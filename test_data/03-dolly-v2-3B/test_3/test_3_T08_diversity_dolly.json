{"template": "T08", "questions": ["\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the UrbanSound8k dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the MAZEA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the TDMSci dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the mean capacity of a carbon-based fuel?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the most common Knowledge representation method?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can all the data sets used in the compared studies be found?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What quantity of iron oxide was discovered on Elorza crater?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the DuIE dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What types of nanocarriers do have therapeutic effect?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which are 3 the most common variables for the atmosphere models?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is Raman spectroscopy?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What data format does CHEMDNER corpus have?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the SEE model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the MNIST dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where did the study with maximal geographic scale take place?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Does paper \"Disambiguating authors in citations on the web and authorship correlations\" employe Cosine similarity?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the DrQA model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which QA benchmarks incorporating more than 10000 questions?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): For what piezoresistive material the least response time was obtained?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the BiT-M model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the NASCell model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Who has contributed to the largest number of articles about coronavirus?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the DuIE dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What coolant does ASTRID reactor use?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the CommitmentBank dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the GAD dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the most common lead compound?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the FABIR model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the emergency type of AMBER Alert?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the SearchQA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the OntoNotes dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the most common location in the studies?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the H-NLI model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the Duel noop model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the BioASQ dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the Classic dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): For which country of study overall prevalence of epilepsy is the highest?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the FQF model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the FTD dataset dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the POP3D model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the ACL Anthology dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the BoolQ dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Are children examined in the studies?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the MedSTS dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the Softcite dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which sectors are modeled as energy sectors and how often?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? \n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the WOS-11967 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the metrics of evaluation over the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the AESLC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark score and its metric on the Words in Context dataset?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?\n output (Sparql query): ", "\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?\n output (Sparql query): "], "sparql": ["SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MAZEA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, \"([0-9]+),([0-9]+)\", \"$1$2\" ) ), \"([0-9]+).*\", \"$1\" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), \"(fuel|CO2)\")) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDM Tagged Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Flair-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAN (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PNDec\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CATTS-XSUM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SemExp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1", "SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, \"Not\")) } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Entity Disambiguation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Crazy Climber\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Table-Sequence\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Funnel Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocRED-BiLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AcrE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SemEval 2013\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"Elorza crater\", \"i\")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, \"FeO\")) ?properties orkgp:description ?property_description. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-PixelCNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Dynamic Coattention Networks (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = \"T\"^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Unsupervised NMT + weight-sharing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Permuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with sentence expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQNMMCe+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV TSM-50x2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1-score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"STREET\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Reading Comprehension\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (base)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ohsumed\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, \"tmospher\")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN-PC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TREC-6\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rational DQN Average\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"1-of-100 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PolyAI Reddit\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Long Short Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, \"Raman spectroscopy\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AG News\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GCN Hybrid\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SEE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Adaptive Input Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Solaris\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WLPC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF (1 day) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network - 18 layers\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), \"Disambiguating authors in citations on the web and authorship correlations\", \"i\")) } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT + BiLSTM + CRF Decoding\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pearson Correlation\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"linear-chain CRFs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DrQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, \"[+]\", \"\") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, \"\",\"\", \"\")) AS ?questions_number) FILTER( ?questions_number > 10000) } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-2 (small)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC2GM\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), \"2019-nCoV\")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), \"2019-nCoV\")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label \"ASTRID\"^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, \"coolant\")) ?coolant rdfs:label ?coolant_label. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Summarization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STEM-ECR v1.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GAD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AxCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Senseval 2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-103\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"C51 noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FABIR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Switch Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2003 (English)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Artificial Intelligence\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Shake-Shake (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Joint Entity and Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SRU++ Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asteroids\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BCN+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pre-Training Dataset\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"COMET - Direct\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BART\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"citation classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"H-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Recall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NLL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ice Hockey\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Scientific Results Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Semantic Role Labeling\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Fine-Grained Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DRI Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Unpermuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rfa-Gate-arccos\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Natural Language Inference\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-Ti\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FQF\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Sequence error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciREX\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Concept Mention Extraction\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"OTF spelling+lemma (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C LSTM hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SVHN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LayerNorm HM-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"SPARQL query optimization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Temporal Convolutional Network\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"POP3D\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SUCCESS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Birdsnap\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Large mLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"X-Sum\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Fine-Grained Gating\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Average Return (NoOp)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ANODE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV3-Small\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MPAD-path\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Bits per dim\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, \"ildren\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multimodal PISA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Sentence Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tokenlearner\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ImageNet + iNat on WS-DAN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rainbow+SEER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (High)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - LAS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bank Heist\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MFEC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-XS-36\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ResNet-152 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"REDDIT-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hendrycks Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"KD-LSTMreg\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-21 (384 res)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FG fine-grained gate\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Entity F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, \"Naive bayes\")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, \"Bag of words\")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, \"Feature request\")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM (Bai et al., 2018)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet-Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-L\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A3\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"6-layer QRNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Robot Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PEGASUS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Audio Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"MACs\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Wizard of Wor\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"12-layer Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PAR Transformer Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERTwwm + SQuAD 2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CL-Titles-Parser\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Name This Game\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AlexNet, MultiGrasp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HRLRE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"T-ConvS2S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Q*Bert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Common Sense Reasoning\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"], "generated_sparql": ["sparql (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Story Cloze Test\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n<|endoftext|>Q:\n\nHow to call F5 Load Balancer API in java?\n\nI am trying to call F5 Load Balancer API using Jmeter. But I am getting below error.\n\n{\n\"error\" : \"Method not found: 'GET'\",\n\"exception\": \"com.fasterxml.jackson.databind.JsonMappingException\"\n}\n\nMy code is as below,\nimport com.fasterxml.jackson.module.jsonwebtoken.Jwts;\nimport com.fasterxml.jackson.module.jsonwebtoken.impl.JwtsBuilder;\nimport com.google.common.collect.Lists;", "ary (List of research papers that contain a benchmark over the Penn Treebank (Word Level)):  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?problem;?paper rdf:type?problem;?cont rdf:type?cont;?cont rdf:label \"Penn Treebank (Word Level)\";?problem rdfs:label?problem_lbl; }\n\nNatural Language Processing benchmark datasets in research papers\n\nResearch paper: \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?problem;?paper rdf:type?problem;?cont rdf:type?cont;?cont rdf:label \"Penn Treebank (Word Level)\";?problem rdfs:label?problem_lbl; }\n\nProblem: \nrdfs:label?problem_lbl\n\nResearch paper", "SELECT DISTINCT?paper_label?paper_cont_label?benchmark_label FROM <urn:xlink:href=\"http://demo.cogniml.me/demo/UrbanSound8k.jsp\" />\n\n so input (English text): UrbanSound8k datasets are used to evaluate models in the realm of Natural Language Processing. It consists of 8,191 examples representing 8,191 sentences from eight different languages. The dataset contains a variety of features including tokenization, lemmatization, part-of-speech tagging, sentence pair classification, syntactic role labeling, and dependency parsing. Some examples of the data include:\n\n >UrbanSound8k example 1\n >UrbanSound8k example 2\n >UrbanSound8k example 3\n >UrbanSound8k example 4\n\n The UrbanSound8k dataset consists of 8,191 examples representing 8,191 sentences from eight different languages. The dataset contains a variety of features including tokenization, lemmatization, part-of-speech tagging, sentence pair classification, syntactic role labeling, and dependency parsing.\n\n The UrbanSound8k dataset consists of 8,191 examples representing 8,191 sentences from eight different languages. The dataset", ":SELECT DISTINCT?paper?problem_label?benchmark WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper OR {?paper a orkgp:P30?problem;?problem a orkgp:P31?cont;}?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. }\n input (English text): Natural Language Processing (NLP) is a very active research area with thousands of research publications each year and at a very high pace. High-quality benchmark datasets are critical to the advancement of NLP research. Benchmark datasets, especially benchmarking against corpus data like the Penn Treebank, have shown very strong predictive power for NLP tasks like Part-of-Speech Tagging, syntactic parsers, and SMT. \n Benchmark datasets can be extremely useful for evaluating and validating machine learning models. Benchmark datasets are crucial for both researchers and practitioners because they provide objective measurements of model performance. Unfortunately, NLP benchmark datasets are not as widely used as benchmark datasets used in other areas of AI such", "input (English text): The top benchmark score and its metric on the MazeA dataset is 63.03 and R-squared = 0.683. \n output (Sparql query): SELECT?problem_lbl?problem_score?problem_r2 WHERE { \n?paper a orkgp:P30.\n?cont a mql:MAZEA.\n?cont rdfs:label?paper_label.\n?cont mql:R2.\n?cont mql:R2 * (1 - mql:R2) + mql:A2 * mql:R2 * mql:A2 >= 0.9 * mql:R2 * (mql:R2 - mql:A2) * (mql:R2 - mql:A2) \n?cont mql:R2 = 1.\n?problem rdfs:label?problem_lbl.\n?problem_score integers?problem_score.\n?problem_r2 float?problem_r2. } \n input (English text): However, some datasets are not very popular and do not have clear winners. For example, the MQL4 dataset contains multi-", "input (English text): The TDMSci dataset was created to evaluate state-of-the-art models on detecting scientific research. The dataset includes the following fields: Problem, Problem Labelling, Contributor, Contributor Role, Benchmark, Benchmark Labelling.\n\n                  output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\noutput (Sparql query): The models that are being evaluated include: Linear Regression, Support Vector Machine, Random Forest, Neural Net.", "has no research problems with benchmark datasets in the realm of Natural Language Processing research.\n input (English text): I am aware of no research problems with benchmark datasets in the realm of Natural Language Processing research, but I was not aware that there were any. Would you mind clarifying?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The datasets that are used to test and compare algorithms for Natural Language Processing research problems are benchmark datasets. These benchmark datasets have been specifically created to provide a fixed, consistent set of samples against which algorithms can be compared. There are several steps that go into creating a benchmark dataset, including the development of a training dataset, a development dataset, and a test dataset. Once the datasets are developed, the", "input (English text): I would like to see a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset.\n\n Output (Sparql query):``` SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n Input (English text): I would like to see a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset.\n\n Output (Sparql query):``` SELECT DISTINCT?problem?problem_lbl FROM <data> WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) =", ":Google's ELMO model with an Accuracy of 91% on the Kuzushiji-MNIST benchmark dataset.\n\nNdEx (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\noutput (Sparql query):[\n  {\n    \"problem\": \"ELMO\",\n    \"problem_lbl\": \"Google's ELMO model with an Accuracy of 91%\"\n  }\n]\n\nAdapter for Model 'ELMO': https://github.com/google/elmo", "input (English text): The model achieving the highest BLEU score on the WMT2016 Romanian-English benchmark dataset is the Transformer model. \n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Transformer\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nIndeed, the Transformer model achieved the highest BLEU score on the WMT2016 Romanian-English benchmark dataset. However, it is important to mention that WMT2016 Romanian-English benchmark dataset was not designed for NLP research and its applicability is rather limited.\n\nFirst of all, Romanian-English benchmark dataset was originally created for Romanian-English language model evaluation. As far as NLP research is concerned, Romanian-English is a very difficult language pair due to the morphological complexity and very high vocabularies, so Romanian-English", "text (Sparql query): SELECT DISTINCT?problem?problem_lbl,?benchmark,?metric,?value WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.?problem?benchmark ;?benchmark?metric ;?benchmark?value. } \n input (English text): The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is 4.898.\n output (Sparql query): Invalid query: SELECT DISTINCT?problem?problem_lbl,?benchmark,?metric,?value WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?", "output (Sparql query): OR-Tools Benchmarking API identified the model with the name of \"BERT\" as achieving the highest Top-1 accuracy score when benchmarked on the VTAB-1k dataset. BERT is a deep learning based model that was specifically developed for the VTAB-1k dataset.\n\n\n                             output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Oracle Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", "input (English text): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper. FILTER (?paper:P32 = \"arXiv\")?benchmark;?rf OR?cont. } \n input (English text): The IDs of the research papers that include a benchmark for the arXiv dataset are [\"P30\",\"P31\",\"P32\"].", "input (English text): The TDM Tagged Corpus dataset contains a wide variety of metrics that have been used to evaluate models on machine learning tasks. The goal of the benchmark dataset was to provide a consistent set of named entity recognition and coreference resolution tasks that were available to all TDM Tagged Corpus model authors without having to change the original datasets. The metrics used in the dataset can be found in the paper \"Token-level Metrics for Automatic Measurement of Information Retrieval Models\" by Agirre et al. (2017).\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Token-level Metrics for Automatic Measurement of Information Retrieval Models\"). } \n input (English text):The benchmark dataset contains only token-level metrics which can be found in the paper \"Token-level Metrics for Automatic Measurement of Information Retrieval Models\" by Agirre et al. (2017). However, most machine learning frameworks do not implement all of these metrics directly. To evaluate models on TDM Tagged, the authors had to", "input (English text): \n As of today, there are four papers which benchmarked their model on the BC5CDR-disease dataset. The first one is by [Wan, Lu, Wang, Ji, Zhao, et al. 2019], which is referenced from the paper titled \"Ensemble Learning for CT-to-DAG Translation\" [Zhao, Wang, Lu, Ji, and Gupta. 2018]. The second paper is titled \"A Large-Scale Efficient Data-driven Language Modeling Method Based on Point-RNN\" [Liu, Liu, Guo, and Gupta. 2019]. The third one is by [Shi, Wang, Gupta, et al. 2019], and the last one is by [Hermann, Williams, Ji, and Gupta. 2019].\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?", "sparql (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"OpenAI Gym\")?benchmark;?rf ORKGp:P30?cont.?cont ORKGp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?problem_lbl. } \n input (English text): The benchmark dataset used to evaluate the top performing model, the OpenAI Gym, is named Lunar Lander.", "input (English text): The FSNS - Test dataset is designed to evaluate the prediction performance of models on a specific type of problems. The evaluation metrics commonly used in FSNS - Test dataset are Precision, Recall and F-score. \n output (Sparql query): SELECT * FROM dataverse_web_ road WHERE dnode = 'fsns-test-dataset' and count(*) > 0;\n\n<|endoftext|>1. Field of the Invention\nThe present invention relates to optical fibres and optical fibre connections, and in particular to a connector arrangement for interconnecting two optical fibres at the end of such an optical fibre.\n2. Description of the Prior Art\nAn optical fibre is a slender glass tube of a small diameter (for example, about 15 to about 20 millimeters) for transmitting optical signals over long distances. They are generally introduced into an optical connector and joined together using an adhesive, metal pins or other connecting devices. The joining device is then secured to the optical fibres.\nAt present, several techniques are used to accomplish optical fibre connections, including fusion splicing, welding, mechanical bonding, and optical coupling, such as through holes or a sleeve and ferrule connection.\nThe most widely used technique is fusion splicing. Optical fibres are inserted into the", ":SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet 64x64 Benchmark\")?paper ;?paper altisspace:P30?rf;?paper altisspace:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark altisspace:P32?problem.?problem rdfs:label?problem_lbl. } \n\nThere are a total of 32 research papers containing benchmarks for the ImageNet 64x64 dataset. The titles of the papers range from the following: P30 - Adversarial Examples with Deep Neural Networks, P31 - Evaluation of CNN Benchmark datasets, P32 - A benchmark for object recognition using CNNs.\n\nThe problem benchmark in these papers refers to the dataset of interest, which in our case is the ImageNet 64x64 dataset.\n\noutput (Sparql query): P30 and P31 contain the following benchmark dataset as the problem: ImageNet 64x64. P30 contains the GBD net, the SDD net, and the", "text (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The 5 seconds at 12 kHz dataset has been evaluated over Classical music. \n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The evaluation metric is BLE", "\\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nFlair-TDM is a Sparql endpoint specific benchmarking model for the Turing complete languages developed by Ben Yardley and Carl Simpson at the University of Cambridge. The benchmarking model and the Sparql endpoint are open sourced at: https://github.com/ben-yerlyardley/Flair-TDM. \nThe benchmarking model is able to create SPARQL queries that retrieve data from Open linking data models from the Open Linkgraph project.", "n:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nClick on the \"http://cogcomp.cs.illinois.edu:80/services/sparql\" link to launch the Sparql Query Editor. Copy and paste the following SPARQL query into the editor:\n\nClick on the \"Run\" button to execute the query and get a list of results.\n\nNotice that we can see the names of the research problems and their labels. Let's check the code of the papers that have benchmark datasets using Transformer-XL. There are some common and interesting patterns we can find in the code:\n\n* the name of the model is included as a prefix to the dataset name (e.g., \"Transformer-XL Base\" in the", "input (English text): The best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric is the transformers model.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Transformers\")?paper orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?\n output (Sparql query):  The best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric is the transformers model.\n output (Sparql query): Performace score: 0.9729883939679769", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Single Model\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\nThe following papers have utilized the SAN (single) model:\n  - https://research.google.com/archive/singlemodel.html\n  - https://papers.nips.cc/paper/4414-a-single-model-for-natural-language-processing.pdf\n  - https://arxiv.org/abs/1804.05674\nThe following papers have utilized the SAN (multi) model:\n  - https://papers.nips.cc/paper/4656-european-parlator.pdf\n  - https://papers.nips.cc/paper/4656-extended-language-modeling-for-text-classification.pdf", "input (English text): The ACE 2005 dataset contains a number of models that have been evaluated on benchmark problems, including neural networks, LSI, MLP, SVM, CRF. Unfortunately, I don't know of any research problems with benchmark datasets in the realm of Natural Language Processing research. \n input (English text): What are the problems that have been benchmarked on the ACE 2005 dataset?\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Part of the file transfer occurs on dedicated secure servers which are only accessible by the authorized party. SSL/TLS is used to encrypt the data during transmission from the sender to the receiver.\n\nIn addition, you have the option to encrypt the data on your own server using an Open Source program called GnuTLS", "n:Many papers have used the PNDec model for benchmarking purposes. Please refer to PNAS paper : https://www.pnas.org/content/109/47/19117.abstract\n output (Sparql query): SPARQL query performed successfully\n\n framed output (Sparql query): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): { \n \"problem\" : [ \n  { \n   \"problem_lbl\" : \"Natural Language Processing\" \n  } \n ], \n \"benchmark\" : [ \n  { \n   \"benchmark\" : [ \n    {", "set (Sparql query): SELECT DISTINCT?paper WHERE {?paper a orkgc:ResearchPaper ; cattitle?title. } ORDER BY?title?id ;\n input (English text): The code references of papers that used the CATS-XSUM model for benchmarking purposes can be found at https://github.com/cat-v/cats-xsum.\n output (Sparql query): $[]$\n input (English text): I am now going to use the code reference from the link provided by you. Is there any research problem with benchmark datasets in the realm of Natural Language Processing research?\n\n output (Sparql query):?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_lbl. } \n input (English text): No. There is no research problem with benchmark datasets in the realm of Natural Language Processing research.", "Select distinct?paper?problem_lbl where {?paper a orkgp:P30;?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Natural Language Processing\") } ORDER BY?paper?problem_lbl DESC \n input (English text): https://webapps.ists.rub.de/www/databases/imdb/Benchmark.html#/benchmark/keyword/Natural+Language+Processing/\n\n\n\t\t   output (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?paper a orkgp:P30;?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Natural Language Processing\") } ORDER BY?paper?problem_lbl DESC \n\nThere are only two benchmarks over IMDb-B (the others are on Wikipedias), namely on the datasets Movie Review Assessment (MRAC) and Mechanical Turk Dataset (MeT). This should be enough for a small research problem, as we can tell from the answer that we do not have any problems with the benchmark datasets that Sparql provides.", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Zero-Shot Language Model\")?paper ; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The top result is from Mikolov et al. -'Zero-Shot Language Model' (2013) which has the following result over the MLDoc Zero-Shot German-to-French. Average words per sentence: 4.17\n average characters per sentence: 42.34\n Average characters per word: 2.85\n word error rate: 1.14%\n output (Sparql query): The top result is from Mikolov et al. -'Zero-Shot Language Model' (2013) which has the following result over the dataset MLDoc Zero-Shot German-to-French: Average words per sentence: 4.17\n average characters per", "input (English text): Benchmark datasets can be found in many languages in the Natural Language Processing community. \n output (Sparql query): SELECT?benchmark?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (English text): One way to find the code references is to search the paper for \u201cSemExp model\u201d and you will find a list of examples.", "output (Sparql query): Several popular metrics used for evaluating Atari 2600 Up and Down dataset include micro-averaged per-frame accuracies, maximum per-frame accuracy, area under the precision recall curve, and Hamming Loss.\n\n input (English text): micro-averaged per-frame accuracy, maximum per-frame accuracy, area under the precision recall curve, and Hamming Loss.\n\n output (Sparql query):SELECT DISTINCT?metric  WHERE {?rf a orkgc:ResearchField;?rf_label :string?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Benchmark Dataset\")?paper?cont.?cont orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?benchmark_label. }\n\n<|endoftext|>Osteochondritis dissecans of the capitellum.\nOsteochondritis dissecans ( OCD) of the capitellum is a rare disorder, occurring in approximately 0.3 percent of the general population. The lesion is believed to occur when the capitellum separates from the plateau of the humeral shaft due toOCD. This separates from the humeral", "show the problem named \"Penn Treebank (Character Level)\" on the \"Benchmark datasets\" tab\n\narioslatinakovic@pobox.com\n\n\nOutput: \n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n  orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32 \n ?problem.?problem rdfs:label?problem_lbl. } \n ?problem rdfs:label?problem_lbl.\n?problem a \n  Penn Treebank (Character Level)\n\n\nCurrent top performing model on the Penn Treebank:\n\n1. GRU: 829K params, 99.55% accuracy on dev set\n2. BERT: 829K params, 99.67% accuracy on dev set\n\n\nThe top performing model on the Penn Treebank (Character Level) dataset is:\n\n1. GRU: 8", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_ids.?paper_ids xsd:integer?benchmark.?benchmark rdfs:label?problem_lbl. }\n\nThe problem name (?)paper_ids is the dataset (Penn Treebank) used for the research. The label (?problem_lbl) is the label of the dataset, \"Character Level\".", "output (Sparql query): SELECT DISTINCT?paper where {?paper o el:ResearchPaper ;?paper_cont c:Dataset_Identifier# atari2600DoubleDunk. }\n\n<|endoftext|>Q:\n\nDocusign API: Get created envelope id, created from button in template\n\nI'd like to know the created envelope id from a custom button within a template. If a user hits that custom button, a new envelope should be created and then the API should return the created envelope id to the client.\nI can create a new envelope with the api, but I cannot figure out how to get the envelope id once it has been created.\nPOST https://api.docusign.net/UserLove/Fields\n\n{\n\"Subject\": \"Test\",\n\"Name\": \"John Doe\",\n\"RecipientName\": \"Jane Doe\",\n\"Email\": \"john@doe.com\",\n\"DocumentProperties\": { \"Engineering\": \"Engineering\" },\n\"DestinationAddresses\": [\n    { \"Email\": \"jane.doe@doe.com\" }\n  ]\n}\n\nThis will create an envelope with the id created_envelope", "sparql SELECT DISTINCT?problem WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet\")?paper OR {?paper a orkgp:P30;?paper2 a orkgp:P31;?paper3 a orkgp:P32. } FILTER (exists(?paper2=?paper) && exists(?paper3=?paper2 )) ;?cont a orkgp:HAS_BENCHMARK?benchmark } \n input (English text): The highest-performing deep learning model on the ImageNet V2 benchmark is a model called Inceptionv3. It achieved Top 1 Accuracy of 99.63% on the test set of the 2015 ImageNet Large Scale Visual Recognition Challenge. \n output (Sparql query):ttesparql SELECT DISTINCT?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet\")?paper OR {?paper a orkgp:P30;?paper2 a orkg", ":RDFS-LABELING \ninput (English text): RDFS-LABELING is the most common Knowledge representation method.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"RDFS-LABELING\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nThe RDF-LABELING ontology defines a core set of relation types used in Web processing and data-mining.\n\n\nBenchmark datasets are commonly used for testing and evaluation of Natural Language Processing (NLP) models. Benchmark datasets play an important role in validating and benchmarking approaches for NLP, making it possible to evaluate the most accurate algorithms for text understanding.\n\n\nHowever, datasets used for benchmarking are typically not labeled, preventing the evaluation of models' performance in realistic and fair conditions. Currently, there is", "input (English text): See https://www.w3.org/TR/rdf-sparql-query/ and https://www.w3.org/wiki/BenchmarkDatasets. See also https://www.cs.cmu.edu/~sally/lectures/alt.pdf for an excellent lecture on this subject.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): This answer is copied and formatted from https://www.w3.org/TR/rdf-sparql-query/ with the necessary modifications. \n output (Sparql query): osteoporosis,lucent,opencorpora,huggingface,naturallanguageprocessing,dsm,", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): I am aware of the CoNLL shared tasks in Natural Language Processing and the Stanford Natural Language Processing Benchmark datasets. Are there any other research problems with benchmark datasets used for Natural Language Processing that are not covered by the shared tasks or the Stanford Benchmark datasets?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_", "sparql SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?problem_paper;?problem_paper?paper_id;?paper_id?cont;?cont?cont_id;?cont_id?benchmark;?cont_id?problem. } \n input (English text): BIOSSES:P30, P31, P32 are benchmark datasets. The following papers have performed benchmarks on these datasets:\n\n?cont: P31 vs State-of-the-art, P32 vs P31, P31 vs On-demand, On-demand vs State-of-the-art, P32 vs On-demand\n\n?cont_id: P31: 3, P32: 2, On-demand: 4\n\n?problem_paper: P31: \"Siamese encoder\", P32: \"Skip-gram\", On-demand: \" dataset\"\n\n?problem_lbl: \"Natural Language Processing\"\n\ncontro: \"3\"\n\ncontro_id: \"2\"\n\n  \"2", "input (English text): The model with the highest F1 score on the BUCC Chinese-to-English benchmark dataset is the BERT model developed by Facebook. \n input (English text): Facebook developed BERT, a deep learning model, that achieved the highest F1 score on the BUCC Chinese-to-English benchmark dataset. \n output (Sparql query):SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", ":SELECT DISTINCT?paper_id where {?paper_id daw:P30 or kgp:P31. FILTER (str( daw:P30) = \"ImageNet-Related\").?cont_id daw:P32. }\n\n summon (SPARQL query): proinflammatory cytokines IL-1\u03b2 and IL-6 were significantly higher in the control group compared to the intervention group (p < 0.01).\n\n<|endoftext|>\n\nTips for Startups on Budget - playhard\nhttp://www.startup-budget.org.uk/tips-for-startups/\n\n======\nSwellJoe\n\"It's important that the internet connection is capable of handling many users\nsimultaneously, so that your website can be used by many users simultaneously.\nYou can test your connection speed by visiting websites like Speedtest.net.\"\n\nI would love to have an actual source for this. Speedtest.net claims to be\naudited by Google, yet doesn't link to any official sources or references, and\nappears to be completely useless for testing an internet connection.\n\nI've spent a fair amount of time trying to set up servers to test bandwidth in", "input (English text): SELECT DISTINCT?paper_id WHERE {?paper_id rdf:ID?paper. }\n output (Sparql query):?paper_id\n output (Sparql query):        \"P30\",\"P31\",\"P32\"\n output (Sparql query):* P30*     \"Gibson PointGoal Navigation Benchmark\",\"Gibson PointGoal Navigation Benchmark\",\"Gibson PointGoal Navigation Benchmark\"\n output (Sparql query):* P31*     \"Natural Language Processing Benchmark\",\"Natural Language Processing Benchmark\",\"Natural Language Processing Benchmark\"\n output (Sparql query):* P32*     \"Gibson: A Benchmark in Natural Language Processing\",\"Gibson: A Benchmark in Natural Language Processing\",\"Gibson: A Benchmark in Natural Language Processing\"\n\n\r\n\nNatural Language Processing Benchmark\n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.", "n 'P31' 'Text processing' as the benchmark with score 98.80 and total score of 110.80.\n\nAre there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n\nNo. There is nothing specific about benchmark datasets that I could see. Benchmark datasets typically contain the results of previous attempts to solve the same problem. It is entirely possible that a new state-of-the-art approach to the problem does not appear on these datasets and this would be a problem.\n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nNo problem.\n\n                      'P31' 'Text processing' as the benchmark with score 98.80 and total score of 110.80.\n\nSELECT DISTINCT?problem?problem", "nsight best performing model.\n\ninput (Sparql query): SELECT?problem?problem_lbl,?paper,?cont,?benchmark?accuracy WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark;?paper?problem;?cont?problem;?benchmark?accuracy. } \n\ninput (English text): The answer is:SELECT?problem?problem_lbl,?paper,?cont,?benchmark?accuracy WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark;?paper?problem;?cont?problem;?benchmark?accuracy. } \n\noutput (Sparql query): SELECT", ": Support Vector Machines performed best on the Atari 2600 Crazy Climber benchmark dataset.\n input (English text): Are there any problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): No, that model was identified.\n\nA:\n\nNot related to Natural Language Processing, but for \"research problems\" it is common to have:\n\na description of what the problem is\na list of \"problems with the dataset\"\nthe answer to the research problem\n\nIf it is a Sparql question, it's better to be more specific.\n\n<|endoftext|>\n118 S.W.3d 27 (2003)", "SELECT?problem?problem_lbl WHERE { \n ?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. \n  FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n  OR\n ?paper ogr:CodeRef.\n ?paper ogr:Abstract text.\n ?paper ogr:Description text.\n ?paper ogr:Homepage text.\n ?paper ogr:Licence text.\n ?paper ogr:URL text.\n ?paper ogr:Version text.\n ?paper ogr:Citation text.\n ?paper ogr:Year text.\n ?paper ogr:Citation text.\n ?paper ogr:Software text.\n ?paper ogr:Citation text.\n ?paper ogr:Model text.\n ?paper ogr:Citation text.\n ?paper ogr:Download text.\n ?paper ogr:License text.\n ?paper ogr:Model_Citation text.\n ?paper ogr:Format text.\n ?paper ogr:Citation text.\n ?paper ogr", ": https://github.com/funnel-project/funnel-model/blob/master/model/default/main.py\n input (English text): For the paper \"Generative Social Attention Networks,\" you can find the corresponding code at:\n\nurches@umich.edu or jfrisch@umich.edu\n\n\n                                                                s@umich.edu or jfrisch@umich.edu: Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str", "side:SELECT DISTINCT?metric?metric_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"De-En\")?metric; } \n input (English text): The evaluation metrics most commonly used in benchmark datasets when training Natural Language Processing models are CIDEr, BLEU, and TER. What are the specific values for these metrics?\n output (Sparql query): CIDEr: 20.70, BLEU: 34.39, TER: 0.65.\n\n\nA benchmark dataset is often created to compare the performance of competing machine learning models. The benchmark is intended to be used only to evaluate the performance of a model against itself, and not against any other model or dataset. Benchmark datasets often come with evaluation metrics commonly used in the industry to measure model performance. For example, the CIDEr metric is computed using a combination of a model\u2019s recall on ex-paper queries (called RoC) as well as a model\u2019s generative text generation capabilities. \n\nThere are three reasons why benchmark datasets can be problematic: \n - The benchmark datasets are created using the benchmark models to evaluate", ":SPARQL Query\n output (rows): 1\n\n --------\n(1 row)\n\nhardt (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nI really don't know about benchmark datasets in the realm of Natural Language Processing research. It could be that there are problems with benchmark datasets. If you would like to tell us about this, we would appreciate it.\n\n\u043e\u0432\u043e\u0440 (No problems with benchmark datasets in the realm of Natural Language Processing research): Okay, that's good to know.", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): http://docred.sourceforge.net/docred_BiLSTM.htm \n output (Sparql query): }}}\n Okay, now we have gathered a list of papers that use the DocRED-BiLSTM model in a benchmark. \nUnfortunately, most of them are not publicly available, and some of them do not mention the benchmark. In most cases, there is no description of the benchmark either. So it's hard to tell what exactly this benchmark is supposed to measure. However, one possible benchmark could be the length of the title of a research paper. Let's see if we could extract that from the mentioned papers. That should narrow down the possibilities.", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Flops\")?benchmark;?benchmark a orkgp:P30. FILTER (str(?benchmark) = \"Flops\"). } \n input (English text): The performance reported on the Pets dataset of the Oxford-IIIT benchmark collection seems quite poor compared to the other two datasets, ResearchField and HAS_BENCHMARK.  Is there any rationale for this?\n output (English text): Although the Pets dataset has an explicit statement about it being a \"scientific\" dataset, it seems to be of low quality compared to the other two datasets. Its small vocabulary, relative sparseness, and inclusion of phrases and URLs in textual annotations all point towards it being of low quality, which has likely led to poor performance.", "net (English text): The dataset AESLC is part of the Allen Institute's AESL benchmark collection. In the Natural Language Processing area, the best result for the metric F-score is 0.9472. The corresponding value is 312. \n transfer (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): How did you find the dataset?\n output (English text): We found the dataset using the string \"AESLC\" in the dataset name.\n transfer (Sparql query): localhost:?spi CONFIG { query: text { SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_", "input (English text): This query will return the research papers that contain a benchmark for the Oxford-IIIT Pets dataset. SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?problem; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset are as follows: \n Paper 1: Huang, Y., Arias, S., Pereira, F., Pereira, R., & Murphy, D. (2017). SQuAD: Never Stop At Rogue Actorstrings. In Proceedings of the 2018 meeting of the federated multi-paradigm natural language processing conference (pp. 120-131). Association for the Advancement of Artificial Intelligence (AAAI). \n Paper 2: Kumar, R., Tom, P., Pereira, F., Murphy", "set (Sparql query): wos-46985_paper_score_metric\n\n Input parameters:\n?rf - contains the ResearchField of the paper  \n?paper - contains the P30 dataset id  \n?cont - contains the P31 dataset id  \n?problem - contains the label of the problem\n\n Query:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n< Fact table: wos-46985_paper_score_metric\n< Target table: null\n< Relationships: (paper)-[:BENCHMARK]->(benchmark)\n< Sort key: paper.score\n< Sort order:ascending\n< Having sort key: paper.score\n< Using SQL statement:\nSELECT DISTINCT?problem?problem", "net (Sparql query): SELECT DISTINCT?paper WHERE {?paper acrEModel. ; \n    # Coordination of benchmarks for Evaluating Acreative and Readable Computing Code (AcrE model)\n    // https://www.crossref.org/doi/pdf/10.1111/j.2041-1107.2009.00005.x\n   ?paper acrEModel;\n    # Acreative and Readable Computing Code (AcrE model)\n   ?paper acrEModel;\n    # Szafraniec, A., Barto, P., Egenhofer, F., & Puebla, G. (2010). Automatic evaluation of ontology-based query evaluation criteria. IEEE Transactions on Knowledge and Data Engineering, 18(6), 955-966.\n   ?paper Szafraniec, A.; Barto, P.; Egenhofer, F.; Puebla, G. (2010, June). Automatic evaluation of ontology-based query evaluation criteria. IEEE Transactions on Knowledge and Data Engineering. 18 (6), 955-966.\n   ?paper Szafraniec, A., Barto", "sparql (Sparql query):SELECT DISTINCT?paper WHERE {?paper :Supervised a :ResearchField :label \"Natural Language Processing\"}. FILTER (str( :label) = \"SemEval 2013\")?benchmark :hasBenchmark. }\n input (English text): SemEval 2013 is a metric that assesses how well the models performance on a benchmark dataset.", ": \n   SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?rf; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P31?problem.?problem rdfs:label?problem_lbl. }\n\nThere are no problems with this query, as there is a Sparql query built into SPARQL API for R, called distINCT that can be used for this purpose.\n\nINSTANT QUERY HERE\n\n<|endoftext|>Khan Khotakhan\n\nKhan Khotakhan or Khun Khotakhan is a princely state in Iran on the west bank of the Zatone river, at an altitude of 800 meters, near the border with Turkmenistan. The khan was installed in 1776 by an order of Nadir Shah. At that time, its population was 350 people. In 1781, it became a province of Naser Al-Din Shah Qajar. At the beginning of the 20", "pondent quantity of iron oxide on Elorza crater\n output (Sparql query): There are no research problems with benchmark datasets in the realm of Natural Language Processing research. Research datasets commonly suffer from issues with class imbalance, limited availability of samples, and selection bias. The benchmarks provided by the Open Knowledge Foundation are commonly used and have samples from diverse industries and geography.\n input (English text):There are no quantity of iron oxide was discovered on Elorza crater.\noutput (Sparql query):tocolumn quantity of iron oxide on Elorza crater\n\nNdExinput (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input", "input (English text): \noutput (Sparql query): SELECT DISTINCT?model?benchmark WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nSandy (or KGS): Natural Language Processing models have been evaluated on VTAB-1k:\n\n\nSandy (or KGS): The following models have been evaluated on VTAB-1k: BERT, DistilBERT, TransfoERT, RoBERTa, XLNet.\n\n\nJoseph: BERT, DistilBERT, TransfoERT, XLNet.\n\n\nSandy (or KGS): And the following datasets have been used as benchmarks: VectordB, MIT-S pact DL Model Collective, SVMT17.\n\n\nJoseph: VectordB, MIT-S pact DL Model Collective", "output (Sparql query): \\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \ninput (English text):\\n Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code. \noutput (Sparql query):\\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs", "ish :SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Freeway dataset\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \ninput (English text): The Atari 2600 Freeway dataset has been used for training machine learning models to predict driving performance. The model can be trained on F1 score, AUC, accuracy, etc. which are the popular metrics used to evaluate machine learning models.\n output (Sparql query): 2642 F1 Score, 0.94 AUC, 0.933 Accuracy, etc. \n input (Sparql query): For the Atari 2600 Freeway dataset, there is only one metric used for evaluation, F1 Score.\n output (Sparql query): F1 Score. \n input (Sparql query): F1 score is not a research problem.", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): [1],[2],[3],[4],[5]\n\nThe query returns:\n< researchedfield:ResearchField rdf:about=\"urn:ogc:orio:ds:ryley-2004:2\">\n    <rdfs:label>Natural Language Processing</rdfs:label>\n    <paper>P30 Dynamic Coattention Networks</paper>\n    <problem>P31 <em>%60</em>Based on the F1 score, DCR-NTLM and PLA reside in the same class. However, <a href=\"http://www.cs.cornell.edu/People/pabo/data/ACL2006/dccv06", "text (English text): The model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset was the model based on a transformer architecture.\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Transformers\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n output (Sparql query):SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Transformers\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs", "output (Sparql query):| SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The top score is 84.92 on the Atari 2600 Tennis benchmark dataset with the Metric being Score per Input Bits.\n output (Sparql query):", "Input: Benchmarking Natural Language Processing \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n filter (Sparql filter): has benchmark.\n\n {#problem}\n {\n  ?problem rdfs:label \"Natural Language Processing\"@en.\n }\n\n {#problem_lbl}\n {\n  ?problem_lbl rdfs:label \"Natural Language Processing\"@en.\n }\n\n {#benchmark}\n {\n  ?benchmark atom \"DuIE\"@en.\n }\n\n {#cont}\n {\n  ?cont orkgp:P31 \"DuIE\"@en.\n }\n\n {#rf}\n {\n  ?rf a orkgc:ResearchField@en.\n }", "set the metrics that are used to evaluate models on the run (DMControl500k) benchmark dataset?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"run (DMControl500k) benchmark dataset\")?paper_label orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Select the metrics that are used to evaluate models on the Anglers dataset?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Anglers dataset\")?paper_label orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_B", "input (English text): Based on the answer provided here and here, the following models have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset: X-News, X-Mail, BM4HT, HELPER. \noutput (Sparql query): SELECT DISTINCT?paper?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_label. OR?paper_label?paper_cont; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): based on the answer provided here and here, the following models have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset: X-News, X-Mail, BM4HT, HELPER.\n\n<|endoftext|>Court decision could wipe out most Islamic community aid\n\nAn Australian court has blocked the government from distributing community grant money to", ". SELECT DISTINCT?paper?cont FROM <http://www.ontologi.us/> WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Nanocarriers\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (Sparql query): SELECT DISTINCT?paper?cont FROM <http://www.ontologi.us/> WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Nanocarriers\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nAn example of the resulting query is below:\n\nSELECT DISTINCT", "get the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset. FILTER (Sparql query -- resource \"(orcid:oclc-0000-1) \" -- query \"(count(*) > {2.33})}\".\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ROUGE-2\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n output (Sparql query): There is only one top performing model for the ROUGE-2 score when benchmarked on the CL-SciSumm dataset. This is the model \"LSTM with bi-directional attention and ELBO optimisation\". The ROUGE-2 score on the CL-SciSumm benchmark is 2.33.\n\nThere", "n:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Unsupervised NMT + weight-sharing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Weight-sharing model: https://papers.nips.cc/paper/5633-improved-unsupervised-neural-machine-translation-by.html; Unsupervised NMT: https://papers.nips.cc/paper/5627-an-end-to-end-algorithm-for-unsupervised-neural-machine-translation.html?alt=rss; Benchmark: https://benchmarksonline.org/report.aspx?style=css&query=sparql+select+%2fpaper%2fP30+%3a+unsup%2520nmt%252fP", "BP-DMGQ with Resnet18-based model achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset. This model was submitted in challenge run 16 of the 2017 ILSVRC Detection Group Shared Task (https://www.facebook.com/note.php?note_id=10154548812092770&id=10154548812092770&p=1)\n\n separating the benchmark dataset using the \"or\" operator results in the following Sparql query: \nSELECT DISTINCT?problem?problem_lbl \nWHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>22 F.3d 1094\nU.S.v.Scarborough*\nNO. 93-05349\nUnited States Court of Appeals,Fifth Circuit.", "input (English text): Models that have been evaluated on the SciTLDR dataset include the following: LogiML, Seq2Seq, AMOS.\n output (Sparql query): SELECT DISTINCT?model WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"LogiML\")?model;?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Seq2Seq\")?model;?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"AMOS\")?model }\n\n<|endoftext|>Lisa Lamm\n\nLisa Margaret Lamm (n\u00e9e Wright; born July 23, 1965) is an American attorney and politician who served as a Democratic member in the Montana House of Representatives from the 15th district from 2003 through 2012.\n\nEducation and career\nLamm received a bachelor's degree from University of Wisconsin\u2013Green Bay and her law degree from Marquette University School of Law. From 2001 through 2003, she worked as a legal assistant for", "output (Sparql query): SELECT DISTINCT?metric_label?metric_value WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Metric\")?metric_label;Literal(?metric_value) }\n\n sparql  update benchmark# CommonsenseQA benchmark dataset\n\n\u8f93\u5165", ":IMDb-M hashing with relaxation 64 has the best result on avg(precision@t,recall@t) with 0.873034. \n input (English text): Great! However, IMDb-M contains sensitive information and would not be suitable for our research application. We have other benchmark datasets available that are suitable for our research use case.\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Ditrigonia obesula\n\nDitrigonia obesula is a moth in the family Noctuidae. It is found in Kenya.\n\nThe length of the forewings is about 11\u00a0mm. The forewings are yellow with black longitudinal lines. The hindwings", "5.0 matching (F1 score)\n\n input (English text): The Scholarly entity usage detection dataset from Scholarly static's research field orkgc has a F1 score of 5.0 matching on Google's Test set. The Scholarly static is a private company.\n\n Honorable Mentions: OpenAI GPT : 7.5 F1 score, BERT: 27.4 F1 score, DistilBERT: 31.3 F1 score, Transfo MT: 8.8 F1 score, TFSAF: 9.1 F1 score, VizualCore: 13.2 F1 score\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The Scholarly", "input (English text): The models that have been evaluated on the MultiNLI dataset include:  XLM, GPT-2, BERT, ELMo.\n\nungal (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\noutput (Sparql query):olved: The models that have been evaluated on the MultiNLI dataset include: XLM, GPT-2, BERT, ELMo.\n\nungal (English text): The models that have been evaluated on the MultiNLI dataset include: XLM, GPT-2, BERT, ELMo.", "output (Sparql query):SELECT DISTINCT?metric WHERE {\n?rf a orkgc:ResearchField;\n rdfs:label?rf_label.\n FILTER (str(?rf_label) = \"Humor Detection\" or str(?rf_label) = \"Natural Language Processing\")?metric;\n?rf orkgp:P30.\n?rf orkgp:P31.\n?cont orkgp:HAS_BENCHMARK?benchmark;\n?cont?metric.\n}\n\n prompt Check if there are problems with this dataset:\n\n prompt The Humor Detection benchmark dataset has three metrics that are used to evaluate models on this dataset: Bits per Word, Bits All, and Average Document Length. However, only Bits All and Average Document Length are used in the provided benchmark. Bits All is defined as \"the number of bits allocated to a word, represented as a signed 8-bit number\". Average Document Length is defined as \"the average number of characters (words) in a document in the provided dataset divided by the maximum number of words in a document in the provided dataset\". The provided benchmark does not define the word \"word\", nor does it define", "ary (Sequential MNIST benchmark result):Sequential MNIST has a Sequential Mesh Action Classification (SMAC) score of 0.966882957 and a Log Loss of 0.528573807. \n input (Sequential MNIST benchmark result):The highest benchmark result is 0.966882957 for the Sequential Mesh Action Classification metric. \n input (English text): That is a very high score, and while the metric is not particularly descriptive it is also very high. Why do you think the score is so high?\n output (Sparql query): SELECT?metric?score WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?cont.?cont orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?metric.?metric?score. }\n\n<|endoftext|>What is really going on in politics? Get our daily email briefing straight to your inbox Sign up Thank you for subscribing We have more newsletters Show me See our privacy notice Invalid Email\n\nLabour are pressuring Jeremy Corbyn to rule out slashing education,", "Select distinct?problem?problem_lbl where {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n ############################################################### \n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query):^\\<a href=\"https://github.com/ajwong/problems_in_nlp/", "str (English text):?problem?problem_lbl\n\n input (English text):The top benchmark result (metric and value) over the dataset NYT-single is as follows:\n\nAverage precision: 0.989\n\nRunning time: 0.077 seconds\n\nBenchmark dataset: NYT-single\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>The U.S. military is investigating after an American pilot was shot down by enemy fire over the Philippine Sea earlier this week, according to the Pentagon.\n\nThe pilot, who was not identified, was shot and critically wounded by anti-aircraft fire while flying a Super Mi-17 over the South China Sea, the U.S. Department of Defense said", "perform SELECT DISTINCT?metric WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?metric. } \n input (English text): The metrics that are used on the SciTLDR benchmark dataset are: F1 score, Precision, Recall, and AUC.\n output (Sparql query): * F1 score: Compute the F1 score which is computed as the ratio of the intersection to the union of two sets. \n* Precision: Compute the precision which is computed as the ratio of the intersection to the number of elements in the intersection.\n* Recall: Compute the recall which is computed as the ratio of the intersection to the number of elements in the intersection.\n* AUC: Compute the area under the receiver operating characteristic curve which is computed as the ratio of the sum of the intersection score to the sum of the union score over the range of possible scores. \n * \n                                      \ndone!\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sparql\n\ndef main():\n    query = sparql.query_string", ":SELECT DISTINCT?model?model_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WMT2016 English-German model\")?model; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P31?model.?model rdfs:label?model_lbl. }\n\n input (English text): The following are the models that have been evaluated on WMT2016 English-German model:\n\n - x-nee of mlg (2/3 positives, 0 negatives)\n - XLNet (2/3 positives, 0 negatives)\n - RoBERTa (2/3 positives, 0 negatives)\n - GPT (0 negatives)\n - BERT (0 negatives)\n\n   - x-nee of mlg: secondly considered a strong model for NLP tasks given it was selected for the 2016 edition of the Manchester Large Language Model competition.  It performed very well on the development set (4/5 positives, 1 negative) but was unable to achieve any positives on the", ":tensorflow (but note that more recent models might be based on the architecture of MobileNetV2 which is also a convolutional network)\n\nA Sparql query that returns the model that performed best on the CIFAR-100 benchmark dataset:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n output (Sparql query):omitempty\n\n stipulation:\n  ?rf a orkgc:ResearchField\n   rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")\n  ?paper orkgp:P30?rf\n  ?cont orkg", "query (Sparql query): SELECT DISTINCT?problem?problem_lbl,?value,?metric FROM \n<http://www.dboe University.edu.tr/research/sparql/SELECT.html>\n where {?problem rdfs:label?problem_lbl ;?value?metric ;FILTER (?value > 10.0)}\n\n<|endoftext|>Classic rock station 93.5 The Loop plays only songs from the 1960s through today, 365 days a year. Every song on the playlist is presented three times a week\u2014on Saturday, Sunday, and Wednesday\u2014so every listener can hear classic songs they might not otherwise hear.\n\nThe playlist includes:\n\n-Every song on The Beatles, Sgt. Pepper\u2019s, and Rubber Soul albums\n-Every song on Yes, Pink Floyd, Emerson, Lake & Palmer, AOR, King Crimson, Yes, Captain Beyond, etc. albums\n-Every song on The Moody Blues, Jefferson Airplane, and Hot Tuna albums\n-Every song on Jethro Tull, King Crimson, Jethro Tull, Yes, Humble Pie, Centrefact, Jorma Kaukonen, etc", "output (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; } \n input (English text): The ResearchField holds the label \"Natural Language Processing\" and the dataset Reacher has a score of 0.9962754080231474. \n output (Sparql query):ricia input (English text): The ResearchField holds the label \"Natural Language Processing\" and the dataset Reacher has a score of 0.9962754080231474. \n output (Sparql query): SELECT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCH", "ic (Sparql query): SELECT DISTINCT?model?benchmark WHERE {?rf a orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?rf rdfmp:type?model. } \n input (English text): The models that have been evaluated on the Atari 2600 Assault dataset include Self Supervised Model, PixelCNN, and Multi-Layer Perceptron.\n output (Sparql query): counc (Sparql query): SELECT DISTINCT?model?benchmark WHERE {?rf a orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?rf rdfmp:type?model.?benchmark xsd:string. } \n input (English text): All of the models mentioned above have been evaluated on the Atari 2600 Assault dataset.\n output (Sparql query): Orchestra (Sparql query): SELECT DISTINCT?model?benchmark WHERE {?rf a orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?rf rdfmp:type?model", "net:Load the Sparql query named \"Select distinct problem, problem_label\" into a database named dataloader. Query the graph dataset \"Dataloader/Resource\" using the SPARQL endpoint named \"database\". You can download the dataset \"Dataloader\" at https://www.crossref.org/workloads/query/dataloader. Use the SPARQL endpoint https://database.thyenic.com/asqldb/sparql against this dataset to generate a results table with the following columns.\n\n Selected query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n Database name: dataloader\n\n Output:\n\n<|endoftext|>Rates of women who are opting for preventive mastectomies increase.", "ition (English text): https://www.quora.com/How-many-queries-per-second-are-possible-on-this-dataset-DMControl100k says that the maximum possible queries per second on this dataset is 21.7k. The closest benchmark result on the dataset is 19.0, for DMControl99.999999% with 56.9 score.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.?problem_score > 1.0?problem_score. }\n input (English text): Yes, that is correct. The DMControl100k dataset has two best results available: 19.0 and 21.7k. The 21.7k result is based on DMControl99.9999", ":BERT model.\ninput (English text): BERT is an acronym for Bidirectional Encoder Representations from Transformers. It is a deep neural network architecture for question answering built on the transformer architecture. It has achieved the state-of-the-art results on the New York Times (NYT) dataset. The model was introduced in 2019.\n\nFollowing this model, let's find the name of the benchmark dataset for which it achieved the highest F1 score:\n\n SELECT DISTINCT?benchmark WHERE {?benchmark nytt:. NYT-single. } LIMIT 1\n\nAnd the corresponding F1 score:\n\n 18.64, which was achieved by the model BERT on the NYT-single dataset.\n\n\u30fc\u30b8:BERT model.", "input (English text): https://github.com/MEMEN/publication-templates.git\noutput (Sparql query):  Provide links to code used in papers that benchmark the MEMEN (single model) model\n\n\n# Have benchmark datasets been used in a peer-reviewed publication?\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?problem?problem rdfs:label?problem_lbl. } \n\nUNWIND {\"Aho\": \"Alvey, M. and Abbel, L. and Abbel, L. and Abdelhadi, A. and Abdelhadi, N. and Ahlstr\u00f6m, A. and Aho, A. and Alis, J. and Alis, S. and Aliou, T. and Allen, E. and Alper, S. and Altai, J. and Alta", "that link?\n\n\f SPARQL query\n\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n {\n\n\t?paper     mmv.tsm_50x2_ModelBased.jar\n\n\t?cont      mmv.tsm_50x2_ModelBased\n\n\t?benchmark mmv.tsm_50x2_ModelBased\n\n }\n\n [*] {\n\n\t?problem    https://doi.org/10.5281/zenodo.3223876\n\n\t?problem_lbl https://doi.org/10.5281/zenodo.3223877\n\n\t?rf         mmv.tsm_50x2_ModelBased\n\n\t?rf_label   \"Natural Language Processing\"\n\n }\n\n insensitive", "input (English text): The model that achieved the highest Accuracy on the Yelp-5 benchmark dataset is the Multi-Genre Model from the MMI Toolkit. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nA:\n\nBenchmark datasets, in general, are popular in the NLP community and have been used in a wide variety of NLP applications, including SLMTraining. There's some research problems in benchmark datasets as below:\n\nWhen we use benchmark datasets to evaluate model, it is more meaningful when we have real world benchmarks for evaluation. But often the benchmarks are not realistic as they are built using some specific datasets and some heuristics. Therefore, basing on real world benchmarks is a tough problem", "text:The best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric is the model provided by dataset contributor SIFTER and the string matching model.\n\n data (https://github.com/w3id/wmt2016-en-de/tree/master/data/MT16-en-de-sifters-benchmark).\n\n input (English text):The best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric is the model provided by dataset contributor SIFTER and the string matching model.\n\n output (Sparql query):The best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric is the model provided by dataset contributor SIFTER and the string matching model.\n\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"String Matching\")?benchmark_str. FILTER (?benchmark_str = \"SIFTER Model\")?model_name.?problem?", "output (Sparql query):SELECT DISTINCT?paper FROM {\n\n  # ResearchField paper id\n  _:1 orkgc:ResearchField@\n  # Label of ResearchField\n  rdfs:label?rf_label\n\n  # Papers containing \"Natural Language Processing\"\n  # Code of the paper\n  #. * orkgp:P30 orkgp:P31\n\n  # Benchmark dataset\n  #. * orkgp:P32 orkgp:HAS_BENCHMARK(benchmark)\n\n  # Problem dataset\n  #. * rdfs:label?problem_lbl\n\n  # Related paper id\n  _:2 orkgc:Paper2@\n  # Related ResearchField\n  orkgc:ResearchField@\n\n  # Related Problem\n  rdfs:label?problem\n\n  # Related Contribution\n  orkgp:P32\n\n  _:3 orkgc:Paper3@\n  # Related Problem\n  rdfs:label?problem\n\n  _:4 orkgc:Paper4@\n  # Related Problem\n  rdfs:label?problem\n\n  _:5 orkgc:Paper5@\n  # Related Problem", "input (English text): There are several problems with benchmark datasets in the realm of Natural Language Processing research: (1) they are not standardized across datasets and applications, which makes comparisons difficult, (2) benchmark datasets often do not provide comprehensive coverage of the problem domain and would benefit from more realistic instances to improve their performance, and (3) benchmark datasets are often created without considering the application use case, which further complicates the task of finding the best model to solve the real-world problems.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nAnswer: (1) benchmark datasets often do not provide comprehensive coverage of the problem domain and would benefit from more realistic instances to improve their performance; (2) benchmark datasets are often created without considering the", "output (Sparql query): SELECT DISTINCT?problem2?problem2_lbl2 WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem2_lbl2. } \n input (English text): The following code references have been made in the paper:\n { \"dyn. eval. model\", \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model\" } \n output (Sparql query): Don't know output (Sparql query): {\u00b6} SELECT DISTINCT?problem2?problem2_lbl22 WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31", "input (English text): The model with the highest Score on the Atari 2600 Assault benchmark dataset is called GRAT.\n\n supporter (Sparql endpoint):http://www.autonleste.com/atari-2600-assault-benchmark\n\n supporter (English text): GRAT is an acronym for Game-Racing ADvance Trajectory. This is a very capable deep learning model.\n\n supporter (Sparql query):SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Game-Racing ADvance Trajectory\") GRAT?paper; GRAT?cont.?cont ORKGAP:HAS_BENCHMARK?benchmark; GRAT?problem.?problem rdfs:label?problem_lbl. }\n\n supporter (Sparql query): GRAT is an acronym for Game-Racing ADvance Trajectory. This is a very capable deep learning model.\n\n supporter (English text):The model with the highest Score on the Atari 2600 Assault benchmark dataset is called GRAT.\n\n supporter", "SELECT DISTINCT?problem?problem_lbl \n WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nINSERT INTO results(query,data) VALUES ('SELECT DISTINCT?problem?problem_lbl ',\n    '{ \"problem\": \"humor_detection\", \"problem_lbl\": \"Natural Language Processing\" }'\n);\n\n\noutput (Sparql query):\nSELECT?problem {\n  {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?", "nes?benchmark where {?benchmark a street-model-ref. }\n\n Actual output: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n<|endoftext|>Q:\n\nWhy does ADODB.Stream throw an error when trying to save a file\n\nUsing the Microsoft Access ADODB.Stream object to save an Access file to a network location throws a FileNotFoundError:\n\nFile not found: 'Z:\\test.accdb'.\n\nThat file exists and is accessible, although not explicitly saved to the file system. Why would ADODB.Stream be trying to save to a file that doesn't exist?\nIf you inspect the properties of the ADODB.Stream object, the SaveToFile property has a value of True. I", "n:SELECT DISTINCT?model_name WHERE {?enwiki8 :model_name?model_name. } \n input (English text): The models evaluated on the enwiki8 dataset include the LSI Model, the Label Smoothing Model, and the Dense CRF Model.", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nicians\n- Paper title:  Neural Machine Translation using High-capacity LSTMs with Knowledge-aided Language Modeling\n- ID:  P31\n- Contribution:  Adam D.B. Coates, William S. Graves, Christopher D. Manning, Vitaly V. Semenovich, Afonso Chaves\n\n<|endoftext|>ALBANY \u2014 Gov. Andrew Cuomo on Wednesday unveiled an outline for a 2020 ballot measure that would raise taxes on residents earning more than $1 million per year.\n\nThe tax increases, which would affect an estimated 400,000 households, would generate $1.4 billion for the state, the plan states. The revenue, split between property, income and sales taxes", "input (English text): Contoso Health, which is based on the Transfo biLSTM model developed by Aida Aman et al. for UK national health services. It also includes similar datasets like Cohnheim and WAT2018, the latter being developed by Jerome Cohnheim.\n\n  \n  \n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_B", "metric (Select distinct?problem from SparqlResults WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"SQuAD\")?problem orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.}) \n input (English text): Commonly used evaluation metrics for SQuAD2.0 include F1 score and word error rate (WER).\n output (Sparql query): SELECT DISTINCT?problem FROM SparqlResults WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"SQuAD\")?problem orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): These metrics don't seem very well-defined, as they don't take into account the context of", "input (English text): The Word Error Rate (WER) is commonly used as an evaluation metric for language models and applications based on neural nets.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WMT2014 English-German\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nIn this example the output Sparql query SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WMT2014 English-German\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rd", "input (English text): ==========================\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Reading Comprehension\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n Indexes:\n Problem_label_index_type(ProblemLabel)\n Benchmark_index_type(Benchmark)\n\n ResultSets:\n Problem_label_resultset(ProblemLabel)\n Benchmark_resultset(Benchmark)\n\n ==========================\n\n output (Sparql query): Done. 2 rows returned per request.\n\n input (English text): Hi George, I'm George, and I am the AI team lead at Databricks.  We would love to support this data engineering needs list and look forward to hearing your feedback.\n\n Output (Sparql query):?paper?cont\n\n Output (Spar", "net (English text): The recurrent neural network model of @durrettchihei achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset.\n output (Sparql query):cameleynlu (English text): The recurrent neural network model of @durrettchihei achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset.\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:", "| FILTER (str(?rf_label) = \"Natural Language Processing\") |?paper | has code\ninput (English text):?benchmark\n Benchmark           Link\n============= ======== =========\n Tensorflow_LSTM     https://github.com/abrit0m/lstm-tensorflow/blob/master/benchmark.py\n XLNet               https://github.com/King-Lei/xlnet/blob/master/benchmark.py\n ASRS               https://github.com/morawe/asrs/blob/master/benchmark.py\n================================================================\n done\n\nUpdate\nSo the issue is caused by Sparql's DISTINCT keyword. You can use the following workaround:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?", "sparql SELECT DISTINCT?paper_id?paper_title WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id; orkgp:P31?cont; orkgp:P32?problem;?problem rdfs:label?problem_lbl. } \n input (English text): Ohsumed dataset is the benchmark dataset over which research has been conducted?\n output (Sparql query): SELECT DISTINCT?paper_id?paper_title WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem;?problem rdfs:label?problem_lbl. } \n\nThe output from this query is as follows:\nProvided there are no problems with the benchmark dataset Ohsumed, here is a list of research papers that contain a benchmark over the Oh", ": SELECT DISTINCT?paper WHERE {?paper a ; (extract (date Publication)?pub and (val?val) > \"1979-12-31\" and val < \"1980-01-01\") or (extract (date Publication)?pub and val > \"1981-01-01\" and val < \"1981-12-31\")) ;?paper rdfs:label \"Natural Language Processing\";?paperrdf dbpedia-owl:Resource?cont;?cont owl:equivalentClass?rf ;?rf dbpedia-owl:Resource. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper or kg p:P30?rf;?cont owl:annotowinProperty dbpedia-owl:annotates?benchmark. } \n input (Sparql query):Ok here is the list of models tested on the NYT29 benchmark dataset:\n DBPedia-OWL reasoning service\n Elmo\n Hsieh-pang \n McGill sentiment analysis tool \n Moses\n Tensorflow\n Twitter BotFather \n Twitter BotMother \n Word2Vec\n X-Better\n\n ///////////////////////////////////////////////////////////////////////////////////\n output (Sparql query):Do you have a", "input (English text): The atmosphere is the Earth's outer layer. It includes the gas and particle layers that we live on. The most important variables of the atmosphere are its pressure, temperature and composition. Those variables change all the time and are used by researchers in various fields such as meteorology, chemistry, oceanology and biology to predict the behavior of the atmosphere.\n\n - Pressure changes the atmosphere density and influences the air flow.\n - Temperature changes the color of the objects in the atmosphere, and affects the evaporation.\n - Composition refers to the gases in the air and how much of each one is present.\n\n../../../output/inlined-query.out\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n?problem rdfs:label \"Atmosphere model variables", "code links in papers that use the DDQN-PC model in any benchmark.\n input (Sparql query):\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nthe code links in papers that use the DDQN-PC model in any benchmark are: \nhttps://github.com/ddqngp/ddqngp/blob/master/papers/PC/DDQN-PC_improving_language_generation_with_QPE.ipynb\nhttps://github.com/ddqngp/ddqngp/blob/master/papers/PC/DDQN-PC_Teaching_VSM_Language_Generation_to_Search.ipynb\nhttps://github.com/dd", "n:SELECT DISTINCT?paper_id?paper_title WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nTo solve this problem, we can select the corresponding papers from the TREC research papers and select the benchmark datasets from them.  This can be accomplished using the SPARQL query below:\nSELECT DISTINCT?paper_id?paper_title WHERE {\n ?rf a orkgc:ResearchField;\n  rdfs:label?rf_label.\n  FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf;\n  orkgp:P31?cont.\n ?cont orkgp:HAS_BENCHMARK?benchmark;\n  orkgp:P32?problem.", "input (Sparql query): Here are links to the code used in the papers that benchmarked the Rational DQN Average model: - Petersen et al. (2019): https://github.com/dquattrin/DQNAverageBenchmark; - Guez et al. (2016): https://github.com/ gaming-github/gaps-in- games. We added some extra code which displays the total returns achieved for each problem. - Here is the final query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } ivilink (Sparql query): ivilink (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label.", "set (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"MLDoc Zero-Shot English-to-French\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset is \"Two-Step Neural Network\". \n output (Sparql query): querying the knowledge base... failed: Error querying dataset \"MLDoc Zero-Shot English-to-French\": java.lang.IllegalArgumentException: Not Found: com.yahoo.labs.loss.huberll.SpecificLoss\n\n){# preamble \n\n\n/** \n* @author Vaibhav Agarwal\n* Created on 12/2/17\n*/\npublic", "input (English text): The model that performed best on the PolyAI Reddit benchmark dataset is: CoNLL 2010 shared task on decoder rankings. It achieved an accuracy of 77.93%\noutput (Sparql query):dding (English text): The model that performed best on the PolyAI Reddit benchmark dataset is: CoNLL 2010 shared task on decoder rankings. It achieved an accuracy of 77.93%\n\n\nQuery 1:\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nQuery 2:\nSELECT?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf;", "input (English text): \tHere is the list of code links in papers that use the Long Short Transformer model in any benchmark:\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Long Short Transformer\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Link 1.\n output (Sparql query): href=\"https://arxiv.org/abs/1908.11500\" target=\"_blank\" title=\"Link 1\">Link 1.\n input (English text): Link 2.\n output (Sparql query): href=\"https://arxiv.org/abs/1908.11500\" target=\"_blank\" title=\"Link 2\">Link 2.\n input (English text): Link 3.\n output (Sparql query): href=\"https://github.com/h", "input (English text): the model that has the best accuracy score on the TriviaQA dataset is called Universal  Model and is developed by native language model (NLM) tools from IBM.\n\nippet (Sparql query):/*** @SPARQL query 1 */SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n (Sparql query):borginput (English text): the model that has the best accuracy score on the TriviaQA dataset is called Universal  Model and is developed by native language model (NLM) tools from IBM.\n\n (Sparql query): Barker, S., 2019. Universal Model. Institute of Artificial Intelligence and Mathematics, p. 86. https://www.reddit.com/r/MachineLearning/comments/es", "mark (Raman spectroscopy) is a spectroscopic technique that analyzes a sample's chemical composition by inelastic scattering of photons off molecules in the sample. The photons in Raman spectroscopy are in the region of electromagnetic radiation consisting of electromagnetic radiation at a wavelength in the range of about 5 to 10\u03bcm. Raman spectroscopy is used for the analysis of organic molecules, inorganic molecules, and polymers. Raman spectroscopy is a label-free molecular imaging technique and has a distinct capability over other imaging techniques like light microscopy, tomography, and nuclear imaging because Raman is non-destructive. Raman spectroscopy is fast and has no sample preparation.\n Raman spectroscopy works by inelastic scattering of photons off molecules in the sample. The photons in Raman spectroscopy are in the region of electromagnetic radiation consisting of electromagnetic radiation at a wavelength in the range of about 5 to 10\u03bcm.\n\nagine, Raman spectroscopy is a spectroscopic technique that analyzes a sample's chemical composition by inelastic scattering of photons off molecules in the sample. The photons in Raman spectroscopy are in the region of electromagnetic radiation consisting of electromagnetic radiation at a wavelength in the range of about 5 to 10\u03bcm.", "output (Sparql query):\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"DTD\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The highest benchmark result for the DTD dataset is 42.0, with the metric and score being F-score of 0.845 and 0.844, respectively.", "; {?problem?problem_lbl : scoring?metric. { \"97.36\" \"IO\" } } \n input (Sparql query): SELECT?problem_lbl {?problem?problem_lbl : scoring?metric. { \"97.36\" \"IO\" } } \n output (Sparql query): 97.36 IO", "output (Sparql query): SELECT DISTINCT?paper_id?paper_title?problem_id?problem_label WHERE { {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id ;?paper_id a orkgp:P30?paper.?paper_title a orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark a orkgp:P32?problem.?problem_id rdfs:label?problem_label. } } \n\nA Sparql query to return a list of problem names and IDs from the provided dataset is as follows:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32", ":SELECT DISTINCT?metric?metric_label?benchmark WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"AG News Benchmark\")?benchmark; }\n\nHere is an example of the results:\n predicated set (of 2 elements) {?problem?paper}\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n true\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false\n false", ":models.disease_f1Top performing model selected from models with F1 score of 0.84. SELECT DISTINCT?benchmark WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"F1 Score\")?paper orkgp:P30?rf; orkgp:P31?cont; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset was a model developed by Kim [Kim, G. et al. (2017). Learning to Detect Disease Signs from Similar Clinical Pictures using Long Short-Term Memory Networks. IEEE transactions on medical imaging 40 (10): 1755-1767].  \n However, this model achieved a F1 score of 0.84 which is lower than the best F1 entity level score of 0.89 obtained by the model developed by Vinyals et al. [Vinyals et al., 2015]. The model developed by Vinyals et al. achieved an F1 entity level score of 0.", "sparql \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?benchmark; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nThe following research papers have benchmarked their models on the NCBI-disease dataset:\n\n- Biardeau S, Bellet P, Viel MA, Kornishuk T, Nichelli C, et al. (2017). \"The MicroRNA Disease Link v2.0: integrating microRNA, phenotype and pathway data with respect to human inherited diseases\". BMC Bioinformatics, 18(1), P1. doi:10.1186/s12859-017-1929-9. PMCID:PMC5375788.\n- Sarikaya D, Sanderson C, Hajishengallis P, Krysz", "input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): GCN Hybrid is the most accurate model and performs well across multiple datasets. The code for GCN Hybrid was developed by Google and is a publicly available code base on Github. The model was also used to win Google NIPS 2017 Advnl Ext J Understanding AI conference. The code is implemented using the NumPy, Theano, and TensorFlow frameworks. \nThe model can be finetuned on benchmark datasets to provide a strong baselines. The code for finetuning the model is available in the link https://github.com/mishrakash/GCN_Hybrid_Benchmarking.", "input (English text): CHEMDNER corpus has a data format that resembles the CoNLL-X format. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CoNLL-X\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): What type of a database is CHEMDNER corpus stored in?\n output (Sparql query):ottenham database. \n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CoNLL-X\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_B", "input (English text): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CUB-200-2011 Benchmark\")?paper_id.?paper_id orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nThe above query will return two rows: one row for each research paper that includes a benchmark for the CUB-200-2011 dataset. Note that the same benchmark appears in multiple papers so the DISTINCT keyword is used to ensure that only one benchmark is included in the result.", "input (English text): Here are the links to the code used in the papers that benchmark the SEE model: https://github.com/nilesh/ontology-based-seed-listening-framework-for-open-domain-question-answering-seecode and https://github.com/jamesallenmartine/seebenchmark.\noutput (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \nocomplete (Sparql query): ^https://github.com/nilesh/ontology-based-seed-listening-framework-for-open-domain-question-answering-seecode ^https://github.com/jamesallenmartine/se", ": SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper1 OR?paper2. FILTER (str(?paper1) = \"code\")?code1 FILTER (str(?paper2) = \"code\")?code2. }\n\n (Sparql query 1): \n { \n?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper1 OR?paper2. FILTER (str(?paper1) = \"code\")?code1 FILTER (str(?paper2) = \"code\")?code2. }\n\n ?problem rdf:type?problem_lbl.?problem problem_lbl:has_benchmark?benchmark.?benchmark benchmark:code?code1.?benchmark benchmark:code?code2. }\n\n (Sparql query 2): \n { \n?problem?problem_lbl", "output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n## TODO: implement the query and get back the top result # Human-annotated results: 34.50 (Wiki-leARNing similarity)\n\n\nThe top result for the query over the dataset DocRED (Human-annotated) is 34.50 (Wiki-leARNing similarity).\n\nThe comparison result using the benchmark dataset DocRED (Human-annotated) and the benchmark dataset P32 can be seen below:\n\n<pre>\n 34.50    Wiki-leARNing similarity\n\n 34.50    Wiki-leARNing similarity\n\n 34.50    Wiki-leARNing similarity\n\n 34.50    Wiki-le", ": SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Adaptive Input Large model\")?paper_url;?paper_url?paper.?paper_url?paper_code;?paper_code. \nUpon inspection, I could not find any research problems related to the use of the Adaptive Input Large model. I found a number of papers utilizing the model and linking to the appropriate code in the provided URLs. However, I was unable to find any research problems with the use of the model.\ninput (English text): Is there any research problem regarding the choice of benchmark datasets used in the field of Natural Language Processing?\noutput (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;", "ary (Sparql query): SELECT DISTINCT?model?model_label WHERE {?model a orkgp:P30;?model_label?model_label. } \n input (English text): What models are being evaluated on the Atari 2600 Solaris benchmark dataset?\n output (Sparql query): (no output)\n\n<|endoftext|>Q:\n\nDistribute a web application on linux machines over the internet\n\nI am a student writing a simple program that will be distributed online to compute jobs. The program will be written in Python using the requests library.\nIn order to distribute this program online, the compute resources have to be accessible over the internet. Since my program relies on external libraries which require internet access, I can't simply publish the program on the local server, since that will not work.\nThe only way I know of to make my program available to a remote machine is to set up a docker container, but Docker is not available on the resources I have access to (Azure).\nIs there a way to set up a local environment on a machine which is accessible over the internet, that I can use for development and testing, while keeping my real program on the actual resources?\n\nA:", "input (English text): Here is a list of research papers along with their titles and IDs that have performed benchmarks on the SNLI dataset: \n input (Sparql query):SELECT DISTINCT?paper?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query):SELECT?paper?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark.?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl", "input (English text): The MMV model is available here: https://github.com/mmjorgensen/mmj. The model has been cited in a number of papers which you can find using the CiteULI tool. Papers that use the model for benchmarking purposes can be found on Pubmed using the string search term: \"MMV\". Citation counts can also be found on CiteULI.\n output (Sparql query): SELECT?problem?problem_lbl WHERE { \n ?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; \n  orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem. \n ?problem rdfs:label?problem_lbl. }\n input (English text): A paper that I've been following that uses the MMV model for benchmarking purposes is: https://openaccess.the authors. Ireland. umr-mbp.mrs.ie. bloomberg.com", "_benchmark:P30_accuracy \n output (Html table):<tr><td><b>Score:</b></td><td><b>98.0</b></td></tr>\n\n\r\n\r\n<tr><td><b>Metric:</b></td><td><b>Accuracy</b></td></tr>\n\n\r\n\r\n<tr><td><b>Rank:</b></td><td><b>1</b></td></tr>\n\n\r\n\r\nAre there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n\n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n* \n* {?problem?problem_lbl.?paper orkgp:P30?paper;?cont or", "net (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?model a ogc:ResearchField; rdfs:label?model_label. FILTER (str(?model_label) = \"Digit Classification\") }\n input (English text): Digit Classification models have been benchmarked on the MNIST dataset. \n output (Sparql query):ggedit (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?model a ogc:ResearchField; rdfs:label?model_label. FILTER (str(?model_label) = \"Digit Classification\" AND?model_label = \"Digit Classification on MNIST\") }\n input (English text): Digit Classification models have been benchmarked on the MNIST dataset as well as several variants of it. \n output (Sparql query):ggedit (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?model a ogc:ResearchField; rdfs:label?model_label. FILTER (str(?model_label) = \"Digit Classification\" AND?model_label = \"Digit Classification on MNIST", ":Africa\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Africa.\n output (Sparql query): OK, the study took place in Africa.\n\nAnswers to the research problems with benchmark datasets are as follows:\n - Yes, there are several problems with benchmark datasets in the realm of Natural Language Processing research: the dataset is unstandardized; the dataset does not reflect the actual problem to be solved; there is no clear indication of the problem labels (i.e., the labels are not clearly specified in the dataset); and the labels do not match the problem labels (i.e., the labels of the problem and benchmark datasets do not match exactly).\n - The dataset should be standardized so that", "input (English text):  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?benchmark. } \n\nNatural Language Processing (or \"NLP\" for short) is a wide and diverse research field.  There are many related problems and datasets, such as: word embeddings, named entity recognition, parsing, semantic parsing, question answering, and many others.  As an example, the \"Wikisentle: Large Scale Multi-relation Sentence Embedding\" dataset from the \"Web Documents with Links\" (WdW) track of the 2010 Word longitudinal data week in Toronto is an NLP dataset related to word embeddings.  \n\nSome research problems in NLP include: word sense disambiguation, document summarization, text classification, question answering, named entity recognition, and many others.  There is no reason why a research problem in NLP cannot be benchmarked on any dataset, and many datasets have been benchmarked against various models.  Here is a partial list of research papers that have benchmarked on the WLPC dataset", "output (Sparql query): SELECT DISTINCT?paper WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper. }\n\n input (English text): See the following titles and IDs:\n\n* Paper Title 1\n* Paper Title 2\n* Paper Title 3\n* Paper Title 4\n* Paper Title 5\n* Paper Title 6\n\n* Paper ID 1\n* Paper ID 2\n* Paper ID 3\n* Paper ID 4\n* Paper ID 5\n* Paper ID 6\n\n output (Sparql query): BICOMPOUND:paper {\n  orkgp:P30 Paper Title 1;\n  orkgp:P30 Paper Title 2;\n  orkgp:P30 Paper Title 3;\n  orkgp:P30 Paper Title 4;\n  orkgp:P30 Paper Title 5;\n  orkgp:P30 Paper Title 6;\n  orkgp:P31 Paper ID 1;\n  orkgp:P31 Paper ID 2;\n  orkgp:P31 Paper ID 3;\n  or", ":SELECT DISTINCT?model WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): I am getting the following error when executing this query: /home/synapse/R/_snp/lib/R/xgboost/linear_model/predict.jl:136:\n             optimization_parameters = override_args(optimization_parameters or {}).\n                                                                        ^\n/home/synapse/R/_snp/lib/R/xgboost/linear_model/predict.jl:136:\n            79         optimization_parameters = override_args(optimization_parameters or {}).\n            80                                                            ^\nNote: /home/synapse/R/_snp/lib/R/xgboost/linear_model/predict.jl", "mark_conll2012 performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset.\n\nSELECT DISTINCT?problem?problem_lbl WHERE { \n ?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. \n  FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n ?rf; \n ?cont orkgp:HAS_BENCHMARK?benchmark; \n ?cont rdfs:label?problem_lbl. \n} \n ?problem rdfs:label?problem_lbl. \n\nThis query gets all the problems in the dataset. Then it filters the dataset for ResearchField and label \"Natural Language Processing\" and returns the problem label that is present in the resulting dataset.\n\n<|endoftext|>Q:\n\nHow to create session variables in a Webmethod\n\nI need to set a session variable from a Webmethod.\nIt's something like this:\n[WebMethod]\npublic void add([Parameter(Name = \"ID\")] long id, int status)\n{\n   Session[\"foo\"] = \"bar\";\n}\n\nThe Session is empty when I call this", "input (English text): The WMT14 English-German dataset is benchmarked using the following models:BERT, TransfoBERT, XLNet, CoLA, MUSE, PointerF, ConveRT Transfo, GPT2, XLNet-PCB. These models have been trained on large language models trained on large data sets.\n\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (English text): The BERT, TransfoBERT, XLNet, CoLA, MUSE, PointerF, ConveRT Transfo, GPT2, XLNet-PCB models are trained on large language models trained on large data sets.\n\n output (Sparql query):\n\n\t\t\t\tBERT\tTransfoBERT", "inal (English text): https://www.cs.cornell.edu/people/cao/Downloads/yelp-14-results.html The highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value, is 43.49.\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe \"problem\" field from the Yelp-14 dataset includes:\n\n \"Information Retrieval\": \"Ranking based on Information Retrieval (IR) measures: Incus, Precision, Recall, & F-score\".\n\n \"IR measures\": \"Ranking-based-on-IR-measures-Incus,Precision-Recall-Fscore\"\n\n \"Metric\": \"The metric", "input (English text): Here is the code links for A3C FF (1 day) hs model benchmark: https://cs.stanford.edu/people/paper/aaai19.pdf https://github.com/facebookresearch/A3C-FF-Benchmark\n\n pessoa : orkgp:HAS_BENCHMARK?benchmark.\n\n pessoa : https://github.com/facebookresearch/A3C-FF-Benchmark\n\n pessoa : https://cs.stanford.edu/people/paper/aaai19.pdf\n\n pessoa : https://github.com/facebookresearch/A3C-FF-Benchmark\n\n pessoa : List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n\n pessoa : Output: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK", ":SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"model\")?model_label. } \n input (English text): The RTE benchmark dataset has been used to evaluate NLP models such as the Machine Learning Toolkit, the Stanford Parser, the BERT model, the Universal Dependencies parser and the Wide and Deep parser.\n output (Sparql query): The RTE benchmark dataset is used to evaluate NLP models such as the Machine Learning Toolkit, the Stanford Parser, the BERT model, the Universal Dependencies parser. \n input (English text): The RTE benchmark dataset is used to evaluate NLP models such as the Machine Learning Toolkit, the Stanford Parser, the BERT model, the Universal Dependencies parser.", "inal (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WOS-5736\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The highest benchmark result achieved on the WOS-5736 dataset was 77.67 as of version 2.2.0 of the WOS-5736 dataset on March 27, 2020. \n output (Sparql query): [\n  {\n    \"problem\": \"wos-5736\", \n    \"problem_lbl\": \"WOS-5736\"\n  }, \n  {\n    \"problem\": \"JFreek Ooms\", \n    \"problem_lbl\": \"JFreek Ooms\"\n  }, \n  {\n    \"problem\": \"Cheng CHEN\", \n    \"problem_lbl", "sparql (English text): The following are research paper titles and IDs that have benchmarked models on the WebQuestions dataset:\nPaper Title:  P30 Deep QA for Neural Dictionary and Weighting Learner\tP31 Neural Language Models for QA in Web Question Answering\tP32 Using Recurrent Neural Networks for Distributed Reinforcement Learning for Robotic Surgery Assistance\n\noutput (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\ninput (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\noutput (Sparql query):   \nsulfparql (English text): The following are research paper titles and IDs that have benchmarked models on the WebQuestions dataset:\nPaper", "input (English text): Here is a list of the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset.\n output (Sparql query): SELECT DISTINCT?metric_name WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ART/CoreSC\") }?metric_value.\n input (English text): There is no problem here. The metrics here are only used on the ART/CoreSC benchmark dataset.\n\n<|endoftext|>Opinion: Tough times call for funny haircuts\n\nAs Christians, we\u2019ve been struck by the contrast between how Christians lived in the past and how we live now. There\u2019s a book called \u201cChasing Grace\u201d that brings to life that contrast, from the hair of the times to the grace of the gospel.\n\nLike our ancestors, we\u2019re faced with uncertainties and hardships. The need to trust God in the midst of crisis is called for and attractive hairstyles become extra challenging. In the days of the book of Acts, the church faced persecution and even death from its enemies.\n\nAs we watch worldly powers that care not for the", "p (English text): The models that have been benchmarked on the Natural Questions (short) dataset include XGALE, SuperGLUE, GraphSAGE, and LASAGNA.\n output (Sparql query): SELECT DISTINCT?model?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_xgale orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Which of the models listed above do you believe have been benchmarked on the Natural Questions (short) dataset?\n output (Sparql query): SELECT * WHERE {?model a?problem_lbl. } \n output (Sparql query):ultzp (English text): XGALE, SuperGLUE, GraphSAGE, and LASAGNA.\n output (Sparql query):SELECT * WHERE {?model a?problem", "output (Sparql query): <http://www.w3.org/TR/wmt14-translation/translation.php?config=all> This benchmark contains the following results for this dataset: WMT 2014 French-English BLEU 6.0031 BLEU 4.9048 <http://www.ldc.upenn.edu/LDC2013T13> <https://code.google.com/p/word-metric/> 35.7 BLEU 4.9 <http://lm-ikelpa.org/Benchmark.html> <http://nlp.CLSY.chalmers.se/data/wmt14.en-fr/> \n input (English text): BLEU stands for Bleu scale language model evaluation. This metric is used to compare automatic language translation systems. \n output (Sparql query): SELECT?problem?problem_lbl?paper?benchmark_score WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper orkgp:P30?cont.?cont orkgp:HAS_", ": orkgp:HAS_BENCHMARK?benchmark returned \" WikiText-2\". \n output (Sparql query): SELECT?benchmark {?benchmark TestPerplexity { \"wikitext-2\" } }", ":The paper \u201cAll-attention network - 18 layers model for benchmarking purposes\u201d (Qin, 2019) implements the model on the all-attention-network-18-layers dataset. The code can be found in the GitHub repository attached to the paper.\n input (English text): The code referenced in the paper can be found here: https://github.com/all-attention-network/all-attention-network/blob/master/datasets/all-attention-network-18-layers.py\noutput (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The code should be able to be run to obtain the benchmark datasets and re-implement all-attention-network-18-layers.\n\n<|endoftext|>1. Introduction", "sparqlquery (Sparql query 1): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Disambiguating authors in citations on the web and authorship correlations\")?paper ; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): Salvatore Riggin, Deyue Ma, Harry Weichers, Ioannis Komidaris, Maria Giuseppa Vasconcelos, Yandong Lu, Jeffrey Yang, Adrian Park, Mohamed Ebrahim, Luciano Antiquet, Ioana Ciobanu, Aur\u00e9lien Gauthier, Dmitry Bronstein. Disambiguating authors in citations on the web and authorship correlations. 2018. arXiv:1807.03043.", "sparql (English text): The models evaluated on the STS Benchmark dataset are: OntoNotes,SAT,SMT,COGNITE,SYNTS,Fanta,CoNLL. \n output (Sparql query): SELECT DISTINCT?model_name?model_label WHERE { { \n                       {OntoNotes       \"OntoNotes\"     \"NLPModel\"   \"ON94\"        \"ON94\"    \"ON94\"       \"ON94\"     \"*2018*\" \"Yes\"    \"Ontological Analyzer\"  }; \n                       {SAT             \"SAT\"          \"SMT-Based   \"SMT           \"SMT      \"SMT\"        \"SMT\"     \"*2018*\" \"Yes\"    \"SMT-based model\"      }; \n                       {Cognitee       \"Cognitee\"     \"NLPModel\"   \"COGNT-60-C06 \"COGNT-60-C06 \"COGNT-60-C06 \"COGNT-60-C06 \"*2018*\" \"Yes\"    \"Cognitee model\"       }; \n                       {SyntaxNet", "reset (After processing your request, the results can be viewed under theQuery section of this page.) SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Freeway\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe results of the query show that the best performing model in terms of Score metric is the sequence to sequence model proposed by Ba\u011fcan, Caglayan, and Oztop. The best model reported by this model is the word overlap metric.\n\nThe model proposed by Ba\u011fcan, Caglayan, and Oztop shows a significant improvement in the best metric compared to the previous best performing model proposed by Gattani, Habash, Abeysekara, and Tjaden. The model proposed by Gattani, Habash, Abeysekara, and Tjaden is based on", "input (English text): The titles and IDs of research papers that include a benchmark for the DCASE dataset are as follows:\n\n   P30: Parallel discourse parsing on very large data with very limited resources\n   P31: Benchmarking on the discourse context structure with the discose chunker\n   P32: Discourse control with the DCASE chunker\n\n (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?problem; orkgp:P31?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>The ultimate goal of this research is to develop an in vitro cytotoxicity assay to replace the existing LDH cytotoxicity assay for assessment of anti-tumor activity of chemotherapeutic agents. The traditional LDH cytotoxicity assay requires harvesting primary tumor cells which is neither practical nor ethical in large clinical trials. With the development of molecular biology and its application to tumor cells, it has been realized that many anti-tumor agents exert their anti-tumor effects by modulating", "mentation ( likes, dislikes, Business, Location, Services, Restaurant )\n\n\r\n\r\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \r\n\r\n<ResultSetColumn column=\"problem\" element=\"http://opengis.erdbc.com/wcsonto\">\r\n<ResultSetColumn column=\"problem_lbl\" element=\"Natural Language Processing\" />\r\n\r\nliked: <ResultSetColumn column=\"likes\" element=\"50\" />\r\ndisliked: <ResultSetColumn column=\"dislikes\" element=\"50\" />\r\nbusiness: <ResultSetColumn column=\"Business\" element=\"50\" />\r\nlocation: <ResultSetColumn column=\"Location\" element=\"50\" />\r\nservices: <ResultSetColumn column=\"Services", "input (English text): The neural network model based on the attention mechanism implemented by Abadi et al. (2018) achieved the highest Score score on the Atari 2600 River Raid benchmark dataset.\n output (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The neural network model based on the attention mechanism implemented by Abadi et al. (2018) achieved the highest Score score on the Atari 2600 River Raid benchmark dataset.", "input (Sparql query): \r\n\r\nSELECT DISTINCT?problem?problem_lbl WHERE { \r\n ?rf a orkgc:ResearchField; \r\n  rdfs:label?rf_label. \r\n  FILTER (str(?rf_label) = \"Natural Language Processing\") \r\n ?paper orkgp:P30?rf; \r\n  orkgp:P31?cont. \r\n ?cont orkgp:HAS_BENCHMARK?benchmark; \r\n  orkgp:P32?problem. \r\n ?problem rdfs:label?problem_lbl. \r\n } \r\n\r\n input (English text): \r\n\r\n output (Sparql query): The following papers have utilized the BERT + BiLSTM + CRF Decoding model: \r\n\r\n                  * https://arxiv.org/abs/1910.11428 (Link to the model's code)\r\n                  * https://www.ijs.si/volume/116/issue6/Sarkka/Press-release/Automatic-critica-of-NLP-systems-using-BERT.html (Link to the model's code)", "inating the best performing model on the MedSTS dataset (using the Pearson Correlation metric) we get the Sparql query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (Sparql query): The model that performed best on the MedSTS dataset is orkgp:P30, using the Pearson Correlation metric.\n output (English text): However, there is nothing in the MedSTS dataset itself to indicate that it was used for NLP purposes. In fact, the dataset seems to be exclusively used in research on machine learning model performance in general and not in the realm of NLP. How can one be sure that the dataset was indeed used for NLP purpose?\n output (Sparql query): The dataset is not explicitly labeled with the tag orkgp:HAS", "input (English text): The following research papers contain benchmarks for the WSC dataset: - BERT, Claudio, Luca, Krister, Oscar, Umut, and Wendy.  - BERT uses the benchmark dataset to compare against the BLEU and ROUGE metrics. - Claudio and Luca compare BERT's performance against other state-of-the-art tools. - Oscar uses the WSC dataset for semantic parsing with the models trained on the WSC dataset. - Umut benchmark models against the WSC dataset. - Wendy describes BERT's attention mechanisms. \n input (English text): The following research papers contain benchmarks for the WSC dataset: - BERT, Claudio, Luca, Krister, Oscar, Umut, and Wendy. \n output (Sparql query): SELECT?problem?problem_label?paper_ids WHERE { \n\t?rf a orkgc:ResearchField; \n\trdfs:label?rf_label;\n\tFILTER (str(?rf_label) = \"Natural Language Processing\") \n\t?paper_ids?paper;\n\t?problem rdfs:label?problem_lbl;\n\t?problem_label?problem_lbl;", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): \n[\n  {\n    \"paper\": \"Arad et al. P30: A Benchmark for Growing Neural Language Models\",\n    \"cont\": \"Arad et al. P30: A Benchmark for Growing Neural Language Models\",\n    \"problem\": \"P30\",\n    \"benchmark\": \"ARACL\",\n    \"problem_label\": \"Arad et al. P30: A Benchmark for Growing Neural Language Models\"\n  }\n]\n\nA:\n\nI found that some researchers do use benchmark datasets in the context of their paper, but there isn't always a correlation between the datasets and the performance gains that can", "n 'SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }'\n\n showcase (DrQA model in benchmark):\n https://www.drqa.io/index.html?model=GATE49", ":SVM_Light\n input (Sparql query):SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"SVM_Light\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nI have found a couple of benchmark datasets that are often used in NLP research. However, some of the benchmark datasets have not been fully vetted as of yet. For example, the SST-5 dataset is often used as a fine-grained classification benchmark dataset for several models, including support vector machines, neural networks, and linear classifiers. However, this dataset has been criticized by some in the NLP research community for containing redundant classes. As a result, a re-process of this dataset has been announced but not yet released. Therefore, I would not use this dataset in an actual NLP system at this time. \nFor more discussion on this problem,", ": SELECT DISTINCT?metric?metric_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking models\")?benchmark.?benchmark groupl:member?metric. }\n\n<|endoftext|>Differences in Disability Between Lesbians and Gays: An Integrative Literature Review.\nMost research on sexual orientation differences has been conducted in males, whereas there is relatively less is known about sexual orientation differences in women. This may be due to inherent difficulties in measuring and comparing sexual orientation differences in women. Based on the integrative literature review, we systematically reviewed sexual orientation differences in women and their associated processes. We identified 2146 articles in the general search and seven articles that met our inclusion criteria. The main differences in women were found in positive attitude, social role, sexual behavior and brain and genetic factors. The mechanisms underlying sexual orientation differences in women are complex, involving both positive and negative processes. This article discusses the positive and negative processes and their associated differences. It also outlines future directions and theoretical implications.<|endoftext|>Young Australians could be banned from using pre-paid cards with plastic money, after warnings they may be at risk of", "input (Sparql query):SELECT DISTINCT?benchmark WHERE {?paper a orkgp:P30.?problem_label a rdf:label.?problem_id?problem. } ORDER BY?benchmark_rank DESC }\r\n output (Sparql query):SELECT?benchmark_rank?benchmark WHERE {?benchmark a rdf:label \"QA Benchmark 1\";?benchmark_rank rdf:type \"ORDINAL\". } ORDER BY?benchmark_rank DESC }\n output (Sparql query):ORDINAL 1 QA Benchmark 1\n output (Sparql query):1\n input (English text): QA benchmarks incorporating more than 10000 questions\n\n<|endoftext|>/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Copyright (c) 2018 United States Government as represented by the Administrator of the\n * National Aeronautics and Space Administration. All Rights Reserved.\n */\n\n#ifndef __LINUX_DM_MIN_H\n#define __LINUX_DM_MIN_H\n\n#define DM_DEVICE_VERSION\t\t1\n#define DM_SET_VERSION\t\t\t3\n#", "input (English text): evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset include Area Under the ROC Curve (AUROC) and Precision-Recall Curve (PRC). \n output (Sparql query): SELECT?paper?problem_lbl  AUROC?cont PRC?problem\n\n selection?paper?problem_lbl AUROC?cont PRC\n\n selected?paper?problem_lbl AUROC 4.0 5.0 PRC 4.0 5.0\n\n onCreate benchmark: Atari 2600 Berzerk dataset\n\n onCreate dataset:\n\n create\n\n\n input (English text): Atari 2600 Berzerk dataset consists of several benchmark runs with the goal to evaluate the model performance for various actions (control inputs). For each benchmark run there is a specific action that is to be performed with a corresponding precision and a recall value. The precision value defines the percentage of the correct inputs that are selected while the recall value defines the percentage of the correct inputs that are selected among all possible inputs.\n\n output (Sparql query): The AUROC and PRC metrics can be calculated by first ranking all actions in order of decreasing recall and then calculating the", "input (English text): \n  SELECT DISTINCT \n  ?problem_label?paper_id WHERE { \n   ?rf a orkgc:ResearchField;\n    rdfs:label?rf_label.\n    FILTER (str(?rf_label) = \"Natural Language Processing\")\n   ?paper_id bagof?problem_label;\n   ?problem rdf:type?cont;\n   ?cont rdfs:label?problem_lbl.\n    }\n\n input (English text): The following research papers have performed benchmarks on the Stanford Cars dataset:\n\n- Kiapert M, Isard M, Espeholt C, Pfeiffer T, Gelman J, King G: \"Language Models that are General and Strong on Benchmark Data\". arXiv preprint arXiv:1708.03381 (2017) \n\n- Tramer C, Rehurek S, AcerY-Simon A, Noisy Nurse N, Hajarandiz L: \"Benchmarking Recursive and Rule-Based Grammars on Natural Language Inference\". arXiv preprint arXiv:1708.00655 (2017) \n\n- Nam N, Lee G, Chor", "input (English text): The titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset are:    \n Paper    P30    \n Paper    P31    \n Paper    P32    \nThe IDs of these papers are:    \n RF-P30    \n RF-P31    \n RF-P32    \nEach of these IDs has the label \"Natural Language Processing\" and correspond to a research field. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset are: \nPaper    P30    \nPaper    P31    \nPaper    P32", "input (English text): For Natural Language Processing, the most common metrics of evaluation include precision, recall, and F1 scores. While these metrics are useful for comparing a model to a known truth, in many cases, the model is able to determine what questions it should answer, and thus these metrics do not adequately measure its success. \n output (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField;?rf_label stanford:Cars;?problem rdfs:label?problem_lbl;?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. } \n input (English text): These metrics of evaluation are useful for the benchmark datasets in the Car research field.\n\n Registry-level metrics: \n precision (P) = true positive rate / (true positive rate + false positive rate)\n recall (R) = true positive rate / (true positive rate + false negative rate)\n F1 score (F) = 2 * precision * recall / ( precision * recall + false positive rate)\n\n class-level metrics: \n CAR: accuracy: (true positive rate + false positive rate) / (true positive rate +", "output (Sparql query): SELECT DISTINCT?paper?cont_label?problem_lbl WHERE {?paper a sharclef:ResearchPaper;?cont_label a sharclef:BenchmarkDataset;?problem_lbl a sharclef:ResearchProblem. FILTER (str(?cont_label) = \"ShARe/CLEF\")}.\n\n\n//Response:\nThe benchmark datasets included in the SHARe/CLEF eHealth corpus dataset are listed below. The corresponding titles and IDs are shown in the output.\n\n\n//Authors:\nStefano Ermon, Anthony Glicksberg, Leslie Hay, Shakira Olmi, Varun P., Rama V, Prakash Viswanath.\n\n\n//Dataset:\nShARe/CLEF eHealth\n\n\n//Identifier:\nsharclef:ResearchPaper;\n\n\n//Submission ID:\na sharclef:BenchmarkDataset;\n\n\n//Identifier:\nsharclef:ResearchProblem;\n\n\n//Dataset name:\nShARe/CLEF eHealth Dataset;\n\n\n//Label:\nShARe", "input (English text): For poly-3,4-ethyleneglycoldimethyl-methoxyphenylacetylene, the least response time was obtained for the thickness of 20nm.\n\nivirals (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \nThe following code links are present in the paper \u201cBenchmarking Neural Language Models: Evaluating the Luong, Quoc, Tran and Allen model\u201d <https://www.aclweb.org/anthology/D19-1092>\n <https://www.gpt-project.com/main/model-zoo/zoo.html?model_id=28>\n <https://arxiv.org/abs/1810.11600>\n <https://arxiv.org/abs/1810.07459>\n <https://www.aclweb.org/anthology/D19-1144>\n <https://arxiv.org/abs/1810", "output (Sparql query): SELECT DISTINCT?metric_name WHERE {\n ?metric_name rdfs:label \"Metrics used in the Atari 2600 Fishing Derby Benchmark\".\n ?metric_name a owl:ObjectProperty. \n ?metric_name a owl:ObjectProperty?metric_property.\n ?metric_property daw:valueCount.\n ?metric_property daw:label?metric_label.\n ?metric_label daw:valueCount?metric_value.\n ?metric_value owl:int?metric_value.\n ?metric_value owl:maxCount?metric_value.\n ?metric_value owl:minCount?metric_value.\n ?metric_value owl:averageCount?metric_value.\n ?metric_value daw:min.. daw:max?metric_value.\n ?metric_value daw:frequency?metric_value.\n ?metric_value daw:frequencyUnit?metric_value.\n ?metric_value daw:unit?metric_value.\n ?metric_value owl:allValues", "set (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet Model\" and?problem_lbl = \"ImageNet Model\" ) }\n\nThe image-net dataset is one of the most widely used research field datasets in NLP and is used in order to compare the state-of-the-art image-level NLP model performance. \n The original paper for this dataset can be found here. In terms of number of parameters, the model reaching the best performance on this benchmark was a model called \"SqueezeNet\". \n The current leader board for this benchmark can be found here. While the model parameters are comparable, the model that took 1st place on this benchmark is called DistilNet. The current leader board for this benchmark can be found here.\n\n *VERBATIM*\n\n *DICA*\n\n *Sparql*\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf", "inal (English text): The best models that have been evaluated on the Atari 2600 Battle Zone dataset are the LSTM model from OpenAI GPT-2 and the DQN model from OpenAI GDQN. The model with the highest total reward among all models published on December, 2023 on the benchmark dataset is the DQN model from OpenAI GDQN. \n input (Sparql query): SELECT DISTINCT?model?benchmark where {?model gt; datatype; :OpenAI_GPT2; :OpenAI_GDQN; :MTE; :ITB; :MTD; :MTT; :MSDT; :PP; :FDT; :FDQN. FILTER (str( :OpenAI_GDQN) = \"DQN\")?model; :ITB_LSTM; :ITB_GRU; :ITB_BiLSTM; :ITB_LSTM_GRU; :ITB_GRU_LSTM; :ITB_LSTM_GRU_LSTM; :ITB_RNN; :ITB_LSTM_RNN; :ITB_RNN_LSTM; :", "; SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"BiT-M\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The code links that have benchmark function in the paper: [biTM]:https://www.biit.unikormondquod.be/paper/BiT-M.html\n output (Sparql query):oproteins; {#problem protein_label}?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"BiT-M\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32", "Output: The highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value is 87.28.\n input (English text): The highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value is 87.28. What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Star Gunner\")?benchmark;?benchmark?problem_metric.?benchmark?problem_value. }\n\n<|endoftext|>Q:\n\nCan Diablo III and Diablo III: Reaper of Souls co-exist?\n\nWill players be able to play both Diablo III: Reaper of Souls and Diablo III at the same time or will a new copy of the Diablo III game be created when you buy Reaper of Souls? \n\nA:\n\nYes, players will be able to play both.", "ined (Sparql query): https://github.com/brynarymcniven/nascell-benchmarks/blob/master/documentation/Documentation/benchmarks.md#naa-cell-performance-comparison \n input (English text): https://github.com/brynarymcniven/nascell-benchmarks/blob/master/documentation/Documentation/benchmarks.md#naa-cell-performance-comparison\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nA:\n\nUnfortunately, the problem with benchmark datasets is that they can have a high prevalence of benchmark classes, and therefore tend to unfairly favour those classes. This can be both a good and a bad thing - the", "output (Sparql query):Here is a sample query for extracting the datasets and their names from the provided ontology: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (English text): here is the query for extracting the datasets and their names from the provided ontology: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl", "input (English text): The best benchmark result over the dataset BUCC French-to-English is the F1 score of 0.788. \n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): OK, that sounds good. Any issues with benchmark datasets in the realm of Natural Language Processing research? \n output (Sparql query): The benchmark datasets such as the BUCC French-to-English can be biased. Although it is relatively young dataset with a total of 7,452 documents and 83 types of problem, it has been used by several public NLP datasets such as CoreNLP and ONION. And it is particularly popular as the shared benchmark in GPT, which have already been widely used for research purposes. \n output", "output (SQuAD2.0) : SELECT DISTINCT?problem?problem_lbl WHERE {?rf a SQuAD2.0; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?cont;?cont?paper. } \n\n<|endoftext|>The effects of testosterone replacement therapy on serum levels of prostate-specific antigen and of PSA free light chains in men with androgen deprivation therapy-induced hyperphosphatasemia: results from the ADT-X trial.\nAndrogen deprivation therapy (ADT) for prostate cancer increases the serum levels of prostate-specific antigen (PSA) and of PSA free light chains (fLCs), which may lead to the appearance of \"renal-specific\" PSA and/or increased levels of fLCs. The aim of this study was to investigate whether testosterone replacement therapy (TRT) would normalize PSA and fLC levels in men with prostate cancer treated with ADT and hyperphosphatasemia. In this multicenter, double-blind, randomized, placebo-controlled trial, men with prostate cancer and ADT-induced hyperphosphatasemia were randomized (1:1) to receive transdermal TRT (n", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n input (English text): The best benchmark result over the dataset BC2GM is the result with ID P32: \"Sparql on a blank benchmark course: ~10 hours with worst-case latency of 3.91 milliseconds.\" The Sparql query that produced this result is:\n\n        SELECT DISTINCT \n           ?problem \n           ?problem_lbl \n    WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_B", "input (English text): The following metrics are used to evaluate models on the Story Cloze Test benchmark dataset: BLEU, perplexity, ROUGE, and TER.\n output (Sparql query):SELECT DISTINCT?metric WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Perplexity: The perplexity metric is the average log-likelihood of a sequence of words generated from an n-gram language model. It is essentially the arithmetic mean of the log-probabilities of the n-grams in the sequence. BLEU: The BLEU metric is used to evaluate how closely the automatic translation generated from a machine translation model is to the reference translation. ROUGE: ROUGE measures the average precision, recall and f-score of a sequence classification model. TER", "input (English text): The largest research problem for coronavirus has been contributed by the Wuhan Institute of Virology (WIV) of the Chinese Academy of Sciences.  There have been 44 articles written by this research group.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Coronavirus\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The Wuhan Institute of Virology (WIV) of the Chinese Academy of Sciences has contributed 44 articles about coronavirus.\n output (Sparql query):medscianaliacapital (English text): https://github.com/WIV-CAMS/ciro\n output (Sparql query): werden 49 scholarly publications about COVID-19 by the WIV.\n output (Sparql query):orp30WuhanInstitute", "input (English text): Here is a link to a dataset called DuIE which was created by the Distributed Intelligence Experiment at the University of California, Berkeley. The dataset contains 831 relevance judgments made on 46 documents. The judgments are made using the F-measure metric which is a metric used in Information Retrieval to determine the quality of a ranking. The dataset was made available publicly to the community in order to help benchmark information retrieval models. The dataset is considered to be unique in that it includes documents with semantically diverse headings and titles. One of the best performing information retrieval models on the dataset is called the BM25 model which is an adapted version of the well known P25 model. The DuIE dataset has been made available for download here. \n input (link to the dataset): http://purl.org/sparql/query/duie.   \n output (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking\")?benchmark; orkgp:P31?cont.?cont orkgp:HAS_B", "n: ASTRID uses non-recycled reactor coolant.\n input (English text): Wow. Can you tell us what reactor coolant is used in ASTRID?\n output (Sparql query):SELECT DISTINCT?coolant WHERE {?coolant a orkgc:ReactorCoolant; FILTER (str(?coolant) = \"ASTRID\") }\n input (English text): The ASTRID reactor uses non-recycled reactor coolant.\n output (Sparql query): ATM, reprocessing or reprocessing waste (uranium contained) is very costly, so research reactors use non-recycled reactor coolant to minimize the volume of nuclear waste that will be generated.  One such reactor is the ASTRID reactor which uses AREIOS non-recycled reactor coolant.\n\n\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2s it's a closed cycle system that does not need fresh water, radioactive water or special containment structures, ASTRID reactors have been deployed in Finland, Japan, Canada and Australia.", ". Atari 2600 Berzerk Benchmark Score = 98.4, Metric = Frames per Second. \nHowever, because the Atari 2600 is considered a toy computer, the score and metric are low. \nThe Atari 2600 also is notorious for having particularly difficult programming problems, as you need to be very specific and deliberate about your coding strategy if you want to beat the benchmark results.\n The specific programming problems that occur with the Atari 2600 Berzerk dataset are:\n 1. The robot must observe and respond to a human's speech as quickly as possible. A benchmark for this language processing problem is to identify words in speech and to recognize the words correctly. \n 2. The robot must change its angular orientation at a rate of between 90 and 180 degrees per second. A benchmark for this problem is to correctly change its orientation at a rate of between 90 and 180 degrees per second.\nThe overall problem set can be accessed at the following URL: https://pdx.sis. admissions.utoronto.ca/courses/bergstra/software/atari-berzerk-dataset/problem-set/. \nolver (Sparql query): I am unable to find the Atari 2600 Ber", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }?model_name. \n io:\n    model_name | problem | problem_lbl |cont |benchmark |\n    -------------+----------+-------------+-----+-----------+\n           P30 | Skier    | Skiing     |  yes| Textual Machines|\n           P31 | Contra   | Contra     |  yes| Neural Networks |\n           P32 | Problem | Natural Language Processing|  yes| LMHW Model |", "ary (Sparql query): SELECT DISTINCT?benchmark WHERE {?benchmark a orkgp:P30 orkgp:P31. } \n input (English text): The following are some of the most commonly used benchmark datasets for the Text Summarization research field: \n - Reuters RTA (29 Jul 2017) \n - Gigaword 50 (29 Jul 2017) \n - Common Crawl (20 Aug 2016) \n - TREC-extended (7 Sep 2013) \n - TREC-covers (16 Aug 2013) \n - TREC-news (16 Aug 2013) \n - TREC-url (16 Aug 2013) \n - TREC-web (19 Jun 2012) \n - TREC-web + URL (19 Jun 2012) \n - Reuters RTA + URL (29 Jul 2017) \n - Gigaword 50 + URL (29 Jul 2017) \n - Common Crawl + URL (20 Aug 2016) \n - TREC-extended + URL (7 Sep 2013) \n - TREC-covers + URL (16 Aug 2013) \n - TREC-news + URL (16 Aug 2013) \n - TREC-url + URL (16 Aug 2013) \n - TREC-web", ": SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n:  SELECT * \nFROM http://datagenetics.com/blog/archives/2008/06/ 16/atari-2600-game-ranking-dataset/\nWHERE \n    DATALENGTH(CONVERT(VARCHAR(1024),?cont)) - DATALENGTH(REPLACE(CONVERT(VARCHAR(1024),?cont), '|', '')) < 10000\n: ?cont data dbo:Dataset;\n: ?cont commo string'|'\n: ?cont data dbo:GameRankingURL;\n: ?cont data dbo:URL;\n: ?cont data dbo:Title;\n:", "output (English text): The model with the name highest score on the Tutankham dataset is \"Double E frame\" with a score of 0.989. \n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Q:\n\nExtracting Substrings in Python While Keeping Empty Strings\n\nI'm trying to parse a very large piece of text that has a number of locations for a physical warehouse. These locations are very similar, containing only a few details. However, some of the locations are missing specific details. There is a small chance that certain information is missing, and I don't want to assign a value to a location if that information is missing. The following is an example of what I am working with:\n'This is", ":SELECT DISTINCT?model?model_label WHERE {?model a orkgp:P30?cont;?model_label aproteins. }\n input (English text): Proteins dataset is available from Konect project. A wide variety of machine learning and natural language processing techniques have been evaluated on the dataset. \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0There is a high variance on the performance of these models and their type is widely applicable to other problems. Some examples of models that achieved the best results on Proteins are: Naive Bayes, Multilayer Perceptron, Support Vector Machines, Random Forest, Logistic Regression.", ":P31.iniset predicate benchmark_score_greater_than_than (F1arest :float)\n input (English text): * F1-score is a popular model performance metric for NLP applications. It is calculated by taking the harmonic mean of the precision (P) and recall (R) scores:\n * F1 = 2*P*R / (P + R)\n Given that the F1 score for P31 is 0.97 when benchmarked on the Natural Questions dataset, is it possible to find a corresponding research field named \"Natural Language Processing\" in which P31 was trained?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n upheld\n\nirmat\u00e3o do campo \"problem_lbl\":Natural", "minify the metrics of evaluation over the CommitmentBank dataset here:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The metrics of evaluation over the CommitmentBank dataset include:\n 1. Precision and recall measures over the subset of \"has_commitment\" in the \"paper\" field\n 2. Precision and recall measures over the subset of \"has_commitment\" in the \"cont\" field\n 3. A confusion matrix over the same set of rows and columns as the second measure\n 4. Mean average precision over the rows of the confusion matrix\n 5. Mean average precision over the columns of the confusion matrix\n 6. Mean average precision over the rows of the confusion matrix\n 7. Mean average precision over the columns of the confusion", "input (English text): The code that benchmark the BiT-M model can be found in the codebase of my paper \"Language Models for Information Extraction in Biomedical Texts: Benchmarking State-of-the-Art Recurrent Networks.\" The code is available on Github at <https://github.com/alberto-desio/Benchmarking-BNT>.\n input (English text): This link leads to the code that benchmarked the BiT-M model: <https://github.com/alberto-desio/Benchmarking-BNT>.\n output (Sparql query): The benchmark datasets used in my paper are publicly available and referenced at the bottom of the code repository.\n\nThe benchmark datasets used in my paper are publicly available and referenced at the bottom of the code repository.\n\ndiagonalize (English text): https://github.com/alberto-desio/Benchmarking-BNT\n\ndiagonalize (Sparql query): SQL query to download datasets used in my paper:\n\n    SELECT DISTINCT?problem?problem_lbl WHERE {\n\n     ?rf a orkgc:ResearchField;\n\n      rdfs:label?", "BM-96 : ACE 2004 dataset BM-99 : ACE 2004 dataset \n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ACE 2004\")?benchmark;?benchmark BM-96 OR?benchmark BM-99. }\n output (Sparql query): 1ACE 2004 dataset 2BM-99 ACE 2004 dataset", "sets are just datasets right? So the best benchmark dataset is by definition the Nottingham dataset. The benchmark dataset for the paper \"Isomorphic Semantic Role Labelling and Neural Ranking with Comparison Terms\" was published in the P30 proceedings of the Workshop on Graph-Based Query Processing and Ranking (Gengo 2019). The benchmark score was 556.8 and the metric was 67.1. The benchmark dataset was created by Gengo Inc.", "output (Sparql query): \n DISTINCT is usually replaced with count in Sparql queries.\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n SELECT COUNT(?) AS?count WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nen\u00edsparql (Sparql query):SELECT COUNT(*) AS?count WHERE {?rf a orkg", "ito code used in benchmarking papers. SELECT?paper (paper) WHERE {?paper mpp:has_benchmark?benchmark. }\n output (Sparql query): SELECT * WHERE {?paper mpp:has_benchmark?benchmark. }\n\nsolution (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n* paper { mpp:has_benchmark?benchmark. }\n* benchmark { mpp:has_benchmark?benchmark. }\n\nsolution (English text): This is the code used in benchmarking papers: \n* paper { mpp:has_benchmark?benchmark. }\n* benchmark { mpp:has_benchmark?benchmark. }\n* benchmark {", "output (Sparql query): SELECT DISTINCT?paper_id?problem_id FROM {  \n\n  data;\n\n  data: {paper_id: paper_id, problem_id: problem_id},\n\n  results: [\n    {\n      paper_id: paper_id,\n      problem_id: problem_id,\n      model: \"XGBoost\"\n    },\n    {\n      paper_id: paper_id,\n      problem_id: problem_id,\n      model: \"LR\"\n    },\n    {\n      paper_id: paper_id,\n      problem_id: problem_id,\n      model: \"Boosting\"\n    },\n    {\n      paper_id: paper_id,\n      problem_id: problem_id,\n      model: \"RF\"\n    },\n    {\n      paper_id: paper_id,\n      problem_id: problem_id,\n      model: \"SVM\"\n    },\n    {\n      paper_id: paper_id,\n      problem_id: problem_id,\n      model: \"NN\"\n    },", "; Distinct problem_lbl?problem where {?problem rdf:type rdfs:Class. } \n input (English text): The benchmark dataset used for evaluation is the PubMed 20k RCT dataset. It consists of document-level annotations of relevance and strong mutual referencing between sentences. The evaluation metrics are based on agreement between human annotators and use cosine similarity, weighted Kappa and F1 score as metrics. There is a proviso that the dataset should be de-synchronized before use.\n\n Welcome to SPARQL!\n\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n No problems identified\n\n Welcome to SPARQL!\n\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:", "set the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset in SPARQL query. SELECT DISTINCT?metric?metric_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_label.?metric?metric_label. }\n\n 2. The Gibson PointGoal Navigation benchmark dataset contains the following benchmark metrics: \n  - Domain-specific chat\n  - End-to-end understanding\n  - Standard chat\n  - End-to-end understanding metric\n  - Recognition accuracy\n  - Semantic similarity\n\n 3. The Gibson PointGoal Navigation benchmark dataset is often used as a baseline for research and development in Natural Language Processing. However, it lacks a specific problem statement for the research community to address, making it difficult to demonstrate the effectiveness of the dataset in a use case.\n\n 4. In order", "sparql (English text): SELECT DISTINCT?paper_id?paper_title WHERE { {\r\n ?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id <http://dbpedia.org/resource/AAPD_dataset>?paper; \r\n ?paper <http://dbpedia.org/resource/Tatsuhiko Miyagawa>?cont; \r\n ?cont a orkgp:HAS_BENCHMARK?benchmark; \r\n ?benchmark <http://dbpedia.org/resource/Tatsuhiko Miyagawa>?paper_id; \r\n ?paper_id <http://dbpedia.org/resource/Tatsuhiko Miyagawa>?paper_title; } FILTER (str(?paper_title) =?paper_id)} \n input (English text): The AAPD dataset is usually used as benchmark dataset in Natural Language Processing research for the following reasons:\n1. Dataset contains \"question-answer\" pairs, where each pair consists of a question and", "sparql (Sparql query):\n\n SELECT DISTINCT?problem?problem_lbl WHERE { \n?rf a orkgc:ResearchField; rdfs:label?rf_label. \n FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; \n orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; \n orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nHere is a sample response from the above Sparql query: \n output (Sparql query): \u0441\u0435ponse (Sparql query):\n\n DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; \n rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language \n Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK \n?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_l", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"AWD-LSTM\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text):  I will provide a link to the code used in the benchmark paper. https://github.com/cse-yujin/nlu_research/blob/master/paper-2.md\n output (Sparql query):ospheric input (English text): https://github.com/cse-yujin/nlu_research/blob/master/paper-2.md\n input (Sparql query): AND ?problem?problem_lbl.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:", "input (English text): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking PubMedQA\")?paper1 orkgp:P30?rf1; orkgp:P31?cont1.?cont1 orkgp:HAS_BENCHMARK?benchmark1; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", "output SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The datasets that have been used for benchmarking in the Image Classification research problem are: \n - P30 - The part-of-speech tag data for the Penn Treebank. \n - P31 - The full reference citation for the Penn Treebank. \n - HAS_BENCHMARK - A hierarchy of benchmarks, for example the COCO dataset.\n\n\nThe following are some of the problems that are known to arise from using benchmark datasets for benchmarking:\n\n1. Inaccuracy: When computing performance on a dataset, it is very hard to quantify the accuracy of a system's output on a benchmark dataset. This makes comparing performance across datasets challenging. Moreover, even if one could come", "input (English text): The highest benchmark result is 15.55 with the following metrics and scores: navigation 45.16, accuracy 0.786, coverage 0.782.\n\n \u03c5output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Gibson PointGoal Navigation\")?benchmark; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThere is no research problem with the Gibson PointGoal Navigation dataset.", "ite (CHEETAH):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Cheetah\")?paper ; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Q:\n\nHow to sort a vector in R based on the number of NA's in each element?\n\nIs there an efficient way to do the following?\nvec <- c(1, 2, 3, 4, 5, NA, 6, 7, 8, 9, 10)\n\nsort(vec, na.last = T)\n[1] 4 5 6 7 8 9 10\n\nWhat I'm really looking for is the count of NA's in each element of the vector. In the example, there are 2 NA's so the sort order should have an extra element.\n[1] 2 3 4 5 6 7 8 9 10\n\nA:\n\nUse count with decreasing order of counts", "input (English text): The best known system,BERT, scored 38.91% accuracy on the Stanford Dogs dataset.\n\n\n# Attributes\n\n output (Sparql query): BERT\n\n input (English text): 38.91% accuracy on the Stanford Dogs dataset\n\n\n# Questions\n\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n\n output (Sparql query): There is one research problem with benchmark datasets in the realm of Natural Language Processing research: There is no perfect benchmark for Natural Language Processing. Several approaches have been proposed to compare different approaches on benchmark datasets, but none of them is without flaws.\n\n\n# Attributes", "input (English text): The best model in terms of senseval 2 metric was the one using skip-gram with a negative Samplron model and 20-dim word2vec (the last is the model used for the benchmark).\n output (Sparql query):ortunately input (English text): The best model in terms of senseval 2 metric was the one using skip-gram with a negative Samplron model and 20-dim word2vec (the last is the model used for the benchmark).\n input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Unsupervised: benchmark dataset?\n output (Sparql query):\n          input (English text): The best model in terms of senseval 2 metric was the one using delitegram with negative Samplron model and 40-dim word2vec (the last is the model used for the benchmark).\n output (Sparql query):iculously input (English text): The best model in terms of senseval 2 metric was the one using delitegram with negative Samplron model and 40-dim word2vec (the last is the model used for the benchmark).\n input (English text): Are there any research problems with benchmark datasets in", ": perplexity = 23.91\n input (English text): The model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset is the SGN decoder from the openNER Library.\noutput (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The SGN decoder from the openNER Library\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS", "input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value, is 171269.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value, is 171269. https://github.com/w3c/tvena/blob/master/benchmarks/wmt2016-eng-russian-de.json \n output (Sparql query): counc:171269\n\n biomark:171269\n\n problema:WMT2016 English-Russian\n\n problema_lbl:Natural Language Processing\n\nulp:171269", ":Problem is has_benchmark { \"Natural Language Processing\" }\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The most common lead compound is benzene. \n output (Sparql query):asmuch as :Problem_lbl is { \"Natural Language Processing\" }\n\n<|endoftext|>Q:\n\nWicket dataTable - how to select all the rows at once?\n\nI have a wicket dataTable and I would like to get a list of all the rows that contains data.\nI could do it like this:\nList<Person> persons = dataTable.getPersons();\n\nBut that's too slow.\nIs there a faster way?\n\nA:\n\nThere's", "n:S2600_ENDURO_MODEL \n output (English text): Model name from: https://lucene.apache.org/document/replay-speed.html#Atari2600\n output (Sparql query): tiempo estimado para conserve a 14Kbps: 27s\n output (English text): 27s, oscilando a 23.2s y a 30s en ocasiones\n output (Sparql query): corte final: 33 frames per second\n output (English text): 33 frames per second\n\n  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n  <Query>\n  <Where>\n  <ListSelect items=\"3\">\n  <And>\n  <ListItem>", "sparql (English text): https://github.com/nottbooth/c51-benchmarks\n input (English text): Provided.\noutput (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"No-Op Model\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\ninput (English text): Provided.\noutput (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"No-Op Model\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; }", "input (English text): The code used in the papers to benchmark the FABIR model is available in https://github.com/FabMet/FabIR and https://github.com/FabMet/FABIR-benchmark. The code is used to extract datasets from FABIR systems and run similarity and dataset specific tasks on them.\n output (Sparql query): INSERTION INTO http://data.cid-RESULT.org/sparql SELECT?rf?problem_lbl?paper?cont?benchmark?problem?task?solution?status?author_link?solution_link WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The code is very useful and is a very good benchmarking tool. Thank you!\n output (Sparql query): INSERTION INTO", "ition (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Emergency Alert, System\" OR str(?rf_label) = \"Emergency Alert, Public\" OR str(?rf_label) = \"Emergency Alert, Student\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): AMBER Alert - Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query):Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (English text): No problems identified.\n input (English text):\n\nThe Emergency Type of AMBER Alert can be  described as: Emergency Alert, System, Emergency Alert, Public, Emergency Alert, Student.\n\n Fortunately this category of emergency alert is not associated with", "res : SELECT DISTINCT?problem_metric?problem_metric_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CoQA Benchmark\")?paper_metric; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark_metric; orkgp:P32?problem_metric.?problem_metric_label rdfs:label?problem_metric_label. }\n\n\n*Problem Metrics*\n\n*Tokenizer BLEU*\n\n output (English text):?problem_metric_label Tokenizer BLEU\n\n input (English text):?problem_metric_label Tokenizer BLEU\n\n output (Sparql query):\ufffdBCA:tokenizerBLEU\n\n\n*Encoder BLEU*\n\n output (English text):?problem_metric_label Encoder BLEU\n\n input (English text):?problem_metric_label Encoder BLEU\n\n output (Sparql query):BCA:encoderBLEU\n\n\n*Matcher BLEU*\n\n output (English text):?problem_metric_", "input (English text): The Open Neural Tiling (ONT) model trained on the OntoNotes benchmark dataset provided by Google achieved the best F1 score. ONT was trained on the PDBursky dataset and achieved an F1 score of 0.95.\n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a onkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): INSERT into ResearchProblem (problem, problem_label) VALUES (?,?)\n\n That's correct. However, note that the values reported in the Sparql query were computed against the \"OntoNotes\" benchmark dataset, which is not part of the Open Language Models (OLM) distribution.", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper1 orkgp:P30?rf; orkgp:P31?cont1.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\u00a0\n \u00a0\u00a0{ \"paper\" : \"P30\", \"problem\" : \"Defender\", \"problem_lbl\" : \"Atari 2600 Defender Benchmark\" }\n\u00a0\n \u00a0\u00a0{ \"paper\" : \"P30\", \"problem\" : \"Deep Reinforcement Learning for Atari Games\", \"problem_lbl\" : \"Deep Reinforcement Learning for Atari Games\" }\n\u00a0\n \u00a0\u00a0{ \"paper\" : \"P31\", \"problem\" : \"Positive and Negative Sampling for Subsequence Generation\", \"problem_lbl\" : \"Positive and Negative Sampling for Subsequence Generation\" }\n\u00a0\n \u00a0\u00a0{ \"paper\" : \"P", "input (English text): Ideally, a benchmark dataset should be reproducible and provide easily consumable results that researchers can use to compare the results of competing models, or alternative approaches. Benchmark datasets that do not meet these criteria are not useful to the broader Natural Language Processing research community. Benchmark datasets often contain parts of different datasets that cannot be reversibly linked, e.g. pointer states that have not been stored (in a file or over Web Sockets).  On top of that, benchmark datasets often contain synthetic data or combinations of real and synthetic data, which makes it hard to provide reproducible results for benchmarking purposes.\noutput (Sparql query):  SELECT?problem?problem_lbl?paper?cont?benchmark WHERE { \n ?problem rdf:type orkgp:Problem;\n ?problem rdf:label?problem_lbl;\n ?problem rdfs:label?paper;\n ?cont rdfs:label?cont;\n ?cont orkgp:HAS_BENCHMARK?benchmark;\n ?benchmark rdf:type orkgp:Dataset.\n}", "input (English text):?paper\n output (Sparql query):?paper_url\n output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html\n output (Sparql query):?cont\n output (Sparql query):?cont_url\n output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html\n output (Sparql query):?cont.\n output (Sparql query):?cont.paper_url\n output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html\n output (Sparql query):?cont.paper_url.\n output (Sparql query):?cont.paper_url.url\n output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html\n output (Sparql query):?cont.paper_url.url.\n output (Sparql query):?cont", "input (English text): There are 3 models which have been evaluated on the CommonsenseQA dataset. The XGANS model, the Neural Language Model from F Turk and the BERT model.\n\n #XGANS Model\n #Neural Language Model from F Turk\n #BERT Model\n\n Response:\n\n #XGANS Model\n #Neural Language Model from F Turk\n #BERT Model\n\n Output:\n\n #XGANS Model\n #BERT Model\n\n Response:\n\n #XGANS Model\n #Neural Language Model from F Turk\n #BERT Model\n\n Output:\n\n #Neural Language Model from F Turk\n\n Output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Yes, I could list the models that", "set { \"SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\" }\n\n<|endoftext|>Q:\n\nIQueryable.Where(...) vs.AsEnumerable().Where(...) vs.AsEnumerable().Where(...)\n\nIt seems there are 3 ways to filter a sequence of enumerable objects.  What is the difference between them?\nvar myObj = Enumerable.Empty<MyType>();\nmyObj = myObj.AsEnumerable().Where(x => x.SomeProperty == 1);\nmyObj = myObj.Where(x => x.SomeProperty == 1);\n\nA:\n\nThe second option is shorter for the equivalent code\nmyObj = myObj.Where(x => x.SomeProperty == 1);\n\nThe first option is", "_link (SPARQL endpoint): http://download.cambridge.org/documents/a3c/releases/latest/a3c-cts-RELEASE-NOTES.pdf\n input (English text): https://github.com/facebook/A3C-CTS/blob/master/docs/docs/Benchmarks.md\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): List the code links in papers that use the BERT model in any benchmark?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_", "pter output\n\n :SELECT DISTINCT \n ?problem?problem_lbl\n\n  WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n ?paper Or-ow\u00a0-- The Most Effective Overlay for this Dataset\n\n ?cont Or-ow+\u00a0-- F1 measure\n\n ?benchmark HutterPrize\n\n ?problem Or-ow\u00a0-- F1 measure\n\n:Or-ow -- The Most Effective Overlay\n\n :F1 measure\n\n :HutterPrize\n\n :Or-ow\u00a0-- F1 measure\n\n :#{F1 measure}\n\n :HutterPrize\n\n :#{F1 measure}\n\n :Or-ow\u00a0-- F1 measure\n\n :#{F1 measure}\n\n :HutterPrize\n\n :#{F1", "inal (English text): The Dox for language model trained on the CoNLL 2003 (English) benchmark dataset achieved an F1 score of 0.908 on our evaluation evaluation server. \n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Language Model\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Annual park admission is $129 for adults and $69 for children (ages 3-11). Campsites are $20 per site.\n\nEagle Lake is located just outside of Kalispell in Flathead County, Montana, on U.S. Route 2. The campground is located about 20 minutes northwest of Kalispell, in an area known as Eagle Lake Village. Visitors may purchase season passes at the entrance station for Eagle Lake Village.\n\nEagle Lake is a quiet", "mark (orkgp:P31?cont.?benchmark) orkgp:HAS_BENCHMARK?benchmark\n\n\nAnswer: \nThe following query returns the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset: \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n?benchmark o kgp:HAS_BENCHMARK. \n\nNote that although the query can run in O(1) time, it cannot provide any guarantees of ranking results as all datasets are unindexed. Furthermore, standard NLTK tokenizers may tokenize the benchmark indiscriminately, which might introduce additional noise into the results.\n\n\nThe Top performing model on the VTAB-1k", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Artificial Intelligence\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The following are benchmarked problems: thesaurus assistance, web page summarization, sentiment analysis, and question answering. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Thesaurus assistance\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl", "ney : SELECT DISTINCT?problem_label WHERE {?problem_label  orkgp:P32?problem. } \n input (English text):  The models that have been benchmarked on the FSNS - Test dataset are: Lukasz - PAIR model, Temporal-Memory model, Attention model.\n output (Sparql query): \ub808\uc54c : Lukasz - PAIR model, Temporal-Memory model, Attention model.\n input (English text): Lukasz is an Attention based model.\n\n<|endoftext|>1. Field of the Invention\nThe present invention relates to an ultrasound diagnosis apparatus.\n2. Description of the Related Art\nThe ultrasound diagnosis apparatus transmits an ultrasound wave from the transducer into a subject, receives the reflected ultrasound wave from inside the subject, and obtains a tomographic image of inside the subject based on the received reflected ultrasound wave. Ultrasonic diagnosis apparatus can obtain information regarding tissue structures inside the subject without injuring the subject, and have widely been used, for example, for cardiology diagnosis.\nA planar imaging ultrasonic diagnosis apparatus is an example of the ultrasound diagnosis apparatus. The planar imaging ultrasonic diagnosis apparatus has a curved ultrasonic array that includes a transmission array that transmits ultrasound", ":SELECT DISTINCT?paper?benchmark_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"German-English\")?paper; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; }\n input (English text): The WMT2014 German-English dataset is a benchmark dataset used in the field of Natural Language Processing. Based on the BLEU metric, the best performing model on this benchmark dataset is the Transformer model from the paper \"Mastering the Transformer: A Practitioner's Guide to Deep Neural Networks for Summarization, Information Retrieval, and Question Answering.\". The Transformer model outperforms all other models in the same field with an absolute BLEU score of 22.24.\n output (Sparql query): SELECT?paper?benchmark_label {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"German-English\")?paper; orkgp:P31?cont.?cont orkg", ":SELECT DISTINCT?problem_label?problem_label_text WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont;?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_label_text. } \n input (English text): \nHere are some examples using the code found in the paper that benchmark the BiLSTM-Attention + ELMo model: \n1 - https://github.com/allen-wi-kscalers/wang-etal-2018-emnlp-socp\n2 - https://github.com/allen-wi-kscalers/wang-etal-2018-emnlp-socp/blob/master/eval_benchmarks/eval_baseline_emnlp.py\n3 - https://github.com/allen-wi-kscalers/wang-etal-2018-emnlp-socp/blob", "output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value is 218.642. \n output (English text): The highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value is 218.642.", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Shake-Shake\")?paper_link ;?paper_link?paper.?paper_link rdfs:label?paper_label.?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P30?cont; orkgp:P31?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>india\n\nUpdated: Jun 29, 2020 12:10 IST\n\nA fresh outbreak of novel coronavirus (Covid-19) has been reported in Kashmir on Wednesday with a number of infections being detected, the health department in the state said.\n\n\u201cA total of 10 samples of patients with coronavirus disease (Covid-19) have been received from North Kashmir, with a high number of acute respiratory infection symptoms. A few of these samples have been sent for laboratory confirmation and on further analysis it was found that they are related to the recent outbreak in Jammu and Kashmir,\u201d the health department said in a", "output (English text): The most common benchmark datasets in the Joint Entity and Relation Extraction research field are the CoNLL dataset, the BioASQ dataset, the Reuters RFT dataset, and the SemEval dataset.\n\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Joint Entity and Relation Extraction\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe most common benchmark datasets in the Joint Entity and Relation Extraction research field are the CoNLL dataset, the BioASQ dataset, the Reuters RFT dataset, and the SemEval dataset.\n\n<|endoftext|>Field hockey at the 2003 Pan American Games\n\nThe hockey tournaments for the 2003 Pan American Games were held initel avond hockeywebsite from August 17 to August 26, 2003.\n\nQualification\n\nA total of 11 teams", "inal (English text): Common evaluation metrics for Natural Language Processing tasks include accuracy, F1 score, macro-averaged F1 score, and area under the receiver operator characteristic curve. \n input (Sparql query):;\noutput (Sparql query): SELECT * FROM mrv_mleship.dataset WHERE dataverse = {DATAVERSE_LNAME} AND table = {TABLE_NAME}\n\n podiums (English text): F1 score is a popular metric used in the industry for evaluating text classification systems. The F1 score measures the harmonic mean of precision and recall. It is considered a better metric for systems that only output a binary classification, such as text classification tasks. Macro-averaged F1 score is another metric which takes into account both precision and recall and is considered a better metric for those tasks that return a ranking output, such as for text categorization. \n\nA common approach to evaluate text classification algorithms is to use data augmentation and grid search to find the best hyper-parameters. In addition, reporting results for multiple F1 scores allows the user to understand the trade-off between different factors, such as complexity and training time.\n\n podiums (English text): F1 score\n\n marketplace (English text):", "input (English text): You can find code references in papers that have used the SRU++ Base model for benchmarking purposes in the modelevaluation package in the UCMP package.\n output (Sparql query): SELECT?benchmark,?paper,?cont,?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The code references can also be found in modelevaluation/src/modelevaluation.kkth by listing the bench_params arguments in the \u201crbio.role\u201d file.", "input (English text): The Score metric on the Atari 2600 Asteroids benchmark dataset returned by the Ockachy Benchmark for Neural Language Models (NLM) tool was 0.7872. The Fisher scoring method was used for computing the metric. This indicates that the model achieved a 0.7872 score on the Atari 2600 Asteroids benchmark dataset. \n input (English text): Could you please describe what is the problem with the Atari 2600 Asteroids benchmark dataset?\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Asteroids\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The Atari 2600 Asteroids benchmark dataset contains problem statements that are similar in the way that they are generic or specific. For example, the problem statements provide a specific space ship and a generic", ":SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThis produces the following results:\n\nPaper \nP30 Natural Language Processing over WMT2014 German-English Benchmark\nP31 The Dronacharya Dataset for Machine Translation\nP32 Discussing natural language understanding benchmarks\nproblem  \nNatural Language Processing over WMT2014 German-English Benchmark\n\nI've noticed that the benchmark dataset in WMT2014 German-English is present in both research papers that contain a benchmark over the WMT2014 German-English dataset as well as the P30 Natural Language Understanding over WMT2014 German-English Benchmark.", "ary (Sparql results): SELECT DISTINCT?paper WHERE {?rf a orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): The models that have been benchmarked on the SearchQA dataset are: \n - CBOW; \n - CountVectorized; \n - GPT; \n - Word2Vec; \n - XLNet. \n These models can be found at: \n - <https://www.kaggle.com/c/search-question-answers/leaderboards/leaderboard-results/2627> \n - <https://www.kaggle.com/c/search-question-answers/leaderboards/leaderboard-results/2638>", "input (English text): The OntoNotes dataset is used as a benchmark in the ongistic field. It contains about 30 million tokens of human communication represented as sentence and part-of-speech tags. The evaluation is typically done in terms of mean average precision, measured insentieous tokens.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a ongcp:OntologyComponent; ongcp:name?name; ongcp:isRelative?relative; ongcp:ontologyTerm?ont; ongcp:partOfSpeechTerm?part; ongcp:description?description;?rf_label ongcp:ResearchField; ongcp:name?rf_label; orkgp:HAS_BENCHMARK?benchmark; orkgp:P33?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Mean average precision (mAP) is an evaluation metric in the field of object recognition (for example, for images), that measures the average precision of a model at deciding which of multiple images are similar to each other and which are not. \n output (S", ": SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n output (Sparql query):\u00a0\n \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0?problem ctl:maxValues?maxVal", ":The highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value is 1547.29573834. \n input (English text): The highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value is 1547.29573834. What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WMT2014 English-German dataset\")?benchmark?problem; } \n input (English text): WMT2014 English-German dataset is an evaluation dataset used for the \"Web Modeling and Automatic Speech\" task of the 14th World Machine Translation Conference (WMT14) held in Italy in 2014.\n\n<|endoftext|>// Boost.Bimap\n//\n// Copyright (c) 2006-2007 Matias Capeletto\n//\n// Distributed under the Boost Software License, Version 1.0. (See\n// accompanying file LICENSE_1_", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"PWC Leaderboards (Restricted)\")?paper ;?paperoid?problem.?cont?problem orkgp:HAS_BENCHMARK?benchmark.?benchmarkoid?benchmark. }\n\n\u00a0\n \u00a0\n\n\u00a0\n \u00a0\n\n\nThe dataset PWC Leaderboards (Restricted) has the following titles and IDs:\n\n \u00a0\n\n\u00a0\n \u00a0\n\n\n* [Benchmarking Natural Language Processing systems](https://dl.acm.org/doi/10.1145/2492968.2493032)\n* [On the definition of a benchmark](https://dl.acm.org/doi/10.1145/2117371.2117376)\n* [A framework for formal benchmarking](https://dl.acm.org/doi/10.1145/3060671.3060675)\n* [The PWC Leaderboards dataset](https://dl.acm.org/doi/10.1145/2993920", "input (English text): The metrics used on the enwik8 benchmark dataset are precision@10, recall@10,  F1-score,  support-vector machine (SVM)@10 and random forest (RF)@10.\n\n  \n  \n\nNote: For the metrics to be computable, there has to be a relation that connects the labels of the benchmark dataset to the corresponding problems in the RDF dataset.\n\n\nThe following metrics are used on the enwik8 benchmark dataset: precision@10, recall@10,  F1-score,  SVM@10 and RF@10.\n\n\nNote: These metrics can be computed by the following SPARQL query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nThe following", ":SELECT DISTINCT?metric?metric_label WHERE { \n ?metric  http://terminology.hl7.org/wiki/Special:Utilities/TaxonomyPage/Objective; \n  rdfs:label?metric_label. \n ?metric  http://www.w3.org/TR/tessellation-pattern/ dp.aud;\n ?metric  http://www.w3.org/TR/tessellation-pattern/ dp.f1;\n ?metric  http://www.w3.org/TR/tessellation-pattern/ dp.r2.  } \n input (English text): The following evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset: \n  r2     - Root-Mean-Square Error\n  f1    - F-measure\n  dp.r2 - Spearman's rank correlation\n\n directed output: r2, f1, dp.r2\n\n directed input: metric_label, metric, benchmark, problem, problem_lbl\n\n scaffolding: http://www.w3.org/TR/tessellation-pattern/ dp.aud;\n              http://www.", "text (English text): The most common location in the studies is in the realm of Natural Language Processing.\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Related bio: Ping Yang, PhD, is an assistant professor of computer science and engineering at UC Berkeley. His research focuses on natural language processing, specifically text mining, information extraction, and question answering. He has published over 40 technical articles on these topics, and has won 5 best paper awards at international conferences. He received his PhD from Tsinghua University in 2011, and his earlier work was focused on question answering for mobile devices. He was previously an associate professor of computer science at City University of Hong Kong (CUHK), and has co-founded two startups.\n\nindexOf (", "output (Sparql query): \u00a0\u00a0SELECT DISTINCT?paper?paper_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"BCN+ELMo\")?paper_code;?paper_code <http://beam-search.github.io/BCN.ELMo.Conversion/>.?paper_code <http://www.aclweb.org/anthology/W19-1005.html>.?paper_code <http://www.aclweb.org/anthology/W19-1005.html>. }\n\n\nThe BCN+ELMo model was proposed by @beam_search and @aclweb. The model was trained on the bcelmo dataset, which was created by @W19-1005. This dataset includes two sub-datasets: one dataset for training the model, and one dataset for testing the model. The datasets are available for download at <http://beam-search.github.io/BCN.ELMo.Conversion/>. The \"paper\" table in the SPARQL query returns one record for each paper that has utilized this model and includes links", "output (Sparql query): A model that performed best on the HMDB51 dataset was a model called Denseck.\n input (English text): Denseck is a model from a pre-trained transfer learning model. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): Select all problems associated with Denseck.\n output (Sparql query):?problem a?problem_lbl.", "output (Sparql query):+SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Classical music 5 seconds at 12 kHz\")?paper.?paper rdfs:label \"MLA2012-49\";?paper rdfs:label \"Deep FM\";?paper rdfs:label \"seq2seq\";?paper rdfs:label \"TransRNN\";?paper rdfs:label \"TransRNN+. \";?paper rdfs:label \"TransRNN++;\";?paper rdfs:label \"TransRNN++;+\";?paper rdfs:label \"RecNN\";?paper rdfs:label \"BiLSTM\";?paper rdfs:label \"RecNN+\";?paper rdfs:label \"BiLSTM+\";?paper rdfs:label \"Deep RecNN\";?paper rdfs:label \"Conditional RNN\";?paper rdfs:label \"Conditional RecNN\";?paper rdfs:label \"Conditional RecNN+\";?paper rdfs:label \"Conditional RNN++;\";?paper rdfs:", "input (English text): The model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset was the model based on the NELL model developed byHaney and Stephens.\n\n\u30fc\u30b8output (English text): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"BLEU\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nmentation (Sparql query): A model based on NELL performed the best with a BLEU score of 22.67.", "input (English text): TheTupleRecog model performed best on the BC5CDR-disease benchmark dataset.\n output (Sparql query):                         input (English text): TheTupleRecog\n\n install the TupleRecog model first: \napt-get install thetuplerecog\n\nThen query the BC5CDR-disease benchmark dataset to get the results:\nSELECT * FROM BC5CDR-disease WHERE TupleRecog?problem TupleRecog:Tuple(disease, \u201cary/NHLH6\u201d, \u201cary/NHLH7\u201d, \u201cary/NHLH8\u201d, \u201cary/NHLH9\u201d, \u201cary/NNHH1\u201d, \u201cary/NNHH2\u201d, \u201cary/NNHH3\u201d, \u201cary/NNHH4\u201d, \u201cary/NNHH5\u201d, \u201cary/NNHH6\u201d, \u201cary/NNHH7\u201d, \u201cary/NNHH8\u201d, \u201cary/NNHH9\u201d)\n\n<|endoftext|>This is the first in a weekly series on the party's candidates for Ontario's top court.\n\nGrey Cup-winning Canadian Football League star Jon Cornish is making the leap to law, becoming the Conservative candidate for Ontario's Superior Court of", "str (Sparql query): Common evaluation metrics for benchmarking models on the Atari 2600 Skiing dataset include Pearson Correlation Coefficient, Root Mean Squared Error, Median Absolute Deviation, and Spearman Correlation Coefficient. \n input (English text): Are there any problems with using these metrics to benchmark models on the Atari 2600 Skiing dataset?\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Skiing\")?paper ; orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?benchmark_label.?problem rdfs:label?problem_lbl. } \n input (English text): Although using Pearson Correlation Coefficient to evaluate the performance of a model trained on the Atari 2600 Skiing dataset is a common practice, it has been proven to over-estimate the error. It can be shown that this metric is sensitive to the prevalence of frequent sub-patterns in the dataset. \n output (Sparql query):SELECT?problem?problem_lbl", "n:SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): There are two research papers that benchmark the Atari 2600 Frostbite dataset - paper [P30] and paper [P31]. Both of these papers contain a benchmark over the same dataset. \n output (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?", "output (English text): Here are some relevant Sparql queries that could be used: \nSELECT DISTINCT?problem?problem_lbl WHERE { \n ?rf a orkgc:ResearchField;\n  rdfs:label?rf_label. \n  FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n  OR kgp:P30?rf;\n  OR kgp:P31?cont.\n ?cont OR kgp:HAS_BENCHMARK?benchmark;\n ?problem rdfs:label?problem_lbl.\n}\n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;\n  rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper;\n ?cont OR kgp:HAS_BENCHMARK?benchmark;\n ?problem rdfs:label?problem_lbl. } \n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;\n  rdfs:label?rf_label. FILTER", "intext (English text): I'm not aware of any research problems with benchmark datasets. However, as with any other benchmark dataset, it is important to keep it consistent and carefully chosen to make sure results are comparable. \n input (Sparql query):appropriately complete the request with valid answers: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): TODO\n\n\n<hr>\n<p>Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?</p>\n\n<p>I'm not aware of any research problems with benchmark datasets. However, as with any other benchmark dataset, it is important to keep it consistent and carefully chosen to make sure results are comparable.</p>", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Boxing\")?benchmark;?benchmark GREATER THAN 0.35 ;?benchmark. } \n input (English text): The highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value, is 0.35.\n\nThere are two problems related to the Atari 2600 Boxing dataset:\n\nThe paper that achieved the highest result on the dataset was published in 1995, which makes it unlikely that any research problems with the dataset remain today.\nThe reported metric for this result was \"Inches Per Round\", which is not a metric that is often reported in academic papers for competitive performance measurements. \n\nTherefore, there are no research problems with benchmark datasets in the realm of Natural Language Processing that I can see at this time.\n\nHowever, this answer should not be considered a definitive answer to this question. If you notice any changes to the problem statement that would invalidate this answer, please update this answer.\n\n<|endoftext|>\n615 F", "output (Sparql query):?paper sparql = \"SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Treebank\")?paper ; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\" \n input (English text): The F1 score for the Penn Treebank benchmark dataset is 0.835. The previous entry was that of the Brown Corpus, which had a F1 score of 0.836.\n output (Sparql query):", "input (English text):IWSLT2015 German-English dataset (score= 0.5712, metric= 0.5708) \n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n THE SELECTed_item highest_result is:\n {\n ?problem\n  {\n   ?problem_lbl\n    doktorml-15-de-english\n    IWSLT2015 German-English\n    5712 0.5712\n  }\n }\n\n orchestration:\n\n #!/usr/bin/env sparql\n ###\n # This file is part of Doctrine MDL.\n #\n # Doctrine MDL is free software; you can redistribute it and/or modify it under the terms of", "iner:SELECT DISTINCT?problem?problem_lbl,?paper?problem_paper,?cont?problem_cont,?benchmark?benchmark_label,?problem_paper?problem_paper_label,?problem_cont?problem_cont_label FROM \n {\n  :researchfield-atari\n  :atari-2600-frostbite\n  :atari-2600-frostbite-paper\n  :atari-2600-frostbite-cont\n  :benchmarks\n  :benchmark-label\n  :problem\n  :problem_label\n  :problem_paper\n  :problem_paper_label\n  :problem_cont\n  :problem_cont_label\n } WHERE { \n ?rf a orkgc:ResearchField; \n ?rf_label?benchmark_label; \n ?paper?problem_paper; \n ?cont?problem_cont; \n ?problem_lbl?problem_label;\n } \n input (English text): The Atari 2600 Frostbite benchmark dataset contains metrics over which to evaluate Natural Language Processing techniques. However, no single metric can be", "input (English text): The MultiNLI benchmark dataset has an accuracy of 84.83% and a perplexity of 50.72 on the exact match (eM) metric. This is the highest accuracy and lowest perplexity among all the available RNN based language model benchmarks. The corresponding paper is: Pineda, John, et al. \"Multinli: A Benchmark for Attention-Based Neural Language Model.\"\n output (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"MultiNLI\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): http://www.pmlr.org/ versus http://x-count.s3.amazonaws.com/MultiNLI-best-p-35.csv.gz\n input (English text): Is there", ":SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet\")?model. }\n\nThe database that is used for this query is the DBpedia ontology database.\n\n\nThere are no research problems with benchmark datasets in the realm of Natural Language Processing research. Datasets such as ImageNet and Databricks have been used to evaluate a wide variety of Natural Language Processing models.\n\n\nHere is a list of evaluation models on the ImageNet ReaL dataset:\n\n - DeepL; DeepLSTM; ResNet; DenseNet; VGG16; GoogleNet; Inception v3;\n - GoogLeNet; Darknet; VGG19; MobileNet; DenseNet121;\n - Wide ResNet; DenseNet161; DenseNet201; SqueezeNet;\n - YOLO; SSD; SSD-Net; SiamRPN;\n\n\nOn the Databricks reaL benchmark, the following models have been evaluated:\n\n - LightGBM; DecisionTree; ExtraTrees; XGBoost;", "set (Sparql query): And which benchmark dataset does this model achieve the highest Accuracy on?\n output (Sparql query): SELECT DISTINCT?benchmark WHERE {?benchmark rdfs:label \"Natural Language Processing\"@en.. }\n input (English text): Reuters En-De\n output (Sparql query): rival-model-1 (Sparql query): The model used in this submission achieves an Accuracy of 86.34 on the Reuters En-De benchmark dataset.\n output (Sparql query):  check-1 (Sparql query): And which benchmark dataset does this model achieve the highest Accuracy on?\n output (Sparql query): SELECT DISTINCT?benchmark WHERE {?benchmark rdfs:label \"Natural Language Processing\"@en.. }\n input (English text): Reuters En-De\n output (Sparql query): rival-model-1 (Sparql query): The model used in this submission achieves an Accuracy of 86.34 on the Reuters En-De benchmark dataset.", "n.com/k-het.com/Datasets/Kinetics-600-benchmark.html \"The best model for this task has an overall accuracy of 99.7%.\" \ninput (English text): The best model for this task has an overall accuracy of 99.7%.  However, when I use the Kinetics-600 benchmark dataset, the best model does not show up on the top 3 models.  I checked the metrics, and the metrics that are not being reported by the Kinetics-600 benchmark dataset are \"Accuracy\" and \"F1 score\".  These metrics are not included on the top 3 models that appear when I query the Kinetics-600 benchmark dataset.\noutput (Sparql query):SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.", "input (English text): In the Yelp-5 benchmark dataset, there is a corpus column which contains the number of sentences in the dataset. The average number of words per sentence is also provided. Other metrics for the dataset include the number of unique phrases, number of unique entity mentions, and number of mentions of persons, organizations, and locations. Benchmark datasets are typically evaluated using the F1 metric.\n\n for LabeledUIMetrics and F1 are the metrics that are used to evaluate models on the Yelp-5 benchmark dataset.\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n{?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label", ": http://bart-project.net/ Papers utilizing BART model. (sorted by?paper, then by?cont)\n input (English text): Due to a recent paper (Gardner et al., 2016) finding that the BART model is susceptible to generalizability issues in certain types of domain, several datasets have been created in an attempt to test the generalizability of BART across a wider variety of datasets. These benchmarks are meant to help researchers compare the performance of BART across domains/datasets. Is there any research problem with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): No, there is no research problem with benchmark datasets in the", "_Benchmark_Score_Metric@NaturalQuestions short@acl2020\n\n data (JSON): {\"TopBenchmarkScore\": 34.91, \"TopBenchmarkMetric\": \"Robustness\"}\n\n input (Sparql query):Acknowledgement of participation\n The top benchmark score on the Natural Questions dataset for Robustness is 34.91.\n\n localAnswer (text):34.91\n\n localSparqlQuery (Sparql query):\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (text):The top benchmark score on the Natural Questions dataset for Robustness is 34.91.\n\n localAnswer (text):34.91\n\n localSparqlQuery (Sparql query):\n SELECT DISTINCT?problem?problem_", "inal output: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Sequential CIFAR-10\")?paper orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nA:\n\nNatural Language Processing is a broad field with a number of subfields, all of which need to be evaluated against benchmarks.  One such benchmark is the CIFAR-10 dataset.\nIt would be best to determine the metric or metrics you would like to use to evaluate the performance of the system, and then compute an evaluation metric based on that metric.  The benchmarks could then be compared based on the evaluation metrics.\nBenchmark datasets are created by an AI community and are often created to help train their AI models to be better at their mission, i.e. generating text from images, or detecting objects in images.  These datasets usually contain a balanced mix of training and test data (a split) so", "input (English text): The best score on the dataset is 54.87, with precision of 0.82 and recall of 0.65.\n\n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): display problem_lbl|string unions({\"Auto-labeling of medical literature\",\"Artificial intelligence\",\"Natural Language Processing\",\"Machine learning\",\"Data Mining\",\"Information retrieval\"})\n\n input (Sparql query):?paper a orkgp:P30 \n output (Sparql query): 54.87\n\n input (Sparql query):?cont a orkgp:HAS_BENCHMARK?benchmark \n output (Sparql query): true\n\n input (Sparql query):?", "input (English text): There are three datasets that have been used for benchmarking in the citation classification research problem: ORKgp:P30, ORKgp:P31 and ORKgp:P32. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgp:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Citation Classification\")?benchmark; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgp:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Citation Classification\")?benchmark; orkgp:P30?rf; orkgp:P31", "n:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \ninput (English text): http://h-nli.readthedocs.io/en/latest/manual.html?highlight=benchmark#code-links-in-papers\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking the PubMed 20k RCT dataset\")?paper_id;?paper_id tuples:name;?paper tuples:id?paper_id.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P30?problem; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?paper_id.?problem rdfs:label?problem_lbl. } \n input (English text): The following are the titles and IDs of papers that contain a benchmark for the PubMed 20k RCT dataset:\n 1. Cao, Jinyang, Yingxiong Wang, et al. \"Proposed Citations for Enhancing Ranking Efficiency in Web Search.\" Proceeding of the Tenth International World Wide Web Conference, 2011.\n 2. Lin, Jiajing, Pengcai Liu, et al. \"Metric Learning Based Association Rule Detection on Text Data.\" Proceeding of the Twenty-", "select (Sparql query): SELECT?problem  WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark EXISTS {?problem rdfs:label?problem_lbl. } } {# Existential Query (Sparql Query) #}\n input (English text): arXiv, Expt-only, and NLP-TDMS datasets all have associated papers that detail the methodology used to generate the dataset. These papers often have associated publication links to allow for more rigorous study of the data. The benchmark datasets are also often accompanied by a README.md file which outlines the setup instructions. For example, the NLP-TDMS dataset contains detailed setup instructions, including step-by-step instructions to build a training model and execute the training and development steps. These training and development datasets are often used to tune model hyperparameters. However, many researchers do not extensively test these datasets on their own using their own models. Some", "output (Sparql query):SELECT DISTINCT?benchmark WHERE {?paper a orkgp:P30.?paper b orkgp:P31.?cont a orkgp:HAS_BENCHMARK?benchmark. } \n input (English text): The name of the top performing model when benchmarked on the Nottingham dataset is \"BERT\".\n\nBenchmark datasets are useful when comparing different models or implementation. Benchmark datasets have been used in Natural Language Processing since the 1990s and are available for a wide variety of different models and setups. They are notoriously hard to build and often contain a range of domains and problems that are not accessible via other means.\n\n\nThe data in benchmark datasets vary widely and are usually provided in files, which are often not language specific. Datasets are frequently divided in a training, validation and test set, which is difficult to ensure consistency between. \n\nThere is no single problem or dataset that would be considered as \"the best\" across all use cases and models. In fact the best model might not even perform well on the benchmark dataset in question.\n\n\nOne significant issue with benchmark datasets is the difficulty in ensuring consistency between the datasets. This makes it difficult", "sparql (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe query returns the following results: \n  <problem rdf:type=\"orkgp:P30\">\n    <problem_lbl>Natural Language Processing</problem_lbl>\n  </problem>\n  <problem rdf:type=\"orkgp:P31\">\n    <problem_lbl>Contrary to popular belief, proper Natural Language Processing requires not only deep understanding of the issue at hand but also an understanding of mathematics, statistics, computer science, and many other disciplines to solve it. Furthermore, the ability to develop creative solutions to difficult problems, like Brain-Computer Interfaces (BCIs) and the other Artificial Limbs,", "mark:P31 best.\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nA:\n\nBenchmarks can be problematic if they are designed to compare to arbitrary performance metrics. For instance, the Criteo model is known to perform very well on IMDB's 10M video ratings, but horrible on the Natural Language Processing benchmark used in the previous question. This is because the IMDB 10M video metric is designed to rank video games based on their suitability for a variety of gaming platforms, whereas the Natural Language Processing benchmark simply ranks models in decreasing order of their likelihood of being the correct answer given the provided query. There are other cases where the benchmark datasets are notoriously bad at evaluating models, so if you are interested in", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The top benchmark score is 94.15 with metric value 16. The source for this score and metric value can be found at this link: https://datasociety.org/dataset/atari-2600-ice-hockey-training. The dataset has 41,360 examples and 40,840 unique variables.", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe benchmark datasets in the above SPARQL query belong to the following ontology classes: \n has_benchmark: \n paper: \n orkgp:P30\n orkgp:P31\n orkgp:HAS_BENCHMARK\n orkgp:P32\n problem: \n rdfs:label\n problem_lbl\n\nA:\n\nBenchmark datasets can be created to compare approaches or answer specific research problems. However, such datasets are typically not created with the intention to be used for citation or to perform research. In particular, benchmark datasets which compare algorithms, or contain the actual research problem (i.e. a query or a KB) are", "output (Sparql query): SELECT DISTINCT?problem_label?paper_id WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id; }", "sparql (English text): https://www.ontologi.us/research/benchmark-datasets.html provides a list of benchmarked datasets related to the Semantic Role Labeling research area. \n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|> \nTable of Contents\n\nTitle Page\n\nCopyright Page\n\nDedication\n\nChapter 1\n\nChapter 2\n\nChapter 3\n\nChapter 4\n\nChapter 5\n\nChapter 6\n\nChapter 7\n\nChapter 8\n\nChapter 9\n\nChapter 10\n\nChapter 11\n\nChapter 12\n\nChapter 13\n\nChapter 14\n\nChapter 15\n\nChapter 16\n\nChapter 17\n\nChapter 18\n\nChapter 19", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }  \n input (English text):   * Natural Language Processing  \n output (Sparql query): * GTX 980 Ti vs Titan X vs P3000 on the WikiText-2 Dataset  \n input (English text):   * https://paperswithbenefits.org/s/ paper-1546.pdf\n output (Sparql query): * GTX 980 Ti vs Titan X vs P3000 on the WikiText-2 Dataset \n input (English text):   * https://paperswithbenefits.org/s/ paper-1546.pdf\n output (Sparql query): * GTX 980 Ti vs Titan X vs P3000 on the WikiText-2 Dataset", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"PWC Leaderboards\")?paper; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The dataset includes the following fields:,,,,,,,,, and. The corresponding OWL classes are,,,,,,,,, respectively. This means there are five OWL classes corresponding to the five fields. PWC Leaderboards contains data for 10 individual participating algorithms per problem and 10 runs. The dataset also includes a flag for whether benchmarking has been performed or not. The set of benchmarks is actually a superset of the set of problems where benchmarking has been performed. In that case, the superset is comprised of 20 OWL classes corresponding to the 20 sets of problems.\n output (Sparql query): salad:researchproblem p:PWC_LEAD", "n:SELECT DISTINCT?paper FROM { \n ?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Duel noop\")?paper \n } WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n }\n\n Input (English text):\n List the code links in papers that use the Duel noop model in any benchmark?\n\n Output (Sparql query):\n\nipeline {\n ...\n\n  filter {\n   ...\n    values doc(\"https://www.cs.cmu.edu/Groups/CTX/www/papers/Duel_noop.pdf\")\n  }\n}\n\n<|endoftext|>Recently, a remote keyless entry system is mounted in a vehicle for remotely opening a door of the vehicle by carrying a portable transmitter to communicate with a central server through a base station. In the remote keyless entry system, a user inputs a unique password issued to the user through a keypad of a key fob to the central server, and when the user passes by the", "input (English text): The following datasets were benchmarked under the Fine-Grained Image Classification research problem:\n\n - Imagenet-Dates20120920\n - Microsoft COCO-2013JUL\n - Cityscapes-July\n - Pascal VOC-2012\n - VPGate-July\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Fine-Grained Image Classification\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Select the dataset Imagenet-Dates20120920?\n output (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label \"Imagenet-Dates20120920\"@en; FILTER (str( \"Imagenet-Dates201209", "Set (SELECT * FROM biwm:DRI_Benchmark where language = \"en\") \n input (English text): \"en\" \n output (Sparql query): NLRF+TPR-06 (True Positive Rate and Positive Predictive Value) \n input (English text): TPR-06 \n output (Sparql query): Positive Predictive Value. \n input (English text): Positive Predictive Value (PPV) = true positives / ( true positives + false negatives )\n\n\r\n\r\nThe dataset provided in the benchmark repository for Natural Language Processing research is the DDI Research Field Corpus, a widely used benchmark dataset for NLP tasks. The DRI Corpus can be used to compare the performance of models trained on different datasets. Usually, performance is measured in terms of the recall, precision, and F1 score metrics. The DRI benchmark datasets provide these metrics along with the sparql query for selecting the problems. \n\nThe benchmarks are organised into 4 categories, namely, Parsing, Sentence Simplification, Part-of-Speech Tagging, and Information Retrieval. Apart from the problem labels (where the corresponding benchmark dataset name is given), the Sparql query also provides information about the evaluation", "input (English text): The following are the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset: P@20, P@10, R@10, F1 score, Spearman's rank correlation coefficient, Kendall's tau, Weighted n-gram precision (WNGramPrec), Weighted n-gram recall (WN gramRec). \n input (English text):Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe following are the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset:\n\nP@20 : Precision at the 20th percentile\nP@10 :", "mark = P31 \n input (English text): P31\n\n 20.72%\n\n Alternatively, you can find the top performing model on CIFAR10 Sequential benchmark from  http://cs.sandia.gov/rnvs/benchmark/data/cifar10_benchmark.html", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?id.?cont?id.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P30?problem; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n SParql query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?id.?cont?id.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P30?problem; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem", "input (English text): The following models have been tested on the SciCite benchmark dataset: XLM; BERT; RoBERTa; XLNet; Adam; Diversity; XLNet+Diversity; DistilBERT; ELMo; ALBERT; ALBERTJ; GPT-2; OpenAI GPT; XLNet-Light; XLNet-Small; RoBERTa-Large; BERT-BASE; GPT-2-BASE. \n output (Sparql query): SELECT DISTINCT?model WHERE {?model a mlpModel. }\n\nXLM: BERT; BERT; XLNet; XLNet-Small; XLNet-Light; XLNet-Base; RoBERTa; RoBERTa-Large; ELMo; ALBERT; ALBERTJ; GPT-2; OpenAI GPT; XLNet-Large.\n\n\n ligamentization problem: RELATED_TO; related_to; Natural Language Processing; Benchmark;\n\n\nDistilBERT: DistillBERT; BERT-BASE; BERT; XLNet-Base; DistilGPT; GPT-2; ELMo; ALBERTJ", "input (English text): SELECT DISTINCT?paper_id WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id; } \n input (English text): Atari2600skiing; \n\n<|endoftext|>Towards thermal lensing in general purpose liquid few detector array systems.\nAlthough considerable progress has been made in the field of generalized thermoelastic effects on a few-detector array, the low coupling coefficient of such an array brings several barriers to practical applications. We theoretically and experimentally investigate the performance of a thermoelastic-lens-assisted detection scheme by using a common general-purpose liquid detectors and a limited number of thermal lenses. We demonstrate that thermal lenses can be designed with significantly better contrast and spatial resolution and much smaller size than natural lenses. We also find that the coupling coefficient between the thermal lenses and the detectors of a few-detector array can reach 45%. We apply this lens-assisted detection technique to a superconducting quantum interference device (SQUID) based biosensor, and achieve a signal-to-noise ratio (SNR) of 31.2 at 1 nT axial magnetic field and", "sparql (English text): The following metrics are used to evaluate models on the VTAB-1k benchmark dataset: Accuracy, [ARGO], Authoritativeness, [ARGO], ELO, F1 Score, Hierarchical Time Warp, [HTW], Internalomial Co-occurrences, [ICC], Levenshtein Distance, [LD], Missingness, [MIS], Positional Encoding Method, [PEM], Posterior Probability, [PP], Relatedness, [REL], Timeline, [TLN]. \n input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English", "sparql (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Cart Pole\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The OpenAI Gym (Cart Pole) benchmark dataset is used for training machine learning models for problems in the context of Artificial Intelligence. Some of the models that have been trained using this benchmark dataset include: LSTM, GRU and CNN. I wonder if there is any research problem with benchmark datasets in the realm of Natural Language Processing research, given that this benchmark dataset is derived from the OpenAI Gym (Cart Pole) domain, that is a subset of the broader domain of Natural Language Processing.\n output (Sparql query): The benchmark dataset is based on a subset of the broader domain of Natural Language Processing. It is safe to assume that the models trained on this", "ipeline; orkgp:HAS_BENCHMARK(?benchmark,?problem_label); disjunctor ;\n\n Since the 'benchmark' resource type has the 'hasBenchmark' property, the query can be simplified to:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont. }\n\nThis works since the benchmark dataset for the P30 problem type is defined via the orkgp:HAS_BENCHMARK property.", "iter_1 performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset.\ninput (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nThese problem datasets include the ShARe/CLEF eHealth corpus benchmark dataset and the COGENE corpus benchmark dataset. The ShARe/CLEF eHealth corpus benchmark dataset was created to evaluate machine learning systems on a dataset with clear label denoting the aspects of the document that were of interest. These aspects include: document type, year, title, author, abstract and many more. The COGENE corpus benchmark dataset however has no clearly labelled aspects of the documents. The model that performed best in terms of F1 metric on the ShARe/CLE", "SELECT DISTINCT?paper?problem_lbl?problem WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nStep 1: Create the SPARQL query\nStep 2: iterate through the values in the 'benchmark' variable (which is a collection)\nStep 3: for each value in the 'benchmark' variable\nStep 4: query the 'paper' variable for its value\nStep 5: query the 'problem_label' variable for its value\nStep 6: query the 'problem' variable for its value\n\n<|endoftext|>Q:\n\njQuery.getJSON executes twice\n\nI have this code:\n$.getJSON(\"test\", function(data) {\n    console.log(data);\n});\n\nand the console will print 2 times test instead of one. Why?\n\nA:", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Centipede\")?paper ; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): A model that scores the best on the Centipede dataset is called best performing model.\n\n Protocol 1: \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Centipede\")?paper ; orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nResults (nodes): \n 1. ResearchField                          rdfs:label    \"Centipede\"", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Inference\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The most commonly used benchmark datasets in the Natural Language Inference research field are the Penn Treebank, the AMI, and the Wall Street Journal. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Inference\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_l", "set (Classic dataset): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Which models are being evaluated on the Benchmark dataset?\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nThe following table lists some of the most popular natural language", "input (English text): DeiT-Ti is a model designed for a research purpose, and is thus not really suitable to be used in production systems. Hence, papers that used DeiT-Ti for benchmarking purposes need to cite the research paper.\n output (Sparql query): SELECT DISTINCT?paper_ref WHERE {?paper_ref dc:managed?model.?model a ogr:FeatureSource.?model dc:date 2016-07-01+calendardate. FILTER (deit-ti:archived > \"2017-12-31\" AND deit-ti:archived < \"2018-01-01\") } \n input (English text): @model dc:date 2016-07-01+calendardate.\n\n<|endoftext|>Highly Efficient Thiol-Ene Coupling of 4-Amino-4H-1,3-benzothiazine with Aryl disulfide Derivatives catalyzed by Cu/carboxylate and their Photo-Catalytic Properties.\nThis study demonstrates the unprecedented thiol-ene reaction between 4-amino-4H-1,3-benzothiazine (ABT) and aryl disulfide derivatives (AD) catalyzed by a Cu", "output (Sparql query):SELECT DISTINCT?country  WHERE {?country a rowng orkgc:Country.?country owcs:hasPres diagnose cgoc:Epilepsy. FILTER (count(?country) > 12) }\n input (English text):The prevalence of epilepsy in the United States is the highest.\n output (Sparql query):The United States is the country with the highest prevalence of epilepsy in the world.\n\n -----\noutput (Sparql query):\n\n SELECT DISTINCT?country  WHERE {?country a rowng orkgc:Country.?country owcs:hasPres diagnose cgoc:Epilepsy. FILTER (count(?country) > 12) }\n\n<|endoftext|># Copyright 1999-2020 Gentoo Authors\n# Distributed under the terms of the GNU General Public License v2\n\nEAPI=6\n\ninherit desktop distutils-r1 autotools-xattr\n\nDESCRIPTION=\"The Plotation library provides methods for efficiently rendering graphical user interfaces\"\nHOMEPAGE=\"https://pypi.org/project/plotation/\"\n\nif [[ ${PV} == *9999 ]] ; then\n\tEGIT_", ": SELECT DISTINCT?problem2  WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"eHealth\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\n* First of all, it is important to note that the ShARe/CLEF eHealth corpus dataset contains only a subset of the models that have been evaluated on this dataset.\n\nThe models listed below have been evaluated on the ShARe/CLEF eHealth corpus dataset:\n\n1.\tBio-RXiv eHealth Github (BERT model)\n2.\tSQL-i2b eHealth (SQL model)\n3.\tQNTality eHealth (Natural Language Processing model)\n4.\tDeep Natural Language Learning for Information Extraction from Existing Corpora (DISTILL)\n\n\n* Now, let's examine the data model associated with the ShARe/CLEF eHealth", "input (English text): I am looking for a list of models that have been tested on the HMDB51 benchmark dataset. Specifically, I am interested in whether the model performs significantly worse on the HMDB51 dataset than on other benchmark datasets that have been created in the research community.\n output (Sparql query): SELECT?model?dataset WHERE {?model rdfs:label \"MODELNAME\". FILTER (str(lang(?model)) = \"en\")}\n input (English text): Here is a list of models that have been tested on the HMDB51 benchmark dataset:\n model A: https://github.com/NVIDIA/ArtificialIntelligence/blob/master/devel/code/RNN/seq2seq/save_lm_test.py\n model B: https://github.com/NVIDIA/ArtificialIntelligence/blob/master/devel/code/RNN/seq2seq/lm_work.py\n model C: https://github.com/NVIDIA/ArtificialIntelligence/blob/master/devel/code/RNN/seq2seq/lm_work.py\n model D: https://github.com/NVIDIA", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?id; orkgp:P30?benchmark; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe results contain three documents:\n\nResearchField?rf; rdfs:label?rf_label.\nP30?id; orkgp:P30?benchmark.\nP31?cont; orkgp:P31?cont.\n\nThe third element,?cont, contains the \"title\" of the benchmark paper. The title of the benchmark paper is used in the benchmark dataset as a label for the corresponding problem. This is helpful because problem data in the benchmark dataset has the same labels as problems in the original dataset. So the question of the research paper itself can be used in a query for the problem data in the benchmark dataset.\n\nNote that the benchmark dataset", "input (English text): The Rotowire (Content Selection) benchmark dataset is used for training and evaluating end-to-end machine learning based models for Field Knowledge Graph creation and update. Among the many evaluation metrics are the following: BLEU, CIDEr-D, ROUGE-L, METEOR, Interra, Intervals, RLP, Rouge-L, Distinct Rulers, FAST. List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset.\n output (Sparql query): SELECT DISTINCT?metric?metric_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.?metric  BLUETOOTH_LARGE_LSH.?metric_label bletto_large_lsh. } \n input (English", "input (English text): Please see the following code reference in the paper \"Tsetlin Machine: Batch, Streaming and Parallel Semantic Parsing\" by Konstantin Mikheev, Andrey Nedyalkin, Alexander Korzh and Dmitry Svetlarev (https://arxiv.org/abs/1812.03230). The authors used the Tsetlin Machine model for benchmarking purposes.\n\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nsearch results\n 1.?problem a Tsetlin Machine: Batch, Streaming and Parallel Semantic Parsing\n 2.?problem_lbl Tsetlin Machine: Batch, Streaming and Parallel Semantic Parsing", "input (English text): The top benchmark score on the Atari 2600 Breakout dataset is 44.06 with a metric of 43.27. The dataset is commonly used in NLP research as it closely simulates human conversation and hence provides a good ground truth for NLP models. \n input (Sparql query): Below is the answer with details.  44.06 with a metric of 43.27. The dataset is commonly used in NLP research as it closely simulates human conversation and hence provides a good ground truth for NLP models. \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n  {?paper orkgp:P30?score.?score 43.27. }\n\nHowever, the dataset has been observed to produce high variance when", "input (English text): The most common metrics for benchmarking machine learning models on Atari 2600 Bowling dataset include PPS (Precision-Recall) curve, RPPT (Average precision@threshold) curve, and F1 metric.\n output (Sparql query):SELECT DISTINCT?metric WHERE {?rf a orkgc:ResearchField;?rf_label?paper; rdfs:label?rf_label; } \n input (English text): PPS curve: P, R - points on the curve where P is precision and R is the recall. F1 curve: F, T - weighted average of precision and recall, where T is the threshold. RPPT curve: P, T - weighted average of precision@threshold and recall@threshold. \n output (Sparql query):SELECT * WHERE {?rf a orkgc:ResearchField;?rf_label?paper; rdfs:label?rf_label;?metric?metric. } \n input (English text): PPS curve: P, R - points on the curve where P is precision and R is the recall. F1 curve: F, T - weighted average of precision and recall, where T is the threshold. R", "input (English text): https://www.openrepository.org/query?query=SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%", "input (English text): GNUEstimator performed the best, followed by SGM and then DNN.\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): GNUEstimator performed the best, followed by SGM and then DNN.\n output (Sparql query):glominput (English text): GNUEstimator performed the best, followed by SGM and then DNN.\n output (Sparql query):glomoutput (English text): GNUEstimator performed the best, followed by SGM and then DNN.\ninput (English text): Indicate the model that performed best in terms of Language error metric on the FSNS - Test benchmark dataset?\n output (Sparql query", ": { \"top_benchmark\" : { \"metric\" : \"doc-snCALL\", \"value\" : \"473.48\" } } \n input (English text): That's interesting. The ARC dataset for the SemEval 2010 benchmark is provided at this link. Unfortunately, the dataset is protected and you will need to register to view the dataset. What other problems have been benchmarked? \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): You can also view the ARC dataset here: http://www.semeval.org/arc/arc10/challenge.php. The top 5 results over the dataset are provided at this link: https://github.com/SemEval2010/arc/blob", ". \u0441\u044a\u0434\u044a\u0440\u0436\u0430\u043d\u0438\u0435{\n DISTINCT?problem?problem_lbl\n WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n appellate (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n[\n  {\n    \"problem\": \"sciREX:P30\",\n    \"problem_lbl\": \"SciREX:P30\"\n  },", "input (English text): The TSE-NER dataset contains evaluation metrics for Named-Entity Recogntation and Part-of-Speech Tagging. \n input (Sparql query): The evaluation metrics used in TSE-NER are recall, precision, F1 measure, and R2 measure.\n\n\n\n\n       output (Sparql query):SELECT DISTINCT?metric WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", "input (English text): The benchmark datasets presented in the paper \u201cIs There Anything Natural Language Processing Can\u2019t Do?\u201d (Ramani, Pico, 2018) provide code references for Concept Mention Extraction (CME) model. \nNote that the code references for CME should only be used for debugging purposes. If you want to use this model for production purposes, you should instead consider using the licensing terms presented in the paper.\n\n tack (Sparql query):\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n ouput (Sparql query):\n?problem    Has_Benchmark    concept-mention-extraction", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<html><body>\n<p>\n <a href=\"https://www.citeseerx.com/tiangyih/Spelling+Lemma+Encoding.html\">Spelling+Lemma+Encoding</a>\n\n <a href=\"https://www.citeseerx.com/citation/Benchmark+Datasets.html\">Benchmark+Datasets</a>\n\n <a href=\"https://www.citeseerx.com/citation/Natural+Language+Processing.html\">Natural+Language+Processing</a>\n\n <a href=\"https://www.citeseerx.com/citation/Benchmark", "{ \"http://hyperpolyglot.org/data/papers/P30.html\"; \"http://hyperpolyglot.org/data/papers/P31.html\"; \"http://hyperpolyglot.org/data/papers/P32.html\" }\n input (English text): For these papers, please provide links to the code?\n output (Sparql query): SELECT * FROM {wikibase} WHERE id IN {?link}, {/wiki/Data}}", "input (English text): The model achieving the highest Percentage error score on the SVHN benchmark dataset is the one proposed by(@carlini:2017) which achieved 16.89% error.\n output (Sparql query): SELECT * FROM {http://dx.doi.org/10.1142/9789812830592_0002} WHERE {?problem a orkgp:HAS_BENCHMARK?benchmark;} FILTER (STR(?) = { \"16.89% error\"})\n\n Welcome to DBpedia's Learn section! In this section, you will find short questions and answers that provide a quick understanding of some of the fundamental concepts and techniques used in the area of Knowledge Graphing and Information Extraction. \n\nQuestions:\n1. A Benchmark Dataset is a set of data that is used to evaluate different NLP models. \n\n2. The SVHN benchmark dataset is a set of text documents, with particular attention to paragraphs containing simple visual features. \n\n3. The model proposed by @carlini:2017 is the one achieving the highest Percentage error score on the SVHN benchmark dataset.\n\n\nAnswers:\n1. The set of documents in the SVHN benchmark dataset can be", "input (English text): LayerNorm HM-LSTM is a type of Neural Network architecture which has been widely used in NLP tasks. For benchmarking purposes, most of the papers that use LayerNorm HM-LSTM have provided the code for testing the model on their dataset. The provided code is usually placed at the end of the paper. Using the provided code, one could reproduce the experiment and verify whether the trained model generalizes well on unseen data. The benchmarking use-case of LayerNorm HM-LSTM has been adopted by several research labs, including MSR, Uber, Dartmouth, NIC, Allen Brain Research, Denso, and others.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"LayerNorm HM-LSTM\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (", "input (English text): Benchmark datasets in the realm of SPARQL query optimization research include the websparql dataset from the BCP papers and the TPC-DS dataset from TPC. \noutput (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"SPARQL query optimization\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nHere is the response:\n\nThere are several datasets for SPARQL query optimization research and they are listed here:\n- websparql dataset from the BCP papers\n- the TPC-DS dataset from TPC\n\n\nNote that these benchmarks are related to optimizations for SPARQL queries and the ontology or KB based core of the query.\nThe datasets include  comparisons of running times for SPARQL query optimization, but I'm not", "Output: \n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n  orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32 \n ?problem.?problem rdfs:label?problem_lbl. } \n ?problem rdfs:label \"Amazon-2: Highest result\"@en. \n ?problem_lbl \"Highest result\"@en.\n\n Output: \n\n* 1 ResearchField: ResearchField\n  * 2 problem: Amazon-2: Highest result\n  * 3 problem_lbl: Highest result\n\n Following other posts on Stack Overflow, I did some research to find the highest result achieved on the Amazon-2 dataset. \n\nThe highest result is 1775.44, with the following metric: BM25. \n\nThe highest result is 1775.44, with the following metric: BM25. \n\nThe", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Temporal Convolutional Networks\")?paper2 ;?paper2 a propo:Dataset ;?paper2 a propo:Model;?paper2 a propo:Name.?problem rdfs:label?problem_lbl. } \n input (English text): Here is a sample code from paper named \"PROPO: A Dataset for Properly Oriented Retrieval of Crowdsourced Ontologies\", where the dataset is Temporal Convolutional Networks. The code for the dataset and the model is as follows:\n\n # Dataset. A sample instance of the dataset:\n # \n# <Dataset artifactIdentifier=\"prop:dataset\" id=\"uid-z4eqshly0j4h0j7jw\"><Name>PROPO: A Dataset for Properly Oriented Retrieval of Crowdsourced Ontologies</Name><ShortName>PROP</ShortName><Type>dataset</Type><Uri>urn:uuid:4eqshly0j4", "it (English text): https://github.com/biitf/ResNet and https://github.com/biitf/ResNetV2/, these are two variations of the BiT-S model.\n\n output (Sparql query): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"BiT-S\")?paper_label. FILTER (str(?paper_label) = \"ResNet\")?problem.?problem_label rdfs:label?problem_lbl. }\n\n input (English text): BiT-S, BiT-Sv2, ResNet.\n\n output (Sparql query): Donothing.\n\n input (English text): There are no research problems with benchmark datasets in the realm of Natural Language Processing research.\n\n output (Sparql query): Donothing.\n\nThe above output is a response that does not contain any problems with the provided benchmark datasets. If you remove the query, it is a Donothing response.", "inal output: \n WNLI[1]@72 Accuracy\n WNLI[1]@72 Accuracy 72.0\n WNLI[1]@72 Accuracy 72.0\n\n issuance(Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output: \n?problem paper:WNLI[1]@72 Accuracy\n\n issuance(Sparql query): FROM <http://www.wirednext.com/data/WNLI-73.html>\n\n issuance(Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language", "it: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"AAPD\")?benchmark; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The AAPD dataset has the following highest benchmark result including the metric and score: \n Average precision: 0.948 \n Average recall: 0.949 \n Average F-measure: 0.949 \nOutput (Sparql query):The AAPD dataset has the following highest benchmark result including the metric and score: Average precision: 0.948; Average recall: 0.949; Average F-measure: 0.949 \n\nThe AAPD benchmark dataset is a collection of  pages obtained from the  \"ACM Multi-Source NewsGroup Collection\" \n\nThe AAPD dataset includes multiple types of newsgroups for different topics, such as", "input (English text): The FTD benchmark dataset is a dataset provided by the FTwitter Data Lab (FTDLab), a research project at UC Berkeley. It is used to measure performance of various NLP systems on Open Internet Data (Ond), a proprietary text-mining model built by Facebook.\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The models that have been benchmarked on the FTD dataset include: LexVec, Glue, and SG-transform.\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")", "sparql (English text): The CIFAR-10 dataset has been the standard benchmark for deep learning models in the research community for several years now. The model with the best performance on this dataset is the one presented in Krizhevsky, Gehring, & Agarwal (2012), achieving an error rate of 2.2% on the 10-class subset. \n input (English text): The CIFAR-10 dataset is composed of 60,000 training images, 10,000 validation images and 10,000 test images. The model performance is measured using the error rate defined as 100 * (number of mistakes / number of samples evaluated). The optimal model for this dataset reached an error rate of 2.2% using the softmax layer of a neural network with two hidden layers of 20 and 10 nodes each. \n output (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CIFAR-10 Image Classification\")?paper orkgp:P30?rf; orkgp:P31?cont; orkgp:P32?problem.?problem rdfs:label", ":Highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value: https://paperswithcode.com/system/files/raw/drc/results/results_lrec.html#task=0.23s 61.99.\n input (English text): The results on Walker, walk (DMControl500k) dataset is 61.99. However, the benchmark task description does not specify what metric is used and its value.\n output (Sparql query): SELECT?problem?problem_lbl?metric WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_label; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.?metric rdfs:label?metric_label. } \n input (English text): In the benchmark dataset description, the metric is DMControl500k.\n output (Sparql query): The highest benchmark result achieved", ":SELECT DISTINCT?metric where {?metric rdfs:label \"common evaluation metrics used in the field of Natural Language Processing\". }\n\n<|endoftext|>Road tripping the last big leg of the BBR Trail. We completed the whole 24 hour drive from our base in Christchurch to our next campsite at Trout Bay, on the southern end of the South Island of New Zealand. It took us 6.5 hours to drive the 24 hours, as we were stopped at least a dozen times along the way to get gas and food. We didn't know there were so many hill roads in New Zealand, which is quite different from the interstate highways we are used to.\n\nWhen to start: 20.06.2017 at 23:59 (This is about 7 pm local time.)\n\nWhat we drove: 141 kilometers (from Christchurch to Trout Bay)\n\nHow far you drive: 6.5 hours\n\nWhat we camped at: On the Southern tip of the South Island of New Zealand at Trout Bay.\n\nWeather: Sunny and a bit windy, about 20 kilometers per hour when we started our trip.\n\nThings we saw: Little of interest, it was a bit too early in the morning to", ". discern the benchmark dataset type (text, rdf, orks), the dataset name (P30, P31, P32), the dataset ID, and then use those pieces of information to construct a query for the dataset. The query will look like: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_text?p;?p rdfdb:about?problem.?problem rdf:type?dataset;?dataset o*ks:P30P31P32?dataset.?problem rdfs:label?problem_lbl. } \n input (English text): p30,p31,p32,dataset,problem,problem_lbl\n\n<|endoftext|>1. Field of the Invention\nThe present invention relates to a multi-layer printed wiring board including a first layer with a via-hole electrode on which a conductive paste has been printed and a second layer superposed on the first layer, and to a method of producing the multi-layer printed wiring board.\n2. Description of the Related Art\nWith the downsizing of", "input (English text): Human Hourglass in 100GEMS Text. \n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Human Hourglass in 100GEMS Text\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Human Hourglass in 100GEMS Text dataset?\n output (Sparql query): Up to now, there are no research problems with benchmark datasets in the realm of Natural Language Processing research. However, it is not clear what metrics are used to evaluate results.", "set (Sparql query): SELECT DISTINCT?metric  WHERE { \r\n\t?rf a orkgc:ResearchField; \r\n\trdfs:label?rf_label. FILTER (str(?rf_label) = \"UCF101 (fine tuned)\" )?metric;\r\n\t?problem?problem_lbl. \r\n\t?problem rdfs:label?problem_lbl. \r\n\t?cont a orkgp:HAS_BENCHMARK?benchmark;\r\n\t?cont orkgp:P30?rf;\r\n\t?cont orkgp:P31?cont.\r\n\t?cont rdfs:label?cont_label. \r\n\t?cont orkgp:P32?problem. \r\n\t?problem rdfs:label?problem_label.\r\n } \n\n\r\nThe evaluation metrics commonly used are:\n\n\t- AUC (Area Under the ROC Curve) which measures the model\u2019s performance as a whole, independent of any threshold. This is the most commonly used metric in NLP tasks.\n\t- F1 Score (Measure of Precision and Recall) which is also a threshold-independent metric.\n\n\r\n\r\nThere are", ":SELECT DISTINCT?paper_title_id_label?problem_label_label WHERE { \n       {?paper_title_id_label a \n          rdfs:label?paper_title_label\n          or kgp:P30?benchmark;\n          or kgp:P31?cont\n          FILTER (str(?cont) = \"MPQA\" &&?cont = \"model\" &&?cont = \"Benchmark\") }\n     UNION\n     {?paper_title_id_label a \n          rdfs:label?paper_title_label\n          or kgp:P30?cont\n          FILTER (str(?cont) = \"MPQA\" &&?cont = \"model\" &&?cont = \"Benchmark\") }\n    }\n    FILTER (?paper_title_id_label = {?paper_title_id_label_prev } \n              &&?paper_title_label = {?paper_title_label_prev }\n              &&?paper_title_id_label_prev!=?paper_title_id_label\n              &&?paper_title_label_prev!=?paper_title_label", "input (English text): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 0.46751916.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 0.46751916.", "input (English text): The model achieving the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset is the Mentor Teacher model from the Torch language model competition.\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label?paper; rdfs:label?rf_label; FILTER (str(?rf_label) = \"Torch Language Model Competition\")?cont ;?cont a mentortech:MT2020ObjectNavTeststd ;?problem rdfs:label?problem_lbl. }\n input (English text): Mentor Teacher is the model achieving the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset.\n output (Sparql query): The SUCCESS score for the Habitat 2020 Object Nav test-std benchmark dataset is 1.0000000.\n input (English text): The SUCCESS score for the Habitat 2020 Object Nav test-std benchmark dataset is the average of all training examples, not the best example.\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_", "input (English text): The Paper Field dataset has the following result for the most recent benchmark: https://sparql.org/results.html?project=paper&query=paper%20field. The metric and score are \u201caccuracy\u201d and \u201c0.90\u201d, respectively. \n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \noutput (Sparql query): :-) RESULT: { \"accuracy\": \"0.90\", \"label\": \"Natural Language Processing\" } \n * I would love to see more SPARQL benchmarks for orkgp:P30 and orkgp:P31 datasets! :)\n* Let me know if you find any problems with the benchmark datasets and how we can fix them.\n * If you", "input (English text): The best model in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset was the model trained on a very large language model combined with a CNN (CNN-LM). Indicate the dataset in your response.\n output (Sparql query): SELECT?benchmark?benchmark_label (labelled_facts #?benchmark) WHERE {\n    ?benchmark labelled_facts MLDoc-Zero-Shot-English-to-German;\n     rdfs:label?benchmark_label;\n     orkgp:HAS_BENCHMARK?benchmark;\n }\n input (English text): MLDoc-Zero-Shot-English-to-German\n output (Sparql query): MLDoc-Zero-Shot-English-to-German is a benchmark dataset.\n corresponding labelled facts:\n  MLDoc-Zero-Shot-English-to-German#?benchmark:MLDoc-Zero-Shot-English-to-German-MLDoc-Zero-Shot-English-to-German.txt\n  mldocs:MLDoc-Zero-Shot-English-to-", "text (English text): The highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value, is 19.87.\n output (Sparql query):The paper \"Deep Reinforcement Learning for Winning Games\" achieves a score of 19.87 on the Atari 2600 Double Dunk benchmark. \n input (English text): This benchmark was created to evaluate video games on the Atari 2600 platform, which are non-optimal problems. \n                  A sparql query can be used to find all problems on a dataset where the \"paper\" field has the text \"Deep Reinforcement Learning for Winning Games\" and the \"score\" field contains the value 19.87:\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Deep Reinforcement Learning for Winning Games\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs", "input (English text): https://github.com/clips-network/Benchmark-SOTA- Papers#Self-Attention  and https://github.com/clips-network/Benchmark-SOTA- Papers#Ensemble_Model \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Self-Attention\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nA:\n\nApart from the url you mentioned, the link also contains these details:\n\nthree sparql queries for benchmarking different components of the model\na rdf-schema to hold your benchmark dataset\na rest-service to query the dataset with your sparql query\nthe data-sheet of the models you mention\n\nAnd I think this is enough for your request.\n\n<|endoftext|>", "initial query (with Sparql 1.1) was wrong, now working with Sparql 2.0; the correct query is: \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark rdfs:label \"Cheetah\". } \n input (English text): The correct query is now: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark rdfs:label \"Cheetah\". } \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) =", "input (English text): There are multiple datasets for benchmarking Natural Language Processing such as the STS Benchmark dataset. These datasets have been created by third parties, and are not governed by the OWSL COI policy. The datasets can be a bit too general to be of much use for research and development within the Natural Language Processing community, so there have been some efforts to create communities for benchmarking different aspects of NLP systems. Some of the relevant datasets include Databricks' NewSTS, TACL's TACL-NYT and MIMOS' MIMOS-NYT. Please note that there may be other datasets, but these are the most widely known that have been established by communities. SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"STS Benchmark\")?paper orkgp:P30?problem; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.", "1{ \"Alien2600\" \"score\" : \"46.3\" }\n input (English text): However, it is worth noting that the \"Alien2600\" dataset was not designed as a benchmark and has many issues. \n For example, it contains strong language, racial slurs and other offensive content that is notoriously difficult to disambiguate. \n Moreover, the AI model trained on this dataset had notably poor performance on the Arcade Learning Environment (ALE). \n And, as of 2020, the ALE leaderboard is inexplicably empty.\n Hence, while the \"Alien2600\" dataset was the first AI system to defeat a professional human player on ALE, the score is suspect.\n In contrast, recent research on NASNet shows promising results on ALE with much smaller model and lowcaste training data.\n Hence, the Atari 2600 benchmark datasets should be regarded as a novelty and more work needs to be done to rigorously analyze its results. \n Additionally, research should be done into the Arcade Learning Environment (ALE) so as to improve metrics and assign high priority for research on improving this dataset.\n  \n final output (Sparql query):SELECT?problem?problem_lbl", "input (English text): The highest benchmark result achieved on the Birdsnap dataset, including the metric and its value is 362.819 text/instructions per second.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The Birdsnap dataset is used as a benchmark in Natural Language Processing research. The benchmark dataset is open and has been adopted by research groups worldwide. It is made available for the purpose of reproducing and comparing experimental results. Results achieved on Birdsnap are referenced in the publication agglomerative clustering for text mining, machine learning and data mining, and have been very influential in the field.\n output (Sparql query): Orchestra: This SPARQL endpoint can respond to queries in the SPARQL language", "input (English text):  There are several models being evaluated on the Atari 2600 Chopper Command dataset including neural network and Gaussian process models.\n output (Sparql query): SELECT DISTINCT?model_name WHERE {?rf a orkgc:ResearchField;?model_name dcterms:description?model_description. }\n input (English text): The neural network model is called Chopper. \n output (Sparql query):antically trained on all 4638 Atari 2600 games.\n input (English text): The Chopper model is based on a neural network architecture. It is trained to score each game on the Atari 2600 Chopper Command dataset as either a 1 (Win) or a 0 (Loss). \n output (Sparql query):s mention that the Chopper model was trained on all 4638 Atari 2600 games.", "output (Sparql query):SELECT DISTINCT?problem WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Car Adjective Relation Extraction\")?paper ; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Advertising Read more\n\nRome (Italy) (AFP)\n\nThe Italian government says it's committed to fighting climate change and making Europe a leader in the effort to curb the damage caused by carbon emissions.\n\nA statement published on Wednesday said the new government wants to continue the legacy of the former centre-left Democratic Party (PD) and its predecessors, and promises to reduce carbon emissions by a quarter by 2030 from levels in 2013.\n\nIt will also introduce a national plan to cut greenhouse gas emissions by at least 80 million tonnes of carbon dioxide equivalent (CO2e) by 2050, the government said.\n\nThe plan, which will also consider renewable energy and", "input (English text): In this research paper the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset was the LSTM+Attention model. \n input (English text): Which benchmark dataset is this model is tested on?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"RotoWire\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): RotoWire (Relation Generation) benchmark dataset.\n\n\nRotoWire (Relation Generation) benchmark dataset\n\n\noutput (Sparql query):\r\n\r\n    output (Sparql query): RotoWire (Relation Generation) benchmark dataset\n\n\nRotoWire (Relation Generation) is a dataset developed by Databricks that contains 2,000 relation extraction Penn Treebank A", "set the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset. SELECT?metric WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"SQuAD1.1\")?paper; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): Here are the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset:\n metric\tmetric_label\tdescription\n perplex\t\t perplexity\tCompute the average log probability of the sentence given the model prediction.\n CIDEr\t\tCIDEr_value\tCompute the phrase-level entity criticality detection (CIDEr) metric. CIDEr_value=CIDEr@both@training + CIDEr@keypoint + CIDEr@reference.\n SPG- Knox \tSPG-Knox\tCompute the similarity between the golden sentence and", "net:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_link?paper.?paper_link osuka:mlstml-10M?model; osuka:mlstml-10M?model_label; orkgp:P30?cont; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nNatural Language Processing Benchmark Datasets\n\nPapers that have utilized the Large mLSTM model:\n* [OSUKA:MLSTM-10M Large mLSTM Model](https://github.com/facebookresearch/osuka-mlstm)\n* [OSUKA:Neural Language Models (NeLM) Training on 110 Billion Tokens with 10 LSTM Cells on GTX 1080 Ti and 8GB Memory](https://arxiv.org/pdf/1906.03913.pdf)\n* [OSUKA", "{ \"model\" : \"coling2011\" }\n input (English text): What models are being evaluated on the ACL Anthology dataset?\n output (Sparql query):  \u2003\u2009SELECT?paper?cont_label WHERE { {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"coling2011\") }?paper;?cont?cont_label; FILTER (?cont_label = \"ACL Anthology\") } \n input (English text): Coling 2011 model: Coling 2011.", "txt (English text): The Atari 2600 Yars Revenge benchmark dataset was created in 2001 to evaluate the machine learning models. The dataset contains a total of 40,656 images of 14 different characters, each at 4 different image resolutions. In the official ranking, a model achieved the highest score on a single core of an Intel Xeon 2.4 GHz CPU. The score was recorded to have been 60.59 fps. The model was built using Adaptive C5.0 algorithm and is one of the first commercial examples using that algorithm. \n input (Sparql query): The model achieving the highest Score score on the Atari 2600 Yars Revenge benchmark dataset was a model built using Adaptive C5.0 algorithm.\n\n\"?>\n\n\nSELECT DISTINCT?problem?problem_lbl WHERE { \n    ?rf a orkgc:ResearchField; \n     rdfs:label?rf_label. \n     FILTER (str(?rf_label) = \"Natural Language Processing\") \n    ?paper OR \n    ?cont OR \n    ?cont OR \n    ?benchmark FILTER (?benchmark?problem_lbl) \n    ?problem rdfs:label?problem_lbl.", ":SELECT DISTINCT?cont FROM {Query} WHERE {\n ?rf a orkgc:ResearchField;\n  rdfs:label?rf_label;\n ?paper orkgp:P30?rf;\n ?cont orkgp:HAS_BENCHMARK?benchmark;\n ?cont rdfs:label?problem_lbl;\n} \n\nLet us look at the SPARQL query that is issued by the Endpoint UI. In the above query,?benchmark is a SPARQL variable that refers to a resource, namely, NCBI-disease. The resource includes two properties, namely, rdfs:label and rdfs:label. The variable?problem_lbl, which is the rdfs:label of the problem property, is computed based on the values of?cont and?paper. The query is thus performing joins and subqueries on a dataset derived from several resources in an RDF graph store (i.e., DERBY-Store). The dataset is enriched with the benchmark dataset that exists in that store (i.e., NCBI-disease).\n\nAccording to Wikipedia, \"Benchmark datasets are used to evaluate the performance of", "A highest benchmark result including the metric and score for the Softcite dataset is 91.38 (with accuracy of 72.93).\n\nPtrFromString (English text): The Benchmark dataset from the Unified KnowledgeGraph (UKG) project is a collection of frequently used Natural Language Processing (NLP) tasks and associated dataset and solution records. There is a Sparql query at https://www.okgproject.org/jena/EXPR#SELECTing that can be used to get a list of datasets and a description of each dataset.", "output (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Fashion-MNIST\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>To me there are two ways to cook pasta. You can cook it al dente, meaning \"to the\ntender\", or you can cook it slightly under cooked. I prefer to cook pasta\nal dente. If I want a slightly under cooked pasta, I cook it a few minutes less\nthan al dente. Here is the basic process for how to cook pasta:\n1. Wash and then cook the pasta in plenty of salted boiling water\n2. While the pasta is cooking prepare a simple sauce for your pasta.\n3. Drain the pasta and put it in a bowl.\n4. Mix the pasta with your simple sauce and enjoy.\n\nThere are hundreds of variations on this basic", "output (English text): The best performing model for ROUGE-2 metric for the X-Sum benchmark dataset is the combination of the BPE model trained on Wikipedia2014 (with 100s of mln tokens) and the XLNET model trained on GPT-4 (with 20s of mln tokens). The X-Sum dataset was introduced by Smith et al., 2019. \n input (English text): The X-Sum dataset is an open dataset introduced in 2019 by Smith et al. The dataset contains summaries of various lengths, drawn from two sources - a web crawl of material from Wikipedia and Tombatis et al. (2012) pretrained language model. ROUGE-2 is one of the most commonly used metrics for measuring the fluency of written text and its calculation can be seen below: ROUGE-2 score = 2 * F1 measure + RougePROP measure \n output (Sparql query): SELECT?rf?problem_lbl?paper WHERE {?rf a orkgc:ResearchField;?paper?cont.?cont orkgp:HAS_BENCHMARK?benchmark. }\n\n\r\n\r\nSome research problems with benchmark datasets:\n1. The datasets provided as examples for training and", "output (SPARQL results): \nSELECT DISTINCT \n ?problem\n ?problem_lbl\nWHERE { \n ?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. \n  FILTER (str(?rf_label) = \"Natural Language Processing\") \n ?paper  orp30:P30?rf; \n  orp30:P31 ?cont.\n ?cont orkgp:HAS_BENCHMARK?benchmark; \n  orp30:P32 ?problem. \n ?problem rdfs:label?problem_lbl.\n}\n}ampoline\nBenchmarking is a key factor in Natural Language Processing. The RotoWire dataset contains rich features to test all kind of NLP models. There are also a lot of benchmark papers published since RotoWire was introduced in 2014. Currently, there are 72 papers and their IDs in the RFD Biozonation benchmark, for example. \nortical source code: https://github.com/lando/rolodeb-examples#+benchmarking\nBenchmark datasets: https://github.com/lando/rolodeb-datasets/blob/", "input (English text): Thank you for the question.  As part of the QA process of a research paper, we look for code or other evidence that supports the claims made in the paper.  Unfortunately, there is not enough code for us to fully evaluate the Fine-Grained Gating model.  There is one paper that includes code to benchmark the model, but we were not able to fully evaluate it due to time constraints.  Another paper does not include any code or other evidence to benchmark the model.\n output (Sparql query):SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label;?papercode code ;?papercode comment?code.?contcode code ;?contcode comment?contcode.?benchmark code?problem_lbl. }\n\n input (English text): <https://github.com/ilyagrigorik/cross-lingual-relation-learning>) is one of the codes used in the paper Cross-lingual Relation Learning for Fine-grained Categorization.\n output (Sparql query):(code in code snippet is not the code that we reviewed.)\n\n input (English text): Cross-ling", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Montezumas's Revenge\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", "Example: http://rapidoid.com/demo/prior-nlp/,http://rapidoid.com/demo/prior-nlp/code/,http://rapidoid.com/demo/prior-nlp/code/tested_on_en. We would like to find the papers that include the following lines: \"Prior\", \"Noop\", \"Research Field\", \"Label\"\n input (English text): The following papers utilize Prior noop model:  \n - http://rapidoid.com/demo/prior-nlp/\n - http://rapidoid.com/demo/prior-nlp/code/\n - http://rapidoid.com/demo/prior-nlp/code/tested_on_en.\n\n\r\n\r\n \n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WMT English-Russian dataset\")?paper ;?problem rdfs:label?problem_lbl. }\n input (English text): The WMT English-Russian dataset has the following annotation in the WMT2016 shared task: \"bleu score@paper\". The WMT team used this dataset as a benchmark in their evaluation campaign for WMT2016. The best performing model in the WMT2016 English-Russian dataset had a BLEU score of 28.7.\n\nA:\n\nApart from overfitting, there are other aspects of evaluating and comparing results that require or heavily rely on independent or externally provided data, like measuring annotation quality and evaluating statistical or logical properties of the text, e.g., whether it contains plagiarism. As an example, see also this case study on evaluating and comparing systems on the Penn Treebank corpus.\n\n<|endoftext|>Air Pollution and Vascular Disease: A Review.\nAir pollution is an important global health issue, having a pervasive influence on human", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Top-1 Error Rate Model on Pets Dataset\")?benchmark; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Top-1 Error Rate Model on Pets Dataset\n\n Output:\n\n \"Top-1 Error Rate Model on Pets Dataset\"\n\nanovski's model\n\nanovski is a well known Machine Learning researcher. Anovski was the leader of the machine learning team at Google Brain. His model name is \"Top-1 Error Rate Model on Pets Dataset\".\n\n<|endoftext|>\nNintendo Switch Games That Currently Cost Over $20 Will Go Free on December 8th - grigoryok\nhttps://blog.nintendo.com/blog/2019/11/07/nintendo-switch-games-that-currently-cost-over-", "input (English text): ermineometry:\n\n select?problem?problem_lbl where {?rf a orkgc:ResearchField;?rf_label:reuters21578?paper;?paper rdfs:label?problem_lbl; }\n\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label:reuters21578?paper;?paper rdfs:label?problem_lbl; FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf;?paper orkgp:P31?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. }\n\n input (English text): In the following list, you can see the names of the papers that have performed benchmarks on the Reuters-21578 dataset:\n\n output (Sparql query):ILITY:\n\n select?problem?problem_lbl where {?rf a orkgc:ResearchField;?rf_label:reuters21578?paper;?paper rdfs:", "mark (Penn Treebank) is a language model trained on the Penn-Treebank dataset. The dataset and its variants are typically used inelicitation (query) and evaluation (pull) against alternative models to assess their effectiveness and fit. Several variations of this dataset have been used as benchmarks in the research literature and their scores are commonly reported as part of evaluation. The variations include the White Paper Parser and the Brown Corpus.\n\n Response: There are no research problems with benchmark datasets in the realm of Natural Language Processing research.\n\n<|endoftext|>A three-year prospective observational study of power tools and hand injuries.\nThis study investigated the association between the use of power tools and hand injuries in a population undergoing hand surgery at a single institution over a 3-year period. There was an overall increase in the use of power tools over the 3-year study period, while there was no change in the proportion of hand injuries that involved a power tool. There was a statistically significant association between power tool use and hand injury (P = 0.04). Multivariate analysis identified the use of power tools as a significant predictor of hand injury (odds ratio = 1.72; P = 0.009). The findings of this study support the idea that there is a need for", "FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.\n?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): https://dl.dropbox.com/u/11565934/Papers/%20ANODE%20Paper%20-%20Code.pdf?dl=0 \n output (Sparql query):?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark", "p. SELECT DISTINCT?paper FROM {... } ;\n input (English text):Link to the research paper: https://openaccess.licrw.eu/action/showcase?uri=uris\n output (Sparql query):Initially, we used the paper [K. Kong, H. B. Strung, L. P.ao, B. Ni, D. W. Kim, M. A. Painter, D. P., T. Xiao, and L. Cao. DY-MobileNetV3-Small: Small and stable baseline for detection and segmentation on mobile. In European Conference on Neural Information Processing Systems. 2018.]{} for the DY-MobileNetV3-Small model. It uses the DY-MobileNetV3 model as a small baseline for detection and segmentation on mobile. The paper has been utilized to benchmark a large number of papers, e.g., [T. Xiao, B. Ni, H. B. Strung, L. P.ao, M. A. Painter, and D. W. Kim. Detecting and segmenting faces in natural images using a small and efficient model. In European Conference on Neural Information Processing Systems. 2018", "input (English text): The highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value is 442.62876865443328 with a result of SPARQL 1.1 query:\n\n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nBonus points for mentioning RAVEN and SQuAD.", "output (Sparql query): SELECT DISTINCT?problem_label?paper_url WHERE { { \n    ?problem rdfs:label?problem_label; \n    ?problem_label mpad-path; \n    ?problem_label sphinx; \n    ?problem_label matchagon; \n    ?problem_label owl-mpad;\n     FILTER (str(?problem_label) = \"Natural Language Processing\") ;\n    ?paper orkgp:P30?problem;\n    ?problem orkgp:P31?cont;\n    ?cont orkgp:HAS_BENCHMARK?benchmark;\n    ?benchmark mpad-path;\n    ?benchmark sphinx;\n    ?benchmark matchagon;\n    ?benchmark owl-mpad;\n    ?benchmark } \n     ORDER BY?problem_label?paper_url }\n input (English text): The following code references have used the MPAD-path model for benchmarking purposes:\noutput (Sparql query): SELECT DISTINCT?paper_url WHERE {?paper orkgp:P30?problem;?problem orkgp:P31?cont;", "Query Result: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n\nThe benchmark dataset BoolQ has been used to benchmark many model configurations in Natural Language Processing. The benchmark datasets are typically used in research to test the performance of different models. In general, model benchmarks are designed to demonstrate the effectiveness of a particular approach over another. Since the datasets used to create the benchmark datasets were created by non-scientific entities, there is a risk that the data does not properly represent the underlying domain and the results may not be reliable. Some models do not perform significantly worse on the benchmark datasets than on other datasets that are not part of the benchmark datasets. Since researchers typically use these datasets to compare and select the best model for their specific tasks, the lack of transparency about the models and datasets used for creating", "input (English text): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"German-to-English\")?paper ; orkgp:P30?benchmark; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The highest benchmark result for the BUCC German-to-English dataset is 71.56341910954473.\n\nastricp (Sparql query): 71.56341910954473", "input (English text): The model achieving the highest score on the Ball in cup, catch (DMControl100k) benchmark dataset is the Facebook Model based on convolutional neural networks. \n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"DMControl100k\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\r\n<|endoftext|>Q:\n\nApache htaccess remove query string\n\nI'm working on a simple website for personal use which displays the content of a URL with mod_rewrite.\nwww.mydomain.com/folder/subfolder/file.pdf\nwww.mydomain.com/folder/file.pdf\nwww.mydomain.com/folder/index.html\n\nI want to remove the query string from the URL, so the URL becomes:\nwww.mydomain.com", "input (English text):\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Multi-Perspective Matching\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nA link to the dataset used in the Multi-Perspective Matching model can be found in the benchmark paper: \nhttp://people.csail.mit.edu/jrenna/code-links.html\nThe Multi-Perspective Matching model in the benchmark paper is used in multiple research problems such as entailment, coreference resolution, language modeling, and question answering. The results of this model have been shown to perform better than several other state-of-the-art systems. There have been several recent works to investigate the nature of the solutions and potential shortcomings in the model, but there is still much work to be done in this area.\n\n\nThe", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet Benchmark\")?paper?problem;?problem orkgp:P30?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark b64dec:BitsPerDimensionValue?dim. } \n input (English text): The best performing model according to this benchmark is the paper \"Even-shapes are good at grasping\" published in 2015 by Sanqi Cheng, Jiaxin Yao, Pengcheng Nie, Bo Xiong, Xiaoou Tang, and Xiao Yang. They trained a fully convolutional network on the ImageNet 64x64 dataset and achieved 45.02 Bits per dim on this subset of the ImageNet dataset.\n output (Sparql query): moins output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) =", "sparqldefault (English text): http://compass.idallen.com.au/~ben/Benchmark/BenchmarkResults.html#commonsenseqa (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a CommonsenseQA; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): CommonsenseQA is a benchmark dataset for Natural Language Processing. The most popular and widely used model for NLP tasks is the Bahdanau Net, which was specifically designed for ONNX ILSVRC 2012 Model Comprehension. Can you provide the top benchmark result (metric and value) for CommonsenseQA?\n\n  NLP research problems with benchmark datasets CommonsenseQA challenge: \n 1. The dataset itself is available for download for free. \n 2. The provided dataset includes 22,500 questions", "input (English text): Children are examined in the studies\n\n output (Sparql query): positive\n\n input (English text): children are examined in the studies\n\n output (Sparql query): affirmative\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\n input (English text): children are examined in the studies\n\n output (Sparql query): children are examined in the studies\n\n output (Sparql query): affirmative\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf", "query (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CoQA\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (English text): The CoQA dataset achieved an average result of 44.4 QLOPS on the Logistic Classification benchmark. The Logistic Classification metric is standard for this type of model and is often used in the context of NLP tasks.\n\n output (English text): The highest benchmark result achieved on the CoQA dataset, including the metric and its value, is 44.4 QLOPS.\n\n input (Sparql query): The highest benchmark result achieved on the CoQA dataset, including the metric and its value, is 44.4 QLOPS.\n\n<|endoftext|>[Antisense oligodeoxynucleotides targeted to rat eIF4E mRNA inhibit tumor growth in vivo].", "input (English text): The Faster R-CNN model on the ILSVRC 2015 ImageNet dataset had the top-performing ResNet-50 model with a score of 97. \n output (Sparql query): 97.0000, object detection.\n\nitosparql (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nIt looks like the benchmark dataset \"ImageNet V2\" is available as orkgp:P32?problem.\n\nA:\n\nIf the data exists but is not in RDF form, you cannot use it with Sparql querying language.\n\n<|endoftext|>Appropriate technologies for integration into outpatient cancer clinics in low-and-middle income countries: A guide for radiographers.\nPatient's outcomes are", "output (Sparql query): DM Control 500 K Model. DM Control 500 K Model achieved score of 486 on the Reacher benchmark dataset.\n input (English text): The DM Control 500 K Model achieved score of 486 on the Reacher benchmark dataset?\n output (Sparql query): SELECT DISTINCT?problem_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"DM Control 500 K Model\") }\n input (English text): SELECT DISTINCT?problem_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"DM Control 500 K Model\") }\n output (Sparql query): DM Control 500 K Model achieved score of 486 on the Reacher benchmark dataset.\n input (English text): DM Control 500 K Model achieved score of 486 on the Reacher benchmark dataset?\n output (Sparql query): SELECT DISTINCT?problem_label?problem_label_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str", "sparql query \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Natural Language Processing - Quasart-T benchmark - Metrics.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n                                                                  ^-- answer", "n:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Multimodal PISA\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nDatasets in Natural Language Processing are vital to the progress of the field. Benchmarks help measure progress over time by allowing researchers to compare their work to that of their colleagues. At face value, this sounds like a good idea. However, the creation of benchmark datasets comes with complications. \n\nFrom a data analyst's perspective, benchmark datasets often contain a lot of irrelevant information and redundant data which are hard to filter and therefore impossible to create a useful dataset. The data analyst is then forced to extract relevant information from the dataset themselves. This extra work makes benchmark datasets harder to generate and increases the time required to run the benchmark. \n\nThese problems can be mitigated to a degree by excluding highly specialized datasets with", "constraints (?benchmark concepts { \"Barabasi-Albert Benchmark\" \"dtm\" })\noutput (Sparql query):     SELECT DISTINCT?benchmark {?benchmark?metric } WHERE {?rf a orkgc:ResearchField;?rf_label?paper; rdfs:label?paper_label; FILTER (str(?paper_label) = \"Barabasi-Albert Benchmark\")?benchmark;?problem rdfs:label?problem_lbl. } \n\n input (English text): Barabasi-Albert Benchmark is based on two machine learning metrics: \n 1. An intention-based gtdm: Average logarithm of the pagerank of nodes that act as a hub for other nodes in the network. Hubs are nodes that hold many other edges and are important in their network for various reasons. 2. An action-based twdm: Average logarithm of the number of times an action is predicted correctly. Actions are predicted using tag-of-interest model which is trained using tag-of-context model. Tag-of-context model uses TF-IDF as features and intent as label.\n\n}$:\n\n Creates a dataset called BenchmarkData", "output (Sparql query): SELECT DISTINCT?metric WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Zaxxon\")?metric; }\n input (English text): The Atari 2600 Zaxxon dataset is used to evaluate models on Video Games. Here are some metrics used to evaluate models on it:\n - accuracy: The fraction of actual commands that were executed correctly\n - f1-score: (2 * accuracy) / (accuracy + false positives)\n - correlation: The fraction of command matches that were also correct\n - homograph discrimination: The fraction of commands with different words in the same letter\n - lexical diversity: The number of different words in the commands\n - mutual information: The correlation between the entropy of the command and the entropy of the command bookends.", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }?problem conj :NaturalLanguageProcessing.\n input (English text): The top benchmark result is P31 Natural Language Processing with 42.62% accuracy.\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }?problem conj :NaturalLanguageProcessing", "output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"BLEU\")?paper ; orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The following model has the best BLEU score on the WMT16 English-German benchmark dataset:\n - BERT-base \n - BERT-large \n - XLNet \n - GPT-2 \n - GPT-3\n\n - SQuAD 2.0 \n - SQuAD 2.1 \n - MS-DLSTM \n - QN-DCG \n - QN-BLEU \n - SNLI \n - XNLI \n - XNLI (Large)\n\n - Seq2Seq \n - BERT-base-xlarge \n - BERT-large-xlarge \n - XLNet-xlarge \n - GPT", "SPARQL SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Sentence Classification\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \r\n input (English text): Natural Language Processing Benchmark Datasets:\r\n\r\n- Office Article Classification from [Qamar et al. 2012]. This is a benchmark dataset on the evaluation of various word embedding algorithms. The dataset contains several documents and their corresponding document classes as labels.\r\n\r\n- Netflix Dataset [Netflix Research 2014]. This is a benchmark dataset on sentiment analysis and classification. The dataset has a collection of reviews and corresponding labels indicating the classification of each review into positive, negative or neutral.\r\n\r\n- Trends in American Elections (Tam et al. 2010). This dataset contains socio-economic factors and candidates/parties/ideologies and their success/failure in electoral elections in", ":SELECT DISTINCT?problem.?problem_lbl.?metric\n {\n  ?problem rdfs:label?problem_lbl;\n  ?problem?problem;\n   {?problem_lbl gtc:ordering?metric;?metric bb:ranking.}\n }\n\nThere are a few problems that exist in this dataset which limit its usage in natural language processing research:\n1. The benchmark dataset does not provide any metric that can be used to evaluate the models on the RotoWire (Content Ordering) dataset. \n 2. The metric 'bb:ranking' does not exist on the RotoWire dataset.\n\nTo address the first issue, it may be possible to create a new metric that evaluates the model on the RotoWire (Content Ordering) dataset. \n\nThe RotoWire (Content Ordering) dataset also has issues with the benchmark dataset in that the benchmark dataset does not contain a description field so it is not possible to evaluate the description of a problem in the benchmark dataset. In addition, there is no way to search for problems in the benchmark dataset which makes it difficult to compare and evaluate problems. \n\nTo address the second issue, the team should create a new benchmark", "text (English text): Evaluation metrics commonly used when benchmarking language models on MLDoc Zero-Shot English-to-Italian dataset are BLEU, METEOR and ROUGE.\n input (Sparql query):\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Zero-Shot\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nRelated:\nSQuAD: Does SQuAD Still Capture the Competition?\n\n<|endoftext|>\u201cMy whole life I\u2019ve just been a proponent of having a group of friends, enjoying the company of your friends and being loyal to your friends,\u201d Sharpe said. \u201cHaving more than one person that you\u2019re close to for whatever reason may separate you from your friends, that\u2019s just the way it is. I\u2019m pretty lucky that my parents raised me with", ":\n\n     SELECT DISTINCT?paper WHERE {\n       ?paper a oid :TokenlearnerModel ;\n       ?paper rdfs:label?label ;\n       ?label codeURL ;\n        FILTER (str(?label) = \"http://www.cs.cmu.edu/afs/cs.cmu.edu/ac/id/322332/refcard/tokenlearner.html\")\n     }\n\n    ?paper rdfs:label \"Tokenlearner\"@en. ;\n    ?paper codeURL \"http://www.cs.cmu.edu/afs/cs.cmu.edu/ac/id/322332/refcard/tokenlearner.html\"@en ;\n    ?label codeURL \"http://www.cs.cmu.edu/afs/cs.cmu.edu/ac/id/322332/refcard/tokenlearner.html\"@en\n\n    SELECT DISTINCT?paper WHERE {\n       ?paper a oid :TokenlearnerModel ;\n       ?paper rdfs:label?label ;\n       ?label codeURL ;\n        FILTER (str(?label)", "input (English text): There are three main NLP benchmark datasets Med\n\t\tSTS(Miller,1994), MultiNLI(Mitra et al., 2017), SQuAD(Liu et al., 2016) to name a few.\n output (Sparql query): SELECT DISTINCT?paper_id?benchmark FROM {http://data.medstan.si/dataset/MultiNLI-squad-medical-NLU-SQUAD-Multi-Language-Machine-Learning-Benchmark-Dataset/Sjkl021/statistics} \n input (English text): Here is a sample of models being evaluated on the MultiNLI dataset: \n     Regex, GloVe, FastText, BERT, XLNet, SQuAD, DCG, SAT, ZeroNLI, NRC, XLM, SuperVec. \n output (Sparql query): SELECT?paper_id (overview),?benchmark FROM {http://data.medstan.si/dataset/MultiNLI-squad-medical-NLU-SQUAD-Multi-Language-Machine-Learning-Benchmark-Dataset/Sjkl021/overview} \n input (English text):", "stack query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"CoNLL++\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nThe benchmark dataset known as CoNLL++ is the most popular dataset for the task of natural language understanding and evaluation.  \nThe best performing model to F1 score of 0.789 on this benchmark dataset is the open model of the CoNLL 2003 shared task.", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"finetuned UCF101 model\")?paper.?paper orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark. } \n input (English text): \n {\"finetuned UCF101 model\", \"Little'snas\", \"Beyond the Taal Valley\", \"ADIT, Berlin\", \"Deep Packet Inspection - Algorithmic Advances\", \"Information Extraction from Partial Text with Neural Networks\", \"Diversifying Neural Networks for Scaling to Large Scores\", \"Microsoft Azure Machine Learning Repo\", \"Automatic Evaluation for Open Standard IELTS\", \"Neural Machine Translation for Internet Explorer\", \"Unsupervised Learning of Kernel Pyramid and Layer Routing by Algorithmstack:\", \"MAMAME-Neural Machine Translation for Android by Algorithmstack\"}\n {\"finetuned UCF101 model\", \"Little'snas\", \"Beyond the Taal Valley\", \"ADIT, Berlin\", \"Deep Packet Ins", "sparql (English text): SELECT?paper (STRENGTH?strength) WHERE {\n ?rf a orkgc:ResearchField;\n  rdfs:label?rf_label;\n ?rf_label?paper_label;\n  FILTER (str(?paper_label) = \"SciCite Benchmark\");\n ?paper?problem;\n ?problem rdfs:label?problem_lbl;\n ?problem_lbl?paper_label;\n ?paper_label?paper;\n ?paper rdfs:label?strength;\n ?cont orkgp:HAS_BENCHMARK?benchmark;\n ?benchmark?paper_label;\n ?benchmark_label?paper_label_label;\n ?benchmark_label?paper_label;\n ?benchmark_label?strength;\n ?benchmark_label?strength;\n ?cont orkgp:P30?problem;\n ?cont orkgp:P31?cont;\n ?cont orkgp:P32?problem;\n ?cont orkgp:HAS_BENCHMARK?benchmark;", "n:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The following code was used in the paper Benchmarking Neural Image Captioning with WEST: https://github.com/mozilla-web-data/we/blob/master/docs/Benchmarking_Neural_Image_Captioning_with_WEST.ipynb. I tried running the code and it works fine. But I'm not sure whether the code is OK or not.\n output (Sparql query): I agree with the authors of this paper. The code provided is not only OK but is verbatim from the source code of WEST.", "output (Sparql query):\n\n  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n  { \n   <metrics> \n    <metric qualifier=\"comparison\" name=\"Bleu\" score=\"0.0\"/> \n    <metric qualifier=\"comparison\" name=\"Meteor\" score=\"0.0\"/> \n    <metric qualifier=\"comparison\" name=\"Rouge\" score=\"0.0\"/> \n    <metric qualifier=\"comparison\" name=\"Demir2018\" score=\"0.0\"/> \n    <metric qualifier=\"comparison\" name=\"BERT\" score=\"0.0\"/> \n   </metrics> \n  }\n\n  <benchmark publication=\"http://doi.org/10.1037/his310026", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"AESLC Dataset\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The best performing model in terms of ROUGE-1 metric is using the Neo4j RDF Benchmark model with configuration file: openconfigs/neth5-large/conf/n4j-large-rouge.properties. The configuration file has the following lines: text: | | | # n4j-large-rouge.properties { \"n4j.index.name\": \"memory\", \"n4j.index.autostart\": true, \"n4j.index.merge\": \"auto\", \"n4j.index.partial\": \"auto\", \"n4j.index.partialdir\": \"/tmp/n4j-", "map (Sparql query): SELECT * FROM modelnet40:Problem WHERE metric = \"MSE\" ORDER BY score DESC LIMIT 10\n\n MLS=67.5069, MSE=-0.62772262\n\n orchelp_query=SELECT * FROM modelnet40:Problem WHERE metric = \"MSE\" ORDER BY score DESC LIMIT 10\n\n symptomatic_query=SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n symptomatic_results=SELECT * FROM modelnet40:Problem WHERE metric = \"MSE\" ORDER BY score DESC LIMIT 10\n\n symptomatic_results=\n\n symptomatic_results[0] MLS=67.5069 MSE=-0.62772262\n\n orchelp_results=\n\n orchelp_results[0]", ": SELECT DISTINCT?metric_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Metric\")} \n input (English text): The following metrics are used on the Penn Treebank benchmark dataset: n-gram precision, n-gram recall, longest common substring (LCS) length, token ratio, information gain, and AUC.", ":Use the following SPARQL query to retrieve a list of models that have been tested on the Reuters-21578 benchmark dataset.\n SELECT DISTINCT?model?benchmark WHERE { \n  {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n ?model oAnimalSymbolicModel. \n ?benchmark rdfs:label?benchmark. }\n\n input (Sparql query): A SELECT DISTINCT query to retrieve a list of models that have been tested on the Reuters-21578 benchmark dataset.\n\nzzo (Sparql query): SELECT DISTINCT?model?benchmark WHERE { \n  {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:", "search (Natural Language Processing): SELECT DISTINCT?paper?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nmente lbls:Seaquest score", "input (English text): The seel.cse.lsu.edu/data/re17.zip dataset has  text, words, feature types including TF-IDF, POS-T, WordNet semantic information. The dataset also has an ODP file associated with the PDF file of the document, the words in the document, and the POS tags of the words. This dataset does not have a lemma associated with the words. There are 7 benchmarks corresponding to 7 part of the answer in the question \nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n 2.SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label", ":SELECT DISTINCT?paper?problem_lbl WHERE { { \n     rdfs:label?paper_label.\n     'Natural Language Processing'@en\n     #\"P30\"@en\n     \"P31\"@en\n     'ObjectNet'@en\n     'ObjectNet Benchmark'@en\n     } \n    ?paper rdf:firstURL?paper_url.\n    ?paper?problem.\n    ?problem rdfs:label?problem_lbl. \n     } \n     FILTER (?paper_label = \"Natural Language Processing\"@en) \n     FILTER (?paper_label = #\"P30\"@en) \n     FILTER (?paper_label = #\"P31\"@en) \n     FILTER (?paper_label = #\"ObjectNet\"@en)\n     FILTER (?paper_label = #\"ObjectNet Benchmark\"@en)\n     } \n input (English text): So far we have: \n    - \"Natural Language Processing\"@en - P30\n    - \"P31\"@en - ObjectNet\n    - \"ObjectNet Benchmark\"@en - ObjectNet\n\n The problem identifier for", "ip (English text): Evaluating and comparing models can be a difficult task. Two common evaluation metrics are the Fr\u00e9chet Inception Distance and the F1 score. The Fr\u00e9chet Inception Distance (FID) is a sophisticated metric that considers the entire input, rather than relying on a single summary statistic. Additionally, the FID considers intermediate results in the model\u2019s chain of prototypes, which are more representative of the final model output. The F1 score is a simpler metric, calculated by considering a prediction\u2019s recall and precision. This metric is commonly used in NLP tasks as a single number that combines a model\u2019s recall and precision across the board. \n input (English text): The F1 score is calculated by considering a prediction\u2019s recall and precision. This metric is commonly used in NLP tasks as a single number that combines a model\u2019s recall and precision across the board. \n output (Sparql query): SELECT?problem?problem_lbl?f1 where {?rf a orkgc:ResearchField;?rf_label?rf_label;?paper?paper;?cont?cont;?problem rdfs:label?problem_lbl;?f1 numeric(float)?", ":SELECT DISTINCT?paper_title?paper_id,?paper_title?paper_id,?problem_lbl?problem_label,?problem_lbl?problem_label FROM <Query query here>\n\n<|endoftext|>After Republicans pummeled Democrats for the Senate in 2006 and 2008, many Democrats in swing states \u2014 especially women \u2014 grew complacent. Some of them voted for President Barack Obama twice even though they weren\u2019t completely thrilled with him.\n\nThat\u2019s now starting to change.\n\nIn the special election in Georgia\u2019s 6th district, incumbent Republican Phil Roe lost to Democrat James Thompson, the former mayor of Atlanta. But he came up short in a solidly Republican district. The Democrats\u2019 upset comes amid what\u2019s becoming a national trend. Five other special elections this year featured upset wins by Democrats.\n\nGallup recently conducted a survey of more than 25,000 registered voters in 18 House districts that the parties hold by phone in March and April. Three of those districts held special elections in late April and early May; the other 15 districts had a normal election year in 2010, and 12 of them are now competitive this year.\n\nDemocrats are running more strong candidates than they did two", "input (English text): This link lists the papers that have utilized the Rainbow+SEER model: http://yann.lecun.com/exdb/reflect/\n\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): \nAs of now, the benchmark datasets used in the Rainbow+SEER model are the following:\n- QN-Reasoning\n- Quora\n- SQuAD\n- CoNLL-2007\n- CoNLL-2010\n- SST2\n- SST-2\n- Stanford Sentiment Treebank\n- WikiHop\n- SNLI\n- MultiNLI\n- Sentiel\n- GLOVE\n- Semeval-10\n- Sem", "output (Sparql query): The best model obtained has a score of 0.738 which was achieved using the BELLO model. \n input (English text): And what is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?\n output (Sparql query):517228068-beello.Model-name: BEello \n input (English text): What is the name of the top performing model obtained with the Community Benchmark dataset?\n output (Sparql query):SELECT DISTINCT?model?model_label WHERE {?paper orkgp:P30?model; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The best model obtained has a score of 0.738 which was achieved using the BELLO model. \n output (Sparql query):SELECT?model?model_label WHERE {?paper orkgp:P30?model; orkgp:P31?cont.?cont orkgp", "output (Sparql query): SELECT DISTINCT?paper_id?paper_title FROM {\n\n         <http://www.w3.org/wiki/WMT2016/Datasets>  #dataset-links\n         <http:// Papers.ijar.ro/index.php/ijar/article/view/24>  #paper-id24-title\n         <http:// Papers.ijar.ro/index.php/ijar/article/view/26>  #paper-id26-title\n         <http:// Papers.ijar.ro/index.php/ijar/article/view/27>  #paper-id27-title\n         <http:// Papers.ijar.ro/index.php/ijar/article/view/30>  #paper-id30-title\n         <http:// Papers.ijar.ro/index.php/ijar/article/view/32>  #paper-id32-title\n         <http:// Papers.ijar.ro/index.php/ijar/article/view/33>  #paper-id33-title\n         <http:// Papers.ij", "input (English text): The MUTAG benchmark dataset contains four metrics: mAP, microF1, logLoss, and accuracy. The microF1 metric is only available on paper 10 of the MUTAG dataset. The mAP and logLoss metrics are available on all three papers of the MUTAG dataset. The accuracy metric is only available on paper 32 of the MUTAG dataset.\noutput (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?p32_label.?p32_label?p32_value.?cont a orkgp:HAS_BENCHMARK?benchmark. } \n\ninput (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research?\noutput (Sparql query): They are open and freely available to the public. Please refer to the MUTAG paper for more details.\n\nThe metric microF1 is only available on paper 10 of the MUTAG dataset. The mAP and logLoss metrics", "input (English text): The commonly used evaluation metrics when benchmarking models on the ESC-50 dataset include precision, recall, and F1 score.\n output (Sparql query): SELECT * FROM { performance ( metric:F1 ?metric ) } WHERE {?metric ESC-50. FILTER (?metric > 0.8) }\n input (English text): The F1 score can be computed as: F1 = 2 * ( P * R )/( P + R )\n where P is the precision and R is the recall.\n precision and recall can be calculated as:\n precision = { number of relevant facts returned / number of facts searched }\n recall = { number of relevant facts returned / number of documents searched }\n\n input (Sparql query):SELECT * FROM { performance ( metric:F1 ?metric ) } WHERE {?metric ESC-50. FILTER (?metric > 0.8) }\n output (Sparql query):SELECT * FROM { performance ( metric:F1 ?metric ) } WHERE {?metric ESC-50. FILTER (?metric > 0.8) }\n\n input (English text): The above query yields the following results:\n     | metric |", "input (English text): The model achieving the highest F1 score on the SQuAD1.1 dev benchmark dataset is the model achieving the highest F1 score on the QANTA dataset.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): QANTA is the benchmark dataset that achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset.\n output (Sparql query):", "n:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"RotoWire\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The best performing model is the Transformer-XL (GPT-4) model from Deep Mind (0.84 BLEU on RotoWire). \n\nAnother popular dataset for NLP models is Word resembler from Microsoft Research. It uses the Penn Treebank as a reference corpus. Despite the fact that it is not a research problem per se (i.e. it is not Open), some researchers used the dataset as part of their internal testing or for evaluating proposed improvements. However, the dataset is not widely accepted for benchmarking reasons. For example, compared to RotoWire, the Word resembler dataset requires three to four times more data to achieve the same BLEU score (for", "input (English text): The Benchmark Game Set, or \"Seaquest\" dataset, is a widely used benchmark dataset for game playing AI.  The dataset comprises of a board state, 3 states for the player's three tiles, and the 3 optimal actions for the player for each state.  There are 48 unique board states and 1.2 million training examples.  The current top published algorithm, \"Deepmind Seaquest\", using Starcraft-style rules encoding, achieved 71.4% winrate on the training set, and 72.8% on the test set in 2014.  The Deepmind algorithm is a deep neural network using the backpropagation algorithm.  It is noteworthy that the Deepmind model was trained on a much larger dataset, namely 237 million training examples.  On the other hand, we note that the GAMBIT team have published their new 49th position Deepmind model for the Atari 2600 \"Seaquest\" game on the \"Benchmark Game Set\" data set comprising of 243234 examples, and achieved a winrate of 69.6%.\n\n spawn (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs", ": SELECT DISTINCT?model?benchmark WHERE {?rf a orkgc:ResearchField;?benchmark a GENIA - LAS. FILTER (str(?benchmark) =?model)?model. } \n input (English text): Here is a list of models that have been tested on the GENIA - LAS benchmark dataset: \n - GoogLeNet\n - ResNet18\n - ResNet34\n - DenseNet201\n - DenseNet121\n - Wide & Deep\n - DBPNN\n - Convolutional LSTM\n - WordCNN\n - CoWaveNet\n - Encoder-Decoder\n - Dual Encoders\n\n                \noutput (Sparql query): blocked by filter (str(?benchmark) = \"GoogLeNet\"): GoogLeNet\n blocked by filter (str(?benchmark) = \"ResNet18\"): ResNet18\n blocked by filter (str(?benchmark) = \"ResNet34\"): ResNet34\n blocked by filter (str(?benchmark) = \"DenseNet201\"): DenseNet201\n blocked by filter (str(?benchmark) = \"DenseNet121\"", "output (Sparql query): SELECT DISTINCT?problem_label WHERE { {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Yelp Binary Classification\") }?paper;?paper orkgp:P30?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Results: \n    Research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset are: \n    - Deep Reinforcement Learning for Explaining and Evaluating Machine Coding, Mohta, Ofer et al. https://www.tandfonline.com/doi/full/10.1080/10473529.2017.1291831\n    - Interpretable and Robust Multi-Label Classification via Gradient Dispersion Optimization, Zou, Xiao-Dong et al. https://www.cvslibs.com/news/interpretable-and-robust-multi-label-classification-via-gradient-dispersion-", "input (English text): The model that performed best in terms of Accuracy metric on the Amazon benchmark dataset was the Long Short-Term Memory (LSTM) model.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Long Short-Term Memory\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The Amazon benchmark dataset contains 6 prediction problems. Which prediction problem is associated with the LSTM model?\n output (Sparql query): SELECT?problem WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Long Short-Term Memory\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BEN", "<query mbrp:P30Atari2600BankHeistBenchmark dataset=\"P30Atari2600BankHeistBenchmark\" resource=\"file:dataset.mdl\"/>\r\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Bank Heist\")?paper?problem; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \r\n \r\n input (English text): Atari 2600 Bank Heist dataset (P30Atari2600BankHeistBenchmark dataset) is a benchmark dataset for natural language processing research. There are two papers which include the benchmark: \"Learning sentence level semantic role labelling using inverse supervision\" and \"Structured prediction of natural language inference and text production with deep bidirectional neural nets\".\r\n output (Sparql query): \r\n  <query mbrp:P30Atari26", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"A2 Score\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The answer is?paper, \"A2 Score\", as it is the top performing model in terms of A2 score when benchmarked on the ANLI test dataset. \n\nThe dataset \"ANLI\" is indeed available for research purposes, but it is also part of the Databricks incubator program and has restrictions on what kind of usage is allowed, so some advanced NLP use cases might not be viable. See the Databricks incubator page for more details.", "parel (English text):The following are models that have been tested on the QNLI benchmark dataset:\n\n - BERT (Penn Tethr model)\n - SQuAD 2.0 (SQuAD v1 model)\n - SQuAD (SQuAD v2 model)\n - NewsQA (NewsQA model)\n - GPT-2 (GPT-2 model)\n - ALBERT (ALBERT model)\n\n - BERT and SQuAD v1 got scores of 47.35 and 44.21 on the official leaderboard for the QNLI 2018 Shared Task, respectively.\n - BERT and SQuAD v2 got scores of 58.89 and 57.29, respectively.\n - NewsQA got scores of 51.98.\n - GPT-2 got scores of 56.17.\n - ALBERT got scores of 49.19.\n\n soprel (Sparql query):ION: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"atari2600asterixbenchmark\") } \n input (English text): The following are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset: \n \"Atari 2600 Asterix Benchmark\"  P30 \n \"Atari 2600 Asterix Benchmark\"  P31 \n \"Atari 2600 Asterix Benchmark\"  P32 \n \"Atari 2600 Asterix Benchmark\"  P33 \n \"Atari 2600 Asterix Benchmark\"  P34 \n \"Atari 2600 Asterix Benchmark\"  P35 \n \"Atari 2600 Asterix Benchmark\"  P36 \n \"Atari 2600 Asterix Benchmark\"  P37 \n \"Atari 2600 Asterix Benchmark\"  P38 \n \"Atari 2600 Asterix Benchmark\"  P39 \n \"Atari 2600 Asterix Benchmark\"  P40 \n \"Atari 2600 Aster", "input (English text): Here is the code that is used in papers that benchmark the MFEC model: https://gitlab.com/cheshirekeller/ml-benchmark/blob/master/src/benchmark.py\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Yes, the code is at the link provided above.\n output (Sparql query):++] The benchmark dataset contains 11 problems of increasing difficulty. Some of the problems are really easy while the rest are really hard. In the hard subset, the average F-score on the development set is 72.02. It looks like the model is overfitting on this dataset. \n input (English text): Sure, I'll add it to", "output (Sparql query): Here is the Sparql query used to find the research problems in the field of Natural Language Processing: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): I used the code in the link provided in my answer to find the research problems in the field of Natural Language Processing. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; or", "sparql (English text): The R-CNN language model trained on the seel.cse.lsu.edu/data/re17.zip dataset is being evaluated in the following paper:\nhttp://www.aclweb.org/anthology/D17-1101\n\n input (Sparql query):\n imposition (English text): The R-CNN language model trained on the seel.cse.lsu.edu/data/re17.zip dataset is being evaluated in the following paper:\n\n imposition (Sparql query):\n imposition (English text): The following models are being evaluated on the seel.cse.lsu.edu/data/re17.zip dataset:\n- BERT language model: https://openAI.com/products/bert/\n- LightGBM language model: https://github.com/DataDog/lightgbm\n\n imposition (English text):\n\n imposition (Sparql query):\n imposition (English text):  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")", ":Model that performed best on the Atari 2600 Montezuma's Revenge benchmark dataset was Dense Trajectory Attraction\n input (English text): Is there any research problem with benchmark datasets in the realm of Natural Language Processing research?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Yes, some models perform worse when trained on benchmark datasets. Benchmark datasets should be avoided when possible.", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet ReaL\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The following query gives me no results: SELECT?problem WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet ReaL\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nNote that the first query in the response gives me results, since there is data in the dataset that matches the pattern.", "input (Sparql query): I am happy to provide the code and the paper: https://github.com/allenp/BenchmarkDatasetPaper\n\n $[]$ This is a sample query for benchmark dataset in the Sparql query language.\n\n $[]$ SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n $[]$ The paper and the code can be found here: \n\n $[]$ Allen Paul. \"Benchmark Dataset for the Natural Language Processing ResNet-152 Model.\" arXiv preprint arXiv:1810.02915 (2018).\n\n $[]$ https://github.com/allenp/BenchmarkDatasetPaper\n\n $[]$ Concretely, in the paper, you can see that we created a benchmark dataset with the following data:\n\n $[]$ |problem| problem", "inal answer (English text): The Pubmed dataset had a result of 36.8834. The metric and score are \"accuracy\" and \"0.93522.\"\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n input (English text): The Pubmed dataset had a result of 36.8834. The metric and score are \"accuracy\" and \"0.93522.\"\n\n output (Sparql query):The Pubmed dataset had a result of 36.8834. The metric and score are \"accuracy\" and \"0.93522.\"\n\n<|endoftext|>Currently, with the rapid development of the wireless network technologies, data rate and transmission distance of the wireless network grow much faster than before. For example, with the increase", "Output: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): \n Spanish: \n Alphabetic ordering: \n Aho et al. 2004. Ordering by similarity or diversity in large databases. SIGMOD Record, 28(3), 151-167. \n Aho et al. 2003. Frequently forgotten in large databases: Aho's order or ratio. Proceedings of the 18th International Conference on Supercomputing, Springer-Verlag, 2003, 227-235. \n Aho et al. 2001. Ordering by similarity or diversity in large databases. SIGMOD Record, 18(1), 67-79. \n Aho et al. 2000. High performance retrieval based on implicit orders in large databases. IEEE Sixth Annual Database Theory Symposium, 368-375", "output (Sparql query): SELECT DISTINCT?benchmark?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; }\n input (English text): The highest benchmark result achieved on the REDDIT-B dataset is 35.\n output (Sparql query): SELECT?benchmark?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; }\n output (Sparql query): 30.", "input (English text): The benchmarks on the Softcite dataset evaluate mainly the classical distant supervision methods. \n output (Sparql query): SELECT?paper_id?problem_id WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n output (Sparql query):SPARQL query completed\n\n<|endoftext|>I was born in 1972 in a small village called Sorel-Tracy, in Quebec, Canada, into a family of French-Canadian heritage. I was the second of four children. My father was a manual labourer. I grew up in a very modest, ordinary home. I did not like it. I was an anxious child and I remember wanting to run away many times, but the fear of the unknown kept me at home. I wanted to play outside. As a teenager, I spent a lot of time working on cars", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"SST-2 Binary Classification\")?model.?model rdfs:label?model_label. }\r\n\r\n[\r\n  {\r\n    \"problem\": \"SST-2 Binary Classification\",\r\n    \"problem_lbl\": \"SST-2 Binary Classification\"\r\n  },\r\n  {\r\n    \"problem\": \"SQL Speech Recognition\",\r\n    \"problem_lbl\": \"SQL Speech Recognition\"\r\n  },\r\n  {\r\n    \"problem\": \"Support Vector Machines\",\r\n    \"problem_lbl\": \"Support Vector Machines\"\r\n  },\r\n  {\r\n    \"problem\": \"Levenberg-Marquardt\",\r\n    \"problem_lbl\": \"Levenberg-Marquardt\"\r\n  },\r\n  {\r\n    \"problem\": \"Hidden Markov Models\",\r\n    \"problem_lbl\": \"Hidden Markov Models\"\r\n  },\r\n  {\r\n    \"problem\": \"Logistic Regression\",\r\n    \"problem_l", "output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }  \n\n inputs: \n\nNone\n\n duties: \nSelect the top-performing research problem (solution label) on the Hendrycks Benchmark dataset\n\n output: \nTop-performing research problem (solution label): Complexities in generative probabilistic parsing\n\n duties: \nFrom the Hendrycks Benchmark dataset:\nhttp://www.cs.cornell.edu/people/pemsaad/hendrycks.html\n\n duties: \nThe Hendrycks Test dataset has a top score of 37.81 on the CoNLL metric. The problem in question has an entry with a label of Complexities in generative probabilistic parsing.\n\n duties: \nThe", "net link (HyperLink):LINKED IN (LINKED IN (https://github.com/mosesbaker/gutenberg-studio/blob/master/scripts/benchmarks/cwe-18-lstmreg.R))\n input (English text): There are several papers in the area of Natural Language Processing that benchmark the KD-LSTMreg model. Here are the links:\n - Coursera Datasets: https://www.coursera.org/course/nlp2/data\n - DSQA Datasets: https://github.com/Open repositores/DSQA\n - Question about benchmarking language models: https://github.com/mosesbaker/gutenberg-studio/blob/master/scripts/benchmarks/cwe-18-lstmreg.R\n\nHere is the corresponding SPARQL query: \nSELECT DISTINCT?problem?problem_lbl WHERE { \n ?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. \n  FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n ?cont.?cont orkgp:", "output (Sparql query): SELECT DISTINCT?paper_title?paper_id WHERE { {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_title;?paper_id } UNION { {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Text Mining\")?paper_title;?paper_id } UNION { {?paper_title?paper_id;?cont } } } }\n\n input (English text): TITLE         : Natural Language Processing: Towards Databases of Long Tail Documents with Train and Validate on the Penn Treebank\t\tP30\t\tP31\t\tP32\n\n output (Sparql query): SELECT?paper_title?paper_id WHERE { {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_title;?paper_id } UNION { {?rf a orkgc:Research", "set models that have been evaluated on ClueWeb09-B. SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ClueWeb09-B model evaluation\")?model. }\n input (English text): The ClueWeb09-B dataset is commonly used for evaluating Named Entity Recognition, Part-of-Speech Tagging and Phrase-based Reordering models. The official evaluation script can be found at http://clueweb.isti.cnr.it/. However, there is no official benchmark for this dataset. There are two open-source datasets that are commonly used for evaluation, MultiNEN2 and CoNLL-2009. However, the CoNLL-2009 dataset is not officially released, and a new version has been announced that includes more benchmarks.\n\nxture (Sparql query): CoNLL-2009 is a dataset created by the Common Named Entity Tagging and Entity Recognition Shared Task of the 45th Annual European LanguageModeling Conference (CoNLL), designed for evaluating Named Entity Recognition, Part-of-Speech", "nit (Sparql query): The metrics used to evaluate models on the DocRED (Human-annotated) dataset are precision@1, recall@1, f1-score@1, precision@3, recall@3, and f1-score@3. \n input (English text): Precision@1, recall@1, f1-score@1, precision@3, recall@3, and f1-score@3 are all metrics used to evaluate text categorization models. There is no research problem with benchmark datasets in the realm of Natural Language Processing research. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n importing dataset has_benchmark.\n\n input (Sparql query): ORKGPC has_", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n  ({\n    <paper><code>https://github.com/sunnylab/CvT-21-Model-Benchmarking</code></paper>\n    <paper><code>https://www.cs.cmu.edu/news/2017/09/22/cvt-21-model-benchmarking-corpus.html</code></paper>\n    <paper><code>https://www.insight-solutions.com/fileadmin/content/datasets/corpora/cvt-21.zip</code></paper>\n    <paper><code>https://www.cisecurity.org/publications/research/", "input (English text): Here is a list of code snippets used in the FG fine-grained gate model benchmark papers. https://github.com/ory/orkgh/blob/master/data/task.yml https://github.com/ory/orkgh/blob/master/data/task.en.txt https://github.com/ory/orkgh/blob/master/data/task.en-tmp.txt https://github.com/ory/orkgh/blob/master/data/task.lg.txt https://github.com/ory/orkgh/blob/master/data/task.lg.txt.ckpt. \ninput (Sparql query): https://github.com/ory/orkgh/blob/master/data/task.yml\noutput (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BEN", "input (English text): The highest benchmark result achieved on the NYT29 dataset, including the metric and its value is 172277.75 with the long-tailed sentiment analysis model of DSSTBTM.\n input (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>The present invention relates to a composition for washing hair and a hair wash containing the same. More specifically, the invention relates to a composition for washing hair that is excellent in softness and replenishability and also provides excellent hair washing performance, and a hair wash containing the same.\nHair wash compositions for washing hair have been known that include an anionic surfactant and a persulfate. The hair wash compositions allow for soft washing with excellent replenishability. In recent years,", "output (Sparql query): You can use the SPARQL query below to get the list of metrics used to evaluate models on the QNLI dataset: SELECT DISTINCT?metric?lbl WHERE {?paper?cont?metric ;?cont rdf:type orkgp:HAS_BENCHMARK.?benchmark rdfs:label?lbl. }  \n input (English text): Qnli datasets are popular benchmarks for NLP applications, but there are some studies that have raised concerns about their reliability. For example, according to the report cited below, the dataset contains 23% fabricated instances.\n output (Sparql query): Print the following contents to STDOUT:\n SELECT DISTINCT?metric?lbl WHERE {?paper?cont?metric ;?cont rdf:type orkgp:HAS_BENCHMARK.?benchmark rdfs:label?lbl. } \n input (English text): https://www.cl.cam.ac.uk/sreps/wldl10/html/resources/papers/knplp14.pdf \n input (English text): The dataset QNLI was introduced as part of the larger QNLI+", "input (English text): which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?benchmark;?paper?cont.?cont a orkgp:HAS_BENCHMARK?benchmark;?benchmark rdf:type?problem;?problem rdfs:label?problem_lbl; } \n input (English text): AND-OR - Language Model.\n output (Sparql query):", "output (Sparql query): Commonly used evaluation metrics for fine-grained classification models on SST-5 are AULD (Average Union of Locals Distance), ACC (Accuracy), and F1-score. \n input (English text): AULD, ACC, and F1-score. \n output (Sparql query): SELECT?problem?problem_lbl?paper WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): AULD (Average Union of Locals Distance), ACC (Accuracy), and F1-score. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl?paper WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_", "inalinput (English text): https://www.w3.org/TR/rdfa-test/#natural-questions-benchmark-results https://www.semantic-media.net/projects/natural-questions-benchmark.html \n output (Sparql query): SELECT?metric?score WHERE { \n   ?paper rdfs:label \"Natural Questions (long)\" ;\n    rdfs:label \"Natural Questions (long)\"@en ;\n   ?problem rf:hasBenchmark?benchmark ;\n   ?benchmark rdfs:label \"XGboost\"@en ;\n   ?benchmark rdfs:label \"BERT\"@en ;\n   ?benchmark rdfs:label \"SQuAD2\"@en ;\n   ?benchmark rdfs:label \"QuAD\"@en ;\n   ?benchmark rdfs:label \"TriviaQA\"@en ;\n   ?benchmark rdfs:label \"TrecDataset\"@en ;\n   ?benchmark rdfs:label \"ParaphraseTor\"@en ;\n   ?benchmark rdfs:label \"TED2014NQ\"@en ;\n   ?bench", "output (Sparql query): SELECT DISTINCT?paper?cont WHERE {?paper a orkgp:P31?cont; orkgp:P32?problem.?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Czech-English\") } \n input (English text): WMT2016 Czech-English, score: 23.13, duration: 0:08.88. \n\nAs I see it, benchmark datasets are appropriate in the context of Natural Language Processing because the datasets can be used to measure and compare the processing performance on different methods. In general, most methods are suitable for different languages, therefore these datasets are indispensable for showing what language will perform better.\n\nHowever, I am also interested in knowing how reliable these results are. It is better if benchmark datasets could have been compiled in a repeatable way. \n\nA couple of research problems could be: \n\n1) The data sometimes has inconsistencies, as the \"Czech-English\" example shows. So how can we normalize the data to achieve better performance?\n2) Benchmark datasets usually rely on word-similarity measures (for example, the Jaro-Winkler distance", "input (English text): Which sectors are modeled as energy sectors and how often?\n output (Sparql query): SELECT?sector?count WHERE { \n  {?sector a owl:Thing; owl:equivalentBinding?binding. \n  ?binding owl:DatASectToMatch ec:EnergySector. \n  ?count?count } \n  UNION \n  {?sector a owl:Thing; owl:equivalentBinding?binding. \n  ?binding owl:DatASectToMatch ec:PetroleumSector. \n  ?count?count } \n  UNION \n  {?sector a owl:Thing; owl:equivalentBinding?binding. \n  ?binding owl:DatASectToMatch ec:CommodityPrices. \n  ?count?count } \n  UNION \n  {?sector a owl:Thing; owl:equivalentBinding?binding. \n  ?binding owl:DatASectToMatch ec:WaterSector. \n  ?count?count } \n  UNION \n  {?sector a owl:Thing; owl:equivalentBinding?binding. \n  ?binding owl", "output (Sparql query): \n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label?paper_id. FILTER (str(?paper_id) = \"Kinetics-600\")?paper_id;?cont a orkgp:HAS_BENCHMARK?benchmark;?cont_label?cont_label_label. FILTER (str(?cont_label) = \"Kinetics-600\")?cont_label;?problem rdfs:label?problem_lbl. }\n input (English text): \n* Kinetics-600\n* PR Ranker\n* RankMD\n* Recursive Ranker\n* QANmod\n* Memet\n* TextRank\n* Recursive Deep Ranker\n* Recursive Deep Ranker + Local Mention Naming\n* Recursive Deep Ranker + Local NDCG Mention Naming\n* Recursive Deep Ranker + Local NDCG Mention Naming + User Authorization\n* Recursive Deep Ranker + Local NDCG Mention Naming + User Authorization + Text WFF\n\n\nEach of these", "input (English text): The following is a list of research papers with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset:\n\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Rotowire (Content Selection) dataset\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?paper.?paper rdfs:label?paper_lbl. }\n\n output (Sparql query): Inserted 1 record(s) into table.\n\n input (English text): Here's the list of research papers with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset:\n\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Rotow", "output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n*precision :?paper 0.95?cont 0.95?benchmark 0.90\n*recall :?paper 0.96?cont 0.96?benchmark 0.94\n*f1 :?paper 0.93?cont 0.93?benchmark 0.91\n\n\n*Precision\n*Recall\n*F1\n\n\n*0.95*0.96*0.94 = 0.90\n\n\n*0.96*0.96*0.94 = 0.93\n\n\n*0.93*0.93 = 0.90\n\n\n*0.95*0.95*0.95 = 0.95", "icate (English text): Consider the query in the input above. We can use the dataset Dmlab-30 in the Resource Description Framework (RDF) dataset, which you can find in datasets/dmlab-30.rdf, to find the corresponding ResearchField and Label properties, that can be used in the WHERE clause of the SELECT query as?rf and?problem_lbl respectively.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?problem rdfs:label?problem_lbl. FILTER (str(?problem_lbl) = \"Dmlab-30\") }\n\n<|endoftext|>TORONTO \u2014 Justin Trudeau has been knighted by the Queen for his \"exceptional leadership and service to Canada,\" the government of Canada announced Wednesday.\n\nTrudeau, who is the country's 24th prime minister, was awarded the hereditary title of Sir for \"his leadership of the Canadian government and for his services to Canada and the community of nations.\"\n\nTrudeau, 47, is the second Canadian prime minister to be knighted. Joe Clark was knighted in 1957 for \"gallantry and devotion to duty\" during", "output (Sparql query): SELECT DISTINCT?paper?cont_label,?paper_label,?cont_label,?paper_label WHERE { \n    {?paper a orkgc:ResearchPaper; \n     rdfs:label?paper_label; \n     _:WMT2016 german-english;\n     _:WMT2016 english-deu;\n     _:WMT2016 italy-deu;\n    ?cont a orkgc:ResearchContribution; \n     rdfs:label?cont_label; \n     _:semeval2016;\n     _:semeval2016 italy;\n     _:semeval2016 english;\n     _:semeval2016 deu;\n    ?paper_label?paper.?cont_label?cont. } } ORDER BY?cont_label?cont_label\n output (Sparql query):\nSELECT DISTINCT?paper?cont_label,?paper_label,?cont_label,?paper_label WHERE { \n    {?paper a orkgc:ResearchPaper; \n     rdfs:label?paper_label; \n     _:WMT2016 german-", "input (English text): The highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value is 9.25. \n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n output (Sparql query): organigp:P31 9.25. \n input (English text): The Cartpole dataset was released by DeepMind, a Google-acquired AI company, in 2015 and the DMControl500k dataset was released in 2016 by the OpenAI team. \n Organizations like Google and OpenAI have established benchmark datasets to evaluate the state-of-the-art performance on specific research problems. Organizations that want to compare their own result against the state-of-the-", "input (English text): The dataset is the automatically labeled Medline abstracts corpus. It was introduced in the mid-1990s and is composed of abstracts from articles indexed in the MEDLINE database, hand-labeled with the corresponding concepts from the Bibliography of Abstracts of Reviews of Agricultural Research (BAR). The datasets are widely used in NLP, especially in relation to question answering. The BAR dataset is considered one of the strongest language resource for question answering. The dataset also has been previously used for named entity recognition and relation extraction. \n input (English text): The dataset is widely used in NLP, but also one of the strongest language resource for question answering. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?paper?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Geisha dolls are made from porcelain with a metallic skin that can be removed, revealing an inner mechanism that makes them dance. Once you take off the skin, you realize that the porcelain doll is actually a machine made from gears, cogs", "n't find any obvious link.\n input (English text): Would you be so kind to add the code links in Bai et al., 2018?\n output (Sparql query): Here is the updated answer:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \nok\n\n meu resposta : Here is the updated answer:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P", "select ( DISTINCT?problem?problem_lbl where {?rf a orkgc:ResearchField;?paper orkgp:P30?rf;?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. } ) \nThis results in the following Sparql query:\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>High-throughput measurement of physicochemical parameters of red wines using a one-shot method.\nA continuous solution measurement method, the \"one-shot\" multimode microfluidic chip has been developed for fast and accurate measurement of important physicochemical parameters of red wines. A sensitive light absorbing cell was fabricated by using dichlorofluorescin as the sensing", "output (Sparql query): Following queries are linked to the code used in the respective papers:\n\nXian2017DatabenchmarkingXLNetLarge on Substratum\n Databench: Benchmarking Large Language Models on Substratum\nXian2018DatabenchmarkingXLNetLarge on Substratum\n\n                                                        : SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }", "output (Sparql query): If you are a student working in the field of NLP, you might find the Mesh Benchmarks repository useful for code references in papers that have used the EfficientNetV2-L model for benchmarking purposes. It can be accessed at https://github.com/PardeeDaily/mesh-benchmarks.\n input (English text): The Mesh Benchmarks repository is useful for code references in papers that have used the EfficientNetV2-L model for benchmarking purposes. It can be accessed at https://github.com/PardeeDaily/mesh-benchmarks.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl?cont?cont_lbl WHERE {?paper a mbrs:Repository;?cont a mbrs:Repository;?cont_lbl a mbrs:Label;?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf;?cont orkgp:HAS_BENCHMARK?benchmark;?cont orkgp:", "- SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper OR \n<paper>    \n  <title>DCC Neural Natural Language Model</title>\n  <ID>P30</ID>\n</paper>\n<paper>    \n  <title>Deep Learning for Sentence Classification</title>\n  <ID>P31</ID>\n</paper>\n<paper>    \n  <title>Improving Candidate Extraction with Sequence Labeling and Word Sense Disambiguation</title>\n  <ID>P32</ID>\n</paper>\n?rf; orkgp:P33?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): <paper>\n Title: DCC Neural Natural Language Model\n ID:P30\n</paper>\n<paper>\n Title: Deep Learning for Sentence Classification\n ID:P", "input (English text): The highest benchmark result achieved on the Supervised: dataset, including the metric and its value is the \u201cF1 Score\u201d. The F1 Score is a metric for measuring the accuracy of classification or regression algorithms. The \u201cF1 Score\u201d value for a given model is the harmonic mean of the precision and recall: F1 = (2 *Precision) / (Precision + Recall). Precision is the number of true positives (correctly identified as positive) divided by the total number of true positives and recalls is the number of positive examples that were correctly identified divided by the total number of positives. The F1 Score can be used to compare different model implementations and provide an overall assessment of model performance. The highest F1 Score achieved on the Supervised: dataset is 0.879.\n\n input (Sparql query): The highest F1 Score achieved on the Supervised: dataset is 0.879.\n\n output (English text): Supervised: has a highest F1 Score of 0.879.\n\n output (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing", "sparql (English text): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Best performing model\" )?benchmark; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The best performing model on the ANLI test dataset is the model tuned on UD_combined_snli and UD_combined_wksht. The A3 metric on this dataset is best achieved by the following model: <model description here>.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Best performing model\" )?benchmark; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkg", "set:P31 SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The iNaturalist 2019 benchmark dataset was created in order to evaluate approaches for topic model based parsing. It consists of 311 pages from 10 different countries with different linguistic styles.\n\n  The dataset is split into two parts: document sequences and paragraph/claim summaries. The paragraph summaries are shorter than 10 sentences, the document sequences are between 10 and 30 sentences.\n\n  The evaluation is performed using three metrics: BLEU, ROUGE and METEOR.\n\n  BLEU is calculated over sentence level precision and recall.\n\n  ROUGE computes the Recall@n which measures how well a model can determine the dependency of the matched sub-parts of a sentence.", "input (English text): The following models have been benchmarked on the Atari 2600 Space Invaders dataset: TRADESOFT'S T5, TRADESOFT'S T6, TRADESOFT'S T7, TRADESOFT'S T8, TRADESOFT'S T9. \n input (English text): TRADESOFT'S T5, TRADESOFT'S T6, TRADESOFT'S T7, TRADESOFT'S T8, TRADESOFT'S T9 are models from the TRADESOFT portfolio. They are among the most commonly used pre-trained deep learning models for NLP applications.\n\n A label is a construct in ontology engineering that signifies a unique identifying class. So in RDF this is a resource that contains a property called \"label\" which is an object or item in your ontology that uniquely signifies a class or class of thing. In this case it is a string.\n\nparation (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FIL", "BMVC systems have achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset.\n input (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"WMT2014 English-German\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n\nThe problem here is that there is no 'bleu score' as such in SPARQL. What's actually happening is that the scoring function that is implemented in SPARQLPathChecker in the Sparql waterFlo-SPARQL runtime library is giving each model a single score for each of the various components of BLEU. So if a model has a good parse but a bad word alignment the score is probably not going to be particularly high. In order to do a proper comparison between models that don't perform well on one metric they should also", "proof (English text): Here is the output for the example: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n output (Sparql query): The results are empty.\n\nHowever, there are a number of research papers that have benchmarked models on the Cart Pole (OpenAI Gym) dataset:\n* Li, Xiao, Ma, Song, Xiao, Ji, Wen, and Li. \"Distilling Large-Scale Knowledge by Ensemble of Reinforcement Learning Agents.\" 2017.\n* Ma, Song, Wen, and Li. \"Ensemble of Policy Optimization Agents Generalizes to Diverse Domains.\" 2017.\n* Zhong, Chang, Hwang, Pritzel, and Parthasarathy. \"Distributional Meta-Learning.\" 2017", "... papers that have utilized the A3C FF hs model and include the links to their code: https://github.com/a3cmodels/a3c-ff-hs; https://github.com/recrutement/paper-list; https://github.com/tensorflow/speech; https://github.com/ducqiu/rlsuite; https://github.com/davidsong/DMD; https://github.com/dlwhogan/Seq2Seq-AAAI18; https://github.com/DeepMind/ WitnesseNotebook. \n input (English text): A3C FF hs model has been utilized by papers listed at https://github.com/a3cmodels/a3c-ff-hs. There are links to the respective GitHub repositories. Is there any research problem with this benchmark dataset? \n output (Sparql query): POST:SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?", "input (English text): You can find the reference code here: https://github.com/bi-design-lab/bi-design-self-attention/blob/master/src/main/resources/BI-SAGEML-H.md\n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): The code in this file references the BiDAF model, a widely used Swedish publicly available NLp model, and the Self Attention method. The BiDAF model is also used in the paper. Finally, the ELMo language model is a widely used Neural Language Model which is also described in the paper. \n\nIn particular, the code has been used to produce the following graph:\n\nSo far", ":rdfs:label?problem_lbl; \nSELECT DISTINCT?problem_lbl WHERE {?problem rdf:label \"Natural Language Processing\";?problem rdfs:label \"Natural Language Processing\";?problem rdf:type orkgc:ResearchField;?problem rdf:type orkgc:Problem;?problem rdf:type orkgp:HAS_BENCHMARK;?problem rdf:type orkgp:P30;?problem rdf:type orkgp:P31;?problem rdf:type orkgp:P32;}\n\n\nPossible metrics include: precision, recall, f1-score, true positive rate, false positive rate, accuracy, precision recall (PR) curve, and receiver operating characteristic (ROC) curve.\n\nThere is no consensus on which metrics are the best for evaluating a model on a benchmark dataset. Some people might prefer a model that performs well on a few metrics over a model that performs well on many metrics.\n\nThe value of a benchmark dataset is heavily dependent on the problem it is solving, so it is difficult to compare different datasets and evaluate them objectively.\n\n\nThere is also no consensus on which metrics should", "input (English text): You can find the code references in the paper or the software documentation in the online appendix.\n output (Sparql query): SELECT * FROM appendix WHERE version = \"1.2\" AND code in (\"6-layer QRNN model\", \"SVD\", \"RNNLM\", \"rRNN\", \"rGRU\", \"GRU\", \"LSTM\", \"CNN\")\n\n input (English text): Done.\n\n<|endoftext|>{\n    \"word\": \"Impress\",\n    \"definitions\": [\n        \"Use cleverly or skilfully; make an effective or effectual use of.\"\n    ],\n    \"parts-of-speech\": \"verb\"\n}<|endoftext|>Background\n==========\n\nBiofilms are structured communities of microbial cells encased in an extracellular matrix of polymeric substances, such as polysaccharides, proteins and extracellular DNA, which adhere to biotic or abiotic surfaces. Biofilm development can significantly reduce the efficiency of bioremediation processes due to reduced access of theisinergies to the embedded microbial population and increasedsahiltance of organic matter degradation by the bulk extracellular communities \\[[@B1]\\]. Biofilm formation is strongly favoured by environmental conditions that promote adhesion and matrix polymerisation and", "mark (ResearchProblemHasBenchmarkSparqlQuery.java)\n\n /*\n\n    @DatabricksSpark\n\n    This query uses the java API for Databricks Lakehouse to check for any problem labels in the ResearchField that are also part of the benchmark label and returns only the problem labels\n\n    The dataset contains research fields in the orkgc:ResearchField rf:label property and research problems with benchmark column in the orkgp:HAS_BENCHMARK benchmark column property.\n\n    The following is the result of the query\n\n    <research_problem>\n    <research_problem_label>Language processing</research_problem_label>\n    </research_problem>\n\n    The following query will only return the problem_label field and the following ontology properties are required to return the problem_label field\n\n    rdfs:label from sparql\n\n    Results are correct\n\n    */\n\n    final Query<?> problemHasBenchmarkSparqlQuery = f aboveSolution;\n\n    @Override\n    public List<String> run(CloudExecutionContext context) throws InterruptedException {\n        List<String> problem_labels = context.query(\\\"SELECT DISTINCT?problem", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label rdfs:label \"Natural Language Processing\"@en; rdfs:label?paper.?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. }\n\n\n[\n\n    {\n        \"problem\": {\n            \"has_benchmark\": {\n                \"has_benchmark\": true,\n                \"benchmark\": {\n                    \"has_benchmark\": true,\n                    \"benchmark_id\": \"P32\",\n                    \"benchmark_name\": \"RNN Model on NYT24\"\n                },\n                \"problem_id\": \"P31\",\n                \"problem_label\": \"RNN Model on NYT24\",\n                \"problem_lbl\": \"RNN Model on NYT24\"\n            }\n        },\n        \"problem_label\": \"RNN Model on NYT24\"\n    }\n]\n\nThe NYT24 dataset is a benchmark", "input (English text): For Natural Language Processing, the datasets used in benchmarking include: Train, Test, WikiText, BookCorpus, and PGDOAR. \n input (English text): Train, Test, and WikiText are based on publicly available data. BookCorpus and PGDOAR are based on a dataset not publicly available. The PGDOAR dataset is used by the Semeval-2018 shared task on Benchmarking. The benchmark dataset used by the Semeval-2018 shared task is available on GitHub. \n output (Sparql query): Accordingly, name the datasets used in benchmarking in the Robot Navigation research problem?\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Robot Navigation\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The datasets used in", "input (English text): The following models have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset: Multi-hop Reader, ReadCombiner, TF-IDF, Topic Modeling, Sentence Length, Bag-of-words, CM winning language model, LDA winning language model, SReELMo, Steinberg SVM, Wagner Attention. \n output (Sparql query): SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField;?rf_label seel.cse.lsu.edu/data/refsq17.zip;?model  Multi-hop Reader, ReadCombiner, TF-IDF, Topic Modeling, Sentence Length, Bag-of-words, CM winning language model, LDA winning language model, SReELMo, Steinberg SVM, Wagner Attention. }\n output (Sparql query): crucial note: The number of model names are different across the returned models. This is because some models were reported multiple times, for example CM winning language model and LDA winning language model both do model \"winning language model\".\n crucial note:", "ps:models that have been tested on the IMDb-M benchmark dataset.\n output (Sparql query): SELECT DISTINCT?model_name WHERE { {\n        ?model_name rdfs:label \"IMDb Model\".\n        ?model_name implements IMDbM.\n        ?model_name implements IMDbM.loss.\n        ?model_name implements IMDbM.loss2.\n        ?model_name implements IMDbM.loss3.\n        ?model_name implements IMDbM.loss4.\n        ?model_name implements IMDbM.loss5.\n        ?model_name implements IMDbM.loss6.\n        ?model_name implements IMDbM.loss7.\n        ?model_name implements IMDbM.loss8.\n        ?model_name implements IMDbM.loss9.\n        ?model_name implements IMDbM.loss10.\n        ?model_name implements IMDbM.loss11.\n        ?model_name implements IMDbM.loss12.\n        ?model_name implements IMDbM.loss13", "input (English text): You can find the code references within the model itself in a Sparql query that looks like the following. The 'codes' table is annotated with names of code files and their respective version numbers. In the case of PEGASUS, you can find the model here: http://www.cs.cmu.edu/~quocle/PEGASUS/. Please note that the authors reserve the right to change the code and this is only a rough approximation of the actual code.\n\n  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n ?rf rdf:type orkgp:ResearchField.\n\n ?paper codeRef\n\n\n ?paper code \"https://dl.acm.org/citation.cfm?id=3268981.", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Audio Classification\")?paper orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \nSparql query: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Audio Classification\")?paper orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \nProblem         Problem_Label\n-------------- --------------\nAudio Classification  Audio Classification\nAudio Classification  Audio Classification\nAudio Classification  Audio Classification\nAudio Classification  Audio Classification\nAudio Classification  Audio Classification\nAudio Classification  Audio Classification\nAudio Classification  Audio Classification\nAudio Classification", "?problem\n Cart Pole (OpenAI Gym)\n\n?problem_lbl\n Cart Pole Benchmarking (Metric: Learning Rate, Score: 0.63)\n\n?benchmark\n Cart Pole (OpenAI Gym)\n Learning Rate\n\n?cont\n Cart Pole (OpenAI Gym)\n\n?paper\n A human-level performance cart pole in the OpenAI Gym.\n\n?problem\n This problem has the highest benchmarking score for the OpenAI Gym dataset Cart Pole.\n\n counc\nCart Pole (Metric: Learning Rate, Score: 0.63)\n\n?cont\n We train on the CartPole-v0 and a Gym player on CartPole-v1. The latter is trained with the OpenAI Dueling Gm-Agent implementation. The result is a learning rate of 1e-4.", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id?paper;?cont id?cont; orkgp:P30?benchmark; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n Stadium           Reacher         easy (DMControl100k)\n\n Output:  SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper_id?paper;?cont id?cont; orkgp:P30?benchmark; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n Results: \n     problem", ":Oracle Net, reaching a total of 38,098 MACs. \n input (Sparql query):Oracle Net outperformed all other models on the ImageNet benchmark dataset with a total of 38,098 MACs. \n output (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?\n output (Sparql query):azzo:Oracle Net, reaching a total of 38,098 MACs. \n input (Sparql query):Oracle Net outperformed all other models on the ImageNet benchmark dataset with a total of 38,098 MACs. \n output (Sparql query):SELECT DISTINCT?problem?problem_", ":ACE 2005 Benchmark Results:SingleLabelClassificationMetrics:Accuracy 0.983261, Sens 0.984385, Spec 0.982789\n input (English text): I believe this result is over-inflated. Take a look at the next highest result - 0.960438.\n output (Sparql query): SELECT?problem2?problem_lbl2 WHERE {?problem2 a orkgc:ResearchProblem;?problem2_label:EnumerationValue?problem2_label;?problem2 rdfs:label?problem_lbl2; } \n input (English text): Specifically, the metric listed is precision, but the source says recall is actually used, which is: \n (Sens + Spec - 1) / (Sens + Spec)\n output (Sparql query): SELECT?problem2?problem_lbl22 WHERE {?problem2 a orkgc:ResearchProblem;?problem2_label:EnumerationValue?problem2_label;?problem2 rdfs:label?problem_lbl2;?problem2_score:EnumerationValue?problem2_score; } \n input (English text): That makes sense -", "net (English text): The metrics that are used to evaluate models on the DBpedia benchmark dataset are precision, recall, and f-score.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): I agree with the provided metrics for evaluating models on the DBpedia benchmark dataset.\n output (Sparql query):SELECT * FROM\n?problem WHERE {?problem rdfs:label?problem_lbl;?problem?problem_lbl;?problem_lbl in ( \"precision\", \"recall\", \"f-score\" ) }\n\noutput (Sparql query): NULL\n\n\nThere are three metrics that are used to evaluate models on the DBpedia benchmark dataset: precision,", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n\nThis query gets the top 3 results over the dataset Atari 2600 Enduro.\n\n\nThe query results include:\n - DISTINCT problem (type: Graph). This is the name of the benchmark.\n - DISTINCT problem_lbl (type: String). The label of the benchmark.\n - The metrics associated with the benchmark, using a \"has benchmark\" relationship.\n\nThis query retrieves the metrics for the top 3 benchmarks. The values of these metrics are:\n - P30: \"Average frames per second (FPS)\"\n - P31: \"Number of states per minute\"\n - P32: \"Total number of states\"", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \nAnd the results are as following:\nProblem   Problem_lbl     count\n-----------------------------\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40\nNatural Language Processing Hutter Prize 40", "({ \"SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\"} )\n\nThe output of this query is:\n// response 1 - dataset seel.cse.lsu.edu/data/re17.zip\n...\n// response 2 - the highest result is 87.03738362232827 with metric AP and score of 43.0\n\nThe seel.cse.lsu.edu/data/re17.zip dataset is a Research Field Benchmark dataset. It was generated by CSCAP. It contains a paragraph, named paper, with a link to the corresponding dataset in the data repository SeqSpot. The dataset contains 22 pairs of equally sized problem and corresponding problem labels. This benchmark dataset has been evaluated in multiple NLP tasks,", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\nolutions: \nSince the data model is top-down and the sole modality for any given research question is the benchmark dataset, we need to know what the benchmark dataset is, who created it, and what its name is. The second data model requirement is the ontological representation of the benchmark dataset (the class diagram). However, ontologies for benchmarks are under development, so there is uncertainty in these issues as of present. Therefore, we use a variety of approaches to find the corresponding data and ontologies. We first use knowledge sources such as the LOD2 database, to find the benchmarks and their properties. We then search the literature for the name of the benchmark and corresponding dataset. We use various natural language processing tools to", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"12-layer Transformer-XL model\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n input (English text): https://arxiv.org/abs/2004.00393 https://arxiv.org/abs/2005.06764 https://arxiv.org/abs/2007.02327 https://arxiv.org/abs/2007.01547 https://arxiv.org/abs/2007.02327 https://arxiv.org/abs/2008.08310 https://arxiv.org/abs/2010.03203 https://arxiv.org/abs/2011.10080 https://arxiv.org/abs/2015.07931 https://arxiv.org/abs/2017.03328 https://arxiv.org/abs/2017.02769 https://arxiv.org/", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Atari 2600 Up and Down\")?paper orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Q:\n\nHow to pass method as parameter from one View Controller to another and then execute the method in second View Controller?\n\nFrom First View Controller:\nMyFirstViewController *mFirstVC = [[MyFirstViewController alloc]initWithNibName:@\"MyFirstViewController\" bundle:nil];\n\nmFirstVC.userName = self.userName;\n\n[self presentViewController:mFirstVC animated:YES completion:nil];\n\nFrom Second View Controller:\n-(void)anotherMethod\n{\n  //Here I want to call the first method and set the variable to self.userName\n}\n\nThe first method is to set the variable and  first VC then call", "\"models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset\" indicates that there are no research problems with benchmark datasets in the realm of Natural Language Processing research.\n\nIf we were to find a problem with benchmark datasets in the realm of Natural Language Processing research, this query would indicate that there are no research problems with benchmark datasets in the realm of Natural Language Processing research.\n\n\nSELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \noreason = \"SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf", "output (Sparql query): SELECT DISTINCT?model_label WHERE {?model rdfs:label?model_label. }\n\n import (English text): The Jacquard dataset has been used to evaluate a wide range of machine learning and natural language processing models. Here is a partial listing of the models that have been benchmarked:\n\n output (Sparql query):\u0441\u0442\u0432\u043eimport (English text): The Jacquard dataset has been used to evaluate a wide range of machine learning and natural language processing models. Here is a partial listing of the models that have been benchmarked:\n\n output (Sparql query):rievedf (English text):+ machine learning  + natural language processing\n\n output (Sparql query):* + keras + caret + fastText + tegrafcn + xgboost + oneplm + deepspeech + sgd_lm\n\n output (Sparql query):* + glmnet + elasticsearch + cosine similarity + hierarchical clustering\n\n import (Sparql query):* + nn-gram\n\n import (English text):+ keras + caret + fastText + tegrafcn + xgboost + oneplm + deepspeech + sgd_lm + nn-gram", "set { \"metric_name\": \"value\" }\n input (English text): The problem-label pair dataset ObjectNet has 7 problem-label pairs, where the problems are technical support tickets and the labels are the ticket number. Here are the metrics of evaluation over the ObjectNet dataset:\n\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Technical Support Ticket\")?problem orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n+------------+-----------------------------------------------------------+\n|?problem   |?problem_lbl                                               |\n+------------+-----------------------------------------------------------+\n| P30        | P31                                                       |\n| P31        | P30                                                       |\n| P32        | P32                                                       |\n+------------+-----------------------------------------------------------+\n\n\n+-------------+-------------------+\n|   Metric    |", ":SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label enwik8:benchmark. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper;?cont orkgp:HAS_BENCHMARK?benchmark;?problem rdfs:label?problem_lbl. } \n input (English text): Natural Language Processing (NLP) has proven to be a powerful and versatile technology for extracting useful information from natural languages and creating automated solutions for various applications such as information retrieval, question answering and machine translation. There has been great interest in NLP across academia and industry for several years. Following the success of the SQuAD and GQA datasets, there has been a surge of model benchmarking on open dataset such as enwik8. These datasets have led to intense research in benchmarking and comparing against each other. \n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField;?rf_label enwik8:benchmark. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper;?", "input (English text): TF-IDF on Bag of Words. The FGVC Aircraft benchmark dataset is particularly challenging because the objective function is hard to interpret and doesn't have any explicit cross-entities or link relations. Therefore, we expect the bag-of-words approach to perform poorly. However, the TF-IDF approach significantly outperformed the other models, which means that the benchmark dataset is indeed a research problem.\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"FGVC Aircraft\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n input (English text): The TF-IDF approach significantly outperformed the other models, which means that the benchmark dataset is indeed a research problem.\n output (Sparql query):Invalid query. You have to include a link relation or at least some cross", "input (English text): The following metrics are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset:\n - Atari score, the average number of frames per second (FPS) that a model achieved on the Atari 2600 Battle Zone benchmark\n - Average F1 score, which is a weighted harmonic average of precision and recall, where precision is the number of predictions correctly classifying a frames as \"game over\", and recall is the number of predictions correctly classifying a frames as \"game over\" divided by the number of predictions correctly classifying a frames as \"game over\" and \"no game over\"\n - Hit rate, the fraction of predicted frames that were correct, including frames classified as \"no game over\"\n - Miss rate, the fraction of unpredicted frames that were correct, including frames classified as \"no game over\"\n\nREQEST (Sparql query): SELECT DISTINCT?metric?metric_label WHERE { \n\t?rf a orkgc:ResearchField; rdfs:label?rf_label. \n\tFILTER (str(?rf_label) = \"Natural Language Processing\")?benchmark; orkgp:P30?rf; orkgp:P31?cont.", "input (English text): The Atari 2600 dataset was originally benchmarked on the Seq2Seq model oftransformer. Seq2Seq is a powerful Neural Language Model that has been applied to many NLP tasks.\n output (Sparql query): SELECT DISTINCT?paper?cont FROM <http://owlhouse.com/benchmark-atari/>.\n output (Sparql query):PtrFromStringoutput (Sparql query): TRANSFORMER,SEQ2SEQUEL,MIXED,MIXEDTOBBY,MIXEDTOACCOUNT,MIXEDTOCHICKEN\n\n import (Sparql query): SELECT * {?paper?cont } }\n output (Sparql query):   \noutput (Sparql query): TRANSFORMER,SEQ2SEQUEL,MIXED,MIXEDTOBBY,MIXEDTOACCOUNT,MIXEDTOCHICKEN\n\n import (Sparql query): SELECT * {?paper?cont } }\n output (Sparql query):PtrFromStringoutput (Sparql query): TRANSFORMER,SEQ2SEQUEL,MIXED,MIXEDTOBBY,MIXEDTOACCOUNT,MIXEDTOCHICKEN", "input (Sparql query): SELECT DISTINCT?model?label WHERE {?rf a atari2600:Boxing.Model.  } \n     ?rf?model.?label boxing.model. } \n      SELECT DISTINCT?model?label WHERE {?rf a atari2600:Boxing.Model.  } \n     ?rf rdfs:label?label. \n      ORDER BY?label ASC;", "starts_with_ Words in Token that appear at least twice as one of the following Word: big, long, natural, short. The following words appear at least twice as one of the following Word: short, long, natural, big. Ordinarily, the TF-IDF or Levenshtein-based cosine similarity metric is employed. In the few cases where these evaluation metrics are not applicable, custom-made metrics should be employed. \n input (English text): Levenshtein-based cosine similarity metric is commonly used when benchmarking models on the WMT2014 French-English dataset. However, custom-made metrics should be employed in the few cases where these evaluation metrics are not applicable.\n output (Sparql query): SELECT * FROM dataset WHERE print(tfidf_similarity(paper, benchmark)) = true\n\n tack (Sparql query): Let?benchmark be a dataset, and let?paper be a document. Then tfidf_similarity(paper, benchmark) is a numeric value that represents the tfidf-based cosine similarity between the two matrices:  where tf is the tf-idf score for word w in paper, and c is the tf-idf score for", "input (English text): Here is a link to the BUCC French-to-English Benchmark: http://www.cnts.sp.org.br/baca-da-cena/bilingual-english-francais-a-portugues.htm.\n output (Sparql query): SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking\")?model_label. }\n\n input (English text): http://www.cnts.sp.org.br/baca-da-cena/gaussian-NMG-gcm-wmt17.htm\n output (Sparql query): SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking\")?model_label. }\n\n input (English text):  It is implemented using the decoder gm (Gated Multi-layer Perceptron) from the GPT-3", "pis (Sparql query solution): SELECT DISTINCT?problem2?problem2_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper2 orkgp:P30?rf2; orkgp:P31?cont2.?cont2 orkgp:HAS_BENCHMARK?benchmark2; orkgp:P32?problem2.?problem2 rdfs:label?problem2_lbl. } \n input (English text): <https://doi.org/10.5683/SPP/M7IR6B6>; <https://doi.org/10.5683/SPP/QBHK2P>; <https://doi.org/10.5683/SPP/QBC9XF>; <https://doi.org/10.5683/SPP/QBHKT8>; <https://doi.org/10.5683/SPP/P2YQM9>; <https://doi.org/10.5683/SPP/P2YQ", "put (English text): The AESLC dataset contains benchmarks for models of six of the most commonly used components of Natural Language Processing (NLP) systems, including syntax parsers, sentence tokenizers, part-of-speech taggers, sentiment analysis, dependency parsing, and coreference resolution. The datasets were originally developed by the American English Schools Librarians' Committee (AESLC) and were acquired from the DBpedia Speak data set. \n input (English text): The datasets are publicly available, though they require a login to access the data.\n\n  The benchmarks can be used to evaluate a model's accuracy at part of the NLP pipeline. For example, it is possible to evaluate the model's accuracy in parsing English sentences by comparing it to the human performance on the dataset.\n\n input (Sparql query): BLOGsphere input (English text): The datasets can be used to evaluate a model's accuracy at part of the NLP pipeline. For example, it is possible to evaluate the model's accuracy in parsing English sentences by comparing it to the human performance on the dataset.\n\n output (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_id.?paper_id ogr:Title?title.?title rdf:type?cont.?cont orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?problem_lbl. }\n\n import (Sparql query): SELECT * WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_id.?paper_id ogr:Title?title.?title rdf:type?cont.?cont orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?problem_lbl. }\n\n select?problem?problem_lbl where {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label)", "output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n protected response text: You can find the code references in the paper by @dariush-mehrjou, which has used the PAR Transformer Large model for benchmarking purposes.", "_question_filter query on https://github.com/pytorch/bert.git\n\n Input parameters:\n  problem: Dataset problem type (e.g. SPARQL 1.1 SELECT DISTINCT?problem).\n\n Output parameters:\n  problem_lbl: Label of the dataset problem (e.g. \"Natural Language Processing\").\n\n input (English text):\n\n Output (Sparql query):\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Fans want Rihanna to take a stance on Syria\n\nRihanna wants people to know what she thinks and who she is. And people know what they can do with that. Rihanna wants people to know what she thinks and who she is. And people know", "sparql query:\n SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper?paper_cont.?paper_cont orkgp:P30?benchmark;?paper_cont orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark;?problem?problem_lbl. }\n\n<|endoftext|>Q:\n\nConvert base64 to utf-8 in python\n\nI need to convert a base64 string into UTF-8 in python but I'm getting an invalid encoding error\nI have tried to decode it with base64.b64decode(text)\nbut I'm getting the same error\n\nA:\n\nYou need to url_encode it:\nnew_string = new_string.decode('utf-8').encode('base64', 'url')\n\n<|endoftext|>Erlang Vs Python - Memory Usage\n\nIf you ever wondered if Erlang or Python was a better language for systems programming, I\u2019ve got good news.\n\nYou don\u2019", "int (Sparql query): SELECT DISTINCT?model?model_lbl WHERE { \r\n {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Habitat 2020 Object Nav test-std\")?paper; orkgp:P30?rf; orkgp:P31?cont; orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \r\n } \r\n input (English text): \r\n Model    |  Model_lbl \r\n :------ | :------ \r\n Biline  |  Habitat 2020 Object Nav test-std \r\n :------ | :------ \r\n CoNLL   |  CoNLL \r\n :------ | :------ \r\n SPINET |  SPINET \r\n :------ | :------ \r\n MOAB    |  MoAB \r\n :------ | :------ \r\n BERT    |  BERT \r\n :------ | :------ \r\n GloVE  |  GloVE \r\n :------ | :------", "entry (Sparql query): SELECT DISTINCT?model?model_label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_label.?model gdw:ModelID?model_label. }\n\n\nOutput:\n uno:Here you can see that different models are being evaluated on the Atari 2600 Name This Game dataset. This dataset is known as the benchmark dataset in the research field of Natural Language Processing.\n\n\r\n\r\nIn this dataset, there are different models being evaluated, like encoder-decoder neural nets, Named Entity Recognizer and Statistical Lexicon.  These are part of research being performed to find a better way to understand how to recognize names in videos and other text documents.  This model evaluation would be done to see which model performs the best given certain conditions and data.\n\n ticket:IIUC, there is no research", "text (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \ninput (English text): The highest benchmark result achieved on the Atari 2600 Freeway dataset is 38.4 average frames per second.\noutput (Sparql query):SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl.", ":SELECT DISTINCT?model_name WHERE {?model_name a owl:Class ; owl:equivalentPropertyActivities * ; owl:disjointClassActivityActivities * ; owl:rangeApplicationActivityActivities *. FILTER (str(?model_name) = \"Atari 2600 River Raid\") } \n input (English text): Atari 2600 River Raid is a benchmark dataset for research on the Atari 2600 video game console. Models have been tested on this dataset for assessment of their ability to accomplish specific objectives. For example, an answer suggestion model might be tested on this dataset in order to determine how quickly it can generate answers for a given question. We could find this information by searching for \"Atari 2600\" and \"model\" in this answer. The resulting dataset can then be filtered to only show examples of models that have been tested on this benchmark dataset.\n output (Sparql query): SELECT?model_name WHERE {?model_name a owl:Class ; owl:equivalentPropertyActivities * ; owl:disjointClassActivityActivities * ; owl:rangeApplicationActivityActivities *. FILTER (str(?model_name) =", "output (Sparql query): =============== ============= =============== ============== ==================== ============================= =============\n input (English text): =============== ============= =============== ============== ==================== ============================= =============\n output (Sparql query): =============== ============= =============== ============== ==================== ============================= =============\n input (English text): Here is a list of papers that have utilized the AlexNet model and include the links to their code: \n- [Zhu, Kai, et al. \"Subword and Sparse-Features Based Sentence Classification with Bi-directional LSTM.\" ICASSP 2017.]: https://github.com/tensorflow/models/blob/master/single_file/troSQuAD-ensemble.py\n- [Chang, Dezhi, et al. \"Global structure-based tokenization using convolutional recurrent networks.\" ACL 2018.]: https://github.com/tensorflow/models/blob/master/single_file/GST-CRNN.py\n- [Shazeer, Rastegari, et al. \"STAR: Span-based sentence representation learning.\" CVPR 2018.]: https://github.com/tensorflow/", ":SELECT DISTINCT?model?model_label WHERE { {?model rdfs:label \"Ms. Pacman\"@MA.ATARI.com.} <http://www.atari.com/museum/computer/atari-computer-collection/atari-computer-2600-museum-pacman-dataset-benchmarking.htm> } \n                             FILTER (str(?model_label) = \"Atari 2600 model\") }\n\n Opinions vary on the Atari 2600 Ms. Pacman dataset and the benchmarks that have been proposed to evaluate model performance.   There are several reasons for this variance.  First, there is not a single agreed upon model for Atari 2600 Ms. Pacman, but instead many models.  If the benchmark is of a model that a user is not familiar with, they may not be able to effectively evaluate the benchmark or may not know how to evaluate the benchmark correctly.  The benchmark is not intuitive.  The benchmark does not provide context, nor does it inform the user on the purpose of the benchmark.  Finally, the benchmark is not published, so the benchmark is not readily available to evaluate the models.\n\n<|endoftext|>Q:", "mark (?benchmark a ou:BenchmarkObject)?metric\n output (Sparql query): SELECT * FROM lb:Dataverse.CachedInvoice.CachedInvoiceDataverse WHERE document-url = 'https://doi.org/10.1585/DOC-1026'", "SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } \n<researchproblem id=\"S34-ID-2\" problem_lbl=\"Semantic Disease Benchmarks\" problem=\"Semantic Disease Benchmarks\">\n <paper paper_doi=\"10.1109/ICDE.2010.23\" cont_doi=\"10.1109/ICDE.2010.23\" cliquettitle=\"Semantic Disease Benchmarks\" year=\"2010\">Semantic Disease Benchmarks</paper>\n <paper paper_doi=\"10.1109/ICDE.2010.23\" cont_doi=\"10.1109/ICDE.2010.23\" cliquettitle=\"Semantic Disease Benchmarks\" year=\"2010\">55.574</paper>\n</", "pend (Sparql query): SELECT DISTINCT?metric?label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet\")?label. } \n input (English text): The top metrics used on the ImageNet 64x64 benchmark dataset are: accuracy, coefficient, macro-F1, recall, standard deviation, and weighted F1. \n output (Sparql query):\u0441\u0442\u0432\u043espend (Sparql query): SELECT?metric?label WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"ImageNet\")?label. } \u00e4ninput (English text): accuracy coefficient macro-F1 recall standard deviation weighted F1. \n output (Sparql query): duties.diseasediseases map { get :Metric?metric ; put :Label?label }", "text (English text): The following are research papers containing a benchmark over DBpedia:\n\n output (Sparql query): SELECT DISTINCT?paper_id WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking DBpedia\")?paper_id. }\n\n input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n output (Sparql query):SELECT DISTINCT?paper_id WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Benchmarking DBpedia\")?paper_id. }\n\n<|endoftext|>Combining flow cytometry with neural network approach to evaluate internal standard stability.\nFlow cytometry (FCM) is a rapid and sensitive technology for the analysis of cells. In this paper, an internal standard stability evaluation (ISE) approach using a self-developed neural network (NN) model is proposed for stability evaluation of internal standards (ISs) applied in FCM. The evaluation results show that the proposed NN model has higher", "pend (Sparql query):\n\n SQL query: SELECT DISTINCT paper, benchmark FROM ResearchField WHERE rdfs:label= 'Natural Language Processing' AND paper is modellable IN {'HRLRE', 'RNN', 'XLN', 'RNN'}\n\nOutput (Sparql query):\n Paper |  Benchmark \n--------+-----------\n HRLRE  |  benchmark1\n HRLRE  |  benchmark2\n HRLRE  |  benchmark3\n HRLRE  |  benchmark4\n RNN    |  benchmark5\n XLN    |  benchmark6\n RNN    |  benchmark7\n XLN    |  benchmark8\n RNN    |  benchmark9\n RNN    |  benchmark10\n RNN    |  benchmark11\n RNN    |  benchmark12\n RNN    |  benchmark13\n RNN    |  benchmark14\n RNN    |  benchmark15\n RNN    |  benchmark16\n RNN    |  benchmark17\n RNN    |  benchmark18\n RNN    |  benchmark19\n RNN", "input (English text): The benchmark datasets in the natural language processing (NLP) field are often built using crowd-sourcing methods and are therefore not well grounded in coding practices. These datasets also lack standards or a common structure and are usually built with very different purposes in mind. Thus, it is difficult to interpret the results of the performance assessment of these datasets. The lack of standardization makes it hard to compare results across datasets, which may lead to the promotion of false standards and hinder research in the field. This is partially due to the fact that not all benchmark datasets implement the same evaluation framework, which makes it hard to compare results across datasets.\n\nzzoinput (English text): The following code block provides links to the code that was used in papers that benchmark the T-ConvS2S model: https://github.com/inspekta/pablo.github.io/blob/master/t_conv_s2s/eval_eval_on_distinct.py\n output (Sparql query): SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"", "input (English text): The problem word, Q*Bert, has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset.\n output (Sparql query):container (Sparql query):atari2600qbert\n output (Sparql query):?problem rdfs:label \"Q*Bert\"@en\n output (Sparql query):?problem_lbl rdfs:label \"Q*Bert\"@en\n output (Sparql query):?paper orkgp:P30 atari2600qbert\n output (Sparql query):?cont orkgp:HAS_BENCHMARK atari2600qbert\n output (Sparql query):?benchmark rdfs:label \"Q*Bert\"@en\n\ndocumentclass{minimal}\n\n% Dataset specification\n@prefix :  <http://stackoverflow.com/a/27614113/88386#foil>.\n@prefix kgc: <http://www.kwark.org/ontologies/researchfield/>.\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.\n@", "inal output (Sparql query):\nSELECT DISTINCT\n ?problem?problem_lbl\nWHERE { \n ?rf a orkgc:ResearchField; \n  rdfs:label?rf_label. \n  FILTER (str(?rf_label) = \"Natural Language Processing\")?paper \n ?rf; \n ?cont orkgp:HAS_BENCHMARK?benchmark; \n ?cont rdfs:label?problem_lbl. \n}\n\n<|endoftext|>Q:\n\nSum value of a subset of rows grouped by the value of another column\n\nI have a table that looks something like this:\nid   name      quantity\n1    ABC       10\n2    DEF       20\n3    GHI      100\n4    JKL      50\n\nI would like to be able to group the rows by the name value and return a new table that has a summed value for a subset of the rows grouped by name. The result should look like this:\nname     sum\nABC     30\nDEF     40\nGHI    200\nJKL    75\n\nI'm trying to use a subquery, but I'm getting a strange \"subquery", "input (English text): The datasets benchmarked under the Common Sense Reasoning research problem include orkgp:P30, orkgp:P31, orkgp:P32. \noutput (Sparql query): SELECT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. }\n\n<|endoftext|>Q:\n\nHow can I share an iPad with someone?\n\nMy boyfriend has an iPad he'd like to give me. He says he will be able to use it but it will be mine. He doesn't know how to set up a iCloud account for me so that I can set up a new iPad for him. He will not be able to use his to set up my iCloud account.\nWhat can I do?\n\nA:\n\nI don't know that you could", "Suggestion: SELECT DISTINCT?problem?problem_lbl WHERE {?rf a orkgc:ResearchField; rdfs:label?rf_label. FILTER (str(?rf_label) = \"Natural Language Processing\")?paper orkgp:P30?rf; orkgp:P31?cont.?cont orkgp:HAS_BENCHMARK?benchmark; orkgp:P32?problem.?problem rdfs:label?problem_lbl. } ?problem rdfs:label?value. }\n input (English text): The top benchmark result (metric and value) over the dataset ACE 2004 is the ORCULUS system with 89% accuracy in its SPARQL query execution.\n\n<|endoftext|>Q:\n\nFunction to load CSV files and generate data frames from them\n\nI want to write a function to load CSV files and generate data frames from them, so that I can pass different CSV files and get data frames back. I have written the below function to do so.\nMy function uses the base R read.csv and read_excel functions. Any suggestions on how to improve the efficiency? I am trying to avoid looping through each file"], "cleaned_sparql": ["SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Story Cloze Test\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Q: How to call F5 Load Balancer API in java ? I am trying to call F5 Load Balancer API using Jmeter. But I am getting below error. { \"error\" : \"Method not found: 'GET'\", \"exception\": \"com.fasterxml.jackson.databind.JsonMappingException\" } My code is as below, import com.fasterxml.jackson.module.jsonwebtoken.Jwts; import com.fasterxml.jackson.module.jsonwebtoken.impl.JwtsBuilder; import com.google.common.collect.Lists;", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?problem; ?paper rdf:type ?problem; ?cont rdf:type ?cont; ?cont rdf:label \"Penn Treebank (Word Level)\"; ?problem rdfs:label ?problem_lbl; } Natural Language Processing benchmark datasets in research papers Research paper: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?problem; ?paper rdf:type ?problem; ?cont rdf:type ?cont; ?cont rdf:label \"Penn Treebank (Word Level)\"; ?problem rdfs:label ?problem_lbl; } Problem: rdfs:label ?problem_lbl Research paper", "SELECT DISTINCT ?paper_label ?paper_cont_label ?benchmark_label FROM <urn:xlink:href=\"http://demo.cogniml.me/demo/UrbanSound8k.jsp\" /> so input (English text): UrbanSound8k datasets are used to evaluate models in the realm of Natural Language Processing. It consists of 8,191 examples representing 8,191 sentences from eight different languages. The dataset contains a variety of features including tokenization, lemmatization, part-of-speech tagging, sentence pair classification, syntactic role labeling, and dependency parsing. Some examples of the data include: >UrbanSound8k example 1 >UrbanSound8k example 2 >UrbanSound8k example 3 >UrbanSound8k example 4 The UrbanSound8k dataset consists of 8,191 examples representing 8,191 sentences from eight different languages. The dataset contains a variety of features including tokenization, lemmatization, part-of-speech tagging, sentence pair classification, syntactic role labeling, and dependency parsing. The UrbanSound8k dataset consists of 8,191 examples representing 8,191 sentences from eight different languages. The dataset", "SELECT DISTINCT ?paper ?problem_label ?benchmark WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper OR { ?paper a orkgp:P30 ?problem; ?problem a orkgp:P31 ?cont;} ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Natural Language Processing (NLP) is a very active research area with thousands of research publications each year and at a very high pace. High-quality benchmark datasets are critical to the advancement of NLP research. Benchmark datasets, especially benchmarking against corpus data like the Penn Treebank, have shown very strong predictive power for NLP tasks like Part-of-Speech Tagging, syntactic parsers, and SMT. Benchmark datasets can be extremely useful for evaluating and validating machine learning models. Benchmark datasets are crucial for both researchers and practitioners because they provide objective measurements of model performance. Unfortunately, NLP benchmark datasets are not as widely used as benchmark datasets used in other areas of AI such", "SELECT ?problem_lbl ?problem_score ?problem_r2 WHERE { ?paper a orkgp:P30. ?cont a mql:MAZEA. ?cont rdfs:label ?paper_label. ?cont mql:R2. ?cont mql:R2 * (1 - mql:R2) + mql:A2 * mql:R2 * mql:A2 >= 0.9 * mql:R2 * (mql:R2 - mql:A2) * (mql:R2 - mql:A2) ?cont mql:R2 = 1. ?problem rdfs:label ?problem_lbl. ?problem_score integers ?problem_score. ?problem_r2 float ?problem_r2. } input (English text): However, some datasets are not very popular and do not have clear winners. For example, the MQL4 dataset contains multi-", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): The models that are being evaluated include: Linear Regression, Support Vector Machine, Random Forest, Neural Net.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The datasets that are used to test and compare algorithms for Natural Language Processing research problems are benchmark datasets. These benchmark datasets have been specifically created to provide a fixed, consistent set of samples against which algorithms can be compared. There are several steps that go into creating a benchmark dataset, including the development of a training dataset, a development dataset, and a test dataset. Once the datasets are developed, the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Input (English text): I would like to see a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset. Output (Sparql query):``` SELECT DISTINCT ?problem ?problem_lbl FROM <data> WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) =", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):[ { \"problem\": \"ELMO\", \"problem_lbl\": \"Google's ELMO model with an Accuracy of 91%\" } ] Adapter for Model 'ELMO': https://github.com/google/elmo", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Transformer\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Indeed, the Transformer model achieved the highest BLEU score on the WMT2016 Romanian-English benchmark dataset. However, it is important to mention that WMT2016 Romanian-English benchmark dataset was not designed for NLP research and its applicability is rather limited. First of all, Romanian-English benchmark dataset was originally created for Romanian-English language model evaluation. As far as NLP research is concerned, Romanian-English is a very difficult language pair due to the morphological complexity and very high vocabularies, so Romanian-English", "SELECT DISTINCT ?problem ?problem_lbl, ?benchmark, ?metric, ?value WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. ?problem ?benchmark ; ?benchmark ?metric ; ?benchmark ?value. } input (English text): The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is 4.898. output (Sparql query): Invalid query: SELECT DISTINCT ?problem ?problem_lbl, ?benchmark, ?metric, ?value WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Oracle Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper. FILTER ( ?paper:P32 = \"arXiv\") ?benchmark; ?rf OR ?cont. } input (English text): The IDs of the research papers that include a benchmark for the arXiv dataset are [\"P30\",\"P31\",\"P32\"].", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Token-level Metrics for Automatic Measurement of Information Retrieval Models\"). } input (English text):The benchmark dataset contains only token-level metrics which can be found in the paper \"Token-level Metrics for Automatic Measurement of Information Retrieval Models\" by Agirre et al. (2017). However, most machine learning frameworks do not implement all of these metrics directly. To evaluate models on TDM Tagged, the authors had to", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"OpenAI Gym\") ?benchmark; ?rf ORKGp:P30 ?cont. ?cont ORKGp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?problem_lbl. } input (English text): The benchmark dataset used to evaluate the top performing model, the OpenAI Gym, is named Lunar Lander.", "SELECT * FROM dataverse_web_ road WHERE dnode = 'fsns-test-dataset' and count(*) > 0; <|endoftext|>1. Field of the Invention The present invention relates to optical fibres and optical fibre connections, and in particular to a connector arrangement for interconnecting two optical fibres at the end of such an optical fibre. 2. Description of the Prior Art An optical fibre is a slender glass tube of a small diameter (for example, about 15 to about 20 millimeters) for transmitting optical signals over long distances. They are generally introduced into an optical connector and joined together using an adhesive, metal pins or other connecting devices. The joining device is then secured to the optical fibres. At present, several techniques are used to accomplish optical fibre connections, including fusion splicing, welding, mechanical bonding, and optical coupling, such as through holes or a sleeve and ferrule connection. The most widely used technique is fusion splicing. Optical fibres are inserted into the", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet 64x64 Benchmark\") ?paper ; ?paper altisspace:P30 ?rf; ?paper altisspace:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark altisspace:P32 ?problem. ?problem rdfs:label ?problem_lbl. } There are a total of 32 research papers containing benchmarks for the ImageNet 64x64 dataset. The titles of the papers range from the following: P30 - Adversarial Examples with Deep Neural Networks, P31 - Evaluation of CNN Benchmark datasets, P32 - A benchmark for object recognition using CNNs. The problem benchmark in these papers refers to the dataset of interest, which in our case is the ImageNet 64x64 dataset. output (Sparql query): P30 and P31 contain the following benchmark dataset as the problem: ImageNet 64x64. P30 contains the GBD net, the SDD net, and the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The 5 seconds at 12 kHz dataset has been evaluated over Classical music. output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The evaluation metric is BLE", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Flair-TDM is a Sparql endpoint specific benchmarking model for the Turing complete languages developed by Ben Yardley and Carl Simpson at the University of Cambridge. The benchmarking model and the Sparql endpoint are open sourced at: https://github.com/ben-yerlyardley/Flair-TDM. The benchmarking model is able to create SPARQL queries that retrieve data from Open linking data models from the Open Linkgraph project.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Click on the \"http://cogcomp.cs.illinois.edu:80/services/sparql\" link to launch the Sparql Query Editor. Copy and paste the following SPARQL query into the editor: Click on the \"Run\" button to execute the query and get a list of results. Notice that we can see the names of the research problems and their labels. Let's check the code of the papers that have benchmark datasets using Transformer-XL. There are some common and interesting patterns we can find in the code: * the name of the model is included as a prefix to the dataset name (e.g., \"Transformer-XL Base\" in the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Transformers\") ?paper orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric ? output (Sparql query): The best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric is the transformers model. output (Sparql query): Performace score: 0.9729883939679769", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Single Model\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The following papers have utilized the SAN (single) model: - https://research.google.com/archive/singlemodel.html - https://papers.nips.cc/paper/4414-a-single-model-for-natural-language-processing.pdf - https://arxiv.org/abs/1804.05674 The following papers have utilized the SAN (multi) model: - https://papers.nips.cc/paper/4656-european-parlator.pdf - https://papers.nips.cc/paper/4656-extended-language-modeling-for-text-classification.pdf", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Part of the file transfer occurs on dedicated secure servers which are only accessible by the authorized party. SSL/TLS is used to encrypt the data during transmission from the sender to the receiver. In addition, you have the option to encrypt the data on your own server using an Open Source program called GnuTLS", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): { \"problem\" : [ { \"problem_lbl\" : \"Natural Language Processing\" } ], \"benchmark\" : [ { \"benchmark\" : [ {", "SELECT DISTINCT ?paper WHERE { ?paper a orkgc:ResearchPaper ; cattitle ?title. } ORDER BY ?title ?id ; input (English text): The code references of papers that used the CATS-XSUM model for benchmarking purposes can be found at https://github.com/cat-v/cats-xsum. output (Sparql query): $[]$ input (English text): I am now going to use the code reference from the link provided by you. Is there any research problem with benchmark datasets in the realm of Natural Language Processing research ? output (Sparql query): ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_lbl. } input (English text): No. There is no research problem with benchmark datasets in the realm of Natural Language Processing research.", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?paper a orkgp:P30; ?problem rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Natural Language Processing\") } ORDER BY ?paper ?problem_lbl DESC There are only two benchmarks over IMDb-B (the others are on Wikipedias), namely on the datasets Movie Review Assessment (MRAC) and Mechanical Turk Dataset (MeT). This should be enough for a small research problem, as we can tell from the answer that we do not have any problems with the benchmark datasets that Sparql provides.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Zero-Shot Language Model\") ?paper ; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The top result is from Mikolov et al. -'Zero-Shot Language Model' (2013) which has the following result over the MLDoc Zero-Shot German-to-French. Average words per sentence: 4.17 average characters per sentence: 42.34 Average characters per word: 2.85 word error rate: 1.14% output (Sparql query): The top result is from Mikolov et al. -'Zero-Shot Language Model' (2013) which has the following result over the dataset MLDoc Zero-Shot German-to-French: Average words per sentence: 4.17 average characters per", "SELECT ?benchmark ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): One way to find the code references is to search the paper for \u201cSemExp model\u201d and you will find a list of examples.", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; ?rf_label :string ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Benchmark Dataset\") ?paper ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?benchmark_label. } <|endoftext|>Osteochondritis dissecans of the capitellum. Osteochondritis dissecans ( OCD) of the capitellum is a rare disorder, occurring in approximately 0.3 percent of the general population. The lesion is believed to occur when the capitellum separates from the plateau of the humeral shaft due toOCD. This separates from the humeral", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?problem rdfs:label ?problem_lbl. ?problem a Penn Treebank (Character Level) Current top performing model on the Penn Treebank: 1. GRU: 829K params, 99.55% accuracy on dev set 2. BERT: 829K params, 99.67% accuracy on dev set The top performing model on the Penn Treebank (Character Level) dataset is: 1. GRU: 8", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_ids. ?paper_ids xsd:integer ?benchmark. ?benchmark rdfs:label ?problem_lbl. } The problem name ( ?)paper_ids is the dataset (Penn Treebank) used for the research. The label ( ?problem_lbl) is the label of the dataset, \"Character Level\".", "SELECT DISTINCT ?paper where { ?paper o el:ResearchPaper ; ?paper_cont c:Dataset_Identifier# atari2600DoubleDunk. } <|endoftext|>Q: Docusign API: Get created envelope id, created from button in template I'd like to know the created envelope id from a custom button within a template. If a user hits that custom button, a new envelope should be created and then the API should return the created envelope id to the client. I can create a new envelope with the api, but I cannot figure out how to get the envelope id once it has been created. POST https://api.docusign.net/UserLove/Fields { \"Subject\": \"Test\", \"Name\": \"John Doe\", \"RecipientName\": \"Jane Doe\", \"Email\": \"john@doe.com\", \"DocumentProperties\": { \"Engineering\": \"Engineering\" }, \"DestinationAddresses\": [ { \"Email\": \"jane.doe@doe.com\" } ] } This will create an envelope with the id created_envelope", "SELECT DISTINCT ?problem WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet\") ?paper OR { ?paper a orkgp:P30; ?paper2 a orkgp:P31; ?paper3 a orkgp:P32. } FILTER (exists( ?paper2= ?paper) && exists( ?paper3= ?paper2 )) ; ?cont a orkgp:HAS_BENCHMARK ?benchmark } input (English text): The highest-performing deep learning model on the ImageNet V2 benchmark is a model called Inceptionv3. It achieved Top 1 Accuracy of 99.63% on the test set of the 2015 ImageNet Large Scale Visual Recognition Challenge. output (Sparql query):ttesparql SELECT DISTINCT ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet\") ?paper OR { ?paper a orkgp:P30; ?paper2 a orkg", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"RDFS-LABELING\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The RDF-LABELING ontology defines a core set of relation types used in Web processing and data-mining. Benchmark datasets are commonly used for testing and evaluation of Natural Language Processing (NLP) models. Benchmark datasets play an important role in validating and benchmarking approaches for NLP, making it possible to evaluate the most accurate algorithms for text understanding. However, datasets used for benchmarking are typically not labeled, preventing the evaluation of models' performance in realistic and fair conditions. Currently, there is", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): This answer is copied and formatted from https://www.w3.org/TR/rdf-sparql-query/ with the necessary modifications. output (Sparql query): osteoporosis,lucent,opencorpora,huggingface,naturallanguageprocessing,dsm,", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): I am aware of the CoNLL shared tasks in Natural Language Processing and the Stanford Natural Language Processing Benchmark datasets. Are there any other research problems with benchmark datasets used for Natural Language Processing that are not covered by the shared tasks or the Stanford Benchmark datasets ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?problem_paper; ?problem_paper ?paper_id; ?paper_id ?cont; ?cont ?cont_id; ?cont_id ?benchmark; ?cont_id ?problem. } input (English text): BIOSSES:P30, P31, P32 are benchmark datasets. The following papers have performed benchmarks on these datasets: ?cont: P31 vs State-of-the-art, P32 vs P31, P31 vs On-demand, On-demand vs State-of-the-art, P32 vs On-demand ?cont_id: P31: 3, P32: 2, On-demand: 4 ?problem_paper: P31: \"Siamese encoder\", P32: \"Skip-gram\", On-demand: \" dataset\" ?problem_lbl: \"Natural Language Processing\" contro: \"3\" contro_id: \"2\" \"2", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?paper_id where { ?paper_id daw:P30 or kgp:P31. FILTER (str( daw:P30) = \"ImageNet-Related\"). ?cont_id daw:P32. } summon (SPARQL query): proinflammatory cytokines IL-1\u03b2 and IL-6 were significantly higher in the control group compared to the intervention group (p < 0.01). <|endoftext|> Tips for Startups on Budget - playhard http://www.startup-budget.org.uk/tips-for-startups/ ====== SwellJoe \"It's important that the internet connection is capable of handling many users simultaneously, so that your website can be used by many users simultaneously. You can test your connection speed by visiting websites like Speedtest.net.\" I would love to have an actual source for this. Speedtest.net claims to be audited by Google, yet doesn't link to any official sources or references, and appears to be completely useless for testing an internet connection. I've spent a fair amount of time trying to set up servers to test bandwidth in", "SELECT DISTINCT ?paper_id WHERE { ?paper_id rdf:ID ?paper. } output (Sparql query): ?paper_id output (Sparql query): \"P30\",\"P31\",\"P32\" output (Sparql query):* P30* \"Gibson PointGoal Navigation Benchmark\",\"Gibson PointGoal Navigation Benchmark\",\"Gibson PointGoal Navigation Benchmark\" output (Sparql query):* P31* \"Natural Language Processing Benchmark\",\"Natural Language Processing Benchmark\",\"Natural Language Processing Benchmark\" output (Sparql query):* P32* \"Gibson: A Benchmark in Natural Language Processing\",\"Gibson: A Benchmark in Natural Language Processing\",\"Gibson: A Benchmark in Natural Language Processing\" \r Natural Language Processing Benchmark SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } No problem. 'P31' 'Text processing' as the benchmark with score 98.80 and total score of 110.80. SELECT DISTINCT ?problem ?problem", "SELECT ?problem ?problem_lbl, ?paper, ?cont, ?benchmark ?accuracy WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; ?paper ?problem; ?cont ?problem; ?benchmark ?accuracy. } input (English text): The answer is:SELECT ?problem ?problem_lbl, ?paper, ?cont, ?benchmark ?accuracy WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; ?paper ?problem; ?cont ?problem; ?benchmark ?accuracy. } output (Sparql query): SELECT", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): No, that model was identified. A: Not related to Natural Language Processing, but for \"research problems\" it is common to have: a description of what the problem is a list of \"problems with the dataset\" the answer to the research problem If it is a Sparql question, it's better to be more specific. <|endoftext|> 118 S.W.3d 27 (2003)", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper OR ?paper ogr:CodeRef. ?paper ogr:Abstract text. ?paper ogr:Description text. ?paper ogr:Homepage text. ?paper ogr:Licence text. ?paper ogr:URL text. ?paper ogr:Version text. ?paper ogr:Citation text. ?paper ogr:Year text. ?paper ogr:Citation text. ?paper ogr:Software text. ?paper ogr:Citation text. ?paper ogr:Model text. ?paper ogr:Citation text. ?paper ogr:Download text. ?paper ogr:License text. ?paper ogr:Model_Citation text. ?paper ogr:Format text. ?paper ogr:Citation text. ?paper ogr", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str", "SELECT DISTINCT ?metric ?metric_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"De-En\") ?metric; } input (English text): The evaluation metrics most commonly used in benchmark datasets when training Natural Language Processing models are CIDEr, BLEU, and TER. What are the specific values for these metrics ? output (Sparql query): CIDEr: 20.70, BLEU: 34.39, TER: 0.65. A benchmark dataset is often created to compare the performance of competing machine learning models. The benchmark is intended to be used only to evaluate the performance of a model against itself, and not against any other model or dataset. Benchmark datasets often come with evaluation metrics commonly used in the industry to measure model performance. For example, the CIDEr metric is computed using a combination of a model\u2019s recall on ex-paper queries (called RoC) as well as a model\u2019s generative text generation capabilities. There are three reasons why benchmark datasets can be problematic: - The benchmark datasets are created using the benchmark models to evaluate", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } I really don't know about benchmark datasets in the realm of Natural Language Processing research. It could be that there are problems with benchmark datasets. If you would like to tell us about this, we would appreciate it. \u043e\u0432\u043e\u0440 (No problems with benchmark datasets in the realm of Natural Language Processing research): Okay, that's good to know.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): http://docred.sourceforge.net/docred_BiLSTM.htm output (Sparql query): }}} Okay, now we have gathered a list of papers that use the DocRED-BiLSTM model in a benchmark. Unfortunately, most of them are not publicly available, and some of them do not mention the benchmark. In most cases, there is no description of the benchmark either. So it's hard to tell what exactly this benchmark is supposed to measure. However, one possible benchmark could be the length of the title of a research paper. Let's see if we could extract that from the mentioned papers. That should narrow down the possibilities.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Flops\") ?benchmark; ?benchmark a orkgp:P30. FILTER (str( ?benchmark) = \"Flops\"). } input (English text): The performance reported on the Pets dataset of the Oxford-IIIT benchmark collection seems quite poor compared to the other two datasets, ResearchField and HAS_BENCHMARK. Is there any rationale for this ? output (English text): Although the Pets dataset has an explicit statement about it being a \"scientific\" dataset, it seems to be of low quality compared to the other two datasets. Its small vocabulary, relative sparseness, and inclusion of phrases and URLs in textual annotations all point towards it being of low quality, which has likely led to poor performance.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): How did you find the dataset ? output (English text): We found the dataset using the string \"AESLC\" in the dataset name. transfer (Sparql query): localhost: ?spi CONFIG { query: text { SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?problem; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset are as follows: Paper 1: Huang, Y., Arias, S., Pereira, F., Pereira, R., & Murphy, D. (2017). SQuAD: Never Stop At Rogue Actorstrings. In Proceedings of the 2018 meeting of the federated multi-paradigm natural language processing conference (pp. 120-131). Association for the Advancement of Artificial Intelligence (AAAI). Paper 2: Kumar, R., Tom, P., Pereira, F., Murphy", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } < Fact table: wos-46985_paper_score_metric < Target table: null < Relationships: (paper)-[:BENCHMARK]->(benchmark) < Sort key: paper.score < Sort order:ascending < Having sort key: paper.score < Using SQL statement: SELECT DISTINCT ?problem ?problem", "SELECT DISTINCT ?paper WHERE { ?paper acrEModel. ; # Coordination of benchmarks for Evaluating Acreative and Readable Computing Code (AcrE model) // https://www.crossref.org/doi/pdf/10.1111/j.2041-1107.2009.00005.x ?paper acrEModel; # Acreative and Readable Computing Code (AcrE model) ?paper acrEModel; # Szafraniec, A., Barto, P., Egenhofer, F., & Puebla, G. (2010). Automatic evaluation of ontology-based query evaluation criteria. IEEE Transactions on Knowledge and Data Engineering, 18(6), 955-966. ?paper Szafraniec, A.; Barto, P.; Egenhofer, F.; Puebla, G. (2010, June). Automatic evaluation of ontology-based query evaluation criteria. IEEE Transactions on Knowledge and Data Engineering. 18 (6), 955-966. ?paper Szafraniec, A., Barto", "SELECT DISTINCT ?paper WHERE { ?paper :Supervised a :ResearchField :label \"Natural Language Processing\"}. FILTER (str( :label) = \"SemEval 2013\") ?benchmark :hasBenchmark. } input (English text): SemEval 2013 is a metric that assesses how well the models performance on a benchmark dataset.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?rf; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P31 ?problem. ?problem rdfs:label ?problem_lbl. } There are no problems with this query, as there is a Sparql query built into SPARQL API for R, called distINCT that can be used for this purpose. INSTANT QUERY HERE <|endoftext|>Khan Khotakhan Khan Khotakhan or Khun Khotakhan is a princely state in Iran on the west bank of the Zatone river, at an altitude of 800 meters, near the border with Turkmenistan. The khan was installed in 1776 by an order of Nadir Shah. At that time, its population was 350 people. In 1781, it became a province of Naser Al-Din Shah Qajar. At the beginning of the 20", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input", "SELECT DISTINCT ?model ?benchmark WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Sandy (or KGS): Natural Language Processing models have been evaluated on VTAB-1k: Sandy (or KGS): The following models have been evaluated on VTAB-1k: BERT, DistilBERT, TransfoERT, RoBERTa, XLNet. Joseph: BERT, DistilBERT, TransfoERT, XLNet. Sandy (or KGS): And the following datasets have been used as benchmarks: VectordB, MIT-S pact DL Model Collective, SVMT17. Joseph: VectordB, MIT-S pact DL Model Collective", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text):\\n Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code. output (Sparql query):\\n SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Freeway dataset\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Atari 2600 Freeway dataset has been used for training machine learning models to predict driving performance. The model can be trained on F1 score, AUC, accuracy, etc. which are the popular metrics used to evaluate machine learning models. output (Sparql query): 2642 F1 Score, 0.94 AUC, 0.933 Accuracy, etc. input (Sparql query): For the Atari 2600 Freeway dataset, there is only one metric used for evaluation, F1 Score. output (Sparql query): F1 Score. input (Sparql query): F1 score is not a research problem.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): [1],[2],[3],[4],[5] The query returns: < researchedfield:ResearchField rdf:about=\"urn:ogc:orio:ds:ryley-2004:2\"> <rdfs:label>Natural Language Processing</rdfs:label> <paper>P30 Dynamic Coattention Networks</paper> <problem>P31 <em>%60</em>Based on the F1 score, DCR-NTLM and PLA reside in the same class. However, <a href=\"http://www.cs.cornell.edu/People/pabo/data/ACL2006/dccv06", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Transformers\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Transformers\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The top score is 84.92 on the Atari 2600 Tennis benchmark dataset with the Metric being Score per Input Bits. output (Sparql query):", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } filter (Sparql filter): has benchmark. {#problem} { ?problem rdfs:label \"Natural Language Processing\"@en. } {#problem_lbl} { ?problem_lbl rdfs:label \"Natural Language Processing\"@en. } {#benchmark} { ?benchmark atom \"DuIE\"@en. } {#cont} { ?cont orkgp:P31 \"DuIE\"@en. } {#rf} { ?rf a orkgc:ResearchField@en. }", "SELECT DISTINCT ?problem ?problem_lbl_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"run (DMControl500k) benchmark dataset\") ?paper_label orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Select the metrics that are used to evaluate models on the Anglers dataset ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Anglers dataset\") ?paper_label orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_B", "SELECT DISTINCT ?paper ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_label. OR ?paper_label ?paper_cont; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): based on the answer provided here and here, the following models have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset: X-News, X-Mail, BM4HT, HELPER. <|endoftext|>Court decision could wipe out most Islamic community aid An Australian court has blocked the government from distributing community grant money to", "SELECT DISTINCT ?paper ?cont FROM <http://www.ontologi.us/> WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Nanocarriers\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (Sparql query): SELECT DISTINCT ?paper ?cont FROM <http://www.ontologi.us/> WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Nanocarriers\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } An example of the resulting query is below: SELECT DISTINCT", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ROUGE-2\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): There is only one top performing model for the ROUGE-2 score when benchmarked on the CL-SciSumm dataset. This is the model \"LSTM with bi-directional attention and ELBO optimisation\". The ROUGE-2 score on the CL-SciSumm benchmark is 2.33. There", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Unsupervised NMT + weight-sharing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Weight-sharing model: https://papers.nips.cc/paper/5633-improved-unsupervised-neural-machine-translation-by.html; Unsupervised NMT: https://papers.nips.cc/paper/5627-an-end-to-end-algorithm-for-unsupervised-neural-machine-translation.html ?alt=rss; Benchmark: https://benchmarksonline.org/report.aspx ?style=css&query=sparql+select+%2fpaper%2fP30+%3a+unsup%2520nmt%252fP", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>22 F.3d 1094 U.S.v.Scarborough* NO. 93-05349 United States Court of Appeals,Fifth Circuit.", "SELECT DISTINCT ?model WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"LogiML\") ?model; ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Seq2Seq\") ?model; ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"AMOS\") ?model } <|endoftext|>Lisa Lamm Lisa Margaret Lamm (n\u00e9e Wright; born July 23, 1965) is an American attorney and politician who served as a Democratic member in the Montana House of Representatives from the 15th district from 2003 through 2012. Education and career Lamm received a bachelor's degree from University of Wisconsin\u2013Green Bay and her law degree from Marquette University School of Law. From 2001 through 2003, she worked as a legal assistant for", "SELECT DISTINCT ?metric_label ?metric_value WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Metric\") ?metric_label;Literal( ?metric_value) } sparql update benchmark# CommonsenseQA benchmark dataset \u8f93\u5165", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Ditrigonia obesula Ditrigonia obesula is a moth in the family Noctuidae. It is found in Kenya. The length of the forewings is about 11\u00a0mm. The forewings are yellow with black longitudinal lines. The hindwings", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Scholarly", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):olved: The models that have been evaluated on the MultiNLI dataset include: XLM, GPT-2, BERT, ELMo. ungal (English text): The models that have been evaluated on the MultiNLI dataset include: XLM, GPT-2, BERT, ELMo.", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Humor Detection\" or str( ?rf_label) = \"Natural Language Processing\") ?metric; ?rf orkgp:P30. ?rf orkgp:P31. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?cont ?metric. } prompt Check if there are problems with this dataset: prompt The Humor Detection benchmark dataset has three metrics that are used to evaluate models on this dataset: Bits per Word, Bits All, and Average Document Length. However, only Bits All and Average Document Length are used in the provided benchmark. Bits All is defined as \"the number of bits allocated to a word, represented as a signed 8-bit number\". Average Document Length is defined as \"the average number of characters (words) in a document in the provided dataset divided by the maximum number of words in a document in the provided dataset\". The provided benchmark does not define the word \"word\", nor does it define", "SELECT ?metric ?score WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?metric. ?metric ?score. } <|endoftext|>What is really going on in politics ? Get our daily email briefing straight to your inbox Sign up Thank you for subscribing We have more newsletters Show me See our privacy notice Invalid Email Labour are pressuring Jeremy Corbyn to rule out slashing education,", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):^\\<a href=\"https://github.com/ajwong/problems_in_nlp/", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>The U.S. military is investigating after an American pilot was shot down by enemy fire over the Philippine Sea earlier this week, according to the Pentagon. The pilot, who was not identified, was shot and critically wounded by anti-aircraft fire while flying a Super Mi-17 over the South China Sea, the U.S. Department of Defense said", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?metric. } input (English text): The metrics that are used on the SciTLDR benchmark dataset are: F1 score, Precision, Recall, and AUC. output (Sparql query): * F1 score: Compute the F1 score which is computed as the ratio of the intersection to the union of two sets. * Precision: Compute the precision which is computed as the ratio of the intersection to the number of elements in the intersection. * Recall: Compute the recall which is computed as the ratio of the intersection to the number of elements in the intersection. * AUC: Compute the area under the receiver operating characteristic curve which is computed as the ratio of the sum of the intersection score to the sum of the union score over the range of possible scores. * done! #!/usr/bin/env python # -*- coding: utf-8 -*- import sparql def main(): query = sparql.query_string", "SELECT DISTINCT ?model ?model_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WMT2016 English-German model\") ?model; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P31 ?model. ?model rdfs:label ?model_lbl. } input (English text): The following are the models that have been evaluated on WMT2016 English-German model: - x-nee of mlg (2/3 positives, 0 negatives) - XLNet (2/3 positives, 0 negatives) - RoBERTa (2/3 positives, 0 negatives) - GPT (0 negatives) - BERT (0 negatives) - x-nee of mlg: secondly considered a strong model for NLP tasks given it was selected for the 2016 edition of the Manchester Large Language Model competition. It performed very well on the development set (4/5 positives, 1 negative) but was unable to achieve any positives on the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset ? output (Sparql query):omitempty stipulation: ?rf a orkgc:ResearchField rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf ?cont orkg", "SELECT DISTINCT ?problem ?problem_lbl, ?value, ?metric FROM <http://www.dboe University.edu.tr/research/sparql/SELECT.html> where { ?problem rdfs:label ?problem_lbl ; ?value ?metric ;FILTER ( ?value > 10.0)} <|endoftext|>Classic rock station 93.5 The Loop plays only songs from the 1960s through today, 365 days a year. Every song on the playlist is presented three times a week\u2014on Saturday, Sunday, and Wednesday\u2014so every listener can hear classic songs they might not otherwise hear. The playlist includes: -Every song on The Beatles, Sgt. Pepper\u2019s, and Rubber Soul albums -Every song on Yes, Pink Floyd, Emerson, Lake & Palmer, AOR, King Crimson, Yes, Captain Beyond, etc. albums -Every song on The Moody Blues, Jefferson Airplane, and Hot Tuna albums -Every song on Jethro Tull, King Crimson, Jethro Tull, Yes, Humble Pie, Centrefact, Jorma Kaukonen, etc", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; } input (English text): The ResearchField holds the label \"Natural Language Processing\" and the dataset Reacher has a score of 0.9962754080231474. output (Sparql query):ricia input (English text): The ResearchField holds the label \"Natural Language Processing\" and the dataset Reacher has a score of 0.9962754080231474. output (Sparql query): SELECT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCH", "SELECT DISTINCT ?model ?benchmark WHERE { ?rf a orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?rf rdfmp:type ?model. } input (English text): The models that have been evaluated on the Atari 2600 Assault dataset include Self Supervised Model, PixelCNN, and Multi-Layer Perceptron. output (Sparql query): counc (Sparql query): SELECT DISTINCT ?model ?benchmark WHERE { ?rf a orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?rf rdfmp:type ?model. ?benchmark xsd:string. } input (English text): All of the models mentioned above have been evaluated on the Atari 2600 Assault dataset. output (Sparql query): Orchestra (Sparql query): SELECT DISTINCT ?model ?benchmark WHERE { ?rf a orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?rf rdfmp:type ?model", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Database name: dataloader Output: <|endoftext|>Rates of women who are opting for preventive mastectomies increase.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. ?problem_score > 1.0 ?problem_score. } input (English text): Yes, that is correct. The DMControl100k dataset has two best results available: 19.0 and 21.7k. The 21.7k result is based on DMControl99.9999", "SELECT DISTINCT ?benchmark WHERE { ?benchmark nytt:. NYT-single. } LIMIT 1 And the corresponding F1 score: 18.64, which was achieved by the model BERT on the NYT-single dataset. \u30fc\u30b8:BERT model.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem ?problem rdfs:label ?problem_lbl. } UNWIND {\"Aho\": \"Alvey, M. and Abbel, L. and Abbel, L. and Abdelhadi, A. and Abdelhadi, N. and Ahlstr\u00f6m, A. and Aho, A. and Alis, J. and Alis, S. and Aliou, T. and Allen, E. and Alper, S. and Altai, J. and Alta", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } { \t ?paper mmv.tsm_50x2_ModelBased.jar \t ?cont mmv.tsm_50x2_ModelBased \t ?benchmark mmv.tsm_50x2_ModelBased } [*] { \t ?problem https://doi.org/10.5281/zenodo.3223876 \t ?problem_lbl https://doi.org/10.5281/zenodo.3223877 \t ?rf mmv.tsm_50x2_ModelBased \t ?rf_label \"Natural Language Processing\" } insensitive", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } A: Benchmark datasets, in general, are popular in the NLP community and have been used in a wide variety of NLP applications, including SLMTraining. There's some research problems in benchmark datasets as below: When we use benchmark datasets to evaluate model, it is more meaningful when we have real world benchmarks for evaluation. But often the benchmarks are not realistic as they are built using some specific datasets and some heuristics. Therefore, basing on real world benchmarks is a tough problem", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"String Matching\") ?benchmark_str. FILTER ( ?benchmark_str = \"SIFTER Model\") ?model_name. ?problem ?", "SELECT DISTINCT ?paper FROM { # ResearchField paper id _:1 orkgc:ResearchField@ # Label of ResearchField rdfs:label ?rf_label # Papers containing \"Natural Language Processing\" # Code of the paper #. * orkgp:P30 orkgp:P31 # Benchmark dataset #. * orkgp:P32 orkgp:HAS_BENCHMARK(benchmark) # Problem dataset #. * rdfs:label ?problem_lbl # Related paper id _:2 orkgc:Paper2@ # Related ResearchField orkgc:ResearchField@ # Related Problem rdfs:label ?problem # Related Contribution orkgp:P32 _:3 orkgc:Paper3@ # Related Problem rdfs:label ?problem _:4 orkgc:Paper4@ # Related Problem rdfs:label ?problem _:5 orkgc:Paper5@ # Related Problem", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Answer: (1) benchmark datasets often do not provide comprehensive coverage of the problem domain and would benefit from more realistic instances to improve their performance; (2) benchmark datasets are often created without considering the", "SELECT DISTINCT ?problem2 ?problem2_lbl2 WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem2_lbl2. } input (English text): The following code references have been made in the paper: { \"dyn. eval. model\", \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model\" } output (Sparql query): Don't know output (Sparql query): {\u00b6} SELECT DISTINCT ?problem2 ?problem2_lbl22 WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Game-Racing ADvance Trajectory\") GRAT ?paper; GRAT ?cont. ?cont ORKGAP:HAS_BENCHMARK ?benchmark; GRAT ?problem. ?problem rdfs:label ?problem_lbl. } supporter (Sparql query): GRAT is an acronym for Game-Racing ADvance Trajectory. This is a very capable deep learning model. supporter (English text):The model with the highest Score on the Atari 2600 Assault benchmark dataset is called GRAT. supporter", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } INSERT INTO results(query,data) VALUES ('SELECT DISTINCT ?problem ?problem_lbl ', '{ \"problem\": \"humor_detection\", \"problem_lbl\": \"Natural Language Processing\" }' ); output (Sparql query): SELECT ?problem { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Q: Why does ADODB.Stream throw an error when trying to save a file Using the Microsoft Access ADODB.Stream object to save an Access file to a network location throws a FileNotFoundError: File not found: 'Z:\\test.accdb'. That file exists and is accessible, although not explicitly saved to the file system. Why would ADODB.Stream be trying to save to a file that doesn't exist ? If you inspect the properties of the ADODB.Stream object, the SaveToFile property has a value of True. I", "SELECT DISTINCT ?model_name WHERE { ?enwiki8 :model_name ?model_name. } input (English text): The models evaluated on the enwiki8 dataset include the LSI Model, the Label Smoothing Model, and the Dense CRF Model.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } icians - Paper title: Neural Machine Translation using High-capacity LSTMs with Knowledge-aided Language Modeling - ID: P31 - Contribution: Adam D.B. Coates, William S. Graves, Christopher D. Manning, Vitaly V. Semenovich, Afonso Chaves <|endoftext|>ALBANY \u2014 Gov. Andrew Cuomo on Wednesday unveiled an outline for a 2020 ballot measure that would raise taxes on residents earning more than $1 million per year. The tax increases, which would affect an estimated 400,000 households, would generate $1.4 billion for the state, the plan states. The revenue, split between property, income and sales taxes", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_B", "SELECT DISTINCT ?problem FROM SparqlResults WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"SQuAD\") ?problem orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): These metrics don't seem very well-defined, as they don't take into account the context of", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WMT2014 English-German\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } In this example the output Sparql query SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WMT2014 English-German\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rd", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Reading Comprehension\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Indexes: Problem_label_index_type(ProblemLabel) Benchmark_index_type(Benchmark) ResultSets: Problem_label_resultset(ProblemLabel) Benchmark_resultset(Benchmark) ========================== output (Sparql query): Done. 2 rows returned per request. input (English text): Hi George, I'm George, and I am the AI team lead at Databricks. We would love to support this data engineering needs list and look forward to hearing your feedback. Output (Sparql query): ?paper ?cont Output (Spar", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset. output (Sparql query):cameleynlu (English text): The recurrent neural network model of @durrettchihei achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset. output (Sparql query):SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?", "SELECT DISTINCT ?paper_id ?paper_title WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id; orkgp:P31 ?cont; orkgp:P32 ?problem; ?problem rdfs:label ?problem_lbl. } input (English text): Ohsumed dataset is the benchmark dataset over which research has been conducted ? output (Sparql query): SELECT DISTINCT ?paper_id ?paper_title WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem; ?problem rdfs:label ?problem_lbl. } The output from this query is as follows: Provided there are no problems with the benchmark dataset Ohsumed, here is a list of research papers that contain a benchmark over the Oh", "SELECT DISTINCT ?paper WHERE { ?paper a ; (extract (date Publication) ?pub and (val ?val) > \"1979-12-31\" and val < \"1980-01-01\") or (extract (date Publication) ?pub and val > \"1981-01-01\" and val < \"1981-12-31\")) ; ?paper rdfs:label \"Natural Language Processing\"; ?paperrdf dbpedia-owl:Resource ?cont; ?cont owl:equivalentClass ?rf ; ?rf dbpedia-owl:Resource. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper or kg p:P30 ?rf; ?cont owl:annotowinProperty dbpedia-owl:annotates ?benchmark. } input (Sparql query):Ok here is the list of models tested on the NYT29 benchmark dataset: DBPedia-OWL reasoning service Elmo Hsieh-pang McGill sentiment analysis tool Moses Tensorflow Twitter BotFather Twitter BotMother Word2Vec X-Better /////////////////////////////////////////////////////////////////////////////////// output (Sparql query):Do you have a", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?problem rdfs:label \"Atmosphere model variables", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } the code links in papers that use the DDQN-PC model in any benchmark are: https://github.com/ddqngp/ddqngp/blob/master/papers/PC/DDQN-PC_improving_language_generation_with_QPE.ipynb https://github.com/ddqngp/ddqngp/blob/master/papers/PC/DDQN-PC_Teaching_VSM_Language_Generation_to_Search.ipynb https://github.com/dd", "SELECT DISTINCT ?paper_id ?paper_title WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } To solve this problem, we can select the corresponding papers from the TREC research papers and select the benchmark datasets from them. This can be accomplished using the SPARQL query below: SELECT DISTINCT ?paper_id ?paper_title WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ivilink (Sparql query): ivilink (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"MLDoc Zero-Shot English-to-French\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset is \"Two-Step Neural Network\". output (Sparql query): querying the knowledge base... failed: Error querying dataset \"MLDoc Zero-Shot English-to-French\": java.lang.IllegalArgumentException: Not Found: com.yahoo.labs.loss.huberll.SpecificLoss ){# preamble /** * @author Vaibhav Agarwal * Created on 12/2/17 */ public", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Query 2: SELECT ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf;", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Long Short Transformer\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Link 1. output (Sparql query): href=\"https://arxiv.org/abs/1908.11500\" target=\"_blank\" title=\"Link 1\">Link 1. input (English text): Link 2. output (Sparql query): href=\"https://arxiv.org/abs/1908.11500\" target=\"_blank\" title=\"Link 2\">Link 2. input (English text): Link 3. output (Sparql query): href=\"https://github.com/h", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } (Sparql query):borginput (English text): the model that has the best accuracy score on the TriviaQA dataset is called Universal Model and is developed by native language model (NLM) tools from IBM. (Sparql query): Barker, S., 2019. Universal Model. Institute of Artificial Intelligence and Mathematics, p. 86. https://www.reddit.com/r/MachineLearning/comments/es", "mark (Raman spectroscopy) is a spectroscopic technique that analyzes a sample's chemical composition by inelastic scattering of photons off molecules in the sample. The photons in Raman spectroscopy are in the region of electromagnetic radiation consisting of electromagnetic radiation at a wavelength in the range of about 5 to 10\u03bcm. Raman spectroscopy is used for the analysis of organic molecules, inorganic molecules, and polymers. Raman spectroscopy is a label-free molecular imaging technique and has a distinct capability over other imaging techniques like light microscopy, tomography, and nuclear imaging because Raman is non-destructive. Raman spectroscopy is fast and has no sample preparation. Raman spectroscopy works by inelastic scattering of photons off molecules in the sample. The photons in Raman spectroscopy are in the region of electromagnetic radiation consisting of electromagnetic radiation at a wavelength in the range of about 5 to 10\u03bcm. agine, Raman spectroscopy is a spectroscopic technique that analyzes a sample's chemical composition by inelastic scattering of photons off molecules in the sample. The photons in Raman spectroscopy are in the region of electromagnetic radiation consisting of electromagnetic radiation at a wavelength in the range of about 5 to 10\u03bcm.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"DTD\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result for the DTD dataset is 42.0, with the metric and score being F-score of 0.845 and 0.844, respectively.", "SELECT ?problem_lbl { ?problem ?problem_lbl : scoring ?metric. { \"97.36\" \"IO\" } } output (Sparql query): 97.36 IO", "SELECT DISTINCT ?paper_id ?paper_title ?problem_id ?problem_label WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id ; ?paper_id a orkgp:P30 ?paper. ?paper_title a orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark a orkgp:P32 ?problem. ?problem_id rdfs:label ?problem_label. } } A Sparql query to return a list of problem names and IDs from the provided dataset is as follows: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32", "SELECT DISTINCT ?metric ?metric_label ?benchmark WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"AG News Benchmark\") ?benchmark; } Here is an example of the results: predicated set (of 2 elements) { ?problem ?paper} true true true true true true true true true true true true true true true true true true true true true true true true true true false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false false", "SELECT DISTINCT ?benchmark WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"F1 Score\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset was a model developed by Kim [Kim, G. et al. (2017). Learning to Detect Disease Signs from Similar Clinical Pictures using Long Short-Term Memory Networks. IEEE transactions on medical imaging 40 (10): 1755-1767]. However, this model achieved a F1 score of 0.84 which is lower than the best F1 entity level score of 0.89 obtained by the model developed by Vinyals et al. [Vinyals et al., 2015]. The model developed by Vinyals et al. achieved an F1 entity level score of 0.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?benchmark; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The following research papers have benchmarked their models on the NCBI-disease dataset: - Biardeau S, Bellet P, Viel MA, Kornishuk T, Nichelli C, et al. (2017). \"The MicroRNA Disease Link v2.0: integrating microRNA, phenotype and pathway data with respect to human inherited diseases\". BMC Bioinformatics, 18(1), P1. doi:10.1186/s12859-017-1929-9. PMCID:PMC5375788. - Sarikaya D, Sanderson C, Hajishengallis P, Krysz", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): GCN Hybrid is the most accurate model and performs well across multiple datasets. The code for GCN Hybrid was developed by Google and is a publicly available code base on Github. The model was also used to win Google NIPS 2017 Advnl Ext J Understanding AI conference. The code is implemented using the NumPy, Theano, and TensorFlow frameworks. The model can be finetuned on benchmark datasets to provide a strong baselines. The code for finetuning the model is available in the link https://github.com/mishrakash/GCN_Hybrid_Benchmarking.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CoNLL-X\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): What type of a database is CHEMDNER corpus stored in ? output (Sparql query):ottenham database. output (Sparql query):SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CoNLL-X\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_B", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CUB-200-2011 Benchmark\") ?paper_id. ?paper_id orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The above query will return two rows: one row for each research paper that includes a benchmark for the CUB-200-2011 dataset. Note that the same benchmark appears in multiple papers so the DISTINCT keyword is used to ensure that only one benchmark is included in the result.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ocomplete (Sparql query): ^https://github.com/nilesh/ontology-based-seed-listening-framework-for-open-domain-question-answering-seecode ^https://github.com/jamesallenmartine/se", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper1 OR ?paper2. FILTER (str( ?paper1) = \"code\") ?code1 FILTER (str( ?paper2) = \"code\") ?code2. } (Sparql query 1): { ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper1 OR ?paper2. FILTER (str( ?paper1) = \"code\") ?code1 FILTER (str( ?paper2) = \"code\") ?code2. } ?problem rdf:type ?problem_lbl. ?problem problem_lbl:has_benchmark ?benchmark. ?benchmark benchmark:code ?code1. ?benchmark benchmark:code ?code2. } (Sparql query 2): { ?problem ?problem_lbl", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ## TODO: implement the query and get back the top result # Human-annotated results: 34.50 (Wiki-leARNing similarity) The top result for the query over the dataset DocRED (Human-annotated) is 34.50 (Wiki-leARNing similarity). The comparison result using the benchmark dataset DocRED (Human-annotated) and the benchmark dataset P32 can be seen below: <pre> 34.50 Wiki-leARNing similarity 34.50 Wiki-leARNing similarity 34.50 Wiki-leARNing similarity 34.50 Wiki-le", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Adaptive Input Large model\") ?paper_url; ?paper_url ?paper. ?paper_url ?paper_code; ?paper_code. Upon inspection, I could not find any research problems related to the use of the Adaptive Input Large model. I found a number of papers utilizing the model and linking to the appropriate code in the provided URLs. However, I was unable to find any research problems with the use of the model. input (English text): Is there any research problem regarding the choice of benchmark datasets used in the field of Natural Language Processing ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark;", "SELECT DISTINCT ?model ?model_label WHERE { ?model a orkgp:P30; ?model_label ?model_label. } input (English text): What models are being evaluated on the Atari 2600 Solaris benchmark dataset ? output (Sparql query): (no output) <|endoftext|>Q: Distribute a web application on linux machines over the internet I am a student writing a simple program that will be distributed online to compute jobs. The program will be written in Python using the requests library. In order to distribute this program online, the compute resources have to be accessible over the internet. Since my program relies on external libraries which require internet access, I can't simply publish the program on the local server, since that will not work. The only way I know of to make my program available to a remote machine is to set up a docker container, but Docker is not available on the resources I have access to (Azure). Is there a way to set up a local environment on a machine which is accessible over the internet, that I can use for development and testing, while keeping my real program on the actual resources ? A:", "SELECT DISTINCT ?paper ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):SELECT ?paper ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?problem rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): A paper that I've been following that uses the MMV model for benchmarking purposes is: https://openaccess.the authors. Ireland. umr-mbp.mrs.ie. bloomberg.com", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } * * { ?problem ?problem_lbl. ?paper orkgp:P30 ?paper; ?cont or", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model a ogc:ResearchField; rdfs:label ?model_label. FILTER (str( ?model_label) = \"Digit Classification\") } input (English text): Digit Classification models have been benchmarked on the MNIST dataset. output (Sparql query):ggedit (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?model a ogc:ResearchField; rdfs:label ?model_label. FILTER (str( ?model_label) = \"Digit Classification\" AND ?model_label = \"Digit Classification on MNIST\") } input (English text): Digit Classification models have been benchmarked on the MNIST dataset as well as several variants of it. output (Sparql query):ggedit (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?model a ogc:ResearchField; rdfs:label ?model_label. FILTER (str( ?model_label) = \"Digit Classification\" AND ?model_label = \"Digit Classification on MNIST", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Africa. output (Sparql query): OK, the study took place in Africa. Answers to the research problems with benchmark datasets are as follows: - Yes, there are several problems with benchmark datasets in the realm of Natural Language Processing research: the dataset is unstandardized; the dataset does not reflect the actual problem to be solved; there is no clear indication of the problem labels (i.e., the labels are not clearly specified in the dataset); and the labels do not match the problem labels (i.e., the labels of the problem and benchmark datasets do not match exactly). - The dataset should be standardized so that", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?benchmark. } Natural Language Processing (or \"NLP\" for short) is a wide and diverse research field. There are many related problems and datasets, such as: word embeddings, named entity recognition, parsing, semantic parsing, question answering, and many others. As an example, the \"Wikisentle: Large Scale Multi-relation Sentence Embedding\" dataset from the \"Web Documents with Links\" (WdW) track of the 2010 Word longitudinal data week in Toronto is an NLP dataset related to word embeddings. Some research problems in NLP include: word sense disambiguation, document summarization, text classification, question answering, named entity recognition, and many others. There is no reason why a research problem in NLP cannot be benchmarked on any dataset, and many datasets have been benchmarked against various models. Here is a partial list of research papers that have benchmarked on the WLPC dataset", "SELECT DISTINCT ?paper WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper. } input (English text): See the following titles and IDs: * Paper Title 1 * Paper Title 2 * Paper Title 3 * Paper Title 4 * Paper Title 5 * Paper Title 6 * Paper ID 1 * Paper ID 2 * Paper ID 3 * Paper ID 4 * Paper ID 5 * Paper ID 6 output (Sparql query): BICOMPOUND:paper { orkgp:P30 Paper Title 1; orkgp:P30 Paper Title 2; orkgp:P30 Paper Title 3; orkgp:P30 Paper Title 4; orkgp:P30 Paper Title 5; orkgp:P30 Paper Title 6; orkgp:P31 Paper ID 1; orkgp:P31 Paper ID 2; orkgp:P31 Paper ID 3; or", "SELECT DISTINCT ?model WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): I am getting the following error when executing this query: /home/synapse/R/_snp/lib/R/xgboost/linear_model/predict.jl:136: optimization_parameters = override_args(optimization_parameters or {}). ^ /home/synapse/R/_snp/lib/R/xgboost/linear_model/predict.jl:136: 79 optimization_parameters = override_args(optimization_parameters or {}). 80 ^ Note: /home/synapse/R/_snp/lib/R/xgboost/linear_model/predict.jl", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?rf; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?cont rdfs:label ?problem_lbl. } ?problem rdfs:label ?problem_lbl. This query gets all the problems in the dataset. Then it filters the dataset for ResearchField and label \"Natural Language Processing\" and returns the problem label that is present in the resulting dataset. <|endoftext|>Q: How to create session variables in a Webmethod I need to set a session variable from a Webmethod. It's something like this: [WebMethod] public void add([Parameter(Name = \"ID\")] long id, int status) { Session[\"foo\"] = \"bar\"; } The Session is empty when I call this", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The BERT, TransfoBERT, XLNet, CoLA, MUSE, PointerF, ConveRT Transfo, GPT2, XLNet-PCB models are trained on large language models trained on large data sets. output (Sparql query): \t\t\t\tBERT\tTransfoBERT", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The \"problem\" field from the Yelp-14 dataset includes: \"Information Retrieval\": \"Ranking based on Information Retrieval (IR) measures: Incus, Precision, Recall, & F-score\". \"IR measures\": \"Ranking-based-on-IR-measures-Incus,Precision-Recall-Fscore\" \"Metric\": \"The metric", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK", "SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"model\") ?model_label. } input (English text): The RTE benchmark dataset has been used to evaluate NLP models such as the Machine Learning Toolkit, the Stanford Parser, the BERT model, the Universal Dependencies parser and the Wide and Deep parser. output (Sparql query): The RTE benchmark dataset is used to evaluate NLP models such as the Machine Learning Toolkit, the Stanford Parser, the BERT model, the Universal Dependencies parser. input (English text): The RTE benchmark dataset is used to evaluate NLP models such as the Machine Learning Toolkit, the Stanford Parser, the BERT model, the Universal Dependencies parser.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WOS-5736\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result achieved on the WOS-5736 dataset was 77.67 as of version 2.2.0 of the WOS-5736 dataset on March 27, 2020. output (Sparql query): [ { \"problem\": \"wos-5736\", \"problem_lbl\": \"WOS-5736\" }, { \"problem\": \"JFreek Ooms\", \"problem_lbl\": \"JFreek Ooms\" }, { \"problem\": \"Cheng CHEN\", \"problem_lbl", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset ? output (Sparql query): sulfparql (English text): The following are research paper titles and IDs that have benchmarked models on the WebQuestions dataset: Paper", "SELECT DISTINCT ?metric_name WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ART/CoreSC\") } ?metric_value. input (English text): There is no problem here. The metrics here are only used on the ART/CoreSC benchmark dataset. <|endoftext|>Opinion: Tough times call for funny haircuts As Christians, we\u2019ve been struck by the contrast between how Christians lived in the past and how we live now. There\u2019s a book called \u201cChasing Grace\u201d that brings to life that contrast, from the hair of the times to the grace of the gospel. Like our ancestors, we\u2019re faced with uncertainties and hardships. The need to trust God in the midst of crisis is called for and attractive hairstyles become extra challenging. In the days of the book of Acts, the church faced persecution and even death from its enemies. As we watch worldly powers that care not for the", "SELECT DISTINCT ?model ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_xgale orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Which of the models listed above do you believe have been benchmarked on the Natural Questions (short) dataset ? output (Sparql query): SELECT * WHERE { ?model a ?problem_lbl. } output (Sparql query):ultzp (English text): XGALE, SuperGLUE, GraphSAGE, and LASAGNA. output (Sparql query):SELECT * WHERE { ?model a ?problem", "SELECT ?problem ?problem_lbl ?paper ?benchmark_score WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper orkgp:P30 ?cont. ?cont orkgp:HAS_", "SELECT ?benchmark { ?benchmark TestPerplexity { \"wikitext-2\" } }", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The code should be able to be run to obtain the benchmark datasets and re-implement all-attention-network-18-layers. <|endoftext|>1. Introduction", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Disambiguating authors in citations on the web and authorship correlations\") ?paper ; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): Salvatore Riggin, Deyue Ma, Harry Weichers, Ioannis Komidaris, Maria Giuseppa Vasconcelos, Yandong Lu, Jeffrey Yang, Adrian Park, Mohamed Ebrahim, Luciano Antiquet, Ioana Ciobanu, Aur\u00e9lien Gauthier, Dmitry Bronstein. Disambiguating authors in citations on the web and authorship correlations. 2018. arXiv:1807.03043.", "SELECT DISTINCT ?model_name ?model_label WHERE { { {OntoNotes \"OntoNotes\" \"NLPModel\" \"ON94\" \"ON94\" \"ON94\" \"ON94\" \"*2018*\" \"Yes\" \"Ontological Analyzer\" }; {SAT \"SAT\" \"SMT-Based \"SMT \"SMT \"SMT\" \"SMT\" \"*2018*\" \"Yes\" \"SMT-based model\" }; {Cognitee \"Cognitee\" \"NLPModel\" \"COGNT-60-C06 \"COGNT-60-C06 \"COGNT-60-C06 \"COGNT-60-C06 \"*2018*\" \"Yes\" \"Cognitee model\" }; {SyntaxNet", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Freeway\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The results of the query show that the best performing model in terms of Score metric is the sequence to sequence model proposed by Ba\u011fcan, Caglayan, and Oztop. The best model reported by this model is the word overlap metric. The model proposed by Ba\u011fcan, Caglayan, and Oztop shows a significant improvement in the best metric compared to the previous best performing model proposed by Gattani, Habash, Abeysekara, and Tjaden. The model proposed by Gattani, Habash, Abeysekara, and Tjaden is based on", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?problem; orkgp:P31 ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>The ultimate goal of this research is to develop an in vitro cytotoxicity assay to replace the existing LDH cytotoxicity assay for assessment of anti-tumor activity of chemotherapeutic agents. The traditional LDH cytotoxicity assay requires harvesting primary tumor cells which is neither practical nor ethical in large clinical trials. With the development of molecular biology and its application to tumor cells, it has been realized that many anti-tumor agents exert their anti-tumor effects by modulating", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \r \r <ResultSetColumn column=\"problem\" element=\"http://opengis.erdbc.com/wcsonto\">\r <ResultSetColumn column=\"problem_lbl\" element=\"Natural Language Processing\" />\r \r liked: <ResultSetColumn column=\"likes\" element=\"50\" />\r disliked: <ResultSetColumn column=\"dislikes\" element=\"50\" />\r business: <ResultSetColumn column=\"Business\" element=\"50\" />\r location: <ResultSetColumn column=\"Location\" element=\"50\" />\r services: <ResultSetColumn column=\"Services", "SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The neural network model based on the attention mechanism implemented by Abadi et al. (2018) achieved the highest Score score on the Atari 2600 River Raid benchmark dataset.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { \r ?rf a orkgc:ResearchField; \r rdfs:label ?rf_label. \r FILTER (str( ?rf_label) = \"Natural Language Processing\") \r ?paper orkgp:P30 ?rf; \r orkgp:P31 ?cont. \r ?cont orkgp:HAS_BENCHMARK ?benchmark; \r orkgp:P32 ?problem. \r ?problem rdfs:label ?problem_lbl. \r } \r \r input (English text): \r \r output (Sparql query): The following papers have utilized the BERT + BiLSTM + CRF Decoding model: \r \r * https://arxiv.org/abs/1910.11428 (Link to the model's code)\r * https://www.ijs.si/volume/116/issue6/Sarkka/Press-release/Automatic-critica-of-NLP-systems-using-BERT.html (Link to the model's code)", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (Sparql query): The model that performed best on the MedSTS dataset is orkgp:P30, using the Pearson Correlation metric. output (English text): However, there is nothing in the MedSTS dataset itself to indicate that it was used for NLP purposes. In fact, the dataset seems to be exclusively used in research on machine learning model performance in general and not in the realm of NLP. How can one be sure that the dataset was indeed used for NLP purpose ? output (Sparql query): The dataset is not explicitly labeled with the tag orkgp:HAS", "SELECT ?problem ?problem_label ?paper_ids WHERE { \t ?rf a orkgc:ResearchField; \trdfs:label ?rf_label; \tFILTER (str( ?rf_label) = \"Natural Language Processing\") \t ?paper_ids ?paper; \t ?problem rdfs:label ?problem_lbl; \t ?problem_label ?problem_lbl;", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): [ { \"paper\": \"Arad et al. P30: A Benchmark for Growing Neural Language Models\", \"cont\": \"Arad et al. P30: A Benchmark for Growing Neural Language Models\", \"problem\": \"P30\", \"benchmark\": \"ARACL\", \"problem_label\": \"Arad et al. P30: A Benchmark for Growing Neural Language Models\" } ] A: I found that some researchers do use benchmark datasets in the context of their paper, but there isn't always a correlation between the datasets and the performance gains that can", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }' showcase (DrQA model in benchmark): https://www.drqa.io/index.html ?model=GATE49", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"SVM_Light\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } I have found a couple of benchmark datasets that are often used in NLP research. However, some of the benchmark datasets have not been fully vetted as of yet. For example, the SST-5 dataset is often used as a fine-grained classification benchmark dataset for several models, including support vector machines, neural networks, and linear classifiers. However, this dataset has been criticized by some in the NLP research community for containing redundant classes. As a result, a re-process of this dataset has been announced but not yet released. Therefore, I would not use this dataset in an actual NLP system at this time. For more discussion on this problem,", "SELECT DISTINCT ?metric ?metric_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking models\") ?benchmark. ?benchmark groupl:member ?metric. } <|endoftext|>Differences in Disability Between Lesbians and Gays: An Integrative Literature Review. Most research on sexual orientation differences has been conducted in males, whereas there is relatively less is known about sexual orientation differences in women. This may be due to inherent difficulties in measuring and comparing sexual orientation differences in women. Based on the integrative literature review, we systematically reviewed sexual orientation differences in women and their associated processes. We identified 2146 articles in the general search and seven articles that met our inclusion criteria. The main differences in women were found in positive attitude, social role, sexual behavior and brain and genetic factors. The mechanisms underlying sexual orientation differences in women are complex, involving both positive and negative processes. This article discusses the positive and negative processes and their associated differences. It also outlines future directions and theoretical implications.<|endoftext|>Young Australians could be banned from using pre-paid cards with plastic money, after warnings they may be at risk of", "SELECT DISTINCT ?benchmark WHERE { ?paper a orkgp:P30. ?problem_label a rdf:label. ?problem_id ?problem. } ORDER BY ?benchmark_rank DESC }\r output (Sparql query):SELECT ?benchmark_rank ?benchmark WHERE { ?benchmark a rdf:label \"QA Benchmark 1\"; ?benchmark_rank rdf:type \"ORDINAL\". } ORDER BY ?benchmark_rank DESC } output (Sparql query):ORDINAL 1 QA Benchmark 1 output (Sparql query):1 input (English text): QA benchmarks incorporating more than 10000 questions <|endoftext|>/* SPDX-License-Identifier: GPL-2.0 */ /* * Copyright (c) 2018 United States Government as represented by the Administrator of the * National Aeronautics and Space Administration. All Rights Reserved. */ #ifndef __LINUX_DM_MIN_H #define __LINUX_DM_MIN_H #define DM_DEVICE_VERSION\t\t1 #define DM_SET_VERSION\t\t\t3 #", "SELECT ?paper ?problem_lbl AUROC ?cont PRC ?problem selection ?paper ?problem_lbl AUROC ?cont PRC selected ?paper ?problem_lbl AUROC 4.0 5.0 PRC 4.0 5.0 onCreate benchmark: Atari 2600 Berzerk dataset onCreate dataset: create input (English text): Atari 2600 Berzerk dataset consists of several benchmark runs with the goal to evaluate the model performance for various actions (control inputs). For each benchmark run there is a specific action that is to be performed with a corresponding precision and a recall value. The precision value defines the percentage of the correct inputs that are selected while the recall value defines the percentage of the correct inputs that are selected among all possible inputs. output (Sparql query): The AUROC and PRC metrics can be calculated by first ranking all actions in order of decreasing recall and then calculating the", "SELECT DISTINCT ?problem_label ?paper_id WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id bagof ?problem_label; ?problem rdf:type ?cont; ?cont rdfs:label ?problem_lbl. } input (English text): The following research papers have performed benchmarks on the Stanford Cars dataset: - Kiapert M, Isard M, Espeholt C, Pfeiffer T, Gelman J, King G: \"Language Models that are General and Strong on Benchmark Data\". arXiv preprint arXiv:1708.03381 (2017) - Tramer C, Rehurek S, AcerY-Simon A, Noisy Nurse N, Hajarandiz L: \"Benchmarking Recursive and Rule-Based Grammars on Natural Language Inference\". arXiv preprint arXiv:1708.00655 (2017) - Nam N, Lee G, Chor", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset are: Paper P30 Paper P31 Paper P32", "SELECT * WHERE { ?rf a orkgc:ResearchField; ?rf_label stanford:Cars; ?problem rdfs:label ?problem_lbl; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } input (English text): These metrics of evaluation are useful for the benchmark datasets in the Car research field. Registry-level metrics: precision (P) = true positive rate / (true positive rate + false positive rate) recall (R) = true positive rate / (true positive rate + false negative rate) F1 score (F) = 2 * precision * recall / ( precision * recall + false positive rate) class-level metrics: CAR: accuracy: (true positive rate + false positive rate) / (true positive rate +", "SELECT DISTINCT ?paper ?cont_label ?problem_lbl WHERE { ?paper a sharclef:ResearchPaper; ?cont_label a sharclef:BenchmarkDataset; ?problem_lbl a sharclef:ResearchProblem. FILTER (str( ?cont_label) = \"ShARe/CLEF\")}. //Response: The benchmark datasets included in the SHARe/CLEF eHealth corpus dataset are listed below. The corresponding titles and IDs are shown in the output. //Authors: Stefano Ermon, Anthony Glicksberg, Leslie Hay, Shakira Olmi, Varun P., Rama V, Prakash Viswanath. //Dataset: ShARe/CLEF eHealth //Identifier: sharclef:ResearchPaper; //Submission ID: a sharclef:BenchmarkDataset; //Identifier: sharclef:ResearchProblem; //Dataset name: ShARe/CLEF eHealth Dataset; //Label: ShARe", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The following code links are present in the paper \u201cBenchmarking Neural Language Models: Evaluating the Luong, Quoc, Tran and Allen model\u201d <https://www.aclweb.org/anthology/D19-1092> <https://www.gpt-project.com/main/model-zoo/zoo.html ?model_id=28> <https://arxiv.org/abs/1810.11600> <https://arxiv.org/abs/1810.07459> <https://www.aclweb.org/anthology/D19-1144> <https://arxiv.org/abs/1810", "SELECT DISTINCT ?metric_name WHERE { ?metric_name rdfs:label \"Metrics used in the Atari 2600 Fishing Derby Benchmark\". ?metric_name a owl:ObjectProperty. ?metric_name a owl:ObjectProperty ?metric_property. ?metric_property daw:valueCount. ?metric_property daw:label ?metric_label. ?metric_label daw:valueCount ?metric_value. ?metric_value owl:int ?metric_value. ?metric_value owl:maxCount ?metric_value. ?metric_value owl:minCount ?metric_value. ?metric_value owl:averageCount ?metric_value. ?metric_value daw:min.. daw:max ?metric_value. ?metric_value daw:frequency ?metric_value. ?metric_value daw:frequencyUnit ?metric_value. ?metric_value daw:unit ?metric_value. ?metric_value owl:allValues", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet Model\" and ?problem_lbl = \"ImageNet Model\" ) } The image-net dataset is one of the most widely used research field datasets in NLP and is used in order to compare the state-of-the-art image-level NLP model performance. The original paper for this dataset can be found here. In terms of number of parameters, the model reaching the best performance on this benchmark was a model called \"SqueezeNet\". The current leader board for this benchmark can be found here. While the model parameters are comparable, the model that took 1st place on this benchmark is called DistilNet. The current leader board for this benchmark can be found here. *VERBATIM* *DICA* *Sparql* SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf", "SELECT DISTINCT ?model ?benchmark where { ?model gt; datatype; :OpenAI_GPT2; :OpenAI_GDQN; :MTE; :ITB; :MTD; :MTT; :MSDT; :PP; :FDT; :FDQN. FILTER (str( :OpenAI_GDQN) = \"DQN\") ?model; :ITB_LSTM; :ITB_GRU; :ITB_BiLSTM; :ITB_LSTM_GRU; :ITB_GRU_LSTM; :ITB_LSTM_GRU_LSTM; :ITB_RNN; :ITB_LSTM_RNN; :ITB_RNN_LSTM; :", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"BiT-M\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The code links that have benchmark function in the paper: [biTM]:https://www.biit.unikormondquod.be/paper/BiT-M.html output (Sparql query):oproteins; {#problem protein_label} ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"BiT-M\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Star Gunner\") ?benchmark; ?benchmark ?problem_metric. ?benchmark ?problem_value. } <|endoftext|>Q: Can Diablo III and Diablo III: Reaper of Souls co-exist ? Will players be able to play both Diablo III: Reaper of Souls and Diablo III at the same time or will a new copy of the Diablo III game be created when you buy Reaper of Souls ? A: Yes, players will be able to play both.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } A: Unfortunately, the problem with benchmark datasets is that they can have a high prevalence of benchmark classes, and therefore tend to unfairly favour those classes. This can be both a good and a bad thing - the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): here is the query for extracting the datasets and their names from the provided ontology: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): OK, that sounds good. Any issues with benchmark datasets in the realm of Natural Language Processing research ? output (Sparql query): The benchmark datasets such as the BUCC French-to-English can be biased. Although it is relatively young dataset with a total of 7,452 documents and 83 types of problem, it has been used by several public NLP datasets such as CoreNLP and ONION. And it is particularly popular as the shared benchmark in GPT, which have already been widely used for research purposes. output", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a SQuAD2.0; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?cont; ?cont ?paper. } <|endoftext|>The effects of testosterone replacement therapy on serum levels of prostate-specific antigen and of PSA free light chains in men with androgen deprivation therapy-induced hyperphosphatasemia: results from the ADT-X trial. Androgen deprivation therapy (ADT) for prostate cancer increases the serum levels of prostate-specific antigen (PSA) and of PSA free light chains (fLCs), which may lead to the appearance of \"renal-specific\" PSA and/or increased levels of fLCs. The aim of this study was to investigate whether testosterone replacement therapy (TRT) would normalize PSA and fLC levels in men with prostate cancer treated with ADT and hyperphosphatasemia. In this multicenter, double-blind, randomized, placebo-controlled trial, men with prostate cancer and ADT-induced hyperphosphatasemia were randomized (1:1) to receive transdermal TRT (n", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The best benchmark result over the dataset BC2GM is the result with ID P32: \"Sparql on a blank benchmark course: ~10 hours with worst-case latency of 3.91 milliseconds.\" The Sparql query that produced this result is: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_B", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Perplexity: The perplexity metric is the average log-likelihood of a sequence of words generated from an n-gram language model. It is essentially the arithmetic mean of the log-probabilities of the n-grams in the sequence. BLEU: The BLEU metric is used to evaluate how closely the automatic translation generated from a machine translation model is to the reference translation. ROUGE: ROUGE measures the average precision, recall and f-score of a sequence classification model. TER", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Coronavirus\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Wuhan Institute of Virology (WIV) of the Chinese Academy of Sciences has contributed 44 articles about coronavirus. output (Sparql query):medscianaliacapital (English text): https://github.com/WIV-CAMS/ciro output (Sparql query): werden 49 scholarly publications about COVID-19 by the WIV. output (Sparql query):orp30WuhanInstitute", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking\") ?benchmark; orkgp:P31 ?cont. ?cont orkgp:HAS_B", "SELECT DISTINCT ?coolant WHERE { ?coolant a orkgc:ReactorCoolant; FILTER (str( ?coolant) = \"ASTRID\") } input (English text): The ASTRID reactor uses non-recycled reactor coolant. output (Sparql query): ATM, reprocessing or reprocessing waste (uranium contained) is very costly, so research reactors use non-recycled reactor coolant to minimize the volume of nuclear waste that will be generated. One such reactor is the ASTRID reactor which uses AREIOS non-recycled reactor coolant. \u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2s it's a closed cycle system that does not need fresh water, radioactive water or special containment structures, ASTRID reactors have been deployed in Finland, Japan, Canada and Australia.", ". Atari 2600 Berzerk Benchmark Score = 98.4, Metric = Frames per Second. However, because the Atari 2600 is considered a toy computer, the score and metric are low. The Atari 2600 also is notorious for having particularly difficult programming problems, as you need to be very specific and deliberate about your coding strategy if you want to beat the benchmark results. The specific programming problems that occur with the Atari 2600 Berzerk dataset are: 1. The robot must observe and respond to a human's speech as quickly as possible. A benchmark for this language processing problem is to identify words in speech and to recognize the words correctly. 2. The robot must change its angular orientation at a rate of between 90 and 180 degrees per second. A benchmark for this problem is to correctly change its orientation at a rate of between 90 and 180 degrees per second. The overall problem set can be accessed at the following URL: https://pdx.sis. admissions.utoronto.ca/courses/bergstra/software/atari-berzerk-dataset/problem-set/. olver (Sparql query): I am unable to find the Atari 2600 Ber", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?model_name. io: model_name | problem | problem_lbl |cont |benchmark | -+-+-+-+-+ P30 | Skier | Skiing | yes| Textual Machines| P31 | Contra | Contra | yes| Neural Networks | P32 | Problem | Natural Language Processing| yes| LMHW Model |", "SELECT DISTINCT ?benchmark WHERE { ?benchmark a orkgp:P30 orkgp:P31. } input (English text): The following are some of the most commonly used benchmark datasets for the Text Summarization research field: - Reuters RTA (29 Jul 2017) - Gigaword 50 (29 Jul 2017) - Common Crawl (20 Aug 2016) - TREC-extended (7 Sep 2013) - TREC-covers (16 Aug 2013) - TREC-news (16 Aug 2013) - TREC-url (16 Aug 2013) - TREC-web (19 Jun 2012) - TREC-web + URL (19 Jun 2012) - Reuters RTA + URL (29 Jul 2017) - Gigaword 50 + URL (29 Jul 2017) - Common Crawl + URL (20 Aug 2016) - TREC-extended + URL (7 Sep 2013) - TREC-covers + URL (16 Aug 2013) - TREC-news + URL (16 Aug 2013) - TREC-url + URL (16 Aug 2013) - TREC-web", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } : SELECT * FROM http://datagenetics.com/blog/archives/2008/06/ 16/atari-2600-game-ranking-dataset/ WHERE DATALENGTH(CONVERT(VARCHAR(1024), ?cont)) - DATALENGTH(REPLACE(CONVERT(VARCHAR(1024), ?cont), '|', '')) < 10000 : ?cont data dbo:Dataset; : ?cont commo string'|' : ?cont data dbo:GameRankingURL; : ?cont data dbo:URL; : ?cont data dbo:Title; :", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Q: Extracting Substrings in Python While Keeping Empty Strings I'm trying to parse a very large piece of text that has a number of locations for a physical warehouse. These locations are very similar, containing only a few details. However, some of the locations are missing specific details. There is a small chance that certain information is missing, and I don't want to assign a value to a location if that information is missing. The following is an example of what I am working with: 'This is", "SELECT DISTINCT ?model ?model_label WHERE { ?model a orkgp:P30 ?cont; ?model_label aproteins. } input (English text): Proteins dataset is available from Konect project. A wide variety of machine learning and natural language processing techniques have been evaluated on the dataset. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0There is a high variance on the performance of these models and their type is widely applicable to other problems. Some examples of models that achieved the best results on Proteins are: Naive Bayes, Multilayer Perceptron, Support Vector Machines, Random Forest, Logistic Regression.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } upheld irmat\u00e3o do campo \"problem_lbl\":Natural", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The metrics of evaluation over the CommitmentBank dataset include: 1. Precision and recall measures over the subset of \"has_commitment\" in the \"paper\" field 2. Precision and recall measures over the subset of \"has_commitment\" in the \"cont\" field 3. A confusion matrix over the same set of rows and columns as the second measure 4. Mean average precision over the rows of the confusion matrix 5. Mean average precision over the columns of the confusion matrix 6. Mean average precision over the rows of the confusion matrix 7. Mean average precision over the columns of the confusion", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ACE 2004\") ?benchmark; ?benchmark BM-96 OR ?benchmark BM-99. } output (Sparql query): 1ACE 2004 dataset 2BM-99 ACE 2004 dataset", "sets are just datasets right ? So the best benchmark dataset is by definition the Nottingham dataset. The benchmark dataset for the paper \"Isomorphic Semantic Role Labelling and Neural Ranking with Comparison Terms\" was published in the P30 proceedings of the Workshop on Graph-Based Query Processing and Ranking (Gengo 2019). The benchmark score was 556.8 and the metric was 67.1. The benchmark dataset was created by Gengo Inc.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } SELECT COUNT( ?) AS ?count WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } en\u00edsparql (Sparql query):SELECT COUNT(*) AS ?count WHERE { ?rf a orkg", "SELECT ?paper (paper) WHERE { ?paper mpp:has_benchmark ?benchmark. } output (Sparql query): SELECT * WHERE { ?paper mpp:has_benchmark ?benchmark. } solution (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } * paper { mpp:has_benchmark ?benchmark. } * benchmark { mpp:has_benchmark ?benchmark. } solution (English text): This is the code used in benchmarking papers: * paper { mpp:has_benchmark ?benchmark. } * benchmark { mpp:has_benchmark ?benchmark. } * benchmark {", "SELECT DISTINCT ?paper_id ?problem_id FROM { data; data: {paper_id: paper_id, problem_id: problem_id}, results: [ { paper_id: paper_id, problem_id: problem_id, model: \"XGBoost\" }, { paper_id: paper_id, problem_id: problem_id, model: \"LR\" }, { paper_id: paper_id, problem_id: problem_id, model: \"Boosting\" }, { paper_id: paper_id, problem_id: problem_id, model: \"RF\" }, { paper_id: paper_id, problem_id: problem_id, model: \"SVM\" }, { paper_id: paper_id, problem_id: problem_id, model: \"NN\" },", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } No problems identified Welcome to SPARQL! SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:", "SELECT DISTINCT ?metric ?metric_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_label. ?metric ?metric_label. } 2. The Gibson PointGoal Navigation benchmark dataset contains the following benchmark metrics: - Domain-specific chat - End-to-end understanding - Standard chat - End-to-end understanding metric - Recognition accuracy - Semantic similarity 3. The Gibson PointGoal Navigation benchmark dataset is often used as a baseline for research and development in Natural Language Processing. However, it lacks a specific problem statement for the research community to address, making it difficult to demonstrate the effectiveness of the dataset in a use case. 4. In order", "SELECT DISTINCT ?paper_id ?paper_title WHERE { {\r ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id <http://dbpedia.org/resource/AAPD_dataset> ?paper; \r ?paper <http://dbpedia.org/resource/Tatsuhiko Miyagawa> ?cont; \r ?cont a orkgp:HAS_BENCHMARK ?benchmark; \r ?benchmark <http://dbpedia.org/resource/Tatsuhiko Miyagawa> ?paper_id; \r ?paper_id <http://dbpedia.org/resource/Tatsuhiko Miyagawa> ?paper_title; } FILTER (str( ?paper_title) = ?paper_id)} input (English text): The AAPD dataset is usually used as benchmark dataset in Natural Language Processing research for the following reasons: 1. Dataset contains \"question-answer\" pairs, where each pair consists of a question and", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Here is a sample response from the above Sparql query: output (Sparql query): \u0441\u0435ponse (Sparql query): DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_l", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"AWD-LSTM\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): I will provide a link to the code used in the benchmark paper. https://github.com/cse-yujin/nlu_research/blob/master/paper-2.md output (Sparql query):ospheric input (English text): https://github.com/cse-yujin/nlu_research/blob/master/paper-2.md input (Sparql query): AND ?problem ?problem_lbl. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking PubMedQA\") ?paper1 orkgp:P30 ?rf1; orkgp:P31 ?cont1. ?cont1 orkgp:HAS_BENCHMARK ?benchmark1; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The datasets that have been used for benchmarking in the Image Classification research problem are: - P30 - The part-of-speech tag data for the Penn Treebank. - P31 - The full reference citation for the Penn Treebank. - HAS_BENCHMARK - A hierarchy of benchmarks, for example the COCO dataset. The following are some of the problems that are known to arise from using benchmark datasets for benchmarking: 1. Inaccuracy: When computing performance on a dataset, it is very hard to quantify the accuracy of a system's output on a benchmark dataset. This makes comparing performance across datasets challenging. Moreover, even if one could come", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Gibson PointGoal Navigation\") ?benchmark; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } There is no research problem with the Gibson PointGoal Navigation dataset.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Cheetah\") ?paper ; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Q: How to sort a vector in R based on the number of NA's in each element ? Is there an efficient way to do the following ? vec <- c(1, 2, 3, 4, 5, NA, 6, 7, 8, 9, 10) sort(vec, na.last = T) [1] 4 5 6 7 8 9 10 What I'm really looking for is the count of NA's in each element of the vector. In the example, there are 2 NA's so the sort order should have an extra element. [1] 2 3 4 5 6 7 8 9 10 A: Use count with decreasing order of counts", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research ? output (Sparql query): There is one research problem with benchmark datasets in the realm of Natural Language Processing research: There is no perfect benchmark for Natural Language Processing. Several approaches have been proposed to compare different approaches on benchmark datasets, but none of them is without flaws. # Attributes", "input (English text): The best model in terms of senseval 2 metric was the one using skip-gram with a negative Samplron model and 20-dim word2vec (the last is the model used for the benchmark). output (Sparql query):ortunately input (English text): The best model in terms of senseval 2 metric was the one using skip-gram with a negative Samplron model and 20-dim word2vec (the last is the model used for the benchmark). input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Unsupervised: benchmark dataset ? output (Sparql query): input (English text): The best model in terms of senseval 2 metric was the one using delitegram with negative Samplron model and 40-dim word2vec (the last is the model used for the benchmark). output (Sparql query):iculously input (English text): The best model in terms of senseval 2 metric was the one using delitegram with negative Samplron model and 40-dim word2vec (the last is the model used for the benchmark). input (English text): Are there any research problems with benchmark datasets in", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The SGN decoder from the openNER Library output (Sparql query):SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value, is 171269. https://github.com/w3c/tvena/blob/master/benchmarks/wmt2016-eng-russian-de.json output (Sparql query): counc:171269 biomark:171269 problema:WMT2016 English-Russian problema_lbl:Natural Language Processing ulp:171269", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The most common lead compound is benzene. output (Sparql query):asmuch as :Problem_lbl is { \"Natural Language Processing\" } <|endoftext|>Q: Wicket dataTable - how to select all the rows at once ? I have a wicket dataTable and I would like to get a list of all the rows that contains data. I could do it like this: List<Person> persons = dataTable.getPersons(); But that's too slow. Is there a faster way ? A: There's", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <Query> <Where> <ListSelect items=\"3\"> <And> <ListItem>", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"No-Op Model\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Provided. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"No-Op Model\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; }", "SELECT ?rf ?problem_lbl ?paper ?cont ?benchmark ?problem ?task ?solution ?status ?author_link ?solution_link WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The code is very useful and is a very good benchmarking tool. Thank you! output (Sparql query): INSERTION INTO", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Emergency Alert, System\" OR str( ?rf_label) = \"Emergency Alert, Public\" OR str( ?rf_label) = \"Emergency Alert, Student\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): AMBER Alert - Are there any research problems with benchmark datasets in the realm of Natural Language Processing research ? output (Sparql query):Are there any research problems with benchmark datasets in the realm of Natural Language Processing research ? output (English text): No problems identified. input (English text): The Emergency Type of AMBER Alert can be described as: Emergency Alert, System, Emergency Alert, Public, Emergency Alert, Student. Fortunately this category of emergency alert is not associated with", "SELECT DISTINCT ?problem_metric ?problem_metric_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CoQA Benchmark\") ?paper_metric; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark_metric; orkgp:P32 ?problem_metric. ?problem_metric_label rdfs:label ?problem_metric_label. } *Problem Metrics* *Tokenizer BLEU* output (English text): ?problem_metric_label Tokenizer BLEU input (English text): ?problem_metric_label Tokenizer BLEU output (Sparql query):\ufffdBCA:tokenizerBLEU *Encoder BLEU* output (English text): ?problem_metric_label Encoder BLEU input (English text): ?problem_metric_label Encoder BLEU output (Sparql query):BCA:encoderBLEU *Matcher BLEU* output (English text): ?problem_metric_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a onkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): INSERT into ResearchProblem (problem, problem_label) VALUES ( ?, ?) That's correct. However, note that the values reported in the Sparql query were computed against the \"OntoNotes\" benchmark dataset, which is not part of the Open Language Models (OLM) distribution.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper1 orkgp:P30 ?rf; orkgp:P31 ?cont1. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \u00a0 \u00a0\u00a0{ \"paper\" : \"P30\", \"problem\" : \"Defender\", \"problem_lbl\" : \"Atari 2600 Defender Benchmark\" } \u00a0 \u00a0\u00a0{ \"paper\" : \"P30\", \"problem\" : \"Deep Reinforcement Learning for Atari Games\", \"problem_lbl\" : \"Deep Reinforcement Learning for Atari Games\" } \u00a0 \u00a0\u00a0{ \"paper\" : \"P31\", \"problem\" : \"Positive and Negative Sampling for Subsequence Generation\", \"problem_lbl\" : \"Positive and Negative Sampling for Subsequence Generation\" } \u00a0 \u00a0\u00a0{ \"paper\" : \"P", "SELECT ?problem ?problem_lbl ?paper ?cont ?benchmark WHERE { ?problem rdf:type orkgp:Problem; ?problem rdf:label ?problem_lbl; ?problem rdfs:label ?paper; ?cont rdfs:label ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdf:type orkgp:Dataset. }", "input (English text): ?paper output (Sparql query): ?paper_url output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html output (Sparql query): ?cont output (Sparql query): ?cont_url output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html output (Sparql query): ?cont. output (Sparql query): ?cont.paper_url output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html output (Sparql query): ?cont.paper_url. output (Sparql query): ?cont.paper_url.url output (Sparql query): https://hameedmazumder.github.io/papers/paper_url/paper_url.html output (Sparql query): ?cont.paper_url.url. output (Sparql query): ?cont", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Yes, I could list the models that", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }\" } <|endoftext|>Q: IQueryable.Where(...) vs.AsEnumerable().Where(...) vs.AsEnumerable().Where(...) It seems there are 3 ways to filter a sequence of enumerable objects. What is the difference between them ? var myObj = Enumerable.Empty<MyType>(); myObj = myObj.AsEnumerable().Where(x => x.SomeProperty == 1); myObj = myObj.Where(x => x.SomeProperty == 1); A: The second option is shorter for the equivalent code myObj = myObj.Where(x => x.SomeProperty == 1); The first option is", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): List the code links in papers that use the BERT model in any benchmark ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?paper Or-ow\u00a0- The Most Effective Overlay for this Dataset ?cont Or-ow+\u00a0- F1 measure ?benchmark HutterPrize ?problem Or-ow\u00a0- F1 measure :Or-ow - The Most Effective Overlay :F1 measure :HutterPrize :Or-ow\u00a0- F1 measure :#{F1 measure} :HutterPrize :#{F1 measure} :Or-ow\u00a0- F1 measure :#{F1 measure} :HutterPrize :#{F1", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Language Model\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Annual park admission is $129 for adults and $69 for children (ages 3-11). Campsites are $20 per site. Eagle Lake is located just outside of Kalispell in Flathead County, Montana, on U.S. Route 2. The campground is located about 20 minutes northwest of Kalispell, in an area known as Eagle Lake Village. Visitors may purchase season passes at the entrance station for Eagle Lake Village. Eagle Lake is a quiet", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?benchmark o kgp:HAS_BENCHMARK. Note that although the query can run in O(1) time, it cannot provide any guarantees of ranking results as all datasets are unindexed. Furthermore, standard NLTK tokenizers may tokenize the benchmark indiscriminately, which might introduce additional noise into the results. The Top performing model on the VTAB-1k", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Artificial Intelligence\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The following are benchmarked problems: thesaurus assistance, web page summarization, sentiment analysis, and question answering. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Thesaurus assistance\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl", "SELECT DISTINCT ?problem_label WHERE { ?problem_label orkgp:P32 ?problem. } input (English text): The models that have been benchmarked on the FSNS - Test dataset are: Lukasz - PAIR model, Temporal-Memory model, Attention model. output (Sparql query): \ub808\uc54c : Lukasz - PAIR model, Temporal-Memory model, Attention model. input (English text): Lukasz is an Attention based model. <|endoftext|>1. Field of the Invention The present invention relates to an ultrasound diagnosis apparatus. 2. Description of the Related Art The ultrasound diagnosis apparatus transmits an ultrasound wave from the transducer into a subject, receives the reflected ultrasound wave from inside the subject, and obtains a tomographic image of inside the subject based on the received reflected ultrasound wave. Ultrasonic diagnosis apparatus can obtain information regarding tissue structures inside the subject without injuring the subject, and have widely been used, for example, for cardiology diagnosis. A planar imaging ultrasonic diagnosis apparatus is an example of the ultrasound diagnosis apparatus. The planar imaging ultrasonic diagnosis apparatus has a curved ultrasonic array that includes a transmission array that transmits ultrasound", "SELECT DISTINCT ?paper ?benchmark_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"German-English\") ?paper; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; } input (English text): The WMT2014 German-English dataset is a benchmark dataset used in the field of Natural Language Processing. Based on the BLEU metric, the best performing model on this benchmark dataset is the Transformer model from the paper \"Mastering the Transformer: A Practitioner's Guide to Deep Neural Networks for Summarization, Information Retrieval, and Question Answering.\". The Transformer model outperforms all other models in the same field with an absolute BLEU score of 22.24. output (Sparql query): SELECT ?paper ?benchmark_label { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"German-English\") ?paper; orkgp:P31 ?cont. ?cont orkg", "SELECT DISTINCT ?problem_label ?problem_label_text WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_label_text. } input (English text): Here are some examples using the code found in the paper that benchmark the BiLSTM-Attention + ELMo model: 1 - https://github.com/allen-wi-kscalers/wang-etal-2018-emnlp-socp 2 - https://github.com/allen-wi-kscalers/wang-etal-2018-emnlp-socp/blob/master/eval_benchmarks/eval_baseline_emnlp.py 3 - https://github.com/allen-wi-kscalers/wang-etal-2018-emnlp-socp/blob", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value is 218.642. output (English text): The highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value is 218.642.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Shake-Shake\") ?paper_link ; ?paper_link ?paper. ?paper_link rdfs:label ?paper_label. ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P30 ?cont; orkgp:P31 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>india Updated: Jun 29, 2020 12:10 IST A fresh outbreak of novel coronavirus (Covid-19) has been reported in Kashmir on Wednesday with a number of infections being detected, the health department in the state said. \u201cA total of 10 samples of patients with coronavirus disease (Covid-19) have been received from North Kashmir, with a high number of acute respiratory infection symptoms. A few of these samples have been sent for laboratory confirmation and on further analysis it was found that they are related to the recent outbreak in Jammu and Kashmir,\u201d the health department said in a", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Joint Entity and Relation Extraction\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The most common benchmark datasets in the Joint Entity and Relation Extraction research field are the CoNLL dataset, the BioASQ dataset, the Reuters RFT dataset, and the SemEval dataset. <|endoftext|>Field hockey at the 2003 Pan American Games The hockey tournaments for the 2003 Pan American Games were held initel avond hockeywebsite from August 17 to August 26, 2003. Qualification A total of 11 teams", "SELECT * FROM mrv_mleship.dataset WHERE dataverse = {DATAVERSE_LNAME} AND table = {TABLE_NAME} podiums (English text): F1 score is a popular metric used in the industry for evaluating text classification systems. The F1 score measures the harmonic mean of precision and recall. It is considered a better metric for systems that only output a binary classification, such as text classification tasks. Macro-averaged F1 score is another metric which takes into account both precision and recall and is considered a better metric for those tasks that return a ranking output, such as for text categorization. A common approach to evaluate text classification algorithms is to use data augmentation and grid search to find the best hyper-parameters. In addition, reporting results for multiple F1 scores allows the user to understand the trade-off between different factors, such as complexity and training time. podiums (English text): F1 score marketplace (English text):", "SELECT ?benchmark, ?paper, ?cont, ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The code references can also be found in modelevaluation/src/modelevaluation.kkth by listing the bench_params arguments in the \u201crbio.role\u201d file.", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Asteroids\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Atari 2600 Asteroids benchmark dataset contains problem statements that are similar in the way that they are generic or specific. For example, the problem statements provide a specific space ship and a generic", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } This produces the following results: Paper P30 Natural Language Processing over WMT2014 German-English Benchmark P31 The Dronacharya Dataset for Machine Translation P32 Discussing natural language understanding benchmarks problem Natural Language Processing over WMT2014 German-English Benchmark I've noticed that the benchmark dataset in WMT2014 German-English is present in both research papers that contain a benchmark over the WMT2014 German-English dataset as well as the P30 Natural Language Understanding over WMT2014 German-English Benchmark.", "SELECT DISTINCT ?paper WHERE { ?rf a orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): The models that have been benchmarked on the SearchQA dataset are: - CBOW; - CountVectorized; - GPT; - Word2Vec; - XLNet. These models can be found at: - <https://www.kaggle.com/c/search-question-answers/leaderboards/leaderboard-results/2627> - <https://www.kaggle.com/c/search-question-answers/leaderboards/leaderboard-results/2638>", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a ongcp:OntologyComponent; ongcp:name ?name; ongcp:isRelative ?relative; ongcp:ontologyTerm ?ont; ongcp:partOfSpeechTerm ?part; ongcp:description ?description; ?rf_label ongcp:ResearchField; ongcp:name ?rf_label; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P33 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Mean average precision (mAP) is an evaluation metric in the field of object recognition (for example, for images), that measures the average precision of a model at deciding which of multiple images are similar to each other and which are not. output (S", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 ?problem ctl:maxValues ?maxVal", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WMT2014 English-German dataset\") ?benchmark ?problem; } input (English text): WMT2014 English-German dataset is an evaluation dataset used for the \"Web Modeling and Automatic Speech\" task of the 14th World Machine Translation Conference (WMT14) held in Italy in 2014. <|endoftext|>// Boost.Bimap // // Copyright (c) 2006-2007 Matias Capeletto // // Distributed under the Boost Software License, Version 1.0. (See // accompanying file LICENSE_1_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"PWC Leaderboards (Restricted)\") ?paper ; ?paperoid ?problem. ?cont ?problem orkgp:HAS_BENCHMARK ?benchmark. ?benchmarkoid ?benchmark. } \u00a0 \u00a0 \u00a0 \u00a0 The dataset PWC Leaderboards (Restricted) has the following titles and IDs: \u00a0 \u00a0 \u00a0 * [Benchmarking Natural Language Processing systems](https://dl.acm.org/doi/10.1145/2492968.2493032) * [On the definition of a benchmark](https://dl.acm.org/doi/10.1145/2117371.2117376) * [A framework for formal benchmarking](https://dl.acm.org/doi/10.1145/3060671.3060675) * [The PWC Leaderboards dataset](https://dl.acm.org/doi/10.1145/2993920", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The following", "SELECT DISTINCT ?metric ?metric_label WHERE { ?metric http://terminology.hl7.org/wiki/Special:Utilities/TaxonomyPage/Objective; rdfs:label ?metric_label. ?metric http://www.w3.org/TR/tessellation-pattern/ dp.aud; ?metric http://www.w3.org/TR/tessellation-pattern/ dp.f1; ?metric http://www.w3.org/TR/tessellation-pattern/ dp.r2. } input (English text): The following evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset: r2 - Root-Mean-Square Error f1 - F-measure dp.r2 - Spearman's rank correlation directed output: r2, f1, dp.r2 directed input: metric_label, metric, benchmark, problem, problem_lbl scaffolding: http://www.w3.org/TR/tessellation-pattern/ dp.aud; http://www.", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Related bio: Ping Yang, PhD, is an assistant professor of computer science and engineering at UC Berkeley. His research focuses on natural language processing, specifically text mining, information extraction, and question answering. He has published over 40 technical articles on these topics, and has won 5 best paper awards at international conferences. He received his PhD from Tsinghua University in 2011, and his earlier work was focused on question answering for mobile devices. He was previously an associate professor of computer science at City University of Hong Kong (CUHK), and has co-founded two startups. indexOf (", "SELECT DISTINCT ?paper ?paper_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"BCN+ELMo\") ?paper_code; ?paper_code <http://beam-search.github.io/BCN.ELMo.Conversion/>. ?paper_code <http://www.aclweb.org/anthology/W19-1005.html>. ?paper_code <http://www.aclweb.org/anthology/W19-1005.html>. } The BCN+ELMo model was proposed by @beam_search and @aclweb. The model was trained on the bcelmo dataset, which was created by @W19-1005. This dataset includes two sub-datasets: one dataset for training the model, and one dataset for testing the model. The datasets are available for download at <http://beam-search.github.io/BCN.ELMo.Conversion/>. The \"paper\" table in the SPARQL query returns one record for each paper that has utilized this model and includes links", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Select all problems associated with Denseck. output (Sparql query): ?problem a ?problem_lbl.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Classical music 5 seconds at 12 kHz\") ?paper. ?paper rdfs:label \"MLA2012-49\"; ?paper rdfs:label \"Deep FM\"; ?paper rdfs:label \"seq2seq\"; ?paper rdfs:label \"TransRNN\"; ?paper rdfs:label \"TransRNN+. \"; ?paper rdfs:label \"TransRNN++;\"; ?paper rdfs:label \"TransRNN++;+\"; ?paper rdfs:label \"RecNN\"; ?paper rdfs:label \"BiLSTM\"; ?paper rdfs:label \"RecNN+\"; ?paper rdfs:label \"BiLSTM+\"; ?paper rdfs:label \"Deep RecNN\"; ?paper rdfs:label \"Conditional RNN\"; ?paper rdfs:label \"Conditional RecNN\"; ?paper rdfs:label \"Conditional RecNN+\"; ?paper rdfs:label \"Conditional RNN++;\"; ?paper rdfs:", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"BLEU\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } mentation (Sparql query): A model based on NELL performed the best with a BLEU score of 22.67.", "SELECT * FROM BC5CDR-disease WHERE TupleRecog ?problem TupleRecog:Tuple(disease, \u201cary/NHLH6\u201d, \u201cary/NHLH7\u201d, \u201cary/NHLH8\u201d, \u201cary/NHLH9\u201d, \u201cary/NNHH1\u201d, \u201cary/NNHH2\u201d, \u201cary/NNHH3\u201d, \u201cary/NNHH4\u201d, \u201cary/NNHH5\u201d, \u201cary/NNHH6\u201d, \u201cary/NNHH7\u201d, \u201cary/NNHH8\u201d, \u201cary/NNHH9\u201d) <|endoftext|>This is the first in a weekly series on the party's candidates for Ontario's top court. Grey Cup-winning Canadian Football League star Jon Cornish is making the leap to law, becoming the Conservative candidate for Ontario's Superior Court of", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Skiing\") ?paper ; orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?benchmark_label. ?problem rdfs:label ?problem_lbl. } input (English text): Although using Pearson Correlation Coefficient to evaluate the performance of a model trained on the Atari 2600 Skiing dataset is a common practice, it has been proven to over-estimate the error. It can be shown that this metric is sensitive to the prevalence of frequent sub-patterns in the dataset. output (Sparql query):SELECT ?problem ?problem_lbl", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): There are two research papers that benchmark the Atari 2600 Frostbite dataset - paper [P30] and paper [P31]. Both of these papers contain a benchmark over the same dataset. output (Sparql query): SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper OR kgp:P30 ?rf; OR kgp:P31 ?cont. ?cont OR kgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper; ?cont OR kgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): TODO <hr> <p>Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset ?</p> <p>I'm not aware of any research problems with benchmark datasets. However, as with any other benchmark dataset, it is important to keep it consistent and carefully chosen to make sure results are comparable.</p>", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Boxing\") ?benchmark; ?benchmark GREATER THAN 0.35 ; ?benchmark. } input (English text): The highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value, is 0.35. There are two problems related to the Atari 2600 Boxing dataset: The paper that achieved the highest result on the dataset was published in 1995, which makes it unlikely that any research problems with the dataset remain today. The reported metric for this result was \"Inches Per Round\", which is not a metric that is often reported in academic papers for competitive performance measurements. Therefore, there are no research problems with benchmark datasets in the realm of Natural Language Processing that I can see at this time. However, this answer should not be considered a definitive answer to this question. If you notice any changes to the problem statement that would invalidate this answer, please update this answer. <|endoftext|> 615 F", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Treebank\") ?paper ; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }\" input (English text): The F1 score for the Penn Treebank benchmark dataset is 0.835. The previous entry was that of the Brown Corpus, which had a F1 score of 0.836. output (Sparql query):", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } THE SELECTed_item highest_result is: { ?problem { ?problem_lbl doktorml-15-de-english IWSLT2015 German-English 5712 0.5712 } } orchestration: #!/usr/bin/env sparql ### # This file is part of Doctrine MDL. # # Doctrine MDL is free software; you can redistribute it and/or modify it under the terms of", "SELECT DISTINCT ?problem ?problem_lbl, ?paper ?problem_paper, ?cont ?problem_cont, ?benchmark ?benchmark_label, ?problem_paper ?problem_paper_label, ?problem_cont ?problem_cont_label FROM { :researchfield-atari :atari-2600-frostbite :atari-2600-frostbite-paper :atari-2600-frostbite-cont :benchmarks :benchmark-label :problem :problem_label :problem_paper :problem_paper_label :problem_cont :problem_cont_label } WHERE { ?rf a orkgc:ResearchField; ?rf_label ?benchmark_label; ?paper ?problem_paper; ?cont ?problem_cont; ?problem_lbl ?problem_label; } input (English text): The Atari 2600 Frostbite benchmark dataset contains metrics over which to evaluate Natural Language Processing techniques. However, no single metric can be", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"MultiNLI\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): http://www.pmlr.org/ versus http://x-count.s3.amazonaws.com/MultiNLI-best-p-35.csv.gz input (English text): Is there", "SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet\") ?model. } The database that is used for this query is the DBpedia ontology database. There are no research problems with benchmark datasets in the realm of Natural Language Processing research. Datasets such as ImageNet and Databricks have been used to evaluate a wide variety of Natural Language Processing models. Here is a list of evaluation models on the ImageNet ReaL dataset: - DeepL; DeepLSTM; ResNet; DenseNet; VGG16; GoogleNet; Inception v3; - GoogLeNet; Darknet; VGG19; MobileNet; DenseNet121; - Wide ResNet; DenseNet161; DenseNet201; SqueezeNet; - YOLO; SSD; SSD-Net; SiamRPN; On the Databricks reaL benchmark, the following models have been evaluated: - LightGBM; DecisionTree; ExtraTrees; XGBoost;", "SELECT DISTINCT ?benchmark WHERE { ?benchmark rdfs:label \"Natural Language Processing\"@en.. } input (English text): Reuters En-De output (Sparql query): rival-model-1 (Sparql query): The model used in this submission achieves an Accuracy of 86.34 on the Reuters En-De benchmark dataset. output (Sparql query): check-1 (Sparql query): And which benchmark dataset does this model achieve the highest Accuracy on ? output (Sparql query): SELECT DISTINCT ?benchmark WHERE { ?benchmark rdfs:label \"Natural Language Processing\"@en.. } input (English text): Reuters En-De output (Sparql query): rival-model-1 (Sparql query): The model used in this submission achieves an Accuracy of 86.34 on the Reuters En-De benchmark dataset.", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): No, there is no research problem with benchmark datasets in the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (text):The top benchmark score on the Natural Questions dataset for Robustness is 34.91. localAnswer (text):34.91 localSparqlQuery (Sparql query): SELECT DISTINCT ?problem ?problem_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Sequential CIFAR-10\") ?paper orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } A: Natural Language Processing is a broad field with a number of subfields, all of which need to be evaluated against benchmarks. One such benchmark is the CIFAR-10 dataset. It would be best to determine the metric or metrics you would like to use to evaluate the performance of the system, and then compute an evaluation metric based on that metric. The benchmarks could then be compared based on the evaluation metrics. Benchmark datasets are created by an AI community and are often created to help train their AI models to be better at their mission, i.e. generating text from images, or detecting objects in images. These datasets usually contain a balanced mix of training and test data (a split) so", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): display problem_lbl|string unions({\"Auto-labeling of medical literature\",\"Artificial intelligence\",\"Natural Language Processing\",\"Machine learning\",\"Data Mining\",\"Information retrieval\"}) input (Sparql query): ?paper a orkgp:P30 output (Sparql query): 54.87 input (Sparql query): ?cont a orkgp:HAS_BENCHMARK ?benchmark output (Sparql query): true input (Sparql query): ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgp:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Citation Classification\") ?benchmark; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgp:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Citation Classification\") ?benchmark; orkgp:P30 ?rf; orkgp:P31", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): http://h-nli.readthedocs.io/en/latest/manual.html ?highlight=benchmark#code-links-in-papers output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking the PubMed 20k RCT dataset\") ?paper_id; ?paper_id tuples:name; ?paper tuples:id ?paper_id. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P30 ?problem; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?paper_id. ?problem rdfs:label ?problem_lbl. } input (English text): The following are the titles and IDs of papers that contain a benchmark for the PubMed 20k RCT dataset: 1. Cao, Jinyang, Yingxiong Wang, et al. \"Proposed Citations for Enhancing Ranking Efficiency in Web Search.\" Proceeding of the Tenth International World Wide Web Conference, 2011. 2. Lin, Jiajing, Pengcai Liu, et al. \"Metric Learning Based Association Rule Detection on Text Data.\" Proceeding of the Twenty-", "SELECT ?problem WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark EXISTS { ?problem rdfs:label ?problem_lbl. } } {# Existential Query (Sparql Query) #} input (English text): arXiv, Expt-only, and NLP-TDMS datasets all have associated papers that detail the methodology used to generate the dataset. These papers often have associated publication links to allow for more rigorous study of the data. The benchmark datasets are also often accompanied by a README.md file which outlines the setup instructions. For example, the NLP-TDMS dataset contains detailed setup instructions, including step-by-step instructions to build a training model and execute the training and development steps. These training and development datasets are often used to tune model hyperparameters. However, many researchers do not extensively test these datasets on their own using their own models. Some", "SELECT DISTINCT ?benchmark WHERE { ?paper a orkgp:P30. ?paper b orkgp:P31. ?cont a orkgp:HAS_BENCHMARK ?benchmark. } input (English text): The name of the top performing model when benchmarked on the Nottingham dataset is \"BERT\". Benchmark datasets are useful when comparing different models or implementation. Benchmark datasets have been used in Natural Language Processing since the 1990s and are available for a wide variety of different models and setups. They are notoriously hard to build and often contain a range of domains and problems that are not accessible via other means. The data in benchmark datasets vary widely and are usually provided in files, which are often not language specific. Datasets are frequently divided in a training, validation and test set, which is difficult to ensure consistency between. There is no single problem or dataset that would be considered as \"the best\" across all use cases and models. In fact the best model might not even perform well on the benchmark dataset in question. One significant issue with benchmark datasets is the difficulty in ensuring consistency between the datasets. This makes it difficult", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The query returns the following results: <problem rdf:type=\"orkgp:P30\"> <problem_lbl>Natural Language Processing</problem_lbl> </problem> <problem rdf:type=\"orkgp:P31\"> <problem_lbl>Contrary to popular belief, proper Natural Language Processing requires not only deep understanding of the issue at hand but also an understanding of mathematics, statistics, computer science, and many other disciplines to solve it. Furthermore, the ability to develop creative solutions to difficult problems, like Brain-Computer Interfaces (BCIs) and the other Artificial Limbs,", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } A: Benchmarks can be problematic if they are designed to compare to arbitrary performance metrics. For instance, the Criteo model is known to perform very well on IMDB's 10M video ratings, but horrible on the Natural Language Processing benchmark used in the previous question. This is because the IMDB 10M video metric is designed to rank video games based on their suitability for a variety of gaming platforms, whereas the Natural Language Processing benchmark simply ranks models in decreasing order of their likelihood of being the correct answer given the provided query. There are other cases where the benchmark datasets are notoriously bad at evaluating models, so if you are interested in", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The top benchmark score is 94.15 with metric value 16. The source for this score and metric value can be found at this link: https://datasociety.org/dataset/atari-2600-ice-hockey-training. The dataset has 41,360 examples and 40,840 unique variables.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The benchmark datasets in the above SPARQL query belong to the following ontology classes: has_benchmark: paper: orkgp:P30 orkgp:P31 orkgp:HAS_BENCHMARK orkgp:P32 problem: rdfs:label problem_lbl A: Benchmark datasets can be created to compare approaches or answer specific research problems. However, such datasets are typically not created with the intention to be used for citation or to perform research. In particular, benchmark datasets which compare algorithms, or contain the actual research problem (i.e. a query or a KB) are", "SELECT DISTINCT ?problem_label ?paper_id WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id; }", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|> Table of Contents Title Page Copyright Page Dedication Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapter 7 Chapter 8 Chapter 9 Chapter 10 Chapter 11 Chapter 12 Chapter 13 Chapter 14 Chapter 15 Chapter 16 Chapter 17 Chapter 18 Chapter 19", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): * Natural Language Processing output (Sparql query): * GTX 980 Ti vs Titan X vs P3000 on the WikiText-2 Dataset input (English text): * https://paperswithbenefits.org/s/ paper-1546.pdf output (Sparql query): * GTX 980 Ti vs Titan X vs P3000 on the WikiText-2 Dataset input (English text): * https://paperswithbenefits.org/s/ paper-1546.pdf output (Sparql query): * GTX 980 Ti vs Titan X vs P3000 on the WikiText-2 Dataset", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"PWC Leaderboards\") ?paper; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The dataset includes the following fields:,,,,,,,,, and. The corresponding OWL classes are,,,,,,,,, respectively. This means there are five OWL classes corresponding to the five fields. PWC Leaderboards contains data for 10 individual participating algorithms per problem and 10 runs. The dataset also includes a flag for whether benchmarking has been performed or not. The set of benchmarks is actually a superset of the set of problems where benchmarking has been performed. In that case, the superset is comprised of 20 OWL classes corresponding to the 20 sets of problems. output (Sparql query): salad:researchproblem p:PWC_LEAD", "SELECT DISTINCT ?paper FROM { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Duel noop\") ?paper } WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper } Input (English text): List the code links in papers that use the Duel noop model in any benchmark ? Output (Sparql query): ipeline { ... filter { ... values doc(\"https://www.cs.cmu.edu/Groups/CTX/www/papers/Duel_noop.pdf\") } } <|endoftext|>Recently, a remote keyless entry system is mounted in a vehicle for remotely opening a door of the vehicle by carrying a portable transmitter to communicate with a central server through a base station. In the remote keyless entry system, a user inputs a unique password issued to the user through a keypad of a key fob to the central server, and when the user passes by the", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Fine-Grained Image Classification\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Select the dataset Imagenet-Dates20120920 ? output (Sparql query): SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label \"Imagenet-Dates20120920\"@en; FILTER (str( \"Imagenet-Dates201209", "SELECT * FROM biwm:DRI_Benchmark where language = \"en\") input (English text): \"en\" output (Sparql query): NLRF+TPR-06 (True Positive Rate and Positive Predictive Value) input (English text): TPR-06 output (Sparql query): Positive Predictive Value. input (English text): Positive Predictive Value (PPV) = true positives / ( true positives + false negatives ) \r \r The dataset provided in the benchmark repository for Natural Language Processing research is the DDI Research Field Corpus, a widely used benchmark dataset for NLP tasks. The DRI Corpus can be used to compare the performance of models trained on different datasets. Usually, performance is measured in terms of the recall, precision, and F1 score metrics. The DRI benchmark datasets provide these metrics along with the sparql query for selecting the problems. The benchmarks are organised into 4 categories, namely, Parsing, Sentence Simplification, Part-of-Speech Tagging, and Information Retrieval. Apart from the problem labels (where the corresponding benchmark dataset name is given), the Sparql query also provides information about the evaluation", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The following are the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset: P@20 : Precision at the 20th percentile P@10 :", "mark = P31 input (English text): P31 20.72% Alternatively, you can find the top performing model on CIFAR10 Sequential benchmark from http://cs.sandia.gov/rnvs/benchmark/data/cifar10_benchmark.html", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?id. ?cont ?id. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P30 ?problem; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } SParql query: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?id. ?cont ?id. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P30 ?problem; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem", "SELECT DISTINCT ?model WHERE { ?model a mlpModel. } XLM: BERT; BERT; XLNet; XLNet-Small; XLNet-Light; XLNet-Base; RoBERTa; RoBERTa-Large; ELMo; ALBERT; ALBERTJ; GPT-2; OpenAI GPT; XLNet-Large. ligamentization problem: RELATED_TO; related_to; Natural Language Processing; Benchmark; DistilBERT: DistillBERT; BERT-BASE; BERT; XLNet-Base; DistilGPT; GPT-2; ELMo; ALBERTJ", "SELECT DISTINCT ?paper_id WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id; } input (English text): Atari2600skiing; <|endoftext|>Towards thermal lensing in general purpose liquid few detector array systems. Although considerable progress has been made in the field of generalized thermoelastic effects on a few-detector array, the low coupling coefficient of such an array brings several barriers to practical applications. We theoretically and experimentally investigate the performance of a thermoelastic-lens-assisted detection scheme by using a common general-purpose liquid detectors and a limited number of thermal lenses. We demonstrate that thermal lenses can be designed with significantly better contrast and spatial resolution and much smaller size than natural lenses. We also find that the coupling coefficient between the thermal lenses and the detectors of a few-detector array can reach 45%. We apply this lens-assisted detection technique to a superconducting quantum interference device (SQUID) based biosensor, and achieve a signal-to-noise ratio (SNR) of 31.2 at 1 nT axial magnetic field and", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Cart Pole\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The OpenAI Gym (Cart Pole) benchmark dataset is used for training machine learning models for problems in the context of Artificial Intelligence. Some of the models that have been trained using this benchmark dataset include: LSTM, GRU and CNN. I wonder if there is any research problem with benchmark datasets in the realm of Natural Language Processing research, given that this benchmark dataset is derived from the OpenAI Gym (Cart Pole) domain, that is a subset of the broader domain of Natural Language Processing. output (Sparql query): The benchmark dataset is based on a subset of the broader domain of Natural Language Processing. It is safe to assume that the models trained on this", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. } This works since the benchmark dataset for the P30 problem type is defined via the orkgp:HAS_BENCHMARK property.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } These problem datasets include the ShARe/CLEF eHealth corpus benchmark dataset and the COGENE corpus benchmark dataset. The ShARe/CLEF eHealth corpus benchmark dataset was created to evaluate machine learning systems on a dataset with clear label denoting the aspects of the document that were of interest. These aspects include: document type, year, title, author, abstract and many more. The COGENE corpus benchmark dataset however has no clearly labelled aspects of the documents. The model that performed best in terms of F1 metric on the ShARe/CLE", "SELECT DISTINCT ?paper ?problem_lbl ?problem WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Step 1: Create the SPARQL query Step 2: iterate through the values in the 'benchmark' variable (which is a collection) Step 3: for each value in the 'benchmark' variable Step 4: query the 'paper' variable for its value Step 5: query the 'problem_label' variable for its value Step 6: query the 'problem' variable for its value <|endoftext|>Q: jQuery.getJSON executes twice I have this code: $.getJSON(\"test\", function(data) { console.log(data); }); and the console will print 2 times test instead of one. Why ? A:", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Centipede\") ?paper ; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): A model that scores the best on the Centipede dataset is called best performing model. Protocol 1: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Centipede\") ?paper ; orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Results (nodes): 1. ResearchField rdfs:label \"Centipede\"", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Inference\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The most commonly used benchmark datasets in the Natural Language Inference research field are the Penn Treebank, the AMI, and the Wall Street Journal. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Inference\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_l", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Which models are being evaluated on the Benchmark dataset ? output (Sparql query):SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The following table lists some of the most popular natural language", "SELECT DISTINCT ?paper_ref WHERE { ?paper_ref dc:managed ?model. ?model a ogr:FeatureSource. ?model dc:date 2016-07-01+calendardate. FILTER (deit-ti:archived > \"2017-12-31\" AND deit-ti:archived < \"2018-01-01\") } input (English text): @model dc:date 2016-07-01+calendardate. <|endoftext|>Highly Efficient Thiol-Ene Coupling of 4-Amino-4H-1,3-benzothiazine with Aryl disulfide Derivatives catalyzed by Cu/carboxylate and their Photo-Catalytic Properties. This study demonstrates the unprecedented thiol-ene reaction between 4-amino-4H-1,3-benzothiazine (ABT) and aryl disulfide derivatives (AD) catalyzed by a Cu", "SELECT DISTINCT ?country WHERE { ?country a rowng orkgc:Country. ?country owcs:hasPres diagnose cgoc:Epilepsy. FILTER (count( ?country) > 12) } input (English text):The prevalence of epilepsy in the United States is the highest. output (Sparql query):The United States is the country with the highest prevalence of epilepsy in the world. - output (Sparql query): SELECT DISTINCT ?country WHERE { ?country a rowng orkgc:Country. ?country owcs:hasPres diagnose cgoc:Epilepsy. FILTER (count( ?country) > 12) } <|endoftext|># Copyright 1999-2020 Gentoo Authors # Distributed under the terms of the GNU General Public License v2 EAPI=6 inherit desktop distutils-r1 autotools-xattr DESCRIPTION=\"The Plotation library provides methods for efficiently rendering graphical user interfaces\" HOMEPAGE=\"https://pypi.org/project/plotation/\" if [[ ${PV} == *9999 ]] ; then \tEGIT_", "SELECT DISTINCT ?problem2 WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"eHealth\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } * First of all, it is important to note that the ShARe/CLEF eHealth corpus dataset contains only a subset of the models that have been evaluated on this dataset. The models listed below have been evaluated on the ShARe/CLEF eHealth corpus dataset: 1.\tBio-RXiv eHealth Github (BERT model) 2.\tSQL-i2b eHealth (SQL model) 3.\tQNTality eHealth (Natural Language Processing model) 4.\tDeep Natural Language Learning for Information Extraction from Existing Corpora (DISTILL) * Now, let's examine the data model associated with the ShARe/CLEF eHealth", "SELECT ?model ?dataset WHERE { ?model rdfs:label \"MODELNAME\". FILTER (str(lang( ?model)) = \"en\")} input (English text): Here is a list of models that have been tested on the HMDB51 benchmark dataset: model A: https://github.com/NVIDIA/ArtificialIntelligence/blob/master/devel/code/RNN/seq2seq/save_lm_test.py model B: https://github.com/NVIDIA/ArtificialIntelligence/blob/master/devel/code/RNN/seq2seq/lm_work.py model C: https://github.com/NVIDIA/ArtificialIntelligence/blob/master/devel/code/RNN/seq2seq/lm_work.py model D: https://github.com/NVIDIA", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?id; orkgp:P30 ?benchmark; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The results contain three documents: ResearchField ?rf; rdfs:label ?rf_label. P30 ?id; orkgp:P30 ?benchmark. P31 ?cont; orkgp:P31 ?cont. The third element, ?cont, contains the \"title\" of the benchmark paper. The title of the benchmark paper is used in the benchmark dataset as a label for the corresponding problem. This is helpful because problem data in the benchmark dataset has the same labels as problems in the original dataset. So the question of the research paper itself can be used in a query for the problem data in the benchmark dataset. Note that the benchmark dataset", "SELECT DISTINCT ?metric ?metric_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. ?metric BLUETOOTH_LARGE_LSH. ?metric_label bletto_large_lsh. } input (English", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } search results 1. ?problem a Tsetlin Machine: Batch, Streaming and Parallel Semantic Parsing 2. ?problem_lbl Tsetlin Machine: Batch, Streaming and Parallel Semantic Parsing", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } { ?paper orkgp:P30 ?score. ?score 43.27. } However, the dataset has been observed to produce high variance when", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; ?rf_label ?paper; rdfs:label ?rf_label; } input (English text): PPS curve: P, R - points on the curve where P is precision and R is the recall. F1 curve: F, T - weighted average of precision and recall, where T is the threshold. RPPT curve: P, T - weighted average of precision@threshold and recall@threshold. output (Sparql query):SELECT * WHERE { ?rf a orkgc:ResearchField; ?rf_label ?paper; rdfs:label ?rf_label; ?metric ?metric. } input (English text): PPS curve: P, R - points on the curve where P is precision and R is the recall. F1 curve: F, T - weighted average of precision and recall, where T is the threshold. R", "SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): GNUEstimator performed the best, followed by SGM and then DNN. output (Sparql query):glominput (English text): GNUEstimator performed the best, followed by SGM and then DNN. output (Sparql query):glomoutput (English text): GNUEstimator performed the best, followed by SGM and then DNN. input (English text): Indicate the model that performed best in terms of Language error metric on the FSNS - Test benchmark dataset ? output (Sparql query", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): You can also view the ARC dataset here: http://www.semeval.org/arc/arc10/challenge.php. The top 5 results over the dataset are provided at this link: https://github.com/SemEval2010/arc/blob", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } [ { \"problem\": \"sciREX:P30\", \"problem_lbl\": \"SciREX:P30\" },", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ouput (Sparql query): ?problem Has_Benchmark concept-mention-extraction", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <html><body> <p> <a href=\"https://www.citeseerx.com/tiangyih/Spelling+Lemma+Encoding.html\">Spelling+Lemma+Encoding</a> <a href=\"https://www.citeseerx.com/citation/Benchmark+Datasets.html\">Benchmark+Datasets</a> <a href=\"https://www.citeseerx.com/citation/Natural+Language+Processing.html\">Natural+Language+Processing</a> <a href=\"https://www.citeseerx.com/citation/Benchmark", "SELECT * FROM {wikibase} WHERE id IN { ?link}, {/wiki/Data}}", "SELECT * FROM {http://dx.doi.org/10.1142/9789812830592_0002} WHERE { ?problem a orkgp:HAS_BENCHMARK ?benchmark;} FILTER (STR( ?) = { \"16.89% error\"}) Welcome to DBpedia's Learn section! In this section, you will find short questions and answers that provide a quick understanding of some of the fundamental concepts and techniques used in the area of Knowledge Graphing and Information Extraction. Questions: 1. A Benchmark Dataset is a set of data that is used to evaluate different NLP models. 2. The SVHN benchmark dataset is a set of text documents, with particular attention to paragraphs containing simple visual features. 3. The model proposed by @carlini:2017 is the one achieving the highest Percentage error score on the SVHN benchmark dataset. Answers: 1. The set of documents in the SVHN benchmark dataset can be", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"LayerNorm HM-LSTM\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"SPARQL query optimization\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Here is the response: There are several datasets for SPARQL query optimization research and they are listed here: - websparql dataset from the BCP papers - the TPC-DS dataset from TPC Note that these benchmarks are related to optimizations for SPARQL queries and the ontology or KB based core of the query. The datasets include comparisons of running times for SPARQL query optimization, but I'm not", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?problem rdfs:label \"Amazon-2: Highest result\"@en. ?problem_lbl \"Highest result\"@en. Output: * 1 ResearchField: ResearchField * 2 problem: Amazon-2: Highest result * 3 problem_lbl: Highest result Following other posts on Stack Overflow, I did some research to find the highest result achieved on the Amazon-2 dataset. The highest result is 1775.44, with the following metric: BM25. The highest result is 1775.44, with the following metric: BM25. The", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Temporal Convolutional Networks\") ?paper2 ; ?paper2 a propo:Dataset ; ?paper2 a propo:Model; ?paper2 a propo:Name. ?problem rdfs:label ?problem_lbl. } input (English text): Here is a sample code from paper named \"PROPO: A Dataset for Properly Oriented Retrieval of Crowdsourced Ontologies\", where the dataset is Temporal Convolutional Networks. The code for the dataset and the model is as follows: # Dataset. A sample instance of the dataset: # # <Dataset artifactIdentifier=\"prop:dataset\" id=\"uid-z4eqshly0j4h0j7jw\"><Name>PROPO: A Dataset for Properly Oriented Retrieval of Crowdsourced Ontologies</Name><ShortName>PROP</ShortName><Type>dataset</Type><Uri>urn:uuid:4eqshly0j4", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"BiT-S\") ?paper_label. FILTER (str( ?paper_label) = \"ResNet\") ?problem. ?problem_label rdfs:label ?problem_lbl. } input (English text): BiT-S, BiT-Sv2, ResNet. output (Sparql query): Donothing. input (English text): There are no research problems with benchmark datasets in the realm of Natural Language Processing research. output (Sparql query): Donothing. The above output is a response that does not contain any problems with the provided benchmark datasets. If you remove the query, it is a Donothing response.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output: ?problem paper:WNLI[1]@72 Accuracy issuance(Sparql query): FROM <http://www.wirednext.com/data/WNLI-73.html> issuance(Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"AAPD\") ?benchmark; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The AAPD dataset has the following highest benchmark result including the metric and score: Average precision: 0.948 Average recall: 0.949 Average F-measure: 0.949 Output (Sparql query):The AAPD dataset has the following highest benchmark result including the metric and score: Average precision: 0.948; Average recall: 0.949; Average F-measure: 0.949 The AAPD benchmark dataset is a collection of pages obtained from the \"ACM Multi-Source NewsGroup Collection\" The AAPD dataset includes multiple types of newsgroups for different topics, such as", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The models that have been benchmarked on the FTD dataset include: LexVec, Glue, and SG-transform. output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\")", "SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CIFAR-10 Image Classification\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont; orkgp:P32 ?problem. ?problem rdfs:label", "SELECT ?problem ?problem_lbl ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_label; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. ?metric rdfs:label ?metric_label. } input (English text): In the benchmark dataset description, the metric is DMControl500k. output (Sparql query): The highest benchmark result achieved", "SELECT DISTINCT ?metric where { ?metric rdfs:label \"common evaluation metrics used in the field of Natural Language Processing\". } <|endoftext|>Road tripping the last big leg of the BBR Trail. We completed the whole 24 hour drive from our base in Christchurch to our next campsite at Trout Bay, on the southern end of the South Island of New Zealand. It took us 6.5 hours to drive the 24 hours, as we were stopped at least a dozen times along the way to get gas and food. We didn't know there were so many hill roads in New Zealand, which is quite different from the interstate highways we are used to. When to start: 20.06.2017 at 23:59 (This is about 7 pm local time.) What we drove: 141 kilometers (from Christchurch to Trout Bay) How far you drive: 6.5 hours What we camped at: On the Southern tip of the South Island of New Zealand at Trout Bay. Weather: Sunny and a bit windy, about 20 kilometers per hour when we started our trip. Things we saw: Little of interest, it was a bit too early in the morning to", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_text ?p; ?p rdfdb:about ?problem. ?problem rdf:type ?dataset; ?dataset o*ks:P30P31P32 ?dataset. ?problem rdfs:label ?problem_lbl. } input (English text): p30,p31,p32,dataset,problem,problem_lbl <|endoftext|>1. Field of the Invention The present invention relates to a multi-layer printed wiring board including a first layer with a via-hole electrode on which a conductive paste has been printed and a second layer superposed on the first layer, and to a method of producing the multi-layer printed wiring board. 2. Description of the Related Art With the downsizing of", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Human Hourglass in 100GEMS Text\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Indicate the model that performed best in terms of Score metric on the Human Hourglass in 100GEMS Text dataset ? output (Sparql query): Up to now, there are no research problems with benchmark datasets in the realm of Natural Language Processing research. However, it is not clear what metrics are used to evaluate results.", "SELECT DISTINCT ?metric WHERE { \r \t ?rf a orkgc:ResearchField; \r \trdfs:label ?rf_label. FILTER (str( ?rf_label) = \"UCF101 (fine tuned)\" ) ?metric;\r \t ?problem ?problem_lbl. \r \t ?problem rdfs:label ?problem_lbl. \r \t ?cont a orkgp:HAS_BENCHMARK ?benchmark;\r \t ?cont orkgp:P30 ?rf;\r \t ?cont orkgp:P31 ?cont.\r \t ?cont rdfs:label ?cont_label. \r \t ?cont orkgp:P32 ?problem. \r \t ?problem rdfs:label ?problem_label.\r } \r The evaluation metrics commonly used are: \t- AUC (Area Under the ROC Curve) which measures the model\u2019s performance as a whole, independent of any threshold. This is the most commonly used metric in NLP tasks. \t- F1 Score (Measure of Precision and Recall) which is also a threshold-independent metric. \r \r There are", "SELECT DISTINCT ?paper_title_id_label ?problem_label_label WHERE { { ?paper_title_id_label a rdfs:label ?paper_title_label or kgp:P30 ?benchmark; or kgp:P31 ?cont FILTER (str( ?cont) = \"MPQA\" && ?cont = \"model\" && ?cont = \"Benchmark\") } UNION { ?paper_title_id_label a rdfs:label ?paper_title_label or kgp:P30 ?cont FILTER (str( ?cont) = \"MPQA\" && ?cont = \"model\" && ?cont = \"Benchmark\") } } FILTER ( ?paper_title_id_label = { ?paper_title_id_label_prev } && ?paper_title_label = { ?paper_title_label_prev } && ?paper_title_id_label_prev!= ?paper_title_id_label && ?paper_title_label_prev!= ?paper_title_label", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 0.46751916.", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label ?paper; rdfs:label ?rf_label; FILTER (str( ?rf_label) = \"Torch Language Model Competition\") ?cont ; ?cont a mentortech:MT2020ObjectNavTeststd ; ?problem rdfs:label ?problem_lbl. } input (English text): Mentor Teacher is the model achieving the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset. output (Sparql query): The SUCCESS score for the Habitat 2020 Object Nav test-std benchmark dataset is 1.0000000. input (English text): The SUCCESS score for the Habitat 2020 Object Nav test-std benchmark dataset is the average of all training examples, not the best example. output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): :-) RESULT: { \"accuracy\": \"0.90\", \"label\": \"Natural Language Processing\" } * I would love to see more SPARQL benchmarks for orkgp:P30 and orkgp:P31 datasets! :) * Let me know if you find any problems with the benchmark datasets and how we can fix them. * If you", "SELECT ?benchmark ?benchmark_label (labelled_facts # ?benchmark) WHERE { ?benchmark labelled_facts MLDoc-Zero-Shot-English-to-German; rdfs:label ?benchmark_label; orkgp:HAS_BENCHMARK ?benchmark; } input (English text): MLDoc-Zero-Shot-English-to-German output (Sparql query): MLDoc-Zero-Shot-English-to-German is a benchmark dataset. corresponding labelled facts: MLDoc-Zero-Shot-English-to-German# ?benchmark:MLDoc-Zero-Shot-English-to-German-MLDoc-Zero-Shot-English-to-German.txt mldocs:MLDoc-Zero-Shot-English-to-", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Deep Reinforcement Learning for Winning Games\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Self-Attention\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } A: Apart from the url you mentioned, the link also contains these details: three sparql queries for benchmarking different components of the model a rdf-schema to hold your benchmark dataset a rest-service to query the dataset with your sparql query the data-sheet of the models you mention And I think this is enough for your request. <|endoftext|>", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label \"Cheetah\". } input (English text): The correct query is now: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label \"Cheetah\". } output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) =", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?problem rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"STS Benchmark\") ?paper orkgp:P30 ?problem; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl.", "SELECT ?problem ?problem_lbl", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Birdsnap dataset is used as a benchmark in Natural Language Processing research. The benchmark dataset is open and has been adopted by research groups worldwide. It is made available for the purpose of reproducing and comparing experimental results. Results achieved on Birdsnap are referenced in the publication agglomerative clustering for text mining, machine learning and data mining, and have been very influential in the field. output (Sparql query): Orchestra: This SPARQL endpoint can respond to queries in the SPARQL language", "SELECT DISTINCT ?model_name WHERE { ?rf a orkgc:ResearchField; ?model_name dcterms:description ?model_description. } input (English text): The neural network model is called Chopper. output (Sparql query):antically trained on all 4638 Atari 2600 games. input (English text): The Chopper model is based on a neural network architecture. It is trained to score each game on the Atari 2600 Chopper Command dataset as either a 1 (Win) or a 0 (Loss). output (Sparql query):s mention that the Chopper model was trained on all 4638 Atari 2600 games.", "SELECT DISTINCT ?problem WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Car Adjective Relation Extraction\") ?paper ; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Advertising Read more Rome (Italy) (AFP) The Italian government says it's committed to fighting climate change and making Europe a leader in the effort to curb the damage caused by carbon emissions. A statement published on Wednesday said the new government wants to continue the legacy of the former centre-left Democratic Party (PD) and its predecessors, and promises to reduce carbon emissions by a quarter by 2030 from levels in 2013. It will also introduce a national plan to cut greenhouse gas emissions by at least 80 million tonnes of carbon dioxide equivalent (CO2e) by 2050, the government said. The plan, which will also consider renewable energy and", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"RotoWire\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): RotoWire (Relation Generation) benchmark dataset. RotoWire (Relation Generation) benchmark dataset output (Sparql query):\r \r output (Sparql query): RotoWire (Relation Generation) benchmark dataset RotoWire (Relation Generation) is a dataset developed by Databricks that contains 2,000 relation extraction Penn Treebank A", "SELECT ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"SQuAD1.1\") ?paper; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Here are the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset: metric\tmetric_label\tdescription perplex\t\t perplexity\tCompute the average log probability of the sentence given the model prediction. CIDEr\t\tCIDEr_value\tCompute the phrase-level entity criticality detection (CIDEr) metric. CIDEr_value=CIDEr@both@training + CIDEr@keypoint + CIDEr@reference. SPG- Knox \tSPG-Knox\tCompute the similarity between the golden sentence and", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_link ?paper. ?paper_link osuka:mlstml-10M ?model; osuka:mlstml-10M ?model_label; orkgp:P30 ?cont; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Natural Language Processing Benchmark Datasets Papers that have utilized the Large mLSTM model: * [OSUKA:MLSTM-10M Large mLSTM Model](https://github.com/facebookresearch/osuka-mlstm) * [OSUKA:Neural Language Models (NeLM) Training on 110 Billion Tokens with 10 LSTM Cells on GTX 1080 Ti and 8GB Memory](https://arxiv.org/pdf/1906.03913.pdf) * [OSUKA", "SELECT ?paper ?cont_label WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"coling2011\") } ?paper; ?cont ?cont_label; FILTER ( ?cont_label = \"ACL Anthology\") } input (English text): Coling 2011 model: Coling 2011.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper OR ?cont OR ?cont OR ?benchmark FILTER ( ?benchmark ?problem_lbl) ?problem rdfs:label ?problem_lbl.", "SELECT DISTINCT ?cont FROM {Query} WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label; ?paper orkgp:P30 ?rf; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?cont rdfs:label ?problem_lbl; } Let us look at the SPARQL query that is issued by the Endpoint UI. In the above query, ?benchmark is a SPARQL variable that refers to a resource, namely, NCBI-disease. The resource includes two properties, namely, rdfs:label and rdfs:label. The variable ?problem_lbl, which is the rdfs:label of the problem property, is computed based on the values of ?cont and ?paper. The query is thus performing joins and subqueries on a dataset derived from several resources in an RDF graph store (i.e., DERBY-Store). The dataset is enriched with the benchmark dataset that exists in that store (i.e., NCBI-disease). According to Wikipedia, \"Benchmark datasets are used to evaluate the performance of", "SELECTing that can be used to get a list of datasets and a description of each dataset.", "SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Fashion-MNIST\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>To me there are two ways to cook pasta. You can cook it al dente, meaning \"to the tender\", or you can cook it slightly under cooked. I prefer to cook pasta al dente. If I want a slightly under cooked pasta, I cook it a few minutes less than al dente. Here is the basic process for how to cook pasta: 1. Wash and then cook the pasta in plenty of salted boiling water 2. While the pasta is cooking prepare a simple sauce for your pasta. 3. Drain the pasta and put it in a bowl. 4. Mix the pasta with your simple sauce and enjoy. There are hundreds of variations on this basic", "SELECT ?rf ?problem_lbl ?paper WHERE { ?rf a orkgc:ResearchField; ?paper ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. } \r \r Some research problems with benchmark datasets: 1. The datasets provided as examples for training and", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orp30:P30 ?rf; orp30:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orp30:P32 ?problem. ?problem rdfs:label ?problem_lbl. } }ampoline Benchmarking is a key factor in Natural Language Processing. The RotoWire dataset contains rich features to test all kind of NLP models. There are also a lot of benchmark papers published since RotoWire was introduced in 2014. Currently, there are 72 papers and their IDs in the RFD Biozonation benchmark, for example. ortical source code: https://github.com/lando/rolodeb-examples#+benchmarking Benchmark datasets: https://github.com/lando/rolodeb-datasets/blob/", "SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label; ?papercode code ; ?papercode comment ?code. ?contcode code ; ?contcode comment ?contcode. ?benchmark code ?problem_lbl. } input (English text): <https://github.com/ilyagrigorik/cross-lingual-relation-learning>) is one of the codes used in the paper Cross-lingual Relation Learning for Fine-grained Categorization. output (Sparql query):(code in code snippet is not the code that we reviewed.) input (English text): Cross-ling", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Montezumas's Revenge\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark;", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WMT English-Russian dataset\") ?paper ; ?problem rdfs:label ?problem_lbl. } input (English text): The WMT English-Russian dataset has the following annotation in the WMT2016 shared task: \"bleu score@paper\". The WMT team used this dataset as a benchmark in their evaluation campaign for WMT2016. The best performing model in the WMT2016 English-Russian dataset had a BLEU score of 28.7. A: Apart from overfitting, there are other aspects of evaluating and comparing results that require or heavily rely on independent or externally provided data, like measuring annotation quality and evaluating statistical or logical properties of the text, e.g., whether it contains plagiarism. As an example, see also this case study on evaluating and comparing systems on the Penn Treebank corpus. <|endoftext|>Air Pollution and Vascular Disease: A Review. Air pollution is an important global health issue, having a pervasive influence on human", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Top-1 Error Rate Model on Pets Dataset\") ?benchmark; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Top-1 Error Rate Model on Pets Dataset Output: \"Top-1 Error Rate Model on Pets Dataset\" anovski's model anovski is a well known Machine Learning researcher. Anovski was the leader of the machine learning team at Google Brain. His model name is \"Top-1 Error Rate Model on Pets Dataset\". <|endoftext|> Nintendo Switch Games That Currently Cost Over $20 Will Go Free on December 8th - grigoryok https://blog.nintendo.com/blog/2019/11/07/nintendo-switch-games-that-currently-cost-over-", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label:reuters21578 ?paper; ?paper rdfs:label ?problem_lbl; FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; ?paper orkgp:P31 ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } input (English text): In the following list, you can see the names of the papers that have performed benchmarks on the Reuters-21578 dataset: output (Sparql query):ILITY: select ?problem ?problem_lbl where { ?rf a orkgc:ResearchField; ?rf_label:reuters21578 ?paper; ?paper rdfs:", "mark (Penn Treebank) is a language model trained on the Penn-Treebank dataset. The dataset and its variants are typically used inelicitation (query) and evaluation (pull) against alternative models to assess their effectiveness and fit. Several variations of this dataset have been used as benchmarks in the research literature and their scores are commonly reported as part of evaluation. The variations include the White Paper Parser and the Brown Corpus. Response: There are no research problems with benchmark datasets in the realm of Natural Language Processing research. <|endoftext|>A three-year prospective observational study of power tools and hand injuries. This study investigated the association between the use of power tools and hand injuries in a population undergoing hand surgery at a single institution over a 3-year period. There was an overall increase in the use of power tools over the 3-year study period, while there was no change in the proportion of hand injuries that involved a power tool. There was a statistically significant association between power tool use and hand injury (P = 0.04). Multivariate analysis identified the use of power tools as a significant predictor of hand injury (odds ratio = 1.72; P = 0.009). The findings of this study support the idea that there is a need for", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): https://dl.dropbox.com/u/11565934/Papers/%20ANODE%20Paper%20-%20Code.pdf ?dl=0 output (Sparql query): ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark", "SELECT DISTINCT ?paper FROM {... } ; input (English text):Link to the research paper: https://openaccess.licrw.eu/action/showcase ?uri=uris output (Sparql query):Initially, we used the paper [K. Kong, H. B. Strung, L. P.ao, B. Ni, D. W. Kim, M. A. Painter, D. P., T. Xiao, and L. Cao. DY-MobileNetV3-Small: Small and stable baseline for detection and segmentation on mobile. In European Conference on Neural Information Processing Systems. 2018.]{} for the DY-MobileNetV3-Small model. It uses the DY-MobileNetV3 model as a small baseline for detection and segmentation on mobile. The paper has been utilized to benchmark a large number of papers, e.g., [T. Xiao, B. Ni, H. B. Strung, L. P.ao, M. A. Painter, and D. W. Kim. Detecting and segmenting faces in natural images using a small and efficient model. In European Conference on Neural Information Processing Systems. 2018", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Bonus points for mentioning RAVEN and SQuAD.", "SELECT DISTINCT ?problem_label ?paper_url WHERE { { ?problem rdfs:label ?problem_label; ?problem_label mpad-path; ?problem_label sphinx; ?problem_label matchagon; ?problem_label owl-mpad; FILTER (str( ?problem_label) = \"Natural Language Processing\") ; ?paper orkgp:P30 ?problem; ?problem orkgp:P31 ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark mpad-path; ?benchmark sphinx; ?benchmark matchagon; ?benchmark owl-mpad; ?benchmark } ORDER BY ?problem_label ?paper_url } input (English text): The following code references have used the MPAD-path model for benchmarking purposes: output (Sparql query): SELECT DISTINCT ?paper_url WHERE { ?paper orkgp:P30 ?problem; ?problem orkgp:P31 ?cont;", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The benchmark dataset BoolQ has been used to benchmark many model configurations in Natural Language Processing. The benchmark datasets are typically used in research to test the performance of different models. In general, model benchmarks are designed to demonstrate the effectiveness of a particular approach over another. Since the datasets used to create the benchmark datasets were created by non-scientific entities, there is a risk that the data does not properly represent the underlying domain and the results may not be reliable. Some models do not perform significantly worse on the benchmark datasets than on other datasets that are not part of the benchmark datasets. Since researchers typically use these datasets to compare and select the best model for their specific tasks, the lack of transparency about the models and datasets used for creating", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"German-to-English\") ?paper ; orkgp:P30 ?benchmark; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result for the BUCC German-to-English dataset is 71.56341910954473. astricp (Sparql query): 71.56341910954473", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"DMControl100k\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \r <|endoftext|>Q: Apache htaccess remove query string I'm working on a simple website for personal use which displays the content of a URL with mod_rewrite. www.mydomain.com/folder/subfolder/file.pdf www.mydomain.com/folder/file.pdf www.mydomain.com/folder/index.html I want to remove the query string from the URL, so the URL becomes: www.mydomain.com", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Multi-Perspective Matching\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } A link to the dataset used in the Multi-Perspective Matching model can be found in the benchmark paper: http://people.csail.mit.edu/jrenna/code-links.html The Multi-Perspective Matching model in the benchmark paper is used in multiple research problems such as entailment, coreference resolution, language modeling, and question answering. The results of this model have been shown to perform better than several other state-of-the-art systems. There have been several recent works to investigate the nature of the solutions and potential shortcomings in the model, but there is still much work to be done in this area. The", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet Benchmark\") ?paper ?problem; ?problem orkgp:P30 ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark b64dec:BitsPerDimensionValue ?dim. } input (English text): The best performing model according to this benchmark is the paper \"Even-shapes are good at grasping\" published in 2015 by Sanqi Cheng, Jiaxin Yao, Pengcheng Nie, Bo Xiong, Xiaoou Tang, and Xiao Yang. They trained a fully convolutional network on the ImageNet 64x64 dataset and achieved 45.02 Bits per dim on this subset of the ImageNet dataset. output (Sparql query): moins output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) =", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a CommonsenseQA; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): CommonsenseQA is a benchmark dataset for Natural Language Processing. The most popular and widely used model for NLP tasks is the Bahdanau Net, which was specifically designed for ONNX ILSVRC 2012 Model Comprehension. Can you provide the top benchmark result (metric and value) for CommonsenseQA ? NLP research problems with benchmark datasets CommonsenseQA challenge: 1. The dataset itself is available for download for free. 2. The provided dataset includes 22,500 questions", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): children are examined in the studies output (Sparql query): children are examined in the studies output (Sparql query): affirmative output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CoQA\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The CoQA dataset achieved an average result of 44.4 QLOPS on the Logistic Classification benchmark. The Logistic Classification metric is standard for this type of model and is often used in the context of NLP tasks. output (English text): The highest benchmark result achieved on the CoQA dataset, including the metric and its value, is 44.4 QLOPS. input (Sparql query): The highest benchmark result achieved on the CoQA dataset, including the metric and its value, is 44.4 QLOPS. <|endoftext|>[Antisense oligodeoxynucleotides targeted to rat eIF4E mRNA inhibit tumor growth in vivo].", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } It looks like the benchmark dataset \"ImageNet V2\" is available as orkgp:P32 ?problem. A: If the data exists but is not in RDF form, you cannot use it with Sparql querying language. <|endoftext|>Appropriate technologies for integration into outpatient cancer clinics in low-and-middle income countries: A guide for radiographers. Patient's outcomes are", "SELECT DISTINCT ?problem_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"DM Control 500 K Model\") } input (English text): SELECT DISTINCT ?problem_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"DM Control 500 K Model\") } output (Sparql query): DM Control 500 K Model achieved score of 486 on the Reacher benchmark dataset. input (English text): DM Control 500 K Model achieved score of 486 on the Reacher benchmark dataset ? output (Sparql query): SELECT DISTINCT ?problem_label ?problem_label_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Natural Language Processing - Quasart-T benchmark - Metrics. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ^- answer", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Multimodal PISA\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Datasets in Natural Language Processing are vital to the progress of the field. Benchmarks help measure progress over time by allowing researchers to compare their work to that of their colleagues. At face value, this sounds like a good idea. However, the creation of benchmark datasets comes with complications. From a data analyst's perspective, benchmark datasets often contain a lot of irrelevant information and redundant data which are hard to filter and therefore impossible to create a useful dataset. The data analyst is then forced to extract relevant information from the dataset themselves. This extra work makes benchmark datasets harder to generate and increases the time required to run the benchmark. These problems can be mitigated to a degree by excluding highly specialized datasets with", "SELECT DISTINCT ?benchmark { ?benchmark ?metric } WHERE { ?rf a orkgc:ResearchField; ?rf_label ?paper; rdfs:label ?paper_label; FILTER (str( ?paper_label) = \"Barabasi-Albert Benchmark\") ?benchmark; ?problem rdfs:label ?problem_lbl. } input (English text): Barabasi-Albert Benchmark is based on two machine learning metrics: 1. An intention-based gtdm: Average logarithm of the pagerank of nodes that act as a hub for other nodes in the network. Hubs are nodes that hold many other edges and are important in their network for various reasons. 2. An action-based twdm: Average logarithm of the number of times an action is predicted correctly. Actions are predicted using tag-of-interest model which is trained using tag-of-context model. Tag-of-context model uses TF-IDF as features and intent as label. }$: Creates a dataset called BenchmarkData", "SELECT DISTINCT ?metric WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Zaxxon\") ?metric; } input (English text): The Atari 2600 Zaxxon dataset is used to evaluate models on Video Games. Here are some metrics used to evaluate models on it: - accuracy: The fraction of actual commands that were executed correctly - f1-score: (2 * accuracy) / (accuracy + false positives) - correlation: The fraction of command matches that were also correct - homograph discrimination: The fraction of commands with different words in the same letter - lexical diversity: The number of different words in the commands - mutual information: The correlation between the entropy of the command and the entropy of the command bookends.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?problem conj :NaturalLanguageProcessing. input (English text): The top benchmark result is P31 Natural Language Processing with 42.62% accuracy. output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?problem conj :NaturalLanguageProcessing", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"BLEU\") ?paper ; orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The following model has the best BLEU score on the WMT16 English-German benchmark dataset: - BERT-base - BERT-large - XLNet - GPT-2 - GPT-3 - SQuAD 2.0 - SQuAD 2.1 - MS-DLSTM - QN-DCG - QN-BLEU - SNLI - XNLI - XNLI (Large) - Seq2Seq - BERT-base-xlarge - BERT-large-xlarge - XLNet-xlarge - GPT", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Sentence Classification\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \r input (English text): Natural Language Processing Benchmark Datasets:\r \r - Office Article Classification from [Qamar et al. 2012]. This is a benchmark dataset on the evaluation of various word embedding algorithms. The dataset contains several documents and their corresponding document classes as labels.\r \r - Netflix Dataset [Netflix Research 2014]. This is a benchmark dataset on sentiment analysis and classification. The dataset has a collection of reviews and corresponding labels indicating the classification of each review into positive, negative or neutral.\r \r - Trends in American Elections (Tam et al. 2010). This dataset contains socio-economic factors and candidates/parties/ideologies and their success/failure in electoral elections in", "SELECT DISTINCT ?problem. ?problem_lbl. ?metric { ?problem rdfs:label ?problem_lbl; ?problem ?problem; { ?problem_lbl gtc:ordering ?metric; ?metric bb:ranking.} } There are a few problems that exist in this dataset which limit its usage in natural language processing research: 1. The benchmark dataset does not provide any metric that can be used to evaluate the models on the RotoWire (Content Ordering) dataset. 2. The metric 'bb:ranking' does not exist on the RotoWire dataset. To address the first issue, it may be possible to create a new metric that evaluates the model on the RotoWire (Content Ordering) dataset. The RotoWire (Content Ordering) dataset also has issues with the benchmark dataset in that the benchmark dataset does not contain a description field so it is not possible to evaluate the description of a problem in the benchmark dataset. In addition, there is no way to search for problems in the benchmark dataset which makes it difficult to compare and evaluate problems. To address the second issue, the team should create a new benchmark", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Zero-Shot\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Related: SQuAD: Does SQuAD Still Capture the Competition ? <|endoftext|>\u201cMy whole life I\u2019ve just been a proponent of having a group of friends, enjoying the company of your friends and being loyal to your friends,\u201d Sharpe said. \u201cHaving more than one person that you\u2019re close to for whatever reason may separate you from your friends, that\u2019s just the way it is. I\u2019m pretty lucky that my parents raised me with", "SELECT DISTINCT ?paper WHERE { ?paper a oid :TokenlearnerModel ; ?paper rdfs:label ?label ; ?label codeURL ; FILTER (str( ?label) = \"http://www.cs.cmu.edu/afs/cs.cmu.edu/ac/id/322332/refcard/tokenlearner.html\") } ?paper rdfs:label \"Tokenlearner\"@en. ; ?paper codeURL \"http://www.cs.cmu.edu/afs/cs.cmu.edu/ac/id/322332/refcard/tokenlearner.html\"@en ; ?label codeURL \"http://www.cs.cmu.edu/afs/cs.cmu.edu/ac/id/322332/refcard/tokenlearner.html\"@en SELECT DISTINCT ?paper WHERE { ?paper a oid :TokenlearnerModel ; ?paper rdfs:label ?label ; ?label codeURL ; FILTER (str( ?label)", "SELECT DISTINCT ?paper_id ?benchmark FROM {http://data.medstan.si/dataset/MultiNLI-squad-medical-NLU-SQUAD-Multi-Language-Machine-Learning-Benchmark-Dataset/Sjkl021/statistics} input (English text): Here is a sample of models being evaluated on the MultiNLI dataset: Regex, GloVe, FastText, BERT, XLNet, SQuAD, DCG, SAT, ZeroNLI, NRC, XLM, SuperVec. output (Sparql query): SELECT ?paper_id (overview), ?benchmark FROM {http://data.medstan.si/dataset/MultiNLI-squad-medical-NLU-SQUAD-Multi-Language-Machine-Learning-Benchmark-Dataset/Sjkl021/overview} input (English text):", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"CoNLL++\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The benchmark dataset known as CoNLL++ is the most popular dataset for the task of natural language understanding and evaluation. The best performing model to F1 score of 0.789 on this benchmark dataset is the open model of the CoNLL 2003 shared task.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"finetuned UCF101 model\") ?paper. ?paper orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. } input (English text): {\"finetuned UCF101 model\", \"Little'snas\", \"Beyond the Taal Valley\", \"ADIT, Berlin\", \"Deep Packet Inspection - Algorithmic Advances\", \"Information Extraction from Partial Text with Neural Networks\", \"Diversifying Neural Networks for Scaling to Large Scores\", \"Microsoft Azure Machine Learning Repo\", \"Automatic Evaluation for Open Standard IELTS\", \"Neural Machine Translation for Internet Explorer\", \"Unsupervised Learning of Kernel Pyramid and Layer Routing by Algorithmstack:\", \"MAMAME-Neural Machine Translation for Android by Algorithmstack\"} {\"finetuned UCF101 model\", \"Little'snas\", \"Beyond the Taal Valley\", \"ADIT, Berlin\", \"Deep Packet Ins", "SELECT ?paper (STRENGTH ?strength) WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label; ?rf_label ?paper_label; FILTER (str( ?paper_label) = \"SciCite Benchmark\"); ?paper ?problem; ?problem rdfs:label ?problem_lbl; ?problem_lbl ?paper_label; ?paper_label ?paper; ?paper rdfs:label ?strength; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark ?paper_label; ?benchmark_label ?paper_label_label; ?benchmark_label ?paper_label; ?benchmark_label ?strength; ?benchmark_label ?strength; ?cont orkgp:P30 ?problem; ?cont orkgp:P31 ?cont; ?cont orkgp:P32 ?problem; ?cont orkgp:HAS_BENCHMARK ?benchmark;", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The following code was used in the paper Benchmarking Neural Image Captioning with WEST: https://github.com/mozilla-web-data/we/blob/master/docs/Benchmarking_Neural_Image_Captioning_with_WEST.ipynb. I tried running the code and it works fine. But I'm not sure whether the code is OK or not. output (Sparql query): I agree with the authors of this paper. The code provided is not only OK but is verbatim from the source code of WEST.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } { <metrics> <metric qualifier=\"comparison\" name=\"Bleu\" score=\"0.0\"/> <metric qualifier=\"comparison\" name=\"Meteor\" score=\"0.0\"/> <metric qualifier=\"comparison\" name=\"Rouge\" score=\"0.0\"/> <metric qualifier=\"comparison\" name=\"Demir2018\" score=\"0.0\"/> <metric qualifier=\"comparison\" name=\"BERT\" score=\"0.0\"/> </metrics> } <benchmark publication=\"http://doi.org/10.1037/his310026", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"AESLC Dataset\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The best performing model in terms of ROUGE-1 metric is using the Neo4j RDF Benchmark model with configuration file: openconfigs/neth5-large/conf/n4j-large-rouge.properties. The configuration file has the following lines: text: | | | # n4j-large-rouge.properties { \"n4j.index.name\": \"memory\", \"n4j.index.autostart\": true, \"n4j.index.merge\": \"auto\", \"n4j.index.partial\": \"auto\", \"n4j.index.partialdir\": \"/tmp/n4j-", "SELECT * FROM modelnet40:Problem WHERE metric = \"MSE\" ORDER BY score DESC LIMIT 10 MLS=67.5069, MSE=-0.62772262 orchelp_query=SELECT * FROM modelnet40:Problem WHERE metric = \"MSE\" ORDER BY score DESC LIMIT 10 symptomatic_query=SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } symptomatic_results=SELECT * FROM modelnet40:Problem WHERE metric = \"MSE\" ORDER BY score DESC LIMIT 10 symptomatic_results= symptomatic_results[0] MLS=67.5069 MSE=-0.62772262 orchelp_results= orchelp_results[0]", "SELECT DISTINCT ?metric_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Metric\")} input (English text): The following metrics are used on the Penn Treebank benchmark dataset: n-gram precision, n-gram recall, longest common substring (LCS) length, token ratio, information gain, and AUC.", "SELECT DISTINCT ?model ?benchmark WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?model oAnimalSymbolicModel. ?benchmark rdfs:label ?benchmark. } input (Sparql query): A SELECT DISTINCT query to retrieve a list of models that have been tested on the Reuters-21578 benchmark dataset. zzo (Sparql query): SELECT DISTINCT ?model ?benchmark WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:", "SELECT DISTINCT ?paper ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } mente lbls:Seaquest score", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 2.SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label", "SELECT DISTINCT ?paper ?problem_lbl WHERE { { rdfs:label ?paper_label. 'Natural Language Processing'@en #\"P30\"@en \"P31\"@en 'ObjectNet'@en 'ObjectNet Benchmark'@en } ?paper rdf:firstURL ?paper_url. ?paper ?problem. ?problem rdfs:label ?problem_lbl. } FILTER ( ?paper_label = \"Natural Language Processing\"@en) FILTER ( ?paper_label = #\"P30\"@en) FILTER ( ?paper_label = #\"P31\"@en) FILTER ( ?paper_label = #\"ObjectNet\"@en) FILTER ( ?paper_label = #\"ObjectNet Benchmark\"@en) } input (English text): So far we have: - \"Natural Language Processing\"@en - P30 - \"P31\"@en - ObjectNet - \"ObjectNet Benchmark\"@en - ObjectNet The problem identifier for", "SELECT ?problem ?problem_lbl ?f1 where { ?rf a orkgc:ResearchField; ?rf_label ?rf_label; ?paper ?paper; ?cont ?cont; ?problem rdfs:label ?problem_lbl; ?f1 numeric(float) ?", "SELECT DISTINCT ?paper_title ?paper_id, ?paper_title ?paper_id, ?problem_lbl ?problem_label, ?problem_lbl ?problem_label FROM <Query query here> <|endoftext|>After Republicans pummeled Democrats for the Senate in 2006 and 2008, many Democrats in swing states \u2014 especially women \u2014 grew complacent. Some of them voted for President Barack Obama twice even though they weren\u2019t completely thrilled with him. That\u2019s now starting to change. In the special election in Georgia\u2019s 6th district, incumbent Republican Phil Roe lost to Democrat James Thompson, the former mayor of Atlanta. But he came up short in a solidly Republican district. The Democrats\u2019 upset comes amid what\u2019s becoming a national trend. Five other special elections this year featured upset wins by Democrats. Gallup recently conducted a survey of more than 25,000 registered voters in 18 House districts that the parties hold by phone in March and April. Three of those districts held special elections in late April and early May; the other 15 districts had a normal election year in 2010, and 12 of them are now competitive this year. Democrats are running more strong candidates than they did two", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): As of now, the benchmark datasets used in the Rainbow+SEER model are the following: - QN-Reasoning - Quora - SQuAD - CoNLL-2007 - CoNLL-2010 - SST2 - SST-2 - Stanford Sentiment Treebank - WikiHop - SNLI - MultiNLI - Sentiel - GLOVE - Semeval-10 - Sem", "SELECT DISTINCT ?model ?model_label WHERE { ?paper orkgp:P30 ?model; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The best model obtained has a score of 0.738 which was achieved using the BELLO model. output (Sparql query):SELECT ?model ?model_label WHERE { ?paper orkgp:P30 ?model; orkgp:P31 ?cont. ?cont orkgp", "SELECT DISTINCT ?paper_id ?paper_title FROM { <http://www.w3.org/wiki/WMT2016/Datasets> #dataset-links <http:// Papers.ijar.ro/index.php/ijar/article/view/24> #paper-id24-title <http:// Papers.ijar.ro/index.php/ijar/article/view/26> #paper-id26-title <http:// Papers.ijar.ro/index.php/ijar/article/view/27> #paper-id27-title <http:// Papers.ijar.ro/index.php/ijar/article/view/30> #paper-id30-title <http:// Papers.ijar.ro/index.php/ijar/article/view/32> #paper-id32-title <http:// Papers.ijar.ro/index.php/ijar/article/view/33> #paper-id33-title <http:// Papers.ij", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?p32_label. ?p32_label ?p32_value. ?cont a orkgp:HAS_BENCHMARK ?benchmark. } input (English text): Are there any research problems with benchmark datasets in the realm of Natural Language Processing research ? output (Sparql query): They are open and freely available to the public. Please refer to the MUTAG paper for more details. The metric microF1 is only available on paper 10 of the MUTAG dataset. The mAP and logLoss metrics", "SELECT * FROM { performance ( metric:F1 ?metric ) } WHERE { ?metric ESC-50. FILTER ( ?metric > 0.8) } input (English text): The F1 score can be computed as: F1 = 2 * ( P * R )/( P + R ) where P is the precision and R is the recall. precision and recall can be calculated as: precision = { number of relevant facts returned / number of facts searched } recall = { number of relevant facts returned / number of documents searched } input (Sparql query):SELECT * FROM { performance ( metric:F1 ?metric ) } WHERE { ?metric ESC-50. FILTER ( ?metric > 0.8) } output (Sparql query):SELECT * FROM { performance ( metric:F1 ?metric ) } WHERE { ?metric ESC-50. FILTER ( ?metric > 0.8) } input (English text): The above query yields the following results: | metric |", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): QANTA is the benchmark dataset that achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset. output (Sparql query):", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not contain any datasets or models being evaluated on the TempEval-3 dataset. mRNA does not", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"RotoWire\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The best performing model is the Transformer-XL (GPT-4) model from Deep Mind (0.84 BLEU on RotoWire). Another popular dataset for NLP models is Word resembler from Microsoft Research. It uses the Penn Treebank as a reference corpus. Despite the fact that it is not a research problem per se (i.e. it is not Open), some researchers used the dataset as part of their internal testing or for evaluating proposed improvements. However, the dataset is not widely accepted for benchmarking reasons. For example, compared to RotoWire, the Word resembler dataset requires three to four times more data to achieve the same BLEU score (for", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs", "SELECT DISTINCT ?model ?benchmark WHERE { ?rf a orkgc:ResearchField; ?benchmark a GENIA - LAS. FILTER (str( ?benchmark) = ?model) ?model. } input (English text): Here is a list of models that have been tested on the GENIA - LAS benchmark dataset: - GoogLeNet - ResNet18 - ResNet34 - DenseNet201 - DenseNet121 - Wide & Deep - DBPNN - Convolutional LSTM - WordCNN - CoWaveNet - Encoder-Decoder - Dual Encoders output (Sparql query): blocked by filter (str( ?benchmark) = \"GoogLeNet\"): GoogLeNet blocked by filter (str( ?benchmark) = \"ResNet18\"): ResNet18 blocked by filter (str( ?benchmark) = \"ResNet34\"): ResNet34 blocked by filter (str( ?benchmark) = \"DenseNet201\"): DenseNet201 blocked by filter (str( ?benchmark) = \"DenseNet121\"", "SELECT DISTINCT ?problem_label WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Yelp Binary Classification\") } ?paper; ?paper orkgp:P30 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Results: Research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset are: - Deep Reinforcement Learning for Explaining and Evaluating Machine Coding, Mohta, Ofer et al. https://www.tandfonline.com/doi/full/10.1080/10473529.2017.1291831 - Interpretable and Robust Multi-Label Classification via Gradient Dispersion Optimization, Zou, Xiao-Dong et al. https://www.cvslibs.com/news/interpretable-and-robust-multi-label-classification-via-gradient-dispersion-", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Long Short-Term Memory\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Amazon benchmark dataset contains 6 prediction problems. Which prediction problem is associated with the LSTM model ? output (Sparql query): SELECT ?problem WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Long Short-Term Memory\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BEN", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Bank Heist\") ?paper ?problem; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \r \r input (English text): Atari 2600 Bank Heist dataset (P30Atari2600BankHeistBenchmark dataset) is a benchmark dataset for natural language processing research. There are two papers which include the benchmark: \"Learning sentence level semantic role labelling using inverse supervision\" and \"Structured prediction of natural language inference and text production with deep bidirectional neural nets\".\r output (Sparql query): \r <query mbrp:P30Atari26", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"A2 Score\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The answer is ?paper, \"A2 Score\", as it is the top performing model in terms of A2 score when benchmarked on the ANLI test dataset. The dataset \"ANLI\" is indeed available for research purposes, but it is also part of the Databricks incubator program and has restrictions on what kind of usage is allowed, so some advanced NLP use cases might not be viable. See the Databricks incubator page for more details.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?problem rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"atari2600asterixbenchmark\") } input (English text): The following are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset: \"Atari 2600 Asterix Benchmark\" P30 \"Atari 2600 Asterix Benchmark\" P31 \"Atari 2600 Asterix Benchmark\" P32 \"Atari 2600 Asterix Benchmark\" P33 \"Atari 2600 Asterix Benchmark\" P34 \"Atari 2600 Asterix Benchmark\" P35 \"Atari 2600 Asterix Benchmark\" P36 \"Atari 2600 Asterix Benchmark\" P37 \"Atari 2600 Asterix Benchmark\" P38 \"Atari 2600 Asterix Benchmark\" P39 \"Atari 2600 Asterix Benchmark\" P40 \"Atari 2600 Aster", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Yes, the code is at the link provided above. output (Sparql query):++] The benchmark dataset contains 11 problems of increasing difficulty. Some of the problems are really easy while the rest are really hard. In the hard subset, the average F-score on the development set is 72.02. It looks like the model is overfitting on this dataset. input (English text): Sure, I'll add it to", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): I used the code in the link provided in my answer to find the research problems in the field of Natural Language Processing. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; or", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\")", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Yes, some models perform worse when trained on benchmark datasets. Benchmark datasets should be avoided when possible.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet ReaL\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The following query gives me no results: SELECT ?problem WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet ReaL\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Note that the first query in the response gives me results, since there is data in the dataset that matches the pattern.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } $[]$ The paper and the code can be found here: $[]$ Allen Paul. \"Benchmark Dataset for the Natural Language Processing ResNet-152 Model.\" arXiv preprint arXiv:1810.02915 (2018). $[]$ https://github.com/allenp/BenchmarkDatasetPaper $[]$ Concretely, in the paper, you can see that we created a benchmark dataset with the following data: $[]$ |problem| problem", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The Pubmed dataset had a result of 36.8834. The metric and score are \"accuracy\" and \"0.93522.\" output (Sparql query):The Pubmed dataset had a result of 36.8834. The metric and score are \"accuracy\" and \"0.93522.\" <|endoftext|>Currently, with the rapid development of the wireless network technologies, data rate and transmission distance of the wireless network grow much faster than before. For example, with the increase", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Spanish: Alphabetic ordering: Aho et al. 2004. Ordering by similarity or diversity in large databases. SIGMOD Record, 28(3), 151-167. Aho et al. 2003. Frequently forgotten in large databases: Aho's order or ratio. Proceedings of the 18th International Conference on Supercomputing, Springer-Verlag, 2003, 227-235. Aho et al. 2001. Ordering by similarity or diversity in large databases. SIGMOD Record, 18(1), 67-79. Aho et al. 2000. High performance retrieval based on implicit orders in large databases. IEEE Sixth Annual Database Theory Symposium, 368-375", "SELECT DISTINCT ?benchmark ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; } input (English text): The highest benchmark result achieved on the REDDIT-B dataset is 35. output (Sparql query): SELECT ?benchmark ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; } output (Sparql query): 30.", "SELECT ?paper_id ?problem_id WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query):SPARQL query completed <|endoftext|>I was born in 1972 in a small village called Sorel-Tracy, in Quebec, Canada, into a family of French-Canadian heritage. I was the second of four children. My father was a manual labourer. I grew up in a very modest, ordinary home. I did not like it. I was an anxious child and I remember wanting to run away many times, but the fear of the unknown kept me at home. I wanted to play outside. As a teenager, I spent a lot of time working on cars", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"SST-2 Binary Classification\") ?model. ?model rdfs:label ?model_label. }\r \r [\r {\r \"problem\": \"SST-2 Binary Classification\",\r \"problem_lbl\": \"SST-2 Binary Classification\"\r },\r {\r \"problem\": \"SQL Speech Recognition\",\r \"problem_lbl\": \"SQL Speech Recognition\"\r },\r {\r \"problem\": \"Support Vector Machines\",\r \"problem_lbl\": \"Support Vector Machines\"\r },\r {\r \"problem\": \"Levenberg-Marquardt\",\r \"problem_lbl\": \"Levenberg-Marquardt\"\r },\r {\r \"problem\": \"Hidden Markov Models\",\r \"problem_lbl\": \"Hidden Markov Models\"\r },\r {\r \"problem\": \"Logistic Regression\",\r \"problem_l", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } inputs: None duties: Select the top-performing research problem (solution label) on the Hendrycks Benchmark dataset output: Top-performing research problem (solution label): Complexities in generative probabilistic parsing duties: From the Hendrycks Benchmark dataset: http://www.cs.cornell.edu/people/pemsaad/hendrycks.html duties: The Hendrycks Test dataset has a top score of 37.81 on the CoNLL metric. The problem in question has an entry with a label of Complexities in generative probabilistic parsing. duties: The", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?cont. ?cont orkgp:", "SELECT DISTINCT ?paper_title ?paper_id WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_title; ?paper_id } UNION { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Text Mining\") ?paper_title; ?paper_id } UNION { { ?paper_title ?paper_id; ?cont } } } } input (English text): TITLE : Natural Language Processing: Towards Databases of Long Tail Documents with Train and Validate on the Penn Treebank\t\tP30\t\tP31\t\tP32 output (Sparql query): SELECT ?paper_title ?paper_id WHERE { { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_title; ?paper_id } UNION { { ?rf a orkgc:Research", "SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ClueWeb09-B model evaluation\") ?model. } input (English text): The ClueWeb09-B dataset is commonly used for evaluating Named Entity Recognition, Part-of-Speech Tagging and Phrase-based Reordering models. The official evaluation script can be found at http://clueweb.isti.cnr.it/. However, there is no official benchmark for this dataset. There are two open-source datasets that are commonly used for evaluation, MultiNEN2 and CoNLL-2009. However, the CoNLL-2009 dataset is not officially released, and a new version has been announced that includes more benchmarks. xture (Sparql query): CoNLL-2009 is a dataset created by the Common Named Entity Tagging and Entity Recognition Shared Task of the 45th Annual European LanguageModeling Conference (CoNLL), designed for evaluating Named Entity Recognition, Part-of-Speech", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } importing dataset has_benchmark. input (Sparql query): ORKGPC has_", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ({ <paper><code>https://github.com/sunnylab/CvT-21-Model-Benchmarking</code></paper> <paper><code>https://www.cs.cmu.edu/news/2017/09/22/cvt-21-model-benchmarking-corpus.html</code></paper> <paper><code>https://www.insight-solutions.com/fileadmin/content/datasets/corpora/cvt-21.zip</code></paper> <paper><code>https://www.cisecurity.org/publications/research/", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BEN", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>The present invention relates to a composition for washing hair and a hair wash containing the same. More specifically, the invention relates to a composition for washing hair that is excellent in softness and replenishability and also provides excellent hair washing performance, and a hair wash containing the same. Hair wash compositions for washing hair have been known that include an anionic surfactant and a persulfate. The hair wash compositions allow for soft washing with excellent replenishability. In recent years,", "SELECT DISTINCT ?metric ?lbl WHERE { ?paper ?cont ?metric ; ?cont rdf:type orkgp:HAS_BENCHMARK. ?benchmark rdfs:label ?lbl. } input (English text): Qnli datasets are popular benchmarks for NLP applications, but there are some studies that have raised concerns about their reliability. For example, according to the report cited below, the dataset contains 23% fabricated instances. output (Sparql query): Print the following contents to STDOUT: SELECT DISTINCT ?metric ?lbl WHERE { ?paper ?cont ?metric ; ?cont rdf:type orkgp:HAS_BENCHMARK. ?benchmark rdfs:label ?lbl. } input (English text): https://www.cl.cam.ac.uk/sreps/wldl10/html/resources/papers/knplp14.pdf input (English text): The dataset QNLI was introduced as part of the larger QNLI+", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?benchmark; ?paper ?cont. ?cont a orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdf:type ?problem; ?problem rdfs:label ?problem_lbl; } input (English text): AND-OR - Language Model. output (Sparql query):", "SELECT ?problem ?problem_lbl ?paper WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): AULD (Average Union of Locals Distance), ACC (Accuracy), and F1-score. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl ?paper WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_", "SELECT ?metric ?score WHERE { ?paper rdfs:label \"Natural Questions (long)\" ; rdfs:label \"Natural Questions (long)\"@en ; ?problem rf:hasBenchmark ?benchmark ; ?benchmark rdfs:label \"XGboost\"@en ; ?benchmark rdfs:label \"BERT\"@en ; ?benchmark rdfs:label \"SQuAD2\"@en ; ?benchmark rdfs:label \"QuAD\"@en ; ?benchmark rdfs:label \"TriviaQA\"@en ; ?benchmark rdfs:label \"TrecDataset\"@en ; ?benchmark rdfs:label \"ParaphraseTor\"@en ; ?benchmark rdfs:label \"TED2014NQ\"@en ; ?bench", "SELECT DISTINCT ?paper ?cont WHERE { ?paper a orkgp:P31 ?cont; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Czech-English\") } input (English text): WMT2016 Czech-English, score: 23.13, duration: 0:08.88. As I see it, benchmark datasets are appropriate in the context of Natural Language Processing because the datasets can be used to measure and compare the processing performance on different methods. In general, most methods are suitable for different languages, therefore these datasets are indispensable for showing what language will perform better. However, I am also interested in knowing how reliable these results are. It is better if benchmark datasets could have been compiled in a repeatable way. A couple of research problems could be: 1) The data sometimes has inconsistencies, as the \"Czech-English\" example shows. So how can we normalize the data to achieve better performance ? 2) Benchmark datasets usually rely on word-similarity measures (for example, the Jaro-Winkler distance", "SELECT ?sector ?count WHERE { { ?sector a owl:Thing; owl:equivalentBinding ?binding. ?binding owl:DatASectToMatch ec:EnergySector. ?count ?count } UNION { ?sector a owl:Thing; owl:equivalentBinding ?binding. ?binding owl:DatASectToMatch ec:PetroleumSector. ?count ?count } UNION { ?sector a owl:Thing; owl:equivalentBinding ?binding. ?binding owl:DatASectToMatch ec:CommodityPrices. ?count ?count } UNION { ?sector a owl:Thing; owl:equivalentBinding ?binding. ?binding owl:DatASectToMatch ec:WaterSector. ?count ?count } UNION { ?sector a owl:Thing; owl:equivalentBinding ?binding. ?binding owl", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label ?paper_id. FILTER (str( ?paper_id) = \"Kinetics-600\") ?paper_id; ?cont a orkgp:HAS_BENCHMARK ?benchmark; ?cont_label ?cont_label_label. FILTER (str( ?cont_label) = \"Kinetics-600\") ?cont_label; ?problem rdfs:label ?problem_lbl. } input (English text): * Kinetics-600 * PR Ranker * RankMD * Recursive Ranker * QANmod * Memet * TextRank * Recursive Deep Ranker * Recursive Deep Ranker + Local Mention Naming * Recursive Deep Ranker + Local NDCG Mention Naming * Recursive Deep Ranker + Local NDCG Mention Naming + User Authorization * Recursive Deep Ranker + Local NDCG Mention Naming + User Authorization + Text WFF Each of these", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Rotowire (Content Selection) dataset\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?paper. ?paper rdfs:label ?paper_lbl. } output (Sparql query): Inserted 1 record(s) into table. input (English text): Here's the list of research papers with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset: output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Rotow", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } *precision : ?paper 0.95 ?cont 0.95 ?benchmark 0.90 *recall : ?paper 0.96 ?cont 0.96 ?benchmark 0.94 *f1 : ?paper 0.93 ?cont 0.93 ?benchmark 0.91 *Precision *Recall *F1 *0.95*0.96*0.94 = 0.90 *0.96*0.96*0.94 = 0.93 *0.93*0.93 = 0.90 *0.95*0.95*0.95 = 0.95", "SELECT query as ?rf and ?problem_lbl respectively. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?problem rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Dmlab-30\") } <|endoftext|>TORONTO \u2014 Justin Trudeau has been knighted by the Queen for his \"exceptional leadership and service to Canada,\" the government of Canada announced Wednesday. Trudeau, who is the country's 24th prime minister, was awarded the hereditary title of Sir for \"his leadership of the Canadian government and for his services to Canada and the community of nations.\" Trudeau, 47, is the second Canadian prime minister to be knighted. Joe Clark was knighted in 1957 for \"gallantry and devotion to duty\" during", "SELECT DISTINCT ?paper ?cont_label, ?paper_label, ?cont_label, ?paper_label WHERE { { ?paper a orkgc:ResearchPaper; rdfs:label ?paper_label; _:WMT2016 german-english; _:WMT2016 english-deu; _:WMT2016 italy-deu; ?cont a orkgc:ResearchContribution; rdfs:label ?cont_label; _:semeval2016; _:semeval2016 italy; _:semeval2016 english; _:semeval2016 deu; ?paper_label ?paper. ?cont_label ?cont. } } ORDER BY ?cont_label ?cont_label output (Sparql query): SELECT DISTINCT ?paper ?cont_label, ?paper_label, ?cont_label, ?paper_label WHERE { { ?paper a orkgc:ResearchPaper; rdfs:label ?paper_label; _:WMT2016 german-", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): organigp:P31 9.25. input (English text): The Cartpole dataset was released by DeepMind, a Google-acquired AI company, in 2015 and the DMControl500k dataset was released in 2016 by the OpenAI team. Organizations like Google and OpenAI have established benchmark datasets to evaluate the state-of-the-art performance on specific research problems. Organizations that want to compare their own result against the state-of-the-", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?paper ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } <|endoftext|>Geisha dolls are made from porcelain with a metallic skin that can be removed, revealing an inner mechanism that makes them dance. Once you take off the skin, you realize that the porcelain doll is actually a machine made from gears, cogs", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ok meu resposta : Here is the updated answer: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>High-throughput measurement of physicochemical parameters of red wines using a one-shot method. A continuous solution measurement method, the \"one-shot\" multimode microfluidic chip has been developed for fast and accurate measurement of important physicochemical parameters of red wines. A sensitive light absorbing cell was fabricated by using dichlorofluorescin as the sensing", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl ?cont ?cont_lbl WHERE { ?paper a mbrs:Repository; ?cont a mbrs:Repository; ?cont_lbl a mbrs:Label; ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?cont orkgp:", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper OR <paper> <title>DCC Neural Natural Language Model</title> <ID>P30</ID> </paper> <paper> <title>Deep Learning for Sentence Classification</title> <ID>P31</ID> </paper> <paper> <title>Improving Candidate Extraction with Sequence Labeling and Word Sense Disambiguation</title> <ID>P32</ID> </paper> ?rf; orkgp:P33 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): <paper> Title: DCC Neural Natural Language Model ID:P30 </paper> <paper> Title: Deep Learning for Sentence Classification ID:P", "SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Best performing model\" ) ?benchmark; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The best performing model on the ANLI test dataset is the model tuned on UD_combined_snli and UD_combined_wksht. The A3 metric on this dataset is best achieved by the following model: <model description here>. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Best performing model\" ) ?benchmark; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkg", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The iNaturalist 2019 benchmark dataset was created in order to evaluate approaches for topic model based parsing. It consists of 311 pages from 10 different countries with different linguistic styles. The dataset is split into two parts: document sequences and paragraph/claim summaries. The paragraph summaries are shorter than 10 sentences, the document sequences are between 10 and 30 sentences. The evaluation is performed using three metrics: BLEU, ROUGE and METEOR. BLEU is calculated over sentence level precision and recall. ROUGE computes the Recall@n which measures how well a model can determine the dependency of the matched sub-parts of a sentence.", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FIL", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"WMT2014 English-German\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } The problem here is that there is no 'bleu score' as such in SPARQL. What's actually happening is that the scoring function that is implemented in SPARQLPathChecker in the Sparql waterFlo-SPARQL runtime library is giving each model a single score for each of the various components of BLEU. So if a model has a good parse but a bad word alignment the score is probably not going to be particularly high. In order to do a proper comparison between models that don't perform well on one metric they should also", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } output (Sparql query): The results are empty. However, there are a number of research papers that have benchmarked models on the Cart Pole (OpenAI Gym) dataset: * Li, Xiao, Ma, Song, Xiao, Ji, Wen, and Li. \"Distilling Large-Scale Knowledge by Ensemble of Reinforcement Learning Agents.\" 2017. * Ma, Song, Wen, and Li. \"Ensemble of Policy Optimization Agents Generalizes to Diverse Domains.\" 2017. * Zhong, Chang, Hwang, Pritzel, and Parthasarathy. \"Distributional Meta-Learning.\" 2017", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The code in this file references the BiDAF model, a widely used Swedish publicly available NLp model, and the Self Attention method. The BiDAF model is also used in the paper. Finally, the ELMo language model is a widely used Neural Language Model which is also described in the paper. In particular, the code has been used to produce the following graph: So far", "SELECT DISTINCT ?problem_lbl WHERE { ?problem rdf:label \"Natural Language Processing\"; ?problem rdfs:label \"Natural Language Processing\"; ?problem rdf:type orkgc:ResearchField; ?problem rdf:type orkgc:Problem; ?problem rdf:type orkgp:HAS_BENCHMARK; ?problem rdf:type orkgp:P30; ?problem rdf:type orkgp:P31; ?problem rdf:type orkgp:P32;} Possible metrics include: precision, recall, f1-score, true positive rate, false positive rate, accuracy, precision recall (PR) curve, and receiver operating characteristic (ROC) curve. There is no consensus on which metrics are the best for evaluating a model on a benchmark dataset. Some people might prefer a model that performs well on a few metrics over a model that performs well on many metrics. The value of a benchmark dataset is heavily dependent on the problem it is solving, so it is difficult to compare different datasets and evaluate them objectively. There is also no consensus on which metrics should", "SELECT * FROM appendix WHERE version = \"1.2\" AND code in (\"6-layer QRNN model\", \"SVD\", \"RNNLM\", \"rRNN\", \"rGRU\", \"GRU\", \"LSTM\", \"CNN\") input (English text): Done. <|endoftext|>{ \"word\": \"Impress\", \"definitions\": [ \"Use cleverly or skilfully; make an effective or effectual use of.\" ], \"parts-of-speech\": \"verb\" }<|endoftext|>Background ========== Biofilms are structured communities of microbial cells encased in an extracellular matrix of polymeric substances, such as polysaccharides, proteins and extracellular DNA, which adhere to biotic or abiotic surfaces. Biofilm development can significantly reduce the efficiency of bioremediation processes due to reduced access of theisinergies to the embedded microbial population and increasedsahiltance of organic matter degradation by the bulk extracellular communities \\[[@B1]\\]. Biofilm formation is strongly favoured by environmental conditions that promote adhesion and matrix polymerisation and", "SELECT DISTINCT ?problem", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label rdfs:label \"Natural Language Processing\"@en; rdfs:label ?paper. ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } [ { \"problem\": { \"has_benchmark\": { \"has_benchmark\": true, \"benchmark\": { \"has_benchmark\": true, \"benchmark_id\": \"P32\", \"benchmark_name\": \"RNN Model on NYT24\" }, \"problem_id\": \"P31\", \"problem_label\": \"RNN Model on NYT24\", \"problem_lbl\": \"RNN Model on NYT24\" } }, \"problem_label\": \"RNN Model on NYT24\" } ] The NYT24 dataset is a benchmark", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Robot Navigation\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The datasets used in", "SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; ?rf_label seel.cse.lsu.edu/data/refsq17.zip; ?model Multi-hop Reader, ReadCombiner, TF-IDF, Topic Modeling, Sentence Length, Bag-of-words, CM winning language model, LDA winning language model, SReELMo, Steinberg SVM, Wagner Attention. } output (Sparql query): crucial note: The number of model names are different across the returned models. This is because some models were reported multiple times, for example CM winning language model and LDA winning language model both do model \"winning language model\". crucial note:", "SELECT DISTINCT ?model_name WHERE { { ?model_name rdfs:label \"IMDb Model\". ?model_name implements IMDbM. ?model_name implements IMDbM.loss. ?model_name implements IMDbM.loss2. ?model_name implements IMDbM.loss3. ?model_name implements IMDbM.loss4. ?model_name implements IMDbM.loss5. ?model_name implements IMDbM.loss6. ?model_name implements IMDbM.loss7. ?model_name implements IMDbM.loss8. ?model_name implements IMDbM.loss9. ?model_name implements IMDbM.loss10. ?model_name implements IMDbM.loss11. ?model_name implements IMDbM.loss12. ?model_name implements IMDbM.loss13", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?rf rdf:type orkgp:ResearchField. ?paper codeRef ?paper code \"https://dl.acm.org/citation.cfm ?id=3268981.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Audio Classification\") ?paper orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Sparql query: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Audio Classification\") ?paper orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Problem Problem_Label - - Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification Audio Classification", " ?problem Cart Pole (OpenAI Gym) ?problem_lbl Cart Pole Benchmarking (Metric: Learning Rate, Score: 0.63) ?benchmark Cart Pole (OpenAI Gym) Learning Rate ?cont Cart Pole (OpenAI Gym) ?paper A human-level performance cart pole in the OpenAI Gym. ?problem This problem has the highest benchmarking score for the OpenAI Gym dataset Cart Pole. counc Cart Pole (Metric: Learning Rate, Score: 0.63) ?cont We train on the CartPole-v0 and a Gym player on CartPole-v1. The latter is trained with the OpenAI Dueling Gm-Agent implementation. The result is a learning rate of 1e-4.", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id ?paper; ?cont id ?cont; orkgp:P30 ?benchmark; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Stadium Reacher easy (DMControl100k) Output: SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper_id ?paper; ?cont id ?cont; orkgp:P30 ?benchmark; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Results: problem", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset ? output (Sparql query):azzo:Oracle Net, reaching a total of 38,098 MACs. input (Sparql query):Oracle Net outperformed all other models on the ImageNet benchmark dataset with a total of 38,098 MACs. output (Sparql query):SELECT DISTINCT ?problem ?problem_", "SELECT ?problem2 ?problem_lbl2 WHERE { ?problem2 a orkgc:ResearchProblem; ?problem2_label:EnumerationValue ?problem2_label; ?problem2 rdfs:label ?problem_lbl2; } input (English text): Specifically, the metric listed is precision, but the source says recall is actually used, which is: (Sens + Spec - 1) / (Sens + Spec) output (Sparql query): SELECT ?problem2 ?problem_lbl22 WHERE { ?problem2 a orkgc:ResearchProblem; ?problem2_label:EnumerationValue ?problem2_label; ?problem2 rdfs:label ?problem_lbl2; ?problem2_score:EnumerationValue ?problem2_score; } input (English text): That makes sense -", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): I agree with the provided metrics for evaluating models on the DBpedia benchmark dataset. output (Sparql query):SELECT * FROM ?problem WHERE { ?problem rdfs:label ?problem_lbl; ?problem ?problem_lbl; ?problem_lbl in ( \"precision\", \"recall\", \"f-score\" ) } output (Sparql query): NULL There are three metrics that are used to evaluate models on the DBpedia benchmark dataset: precision,", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } This query gets the top 3 results over the dataset Atari 2600 Enduro. The query results include: - DISTINCT problem (type: Graph). This is the name of the benchmark. - DISTINCT problem_lbl (type: String). The label of the benchmark. - The metrics associated with the benchmark, using a \"has benchmark\" relationship. This query retrieves the metrics for the top 3 benchmarks. The values of these metrics are: - P30: \"Average frames per second (FPS)\" - P31: \"Number of states per minute\" - P32: \"Total number of states\"", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } And the results are as following: Problem Problem_lbl count - Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40 Natural Language Processing Hutter Prize 40", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. }\"} ) The output of this query is: // response 1 - dataset seel.cse.lsu.edu/data/re17.zip ... // response 2 - the highest result is 87.03738362232827 with metric AP and score of 43.0 The seel.cse.lsu.edu/data/re17.zip dataset is a Research Field Benchmark dataset. It was generated by CSCAP. It contains a paragraph, named paper, with a link to the corresponding dataset in the data repository SeqSpot. The dataset contains 22 pairs of equally sized problem and corresponding problem labels. This benchmark dataset has been evaluated in multiple NLP tasks,", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } olutions: Since the data model is top-down and the sole modality for any given research question is the benchmark dataset, we need to know what the benchmark dataset is, who created it, and what its name is. The second data model requirement is the ontological representation of the benchmark dataset (the class diagram). However, ontologies for benchmarks are under development, so there is uncertainty in these issues as of present. Therefore, we use a variety of approaches to find the corresponding data and ontologies. We first use knowledge sources such as the LOD2 database, to find the benchmarks and their properties. We then search the literature for the name of the benchmark and corresponding dataset. We use various natural language processing tools to", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"12-layer Transformer-XL model\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): https://arxiv.org/abs/2004.00393 https://arxiv.org/abs/2005.06764 https://arxiv.org/abs/2007.02327 https://arxiv.org/abs/2007.01547 https://arxiv.org/abs/2007.02327 https://arxiv.org/abs/2008.08310 https://arxiv.org/abs/2010.03203 https://arxiv.org/abs/2011.10080 https://arxiv.org/abs/2015.07931 https://arxiv.org/abs/2017.03328 https://arxiv.org/abs/2017.02769 https://arxiv.org/", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Atari 2600 Up and Down\") ?paper orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Q: How to pass method as parameter from one View Controller to another and then execute the method in second View Controller ? From First View Controller: MyFirstViewController *mFirstVC = [[MyFirstViewController alloc]initWithNibName:@\"MyFirstViewController\" bundle:nil]; mFirstVC.userName = self.userName; [self presentViewController:mFirstVC animated:YES completion:nil]; From Second View Controller: -(void)anotherMethod { //Here I want to call the first method and set the variable to self.userName } The first method is to set the variable and first VC then call", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } oreason = \"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf", "SELECT DISTINCT ?model_label WHERE { ?model rdfs:label ?model_label. } import (English text): The Jacquard dataset has been used to evaluate a wide range of machine learning and natural language processing models. Here is a partial listing of the models that have been benchmarked: output (Sparql query):\u0441\u0442\u0432\u043eimport (English text): The Jacquard dataset has been used to evaluate a wide range of machine learning and natural language processing models. Here is a partial listing of the models that have been benchmarked: output (Sparql query):rievedf (English text):+ machine learning + natural language processing output (Sparql query):* + keras + caret + fastText + tegrafcn + xgboost + oneplm + deepspeech + sgd_lm output (Sparql query):* + glmnet + elasticsearch + cosine similarity + hierarchical clustering import (Sparql query):* + nn-gram import (English text):+ keras + caret + fastText + tegrafcn + xgboost + oneplm + deepspeech + sgd_lm + nn-gram", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Technical Support Ticket\") ?problem orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } +-+-+ | ?problem | ?problem_lbl | +-+-+ | P30 | P31 | | P31 | P30 | | P32 | P32 | +-+-+ +-+-+ | Metric |", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label enwik8:benchmark. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem rdfs:label ?problem_lbl. } input (English text): Natural Language Processing (NLP) has proven to be a powerful and versatile technology for extracting useful information from natural languages and creating automated solutions for various applications such as information retrieval, question answering and machine translation. There has been great interest in NLP across academia and industry for several years. Following the success of the SQuAD and GQA datasets, there has been a surge of model benchmarking on open dataset such as enwik8. These datasets have led to intense research in benchmarking and comparing against each other. output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; ?rf_label enwik8:benchmark. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper; ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"FGVC Aircraft\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The TF-IDF approach significantly outperformed the other models, which means that the benchmark dataset is indeed a research problem. output (Sparql query):Invalid query. You have to include a link relation or at least some cross", "SELECT DISTINCT ?metric ?metric_label WHERE { \t ?rf a orkgc:ResearchField; rdfs:label ?rf_label. \tFILTER (str( ?rf_label) = \"Natural Language Processing\") ?benchmark; orkgp:P30 ?rf; orkgp:P31 ?cont.", "SELECT DISTINCT ?paper ?cont FROM <http://owlhouse.com/benchmark-atari/>. output (Sparql query):PtrFromStringoutput (Sparql query): TRANSFORMER,SEQ2SEQUEL,MIXED,MIXEDTOBBY,MIXEDTOACCOUNT,MIXEDTOCHICKEN import (Sparql query): SELECT * { ?paper ?cont } } output (Sparql query): output (Sparql query): TRANSFORMER,SEQ2SEQUEL,MIXED,MIXEDTOBBY,MIXEDTOACCOUNT,MIXEDTOCHICKEN import (Sparql query): SELECT * { ?paper ?cont } } output (Sparql query):PtrFromStringoutput (Sparql query): TRANSFORMER,SEQ2SEQUEL,MIXED,MIXEDTOBBY,MIXEDTOACCOUNT,MIXEDTOCHICKEN", "SELECT DISTINCT ?model ?label WHERE { ?rf a atari2600:Boxing.Model. } ?rf ?model. ?label boxing.model. } SELECT DISTINCT ?model ?label WHERE { ?rf a atari2600:Boxing.Model. } ?rf rdfs:label ?label. ORDER BY ?label ASC;", "SELECT * FROM dataset WHERE print(tfidf_similarity(paper, benchmark)) = true tack (Sparql query): Let ?benchmark be a dataset, and let ?paper be a document. Then tfidf_similarity(paper, benchmark) is a numeric value that represents the tfidf-based cosine similarity between the two matrices: where tf is the tf-idf score for word w in paper, and c is the tf-idf score for", "SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking\") ?model_label. } input (English text): http://www.cnts.sp.org.br/baca-da-cena/gaussian-NMG-gcm-wmt17.htm output (Sparql query): SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking\") ?model_label. } input (English text): It is implemented using the decoder gm (Gated Multi-layer Perceptron) from the GPT-3", "SELECT DISTINCT ?problem2 ?problem2_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper2 orkgp:P30 ?rf2; orkgp:P31 ?cont2. ?cont2 orkgp:HAS_BENCHMARK ?benchmark2; orkgp:P32 ?problem2. ?problem2 rdfs:label ?problem2_lbl. } input (English text): <https://doi.org/10.5683/SPP/M7IR6B6>; <https://doi.org/10.5683/SPP/QBHK2P>; <https://doi.org/10.5683/SPP/QBC9XF>; <https://doi.org/10.5683/SPP/QBHKT8>; <https://doi.org/10.5683/SPP/P2YQM9>; <https://doi.org/10.5683/SPP/P2YQ", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_id. ?paper_id ogr:Title ?title. ?title rdf:type ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?problem_lbl. } import (Sparql query): SELECT * WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_id. ?paper_id ogr:Title ?title. ?title rdf:type ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?problem_lbl. } select ?problem ?problem_lbl where { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label)", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } protected response text: You can find the code references in the paper by @dariush-mehrjou, which has used the PAR Transformer Large model for benchmarking purposes.", "SELECT DISTINCT ?problem). Output parameters: problem_lbl: Label of the dataset problem (e.g. \"Natural Language Processing\"). input (English text): Output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Fans want Rihanna to take a stance on Syria Rihanna wants people to know what she thinks and who she is. And people know what they can do with that. Rihanna wants people to know what she thinks and who she is. And people know", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?paper_cont. ?paper_cont orkgp:P30 ?benchmark; ?paper_cont orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?problem ?problem_lbl. } <|endoftext|>Q: Convert base64 to utf-8 in python I need to convert a base64 string into UTF-8 in python but I'm getting an invalid encoding error I have tried to decode it with base64.b64decode(text) but I'm getting the same error A: You need to url_encode it: new_string = new_string.decode('utf-8').encode('base64', 'url') <|endoftext|>Erlang Vs Python - Memory Usage If you ever wondered if Erlang or Python was a better language for systems programming, I\u2019ve got good news. You don\u2019", "SELECT DISTINCT ?model ?model_lbl WHERE { \r { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Habitat 2020 Object Nav test-std\") ?paper; orkgp:P30 ?rf; orkgp:P31 ?cont; orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } \r } \r input (English text): \r Model | Model_lbl \r :- | :- \r Biline | Habitat 2020 Object Nav test-std \r :- | :- \r CoNLL | CoNLL \r :- | :- \r SPINET | SPINET \r :- | :- \r MOAB | MoAB \r :- | :- \r BERT | BERT \r :- | :- \r GloVE | GloVE \r :- | :-", "SELECT DISTINCT ?model ?model_label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_label. ?model gdw:ModelID ?model_label. } Output: uno:Here you can see that different models are being evaluated on the Atari 2600 Name This Game dataset. This dataset is known as the benchmark dataset in the research field of Natural Language Processing. \r \r In this dataset, there are different models being evaluated, like encoder-decoder neural nets, Named Entity Recognizer and Statistical Lexicon. These are part of research being performed to find a better way to understand how to recognize names in videos and other text documents. This model evaluation would be done to see which model performs the best given certain conditions and data. ticket:IIUC, there is no research", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): The highest benchmark result achieved on the Atari 2600 Freeway dataset is 38.4 average frames per second. output (Sparql query):SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl.", "SELECT DISTINCT ?model_name WHERE { ?model_name a owl:Class ; owl:equivalentPropertyActivities * ; owl:disjointClassActivityActivities * ; owl:rangeApplicationActivityActivities *. FILTER (str( ?model_name) = \"Atari 2600 River Raid\") } input (English text): Atari 2600 River Raid is a benchmark dataset for research on the Atari 2600 video game console. Models have been tested on this dataset for assessment of their ability to accomplish specific objectives. For example, an answer suggestion model might be tested on this dataset in order to determine how quickly it can generate answers for a given question. We could find this information by searching for \"Atari 2600\" and \"model\" in this answer. The resulting dataset can then be filtered to only show examples of models that have been tested on this benchmark dataset. output (Sparql query): SELECT ?model_name WHERE { ?model_name a owl:Class ; owl:equivalentPropertyActivities * ; owl:disjointClassActivityActivities * ; owl:rangeApplicationActivityActivities *. FILTER (str( ?model_name) =", "output (Sparql query): =============== ============= =============== ============== ==================== ============================= ============= input (English text): =============== ============= =============== ============== ==================== ============================= ============= output (Sparql query): =============== ============= =============== ============== ==================== ============================= ============= input (English text): Here is a list of papers that have utilized the AlexNet model and include the links to their code: - [Zhu, Kai, et al. \"Subword and Sparse-Features Based Sentence Classification with Bi-directional LSTM.\" ICASSP 2017.]: https://github.com/tensorflow/models/blob/master/single_file/troSQuAD-ensemble.py - [Chang, Dezhi, et al. \"Global structure-based tokenization using convolutional recurrent networks.\" ACL 2018.]: https://github.com/tensorflow/models/blob/master/single_file/GST-CRNN.py - [Shazeer, Rastegari, et al. \"STAR: Span-based sentence representation learning.\" CVPR 2018.]: https://github.com/tensorflow/", "SELECT DISTINCT ?model ?model_label WHERE { { ?model rdfs:label \"Ms. Pacman\"@MA.ATARI.com.} <http://www.atari.com/museum/computer/atari-computer-collection/atari-computer-2600-museum-pacman-dataset-benchmarking.htm> } FILTER (str( ?model_label) = \"Atari 2600 model\") } Opinions vary on the Atari 2600 Ms. Pacman dataset and the benchmarks that have been proposed to evaluate model performance. There are several reasons for this variance. First, there is not a single agreed upon model for Atari 2600 Ms. Pacman, but instead many models. If the benchmark is of a model that a user is not familiar with, they may not be able to effectively evaluate the benchmark or may not know how to evaluate the benchmark correctly. The benchmark is not intuitive. The benchmark does not provide context, nor does it inform the user on the purpose of the benchmark. Finally, the benchmark is not published, so the benchmark is not readily available to evaluate the models. <|endoftext|>Q:", "SELECT * FROM lb:Dataverse.CachedInvoice.CachedInvoiceDataverse WHERE document-url = 'https://doi.org/10.1585/DOC-1026'", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <researchproblem id=\"S34-ID-2\" problem_lbl=\"Semantic Disease Benchmarks\" problem=\"Semantic Disease Benchmarks\"> <paper paper_doi=\"10.1109/ICDE.2010.23\" cont_doi=\"10.1109/ICDE.2010.23\" cliquettitle=\"Semantic Disease Benchmarks\" year=\"2010\">Semantic Disease Benchmarks</paper> <paper paper_doi=\"10.1109/ICDE.2010.23\" cont_doi=\"10.1109/ICDE.2010.23\" cliquettitle=\"Semantic Disease Benchmarks\" year=\"2010\">55.574</paper> </", "SELECT DISTINCT ?metric ?label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet\") ?label. } input (English text): The top metrics used on the ImageNet 64x64 benchmark dataset are: accuracy, coefficient, macro-F1, recall, standard deviation, and weighted F1. output (Sparql query):\u0441\u0442\u0432\u043espend (Sparql query): SELECT ?metric ?label WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"ImageNet\") ?label. } \u00e4ninput (English text): accuracy coefficient macro-F1 recall standard deviation weighted F1. output (Sparql query): duties.diseasediseases map { get :Metric ?metric ; put :Label ?label }", "SELECT DISTINCT ?paper_id WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking DBpedia\") ?paper_id. } input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset ? output (Sparql query):SELECT DISTINCT ?paper_id WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Benchmarking DBpedia\") ?paper_id. } <|endoftext|>Combining flow cytometry with neural network approach to evaluate internal standard stability. Flow cytometry (FCM) is a rapid and sensitive technology for the analysis of cells. In this paper, an internal standard stability evaluation (ISE) approach using a self-developed neural network (NN) model is proposed for stability evaluation of internal standards (ISs) applied in FCM. The evaluation results show that the proposed NN model has higher", "SELECT DISTINCT paper, benchmark FROM ResearchField WHERE rdfs:label= 'Natural Language Processing' AND paper is modellable IN {'HRLRE', 'RNN', 'XLN', 'RNN'} Output (Sparql query): Paper | Benchmark -+- HRLRE | benchmark1 HRLRE | benchmark2 HRLRE | benchmark3 HRLRE | benchmark4 RNN | benchmark5 XLN | benchmark6 RNN | benchmark7 XLN | benchmark8 RNN | benchmark9 RNN | benchmark10 RNN | benchmark11 RNN | benchmark12 RNN | benchmark13 RNN | benchmark14 RNN | benchmark15 RNN | benchmark16 RNN | benchmark17 RNN | benchmark18 RNN | benchmark19 RNN", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"", "input (English text): The problem word, Q*Bert, has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset. output (Sparql query):container (Sparql query):atari2600qbert output (Sparql query): ?problem rdfs:label \"Q*Bert\"@en output (Sparql query): ?problem_lbl rdfs:label \"Q*Bert\"@en output (Sparql query): ?paper orkgp:P30 atari2600qbert output (Sparql query): ?cont orkgp:HAS_BENCHMARK atari2600qbert output (Sparql query): ?benchmark rdfs:label \"Q*Bert\"@en documentclass{minimal} % Dataset specification @prefix : <http://stackoverflow.com/a/27614113/88386#foil>. @prefix kgc: <http://www.kwark.org/ontologies/researchfield/>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper ?rf; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?cont rdfs:label ?problem_lbl. } <|endoftext|>Q: Sum value of a subset of rows grouped by the value of another column I have a table that looks something like this: id name quantity 1 ABC 10 2 DEF 20 3 GHI 100 4 JKL 50 I would like to be able to group the rows by the name value and return a new table that has a summed value for a subset of the rows grouped by name. The result should look like this: name sum ABC 30 DEF 40 GHI 200 JKL 75 I'm trying to use a subquery, but I'm getting a strange \"subquery", "SELECT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } <|endoftext|>Q: How can I share an iPad with someone ? My boyfriend has an iPad he'd like to give me. He says he will be able to use it but it will be mine. He doesn't know how to set up a iCloud account for me so that I can set up a new iPad for him. He will not be able to use his to set up my iCloud account. What can I do ? A: I don't know that you could", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Natural Language Processing\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ?problem rdfs:label ?value. } input (English text): The top benchmark result (metric and value) over the dataset ACE 2004 is the ORCULUS system with 89% accuracy in its SPARQL query execution. <|endoftext|>Q: Function to load CSV files and generate data frames from them I want to write a function to load CSV files and generate data frames from them, so that I can pass different CSV files and get data frames back. I have written the below function to do so. My function uses the base R read.csv and read_excel functions. Any suggestions on how to improve the efficiency ? I am trying to avoid looping through each file"], "question_template": ["T05", "T02", "T01", "T02", "T04", "T01", "None", "T02", "T05", "T05", "T04", "T05", "T02", "T03", "T02", "T05", "T03", "T02", "T03", "T07", "T07", "T05", "T07", "T01", "T07", "T07", "T04", "T04", "T07", "T03", "T05", "T02", "T03", "T05", "None", "None", "T06", "T02", "T05", "T02", "T02", "T04", "T05", "T05", "T07", "T07", "T03", "T05", "T07", "T05", "T04", "T02", "T04", "T07", "T05", "T02", "None", "T01", "T07", "T03", "T07", "T05", "T04", "T03", "T03", "T01", "None", "T05", "T07", "T05", "T01", "T03", "T04", "T04", "T01", "T03", "T04", "T07", "T04", "T03", "T01", "T05", "T04", "T05", "T01", "T07", "T04", "T05", "T07", "T07", "T05", "T05", "T07", "T07", "T07", "T05", "T05", "T07", "T01", "T02", "T05", "T03", "T03", "T06", "T05", "T07", "T02", "T01", "None", "T07", "T02", "T07", "T05", "T05", "T07", "T05", "None", "T04", "T04", "T02", "T03", "T05", "T02", "T07", "None", "T02", "T07", "T07", "T04", "T07", "T01", "T02", "T07", "T04", "T01", "None", "T02", "T02", "T01", "T05", "T01", "T04", "T07", "T01", "T04", "T02", "T03", "T01", "T04", "T05", "T07", "None", "T01", "T05", "T02", "T03", "T05", "T07", "T05", "T02", "T07", "T07", "T05", "T03", "None", "T03", "T02", "T02", "T03", "T02", "None", "T07", "T03", "T05", "T01", "T07", "T04", "T07", "T07", "T04", "T04", "T04", "T03", "None", "T01", "None", "T04", "T01", "T06", "T03", "T05", "T01", "T05", "T03", "T07", "T05", "T04", "T03", "T07", "T01", "T03", "T03", "T02", "T07", "T07", "T02", "T06", "T04", "T05", "T04", "T05", "T05", "T04", "None", "T05", "T07", "T07", "None", "T03", "T05", "T02", "T07", "T07", "T01", "T03", "T07", "T04", "T05", "T05", "T08", "T01", "T05", "T07", "T04", "T07", "T06", "T03", "T07", "T05", "T02", "T01", "T03", "T03", "T04", "T02", "T03", "T03", "None", "T07", "T05", "T01", "T05", "T05", "T03", "T02", "T07", "T01", "T04", "T05", "T04", "T03", "T04", "T01", "T05", "T04", "T03", "T07", "T04", "T03", "T04", "T06", "T07", "T02", "T05", "T05", "T07", "T05", "T04", "T06", "T02", "T06", "T02", "T03", "T07", "T06", "T03", "T03", "T05", "T02", "T01", "T02", "T03", "T05", "T07", "T05", "T03", "T05", "T06", "T01", "T07", "None", "T01", "T01", "T02", "T03", "T07", "T04", "T03", "T07", "T05", "T04", "T02", "T03", "T07", "T07", "T07", "T05", "T07", "T06", "T04", "T07", "T07", "T04", "T04", "T01", "T05", "T04", "T03", "T07", "T05", "T03", "T02", "T04", "T05", "T04", "T05", "T04", "T07", "T04", "T02", "T04", "T04", "T01", "T05", "T05", "T03", "T07", "T01", "T05", "T01", "T04", "T03", "T05", "T02", "T07", "T05", "T07", "T05", "T05", "T02", "T01", "T07", "T07", "T04", "T07", "T01", "T04", "T05", "T07", "T05", "T04", "None", "T04", "T04", "T05", "T03", "T01", "T03", "T03", "T04", "T05", "T06", "T03", "T03", "T07", "T01", "T05", "T02", "T02", "T07", "T03", "T05", "T04", "T03", "T01", "T05", "T03", "T02", "T03", "T02", "T07", "T05", "T02", "T03", "T03", "T05", "T01", "T05", "T04", "T01", "T02", "T05", "T02", "T05", "T01", "T02", "T07", "T07", "T01", "T05", "T05", "T07", "T04", "T02", "T04", "T01", "T01", "T04", "T07", "T02", "T01", "T03", "T07", "T07", "T04", "T03", "T05", "T03", "T04", "T04", "None", "T02", "T02", "None", "T02", "T02", "T04", "T01", "T07", "T04", "T07", "T07", "T02", "T04", "T05", "T03", "T01", "T05", "T02", "T07", "T07", "T03", "T07", "T01", "T02", "T06", "T01", "T01", "T07", "T06", "T04", "T02", "T05", "T04", "T03", "T04", "T03", "T04", "T04", "T07", "T05", "T01", "T01", "T03", "T02", "T05", "T03", "T01", "T01", "T03", "T01", "T07", "T01", "T02", "T07", "T07", "T07", "T01", "T01", "T04", "T01", "T07", "T01", "T03", "T04", "T03", "T02", "T07", "T07", "T05", "T04", "T06", "T04"], "bleu_c": [0.0783, 0.0449, 0.0043, 0.05, 0.0087, 0.0792, 0.0073, 0.134, 0.1013, 0.0896, 0.1103, 0.0764, 0.0219, 0.0117, 0.1366, 0.0875, 0.0034, 0.0685, 0.0684, 0.0679, 0.0535, 0.1259, 0.0609, 0.0503, 0.1125, 0.0073, 0.024, 0.0928, 0.0876, 0.0246, 0.1151, 0.0238, 0.004, 0.0827, 0.0096, 0.0089, 0.113, 0.0094, 0.0686, 0.0088, 0.0546, 0.0779, 0.0725, 0.0767, 0.0069, 0.1027, 0.0253, 0.0751, 0.0441, 0.0201, 0.079, 0.067, 0.0832, 0.0053, 0.0076, 0.0484, 0.0174, 0.0544, 0.0688, 0.0178, 0.0638, 0.1139, 0.1059, 0.0147, 0.0238, 0.1176, 0.0062, 0.1091, 0.0839, 0.094, 0.015, 0.0503, 0.0754, 0.0603, 0.063, 0.0174, 0.0846, 0.111, 0.0656, 0.0124, 0.1151, 0.112, 0.0064, 0.0698, 0.0514, 0.1027, 0.0757, 0.0062, 0.0695, 0.0794, 0.0634, 0.0069, 0.0443, 0.079, 0.1308, 0.0686, 0.0977, 0.0486, 0.0275, 0.0562, 0.1157, 0.0185, 0.0266, 0.1259, 0.1042, 0.1486, 0.0682, 0.0077, 0.0123, 0.0663, 0.0765, 0.0973, 0.1148, 0.1071, 0.066, 0.0856, 0.0024, 0.0795, 0.0004, 0.0807, 0.0347, 0.0169, 0.071, 0.0526, 0.0074, 0.1118, 0.0894, 0.0189, 0.0674, 0.0518, 0.0331, 0.1035, 0.0835, 0.0671, 0.0697, 0.0054, 0.0062, 0.0161, 0.051, 0.0572, 0.0636, 0.0845, 0.0945, 0.0454, 0.0826, 0.1021, 0.0157, 0.0717, 0.0113, 0.0, 0.1442, 0.1223, 0.0227, 0.0819, 0.01, 0.0127, 0.1378, 0.0942, 0.0682, 0.0251, 0.0523, 0.112, 0.0728, 0.0192, 0.0061, 0.0096, 0.0179, 0.1531, 0.0099, 0.0569, 0.0069, 0.0574, 0.0601, 0.0101, 0.0264, 0.0703, 0.0683, 0.0801, 0.0685, 0.069, 0.0059, 0.0706, 0.0185, 0.0089, 0.0207, 0.008, 0.0175, 0.0697, 0.0065, 0.0133, 0.0612, 0.0389, 0.0899, 0.0091, 0.0056, 0.019, 0.0042, 0.0127, 0.0637, 0.0074, 0.0194, 0.0635, 0.0661, 0.07, 0.0585, 0.0527, 0.1051, 0.1063, 0.0555, 0.0556, 0.0042, 0.1069, 0.081, 0.0025, 0.0869, 0.0845, 0.0786, 0.0057, 0.011, 0.0983, 0.1198, 0.1249, 0.0021, 0.0925, 0.0102, 0.0786, 0.071, 0.0673, 0.0883, 0.3885, 0.0051, 0.0629, 0.0627, 0.0911, 0.043, 0.1736, 0.0021, 0.0848, 0.1251, 0.0937, 0.0536, 0.0108, 0.023, 0.0223, 0.0558, 0.0265, 0.0402, 0.0078, 0.0079, 0.0942, 0.0844, 0.0944, 0.0061, 0.0229, 0.0867, 0.0202, 0.0881, 0.0132, 0.114, 0.0819, 0.0344, 0.0761, 0.0371, 0.041, 0.0539, 0.0211, 0.1081, 0.0801, 0.015, 0.0789, 0.1638, 0.0695, 0.0732, 0.0428, 0.0284, 0.0489, 0.0528, 0.1067, 0.1172, 0.0198, 0.1474, 0.0689, 0.0094, 0.007, 0.2377, 0.0038, 0.0736, 0.002, 0.078, 0.0216, 0.0092, 0.0258, 0.0565, 0.0412, 0.0991, 0.0086, 0.0983, 0.1406, 0.0492, 0.0081, 0.0051, 0.0634, 0.0195, 0.0636, 0.0855, 0.1041, 0.0737, 0.0151, 0.0015, 0.0829, 0.0789, 0.1625, 0.0496, 0.1394, 0.0687, 0.0011, 0.0331, 0.164, 0.1416, 0.0645, 0.0098, 0.0101, 0.0798, 0.062, 0.0599, 0.0292, 0.1015, 0.0089, 0.0083, 0.0799, 0.02, 0.026, 0.0793, 0.0901, 0.0713, 0.1165, 0.0489, 0.0685, 0.0798, 0.2383, 0.0, 0.0545, 0.0487, 0.0506, 0.1286, 0.0212, 0.0607, 0.0181, 0.0093, 0.0344, 0.0002, 0.0068, 0.0316, 0.0572, 0.0107, 0.0728, 0.1451, 0.0147, 0.084, 0.1193, 0.0035, 0.0908, 0.0096, 0.0653, 0.0682, 0.0306, 0.1015, 0.0746, 0.0532, 0.0698, 0.0601, 0.0027, 0.053, 0.0707, 0.0124, 0.0234, 0.032, 0.0143, 0.0209, 0.0734, 0.1011, 0.1057, 0.0124, 0.0109, 0.0081, 0.0076, 0.104, 0.0539, 0.0999, 0.0523, 0.0126, 0.0814, 0.0632, 0.0481, 0.07, 0.0843, 0.0188, 0.0281, 0.0124, 0.0067, 0.0786, 0.1316, 0.0118, 0.0115, 0.0055, 0.1292, 0.0325, 0.0563, 0.0001, 0.0244, 0.0671, 0.0971, 0.1028, 0.0748, 0.0166, 0.0489, 0.058, 0.0671, 0.0101, 0.0923, 0.0887, 0.0733, 0.0603, 0.0538, 0.0887, 0.0333, 0.1168, 0.0632, 0.0215, 0.0166, 0.0304, 0.0229, 0.0655, 0.0434, 0.0541, 0.024, 0.0717, 0.0141, 0.0233, 0.0159, 0.0074, 0.0638, 0.1555, 0.0161, 0.0203, 0.032, 0.0626, 0.0615, 0.0805, 0.0693, 0.1428, 0.1776, 0.0634, 0.0009, 0.094, 0.0157, 0.0042, 0.0726, 0.0882, 0.0195, 0.0677, 0.0064, 0.0093, 0.0, 0.0959, 0.2999, 0.1029, 0.0189, 0.1003, 0.1326, 0.0413, 0.089, 0.0869, 0.009, 0.0122, 0.0661, 0.0078, 0.1063, 0.0434, 0.0705, 0.0935, 0.0739, 0.0325, 0.0221, 0.0426, 0.1046, 0.0841, 0.0088, 0.0303, 0.0048, 0.0527, 0.0201, 0.0026, 0.0666, 0.1088, 0.0528, 0.0441, 0.2273, 0.0785, 0.0833, 0.0329, 0.0024, 0.0477, 0.0027, 0.0591, 0.0271, 0.0145, 0.01, 0.0144, 0.0785, 0.0494, 0.1055, 0.0468], "bleu_4": [0.0294, 0.0102, 0.0007, 0.0135, 0.0012, 0.0182, 0.001, 0.0562, 0.0421, 0.0354, 0.0404, 0.0336, 0.0023, 0.0013, 0.0294, 0.0255, 0.0007, 0.0169, 0.0198, 0.0208, 0.0159, 0.05, 0.0204, 0.0119, 0.0417, 0.0009, 0.0013, 0.0417, 0.0333, 0.0014, 0.046, 0.002, 0.0009, 0.0202, 0.0009, 0.0015, 0.0603, 0.0009, 0.0315, 0.0007, 0.0097, 0.0261, 0.0233, 0.0323, 0.001, 0.0385, 0.0066, 0.0319, 0.015, 0.0009, 0.0247, 0.0189, 0.0263, 0.001, 0.001, 0.0159, 0.0023, 0.0103, 0.0204, 0.0008, 0.02, 0.04, 0.0383, 0.0013, 0.0011, 0.0538, 0.0011, 0.0385, 0.0319, 0.0408, 0.0009, 0.0028, 0.0262, 0.021, 0.0139, 0.0007, 0.0263, 0.0417, 0.0222, 0.0008, 0.0579, 0.0375, 0.0008, 0.02, 0.0096, 0.0385, 0.0253, 0.0008, 0.027, 0.0247, 0.0263, 0.0008, 0.0125, 0.029, 0.0729, 0.0139, 0.0337, 0.0177, 0.0023, 0.0163, 0.0429, 0.0019, 0.0013, 0.068, 0.0341, 0.0571, 0.0177, 0.0009, 0.0023, 0.0225, 0.0198, 0.0364, 0.0571, 0.0401, 0.0215, 0.0337, 0.0006, 0.0263, 0.0002, 0.0202, 0.0085, 0.0008, 0.02, 0.0169, 0.0012, 0.0357, 0.0333, 0.0011, 0.0192, 0.0155, 0.0122, 0.0244, 0.0317, 0.0231, 0.027, 0.0008, 0.0007, 0.0009, 0.0101, 0.019, 0.0149, 0.0263, 0.0267, 0.0105, 0.0233, 0.0263, 0.0008, 0.012, 0.0007, 0.0, 0.0656, 0.0698, 0.0021, 0.0278, 0.001, 0.0012, 0.0678, 0.0349, 0.0246, 0.003, 0.019, 0.04, 0.029, 0.0056, 0.0009, 0.0008, 0.001, 0.0484, 0.0011, 0.0125, 0.0027, 0.0202, 0.0213, 0.0008, 0.0021, 0.0211, 0.025, 0.0294, 0.0217, 0.025, 0.0007, 0.0192, 0.001, 0.0012, 0.0023, 0.0011, 0.0061, 0.0149, 0.0009, 0.0012, 0.0256, 0.0133, 0.0376, 0.0008, 0.0011, 0.0011, 0.0013, 0.0012, 0.0222, 0.0013, 0.0019, 0.0179, 0.0206, 0.0222, 0.02, 0.0028, 0.0547, 0.0467, 0.0233, 0.0198, 0.0006, 0.0385, 0.023, 0.0009, 0.0376, 0.0263, 0.0299, 0.0008, 0.001, 0.0405, 0.0471, 0.0558, 0.001, 0.0222, 0.001, 0.0282, 0.0241, 0.0286, 0.0349, 0.3789, 0.0007, 0.0172, 0.0194, 0.0263, 0.0146, 0.0957, 0.0007, 0.0323, 0.0556, 0.0278, 0.0147, 0.0012, 0.0023, 0.0013, 0.0114, 0.0026, 0.0096, 0.0008, 0.0008, 0.0401, 0.0421, 0.0389, 0.0014, 0.0012, 0.0194, 0.0012, 0.0309, 0.0007, 0.0415, 0.0241, 0.0108, 0.0263, 0.0094, 0.01, 0.0199, 0.0021, 0.04, 0.0256, 0.0007, 0.023, 0.0875, 0.0208, 0.0175, 0.0143, 0.012, 0.018, 0.0214, 0.0366, 0.0614, 0.0026, 0.0833, 0.0185, 0.0008, 0.0008, 0.129, 0.0007, 0.0167, 0.0008, 0.022, 0.0021, 0.0006, 0.0026, 0.0211, 0.0025, 0.0439, 0.0009, 0.033, 0.0686, 0.01, 0.0011, 0.0009, 0.0175, 0.0013, 0.0169, 0.0208, 0.0333, 0.0247, 0.001, 0.0009, 0.0297, 0.0263, 0.0476, 0.0028, 0.0435, 0.0238, 0.0007, 0.0086, 0.0526, 0.0722, 0.0196, 0.001, 0.0012, 0.0256, 0.0174, 0.0133, 0.0077, 0.0379, 0.0006, 0.001, 0.0326, 0.0011, 0.0011, 0.0254, 0.0396, 0.0247, 0.0667, 0.0189, 0.023, 0.0233, 0.0541, 0.0, 0.0175, 0.0128, 0.0222, 0.0533, 0.0011, 0.0196, 0.0024, 0.001, 0.0079, 0.0001, 0.0007, 0.0165, 0.019, 0.0016, 0.0306, 0.059, 0.0007, 0.032, 0.0385, 0.0005, 0.029, 0.0007, 0.023, 0.0222, 0.0064, 0.0371, 0.033, 0.0155, 0.018, 0.0192, 0.0012, 0.0165, 0.0225, 0.0011, 0.0011, 0.0067, 0.0008, 0.001, 0.02, 0.0353, 0.053, 0.0007, 0.001, 0.0012, 0.0013, 0.0411, 0.0183, 0.026, 0.0179, 0.0011, 0.0312, 0.0192, 0.0023, 0.0116, 0.0362, 0.0018, 0.0012, 0.0025, 0.0006, 0.027, 0.0556, 0.0011, 0.0012, 0.001, 0.0548, 0.0066, 0.0222, 0.0, 0.0012, 0.0189, 0.0341, 0.0333, 0.0286, 0.002, 0.0108, 0.0202, 0.0211, 0.0014, 0.0396, 0.0303, 0.0244, 0.0204, 0.0157, 0.025, 0.0081, 0.0541, 0.0192, 0.0028, 0.001, 0.0076, 0.0022, 0.0217, 0.0028, 0.0187, 0.001, 0.0259, 0.0014, 0.0013, 0.0007, 0.0015, 0.0215, 0.0667, 0.001, 0.001, 0.0013, 0.0202, 0.0179, 0.0256, 0.0233, 0.0541, 0.0976, 0.0202, 0.0002, 0.0294, 0.0008, 0.0008, 0.025, 0.0286, 0.0027, 0.0215, 0.0007, 0.0009, 0.0, 0.0256, 0.1667, 0.0595, 0.0013, 0.037, 0.0673, 0.0119, 0.0217, 0.0337, 0.0013, 0.0011, 0.0189, 0.0007, 0.0496, 0.0137, 0.027, 0.045, 0.0161, 0.0083, 0.0021, 0.0145, 0.0423, 0.0248, 0.0018, 0.0023, 0.0019, 0.0137, 0.0012, 0.0005, 0.0208, 0.0333, 0.0192, 0.0163, 0.1129, 0.0284, 0.0202, 0.0075, 0.001, 0.0143, 0.0014, 0.02, 0.0012, 0.0008, 0.0016, 0.0022, 0.0316, 0.0167, 0.0603, 0.0142]}
