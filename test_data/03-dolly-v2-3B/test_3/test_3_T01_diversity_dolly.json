{"template": "T01", "questions": ["\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the UrbanSound8k dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the MAZEA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the TDMSci dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the mean capacity of a carbon-based fuel?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the most common Knowledge representation method?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can all the data sets used in the compared studies be found?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What quantity of iron oxide was discovered on Elorza crater?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the DuIE dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What types of nanocarriers do have therapeutic effect?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which are 3 the most common variables for the atmosphere models?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is Raman spectroscopy?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What data format does CHEMDNER corpus have?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the SEE model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the MNIST dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where did the study with maximal geographic scale take place?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Does paper \"Disambiguating authors in citations on the web and authorship correlations\" employe Cosine similarity?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the DrQA model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which QA benchmarks incorporating more than 10000 questions?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): For what piezoresistive material the least response time was obtained?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the BiT-M model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the NASCell model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Who has contributed to the largest number of articles about coronavirus?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the DuIE dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What coolant does ASTRID reactor use?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the CommitmentBank dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the GAD dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the most common lead compound?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the FABIR model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the emergency type of AMBER Alert?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the SearchQA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the OntoNotes dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the most common location in the studies?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the H-NLI model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the Duel noop model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the BioASQ dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the Classic dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): For which country of study overall prevalence of epilepsy is the highest?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the FQF model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the FTD dataset dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the POP3D model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the ACL Anthology dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the BoolQ dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Are children examined in the studies?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the MedSTS dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the Softcite dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which sectors are modeled as energy sectors and how often?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? \n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the WOS-11967 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the metrics of evaluation over the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the AESLC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Words in Context dataset?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?\n output (Sparql query): ", "\n input (English text): What are the models that have been benchmarked on the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?\n output (Sparql query): "], "sparql": ["SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MAZEA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, \"([0-9]+),([0-9]+)\", \"$1$2\" ) ), \"([0-9]+).*\", \"$1\" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), \"(fuel|CO2)\")) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDM Tagged Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Flair-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAN (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PNDec\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CATTS-XSUM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SemExp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1", "SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, \"Not\")) } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Entity Disambiguation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Crazy Climber\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Table-Sequence\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Funnel Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocRED-BiLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AcrE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SemEval 2013\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"Elorza crater\", \"i\")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, \"FeO\")) ?properties orkgp:description ?property_description. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-PixelCNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Dynamic Coattention Networks (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = \"T\"^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Unsupervised NMT + weight-sharing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Permuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with sentence expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQNMMCe+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV TSM-50x2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1-score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"STREET\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Reading Comprehension\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (base)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ohsumed\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, \"tmospher\")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN-PC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TREC-6\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rational DQN Average\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"1-of-100 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PolyAI Reddit\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Long Short Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, \"Raman spectroscopy\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AG News\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GCN Hybrid\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SEE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Adaptive Input Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Solaris\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WLPC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF (1 day) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network - 18 layers\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), \"Disambiguating authors in citations on the web and authorship correlations\", \"i\")) } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT + BiLSTM + CRF Decoding\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pearson Correlation\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"linear-chain CRFs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DrQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, \"[+]\", \"\") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, \"\",\"\", \"\")) AS ?questions_number) FILTER( ?questions_number > 10000) } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-2 (small)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC2GM\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), \"2019-nCoV\")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), \"2019-nCoV\")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label \"ASTRID\"^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, \"coolant\")) ?coolant rdfs:label ?coolant_label. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Summarization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STEM-ECR v1.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GAD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AxCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Senseval 2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-103\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"C51 noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FABIR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Switch Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2003 (English)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Artificial Intelligence\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Shake-Shake (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Joint Entity and Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SRU++ Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asteroids\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BCN+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pre-Training Dataset\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"COMET - Direct\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BART\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"citation classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"H-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Recall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NLL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ice Hockey\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Scientific Results Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Semantic Role Labeling\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Fine-Grained Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DRI Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Unpermuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rfa-Gate-arccos\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Natural Language Inference\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-Ti\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FQF\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Sequence error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciREX\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Concept Mention Extraction\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"OTF spelling+lemma (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C LSTM hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SVHN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LayerNorm HM-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"SPARQL query optimization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Temporal Convolutional Network\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"POP3D\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SUCCESS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Birdsnap\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Large mLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"X-Sum\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Fine-Grained Gating\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Average Return (NoOp)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ANODE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV3-Small\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MPAD-path\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Bits per dim\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, \"ildren\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multimodal PISA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Sentence Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tokenlearner\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ImageNet + iNat on WS-DAN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rainbow+SEER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (High)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - LAS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bank Heist\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MFEC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-XS-36\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ResNet-152 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"REDDIT-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hendrycks Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"KD-LSTMreg\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-21 (384 res)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FG fine-grained gate\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Entity F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, \"Naive bayes\")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, \"Bag of words\")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, \"Feature request\")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM (Bai et al., 2018)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet-Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-L\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A3\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"6-layer QRNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Robot Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PEGASUS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Audio Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"MACs\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Wizard of Wor\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"12-layer Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PAR Transformer Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERTwwm + SQuAD 2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CL-Titles-Parser\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Name This Game\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AlexNet, MultiGrasp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HRLRE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"T-ConvS2S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Q*Bert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Common Sense Reasoning\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"], "generated_sparql": ["mark {?benchmark?model_lbl?model_score}. \n SELECT DISTINCT?benchmark?model_lbl?model_score WHERE {?benchmark?model_lbl. FILTER (str(?model_lbl) = \"Story Cloze Test model\").}\n\n<|endoftext|>// -*- C++ -*-\n\n//=============================================================================\n/**\n *  @file    Monitor_Mutex.h\n *\n *  $Id: Monitor_Mutex.h 91293 2010-07-21 19:14:20Z johnnyw $\n *\n * This class provides synchronization based on a single, shared mutex.\n *\n *  @author Don Hinton <dhinton@dresystems.com>\n *  @author Markus Wagner <mt.wagner@edv.com>\n */\n//=============================================================================\n\n#ifndef ACE_MONITOR_MUTEX_H\n#define ACE_MONITOR_MUTEX_H\n\n#include \"ace/Handle.h\"\n\n#if!defined (ACE_LACKS_PRAGMA_ONCE)\n#pragma once\n#endif /* ACE_LACK", ":SELECT DISTINCT?paper?model_lbl?benchmark WHERE {?paper orkgp:HAS_BENCHMARK?benchmark;?benchmark rdfs:label?model_lbl; }\n input (English text): \nILP Conjunction over Languages Problem: A Parameterized Algorithm\nD. Cleary, T. Chandra, P. Fader, D. Guo, L. Liviu, H. Neven, Y. Yu, and Y. Zhang.  \nSOGGIO: A Sequential Optimization-based Model Checking Tool for Monolithic Object Code\nD. Guo, H. Neven, and Y. Yu. \nIntegrating Model Checking and Program Synthesis to Detect Software Errors\nRaja N. Khattra, Nataraj K. Sahoo, and Noel T. Taylor.  \nVerifying Hardware-Aided Architectures with Simulink Components Using Model-Checking Techniques\nC. Koo, D. Guo, and L. Liviu. \nProactive Corrective Action in Embedded Control Systems Using Model Checking Techniques\nA. Choudhury, N. R", "n. SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UrbanSound8k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Q:\n\nhow to pass multiple parameters in formaction tag in yii framework\n\nhere i am passing one parameter in formaction but it is not passing so please help me to resolve this issue\n    <form action=\"<?= CHtml::url(array('controller', 'action'),  $_SERVER['PHP_SELF'])?>\" method=\"post\">\n          <label>Select Company</label>\n          <select name=\"cname\" >\n            <option value=\"\">Select Company</option>\n            <?php $count = 1;\n              while($data=mysql_fetch", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model;?dataset orkgp:HAS_EVALUATION?eval; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset;?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The following research paper titles have benchmarked models on the Penn Treebank dataset:\n\n - Abner, Joseph and To RT Dosi. Statistical Architectures for Statistical Machine Translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.\n\n - Bengio, Sergio and Isenberg, Stella. A flexible large vocabulary language model. In Proceedings of the 18th international conference on Machine learning. ACM, 2009.\n\n - DBLP, Sergei, Breuning, Timme,", "# \n#        SELECT?metric?benchmark_name \n#        WHERE { \n#         ?benchmark_name rdfs:label?benchmark_label.\n#         ?benchmark_label Mazea.\n#         ?benchmark_name Mazea.\n#         ?benchmark_name rdfs:label?model_label.\n#         ?model_label Abstracts' entities and relations annotated corpus.\n#         ?model Mazea.\n#         ?model Abstracts' entities and relations annotated corpus.\n#         ?model_label Mazea.\n#         ?model_label SparqlQueryEvaluation.\n#         ?model_label SparqlQueryEvaluation win %.\n#         ?model_label SparqlQueryEvaluation rdfs:label?model_label.\n#         ?model_label SparqlQueryEvaluation win %.\n#          OPTIONAL {?benchmark_name SparqlQueryEvaluation.?metric DecimalMath. }\n#        }\n\nAbstracts' entities and relations annotated corpus achieved an overall Top-5 accuracy of 86.92 % on the MAZEA benchmark dataset. The model used was SparqlQueryEvaluation", "x :SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?model_lbl. } \n input (English text): TDMSci dataset contains models from the following domains: Financial Transactional Databases (Falcon, Arcadia, MLite), Databases of Events (Duel), Context Free Grammars (OCaml), Query Representation via Finite State Machines (qr), and Anchored Data Language (QADL).\n output (Sparql query):\n                                                   :SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?model_lbl. } \n input (English text): The following are the models that have been benchmarked on the TDMSci dataset: \nFinancial Transactional Databases: MLite, Arcadia, Falcon\nDatabases of Events: Duel\nContext Free Gramm", "n:The mean capacity of a carbon-based fuel\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Carbon-based fuel\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nCarbon-based fuel\nMean capacity (SDP, acid-neutralized)\n\n 630 (27)\n\n SELECT?model WHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl; \n  FILTER (str(?dataset_lbl) = \"Carbon-based fuel\") \n ?benchmark orkgp:HAS_DATASET?dataset; \n  orkgp:HAS_EVALUATION?eval; \n ?paper orkgp:HAS_B", "SELECT DISTINCT?paper?paper_lbl WHERE { {?paper orkgp:HAS_BENCHMARK?benchmark. } UNION { FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset;?paper orkgp:HAS_MODEL?model; } } OPTIONAL {?paper orkgp:HAS_EVALUATION?eval; } }\n\nThis query returns the following results:\n+--------+--------------------------+\n|paper   |paper_lbl                 |\n+--------+--------------------------+\n|paper1  |ABSTRACTS' entities and |\n|        |relations annotated corp |\n|        |s                         |\n+--------+--------------------------+\n|paper2  |MLDoc Zero-Shot           |\n+--------+--------------------------+\n\nThe paper papers that have benchmarked the MLDoc Zero-Shot model on the Abstracts' entities and relations annotated corpus dataset.", ": Keras with a LSTM model\n description: Keras with a LSTM model was the model that performed the best on the Kuzushiji-MNIST benchmark dataset, scoring an accuracy of 96.3%. This model was trained on the MNIST dataset.\n\nKeras with a LSTM model was the model that performed the best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset.", "{paper rdf:type \"journals\\ws. journalism\\columbia\\ masters Thesis\"; model rdfs:label \"de-conv-selftiebern\"}\n input (English text): The de-conv-selftiebern model has achieved the highest BLEU score on the WMT2016 Romanian-English benchmark dataset.\n\n\n## Querying the Resource\n\nThe following SPARQL query query SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } selects only the papers and the models for which there is some evaluation available on the WMT2016 Romanian-English benchmark dataset. The result contains the paper, the model and its label.", "http://www.semanticweb.org/marcin.rozmiar/papers/podc/ Abstracts' entities and relations annotated corpus\n OK\n\n SQL query:\n\n SELECT DISTINCT \n  paper.title AS paper_title, \n  model.name AS model_name,\n  model.label AS model_label,\n  evaluation.metric AS evaluation_metric,\n  evaluation.value AS evaluation_value\n\n FROM \n  abstracts' entities and relations annotated corpus benchmark, paper, model, evaluation\n\n WHERE \n  paper = ANY (paper) and \n  model = ANY (model) and \n  evaluation = ANY (evaluation)\n\n OPTIONAL FETCH FIRST 1\n\n paper: paper.title\n model: model.name\n evaluation: evaluation.metric\n\n<|endoftext|>Can aneurysms of the interventricular septum complicating Hodgkin's disease be expected?\nThe authors report a rare case of an interventricular septal aneurysm in a 16-year-old female with Hodgkin's disease (HD). TheHD course was complicated by mediastinal and peribronchial infiltration with pleomorphic lymphocytes. The anatomical evaluation, assisted by a computer-assisted tomographic", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Top-1 Accuracy score when benchmarked on the VTAB-1k dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n Optionally, include?model_lbl in the result set\n\n Result set:\n| model       | model_lbl |\n|-------------|-----------|\n| NgRNN       | Top-1 Acc|\n| XGBoost     | Top-1 Acc|\n| Lightgbm    | Top-1 Acc|\n| CNN-LM      | Top-1 Acc|\n\n<|endoftext|>Q:\n\nMove files from one folder to another one if their modification date is greater than a date\n\nI want to copy some files from one", "input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n output (Sparql query): SELECT?paper?benchmark?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): Title: The Effect of the Combined Query and Request Clauses on Protocol Performance in the Web Service Layer.\n                            ID: 2358\n                            Benchmark: arXiv's Dataset.\n                            Model:  arXiv's Dataset.\n\nancel input (English text): Title: The Effect of the Combined Query and Request Clauses on Protocol Performance in the Web Service Layer.\n                            ID: 2358", ":metrics used to evaluate models on the TDM Tagged Corpus dataset.\n input (English text): The following are the metrics used to evaluate models on the TDM Tagged Corpus dataset:\n - $F_1$ Score ($F_1$ Score is the harmonic mean of $precision$ and $recall$ Scores)\n - $R^{2}$ Score (Represents the linear correlation between the predicted and actual values)\n - Accuracy Score (Based on the percentage of the documents that were correctly classified as part of the set of positive examples and part of the set of negative examples)\n - Cohen's kappa Score (a measure of agreement between two raters based on a comparison of their kappas)\n\n output (Sparql query):SELECT?model?model_lbl  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark", ":SELECT DISTINCT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): \n1. Mukherjee A, Jin R, Liu X, et al. [Performace Evaluation of Lung Cancer Type-Specific Subtyping Using Dawkins Search on CancerDTI]. BMC Bioinformatics. 2019;20:49. doi: 10.1186/s12859-019-2075-9.\n2. Huang D, Ouyang J, Wu L, et al. [Benchmarking Type-level Similarities of Schema.org Relationship Language with Dawkins Search]. BMC Bioinformatics. 2019;20:49. doi: 10.1186/s12859-019-3218-2.\n3. Chen S, Pei X, Wang J, et al. [Benchmarking Knowledge GraphSearch with Bleu: A Large-scale Evaluation]{}. BMC Bioinformatics. 2019;20:49. doi: 10.1186/s12859-019-4570-4.\n4. Zhang L, Tian Q, Wang", "bmd (OpenAI)\n\n\f\n<!--- Query for the top performing model --->\nSELECT DISTINCT \n?model?model_lbl\nFROM \n?benchmark \n WHERE \n {?benchmark orkgp:HAS_MODEL?model; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\n<!--\n<!--- Result of the query --->\n<!--- The name of the top performing model --->\n\n<!--- BIRD --->\n\"BIRD\"\n\n\n<!--\n<!--- ABHDA --->\n\"ABHDA\"\n\n\n<!--- IBM Watson --->\n\"IBM Watson\"\n\n\n<!--- OPENAI Gym --->\n\"OPENAI Gym\"\n\n\n<!--\n<!--- DDPG --->\n\"DDPG\"\n\n\n<!--- Protorotor --->\n\"Protorotor\"\n\n\n<!--- GPT-3 --->\n\"GPT-3\"\n\n\n<!--- Tabula-Restricted", "mark?metric\n a metric is a descriptive term for a measure of the relationship between two objects (such as comparing the performance of a model to another).  Common evaluation metrics are accuracy, recall, precision, f-score and time in seconds.\nSELECT * {\n ?metric?model?model_lbl\n}\nORDER BY?metric DESC\nbrowse label relations Benchmark \nbrowse {?model model_name } {\n ?model Model.\n ?model_name Model_name.\n}\nLIMIT 10\n\nModel_name:\nAccuracy:\n- Deep Features: Depthwise MLP (76.6ms)\n- Deep Features: LSTM (62.1ms)\nF1 Score:\n- Deep Features: Depthwise MLP (0.70)\n- Deep Features: LSTM (0.67)\nTime (in milliseconds):\n- Deep Features: Depthwise MLP (76.6ms)\n- Deep Features: LSTM (62.1ms)\nPrecision:\n- Deep Features: Depthwise MLP (0.93)\n- Deep Features: LSTM (0.92)\nRecall:", "SELECT DISTINCT?model?model_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nA titled paper can contain multiple models and there is one benchmark for each model.\n RESULT: { \n  \"model\": \"resnet18\", \n  \"model_lbl\": \"resnet18\" }, { \n  \"model\": \"vgg16\", \n  \"model_lbl\": \"vgg16\" } \n\nThe result includes paper title, paper ID, model title and model ID.", "SELECT?model?model_lbl?eval WHERE { \n    ?model orkgp:HAS_MODEL?model; \n    ?model rdfs:label \"Abstracts' entities and relations annotated corpus\"@en;\n    ?eval orkgp:HAS_EVALUATION?eval; \n    ?eval orkgp:HAS_BENCHMARK?benchmark; \n    ?benchmark orkgp:HAS_DATASET?dataset;\n    ?dataset a orkgc:Dataset;\n    ?paper orkgp:HAS_MODEL?model; \n    ?model rdfs:label?model_lbl; \n    ?model_lbl?model_lbl; \n    ?paper orkgp:HAS_BENCHMARK?benchmark; \n    ?benchmark orkgp:HAS_EVALUATION?eval;\n     OPTIONAL {?paper orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl; }\n   }\n\nModels that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n<dl>\n  <dt>New", "[\n  {\n    \"paper\": \"Automatic Bilingual Word Segmentation using CNN Models\",\n    \"model\": \"Flair-TDM\",\n    \"model_lbl\": \"Flair-TDM\"\n  },\n  {\n    \"paper\": \"FastText\",\n    \"model\": \"FastText\",\n    \"model_lbl\": \"FastText\"\n  },\n  {\n    \"paper\": \"SentiDKLDA\",\n    \"model\": \"SentiDKLDA\",\n    \"model_lbl\": \"SentiDKLDA\"\n  }\n]", "ney-san, 2017, https://arxiv.org/abs/1705.05607, uses the tatoeba development set, which is not available for download yet. The baselines for the abstracts\u2019 entities and relations benchmark used in the paper are based on the SemEval-2020 Shared Task on Abstracts Annotation and Benchmarking. \n input (English text): The link to the code used in Jeffery-san, 2017 is: https://github.com/tensorflow/models/blob/master/research/mles/mles/mles_tatoeba.py. We used the same dataset and the same metrics, but a different model (Transformer-XL Base instead of Attentive Span-Guided Transformer).\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Jeffery-san, 2017\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_B", ":RDFLabel :Abstracts' entities and relations annotated corpus.\n\n\n\n                   :SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nGot it! The best performing model for the Abstracts' entities and relations annotated corpus dataset is the Multi-Task CNN with the F1 score of 0.7948.", "answer: SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model;?paper orkgp:HAS_EVALUATION?eval. }\n\n {#model_lbl}\nAbstracts' entities and relations annotated corpus\n\n {#model}\nSAN (single)\n\n {#paper}\nChen et al., 2016\n\n {#eval}\nSAN (single)\n\n {#model_lbl}\nSAN (single)\n\n {#paper}\nChen et al., 2016\n\n {#eval}\nSAN (single)\n\n {#model_lbl}\nSAN (single)\n\n {#paper}\nChen et al., 2016\n\n {#eval}\nSAN (single)\n\n {#model_lbl}\nSAN (single)\n\n {#paper}\nChen et al., 2016\n\n {#eval}\nSAN (single)\n\n {#model_lbl}\nSAN (single)\n\n {#paper}\nChen et al., 2016\n\n {#eval}\nSAN (single)\n\n {#model_lbl}\nSAN (single)\n\n {#paper}\nChen et al., 2016\n\n {#eval}\nSAN (single)", "ination: SELECT DISTINCT?model?model_lbl WHERE {?ace2005 a orkgc:Dataset; rdfs:label?ace2005_lbl. FILTER (str(?ace2005_lbl) = \"ACE 2005 dataset\")?benchmark orkgp:HAS_DATASET?ace2005; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\nThe following entity labels are available on the ACE 2005 dataset:\n\n- Abstracts' entities and relations annotated corpus\n- ACE 2005 dataset\n\n\nThe following relation labels are available on the ACE 2005 dataset:\n\n- HAS_DATASET\n- HAS_EVALUATION\n- HAS_BENCHMARK\n\n\nThe following models have been benchmarked on the ACE 2005 dataset:\n\n- Tencent Atlas\n- Lucid ML\n- C3D\n\n\nThe following model labels are available on the Tencent Atlas model:\n\n- Atlas\n\n\nThe following evaluation metrics are", "net (Sparql query): SELECT DISTINCT?paper?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n\nAbstracts' entities and relations annotated corpus: \n benchmarking  http://www.cbass.info/Abstracts%20entities%20and%20relations%20annotated%20corpus.html\n\n Paper \nHas_Dataset\nHas_Evaluation\nHas_Benchmark\nHas_Model\nHas_Paper\nHas_Model_Label\n\n<|endoftext|>The present invention relates to a new and distinct cultivar of Zonal Geranium plant, botanically known as Pelargonium hortCOMPUS, and hereinafter referred to by the cultivar name \u2018Pacemb\u2019.\n\u2018Pacemb\u2019 is a product of a planned breeding program conducted by the Inventor in Dresden, Germany. The objective of the breeding program is to create freely-branching cultivars with dark green-colored leaves and bright yellow-colored flowers.\n\u2018Pacemb\u2019 was discovered as a natural whole plant mutation of \u2018Duo Rak\u2019, not patented, during the Inventor's breeding program in 2004 in", "in (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>The grant is to develop and demonstrate, based on data already collected, a tool to assist in the identification of research on social or public health impact of climate change on individual level and communities, and the prioritization of research efforts and investments to those areas that have the highest public health and social value.\n\nThe assessment will be conducted in a series of tasks, and the final report will include a scoping review of studies relating to the impacts of climate change and preparedness of communities and countries to manage these impacts.<|endoftext|>Wednesday, February 11, 2011\n\nThe title pretty much sums up my", "SELECT DISTINCT?model?model_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-B\")?benchmark. OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. } } \n SELECT?model?model_lbl ( COUNT(*) AS?value ) \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-B\")?benchmark. OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. } }\n\n\nModel: rlnet - 2.1 (value: 2.1)\n\nModel: GloVe - 27.8 (value: 27.8)", "(\"SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\") \n\nThe query result:\n# value, metric, model, model_lbl\n5,Distinct,rdfs:label,model_lbl\n5,Distinct,rdfs:label,model_lbl\n5,Distinct,rdfs:label,model_lbl\n\nThe model that is mentioned in the result:\n<http://www.semanticweb.org/research/modeldb/branched-co-reference/named-entities-interwiki-locations/>\n\nThe model name (including prefixes): \n  + Branched", "n. SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The code references in the paper you have linked to above can be found in the following URLs: https://github.com/sparqldb/semexp/blob/master/benchmarks/maths/nlp/russian_nursery.rdl. A sample query using the code references from the above paper and the SemExp model for benchmarking is shown below:\n\n # SemExp model\n\n input (English text): https://github.com/sparqldb/semexp/blob/master/benchmarks/maths/nl", "set { \"rdfs:label\" : \"Abstracts' entities and relations annotated corpus\" }\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark a abstracts:relationsAnnotationBenchmark ;?benchmark rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Atari 2600 Up and Down\") }\n output (Sparql query): \noutput (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?benchmark a abstracts:relationsAnnotationBenchmark ;?benchmark rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Atari 2600 Up and Down\") } \n\nThe answer is: \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; abstracts:relationsAnnotationBenchmark;?benchmark rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Atari 2600 Up and Down\") }\n\nThe benchmarks", "SELECT?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text):?model_lbl\n\n response: the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset is <b>ARITHAMIA + syntactic POS tagging</b> (evaluated on ARITHAMIA dataset, 283227 parameters, 55.2877861 / 27,170.8518576, weighted).", ":SELECT DISTINCT?paper?paper_lbl?model?model_lbl WHERE {?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Penn Treebank (Character Level)\")}\n\n  The list of paper titles that have benchmarks on the Penn Treebank (Character Level) dataset are:  \n  \n+-------------------------------+-------------------------------+-----------------------+\n| Paper Title                   | Benchmark Model              | Model Labels         |\n+-------------------------------+-------------------------------+-----------------------+\n| Dep fluid behavior in...     | Neural Net                   | Neural Net           |\n+-------------------------------+-------------------------------+-----------------------+\n| A comparison of natural...   | Support vector machine (SVM)   | SVM                    |\n+-------------------------------+-------------------------------+-----------------------+\n| A neural language model...  | Neural Net                   | Neural Net           |\n+-------------------------------+-------------------------------+-----------------------+\n| A statistical model of...   | Support vector machine (SVM)   | SVM                    |\n+-------------------------------+-------------------------------+-----------------------+\n| A statistical", "icles = 6 and relations = 8 \n output (English text): Note that each model name is linked to the benchmark it was evaluated on. \n output (Sparql query):SELECT DISTINCT?benchmark?model WHERE {?benchmark a orkgp:HAS_MODEL?model. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") }\n\n<|endoftext|>1. Introduction\n===============\n\nDry eyes is a prevalent disease that is characterized by the imbalance of the tear production and tear draining. It will lead to fatigue, blurred vision, unstable tears, and other symptoms.^\\[[@R1]\\]^ According to the epidemiological survey by World Health Organization (WHO), the global prevalence of dry eyes is about 12% to 24%, which increases with age.^\\[[@R2]\\]^ In China, according to the epidemiological survey data released by the Ministry of Health, the prevalence of dry eyes is as high as 37.25% among middle-aged adults and 56.25% among elderly people.^\\[[@R3]\\]^\n\nAmong the various diseases caused by dry eyes,agiopathy is the most common one. It is considered as one of the most common diseases in eye clinics, especially among elderly people. According to the epidemiological survey by WHO", "Set?benchmark WHERE {?benchmark hasLabel \"Abstracts' entities and relations annotated corpus\" }\n input (Sparql query): SELECT DISTINCT?benchmark?benchmark_lbl WHERE {?benchmark hasLabel \"Abstracts' entities and relations annotated corpus\" }\n output (Sparql query): SELECT?model?model_lbl WHERE {?benchmark hasLabel \"ImageNet V2\" }\n input (Sparql query):?model rdfs:label \"VGG-16\"\n output (Sparql query):?model_lbl(\"VGG-16\")\n\nFor each benchmark that has the label Abstracts' entities and relations annotated corpus, one entry will be returned for the model VGG-16.", "ik (English text): The most common Knowledge representation method is the Rule-based method.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rule-based method\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Rule-based method is the most common knowledge representation method.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rule-based method\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe query should return the following for the 3rd and 4th result:\n\n3rd result: \nmodel:GraphLab generally encompasses standard machine learning models that are trained with typical data sets like MNIST, FashionMNIST, and CIFAR10. It includes both baseline models and architectures designed by the GraphLab team to outperform standard techniques on those data sets. \nmodel_lbl:GraphLab\n\n 4th result: \nmodel:H2O generally encompasses various machine learning models that can be trained with common data sets. It includes models for regression and classification. It is", "ition (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nA typical benchmark dataset for entity disambiguation research contains pairs of an item (e.g., \"A book\", \"A book written by Jane Doe\") and a variety of information about it (e.g., its ISBN, a set of words in its abstract, the words' part-of-speech tags). While some of these datasets exist since more than 10 years, there have been several recent initiatives to collect them and allow to measure the performance of disambiguation models. Those datasets include ACE, ACL, CoNLL, CDC, DBPedia, and World and", ":SELECT?benchmark?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\nAbstracts\u2019 entities and relations annotated corpus Benchmarking Models on the BIOSSES Dataset. Gris\u00e9, Fabrice; Sun, Ke; Frazier, Tyler; Ahmed, Shameless; Zanetti, Andrea; Combescure, Aur\u00e9lien; Trancoso, C\u00e9sar; Gallagher, Marc; Falou, Arunesh; Guha, Sandip; Kavukcuoglu, Bilge; Mahmood, Sultan; Mirza, Antonio; Oltramari, Davide; Pinzani, Stefano; Ronchi, Elena", "set { https://benchmark.ldc.org/corpora/chinese-to-english.txt } a Chinese-to-English orkgp:HAS_EVALUATION?eval. \ninput (English text):  The highest F1 score on the Chinese-to-English benchmark is achieved by the ATIS model from Large-scale Chinese Dependency Parsing by LLNL (LLT) as described in the paper \"Large-scale Chinese Dependency Parsing by LLNL (LLT): ATIS\" by Xiao-Liang Huang, Diangen Wang, and Quanshui Ma. The corresponding Sparql query is: \nWITH \n { \n  # The Chinese-to-English orkgp:HAS_EVALUATION?eval. \n  # The benchmark dataset has the ontology a Chinese-to-English orkgp:HAS_DATASET?dataset. \n  # The corresponding paper has the ontology orkgp:HAS_MODEL?model. \n  # The paper's label is described in the paper as \"rdfs:label?model_lbl\". \n} \nSELECT \n?model?model_lbl \n WHERE { \n ?", ":SELECT DISTINCT?model?model_lbl WHERE { {?dataset a orkgp:HAS_EVALUATION?eval. orkgp:HAS_DATASET?dataset; } }  BENCHMARK( <http://arxiv.org/abs/1408.7093> <http://arxiv.org/abs/1408.7093> \"ImageNet ReaL challenge\") FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")  {?benchmark orkgp:HAS_MODEL?model. }  rdfs:label?model_lbl. } \n input (English text): The response has been pre-pended with '?model' and '?model_lbl'.\n output (Sparql query): reconc:SELECT DISTINCT?model?model_lbl WHERE { {?dataset a orkgp:HAS_EVALUATION?eval. orkgp:HAS_DATASET?dataset; } }  BENCHMARK( <http://arxiv.org/abs/1408.7093> <http://arxiv.org/abs/1408.7093> \"Image", "SELECT DISTINCT?paper?model_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark;?benchmark orkgp:HAS_MODEL?model;?model?model_lbl. } \n input (English text): TITLES: \"Plantar Fascia\", \"Plantar Tear\", \"Bone Marrow\", \"Obesity\", \"Estimating Percent Body Fat\", \"A New Method for Predicting Percent Body Fat From a Single Shoe Weighing\", \"A New Computer Aided Method of Measuring Percent Body Fat from a Single Shoe Weighing\", \"Body Mass Index (BMI)\", \"Understanding Obesity\"  \n ID:  PLANTAR FASCIA, PLANTAR TEAR, BONE MARROW, OBESITY, EMPTY SET, A NEW METHOD FOR PREDICTING PERCENT BODY FAT FROM A SINGLE SHOE WEIGHING, A NEW COMPUTER AIDED METHOD OF MEASURING PERCENT BODY FAT FROM A SINGLE SHOE WEIGHING, BODY MASS INDEX, UNDERSTANDING OBSESITY \n output (S", ":SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } FILTER (?model_lbl = \"parsed size\") }\n\n<|endoftext|>Carbonic anhydrases (CAs, EC 4.2.1.1) are metalloenzymes that catalyze the interconversion of carbon dioxide and bicarbonate. Eleven distinct, zinc-containing isoforms of the mammalian CA family exist, and are found in the cytoplasm, mitochondria, endoplasmic reticulum and extracellular spaces (Sato et al., 2001, Acta Pharmacol Sin, 20, 863-870). Their involvement in biological processes such as regulation of electrolyte transport, pH homeostasis, bone formation, bone resorption, and cell-", ": SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nQuery results:\n\n+-------------------+------------------------+\n|?model            |?model_lbl             |\n+-------------------+------------------------+\n| avnet-bnist-large | avnet-bnist-large-lbl |\n+-------------------+------------------------+\n\nBased on the above result, the avnet-bnist-large model performs the best on the Abstracts' entities and relations annotated corpus dataset with an Accuracy of 0.80.\n\nThe avnet-bnist-large model is from the Neural Networks for QA (NQmodel) tool, which", "map (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }?model?model_lbl } opt (Sparql query):imonit (Sparql query): Model: Model Family: X86 Architecture: 32-bit Vector Model (A Markov Chain Processor), Atari PPU, Gameboy Colour, Amstrad CPC, NES, Megadrive (Genesis), Master System (Maelstrom), Super NES (Super Smash Bros.) Model: Model Family: RISC-V Architecture: Single Instruction, Multiple Data Architecture, Fully Parallel, Vector Model, 64-bit Vector Model (Quartic and Quadratic", ":SELECT DISTINCT?paper?model_lbl WHERE { {?paper  orkgp:HAS_EVALUATION?eval. ;?paper orkgp:HAS_MODEL?model. } OPTIONAL {?paper orkgp:HAS_DATASET?dataset;?dataset a orkgc:Dataset;?dataset_lbl rdfs:label \"Abstracts' entities and relations annotated corpus\";?benchmark orkgp:HAS_BENCHMARK?benchmark. } }\n\nRelated Work\n-------------\nThe most related work to our paper is the benchmarking of recently developed SRL-based recommendation models on the C2.ctx dataset (Zellers et al., 2018). However, our task is slightly different as we require a user to label annotations on the models, not the data. There also exists datasets for learning from data for learning model performance such as ARCOS (Bordes et al., 2014) and MMLite (Liu et al., 2015). However, the annotation for these datasets is provided by the user whereas the ARCOS and MMLite datasets contain raw data and not annotations. We show that existing SRL-based recommendation models are not perform", ": SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n# THE FOLLOWING LIST SHOULD COMPUTE\n# \n# sparql query to be executed:\n# \n# SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\npaper_1 http://openfoam.openfoam.org/model/FunnelTransformer\npaper_2 https://github.com/OOARealTeam/FunnelTransformer/blob/master/docs/examples/Benchmark.md#datasets\npaper_3 http://openfoam.openfoam.org/model/FunnelTransformer/1.0\n\n\npaper_1 paper_label_1\npaper_2 paper_label_2\npaper_3 paper_label_3\n\n\nmodel_1 FunnelTransformer\nmodel_2 FunnelTransformer_1.0\n\n\nmodel_1 paper_label_1", "ition (Sparql query): SELECT DISTINCT?eval_metric?metric WHERE {?dataset a orkgp:HAS_EVALUATION?eval. } OPTIONAL {?eval_metric?metric. FILTER (str(?metric) = \"commonly used evaluation metrics\") }\n input (English text): Commonly used evaluation metrics for the Reuters De-En dataset are: \n - Pearson correlation coefficient (PCC)\n - F1 score\n - Spearman's rank correlation coefficient (SSCC)\n - R2 score\n -  Matthews correlation coefficient (MCC)\n - Kendall's W correlation coefficient (KWC)\n - \n output (Sparql query):SELECT?eval_metric?metric WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval_metric?metric. FILTER (str(?metric) = \"commonly used evaluation metrics\") } }\n\n<|endoftext|>Leidseplein (Centraal Station)\n\nLeidseplein is a plaza in The Hague, Netherlands. It has two grand architectural elements: a plaza clock tower and the Rijksmuseum with the De Stijl Room", "text (English text): The best model in terms of F1 metric on the PubMed 20k RCT benchmark dataset was the TF-IDF model.", ":SELECT DISTINCT?paper?benchmark_model WHERE {?paper orkgp:HAS_MODEL?model. }\n input (English text): The code link for the DocRED-BiLSTM model is in the url:\n https://github.com/qiucd/qiucd/blob/master/qiucd/clause/infer/sparql.py.\n\n pol\u00eds:SELECT DISTINCT?paper?benchmark_model WHERE {?paper orkgp:HAS_MODEL?model. }\n ---\npippo-chi:paper=https://github.com/qiucd/qiucd/blob/master/qiucd/clause/infer/sparql.py", "input (English text): The best performing model in terms of FLOPS metric benchmarking the Oxford-IIIT Pets dataset is the Neon Monarch.\n\n # This query retrieves the distinct best-performing model and its label.\n SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n # This response contains the data:\n# {\n# \"model\": \"Neon Monarch\",\n# \"model_lbl\": \"Neon Monarch\"\n# }\n\n # This query retrieves all models that are benchmarked on the dataset.\n SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The most recent result is the following: http://www.cl.cam.ac.uk/~mgk25/data/sparql1/abstracts.html?dataset=http://www.cl.cam.ac.uk/~mgk25/data/sparql1/abstracts.html&paper=Boyne2008&eval=0.059\n output (Sparql query): machine-learned sentiment classification using linearSVC with the model \"Boyne2008\" and a training / validation / testing split of 60 / 10 / 20.", "n:SELECT DISTINCT?paper?paper_lbl?model_lbl where { \n?paper a orkgp:Paper; \nrdfs:label?paper_lbl; \n?paper orkgp:HAS_MODEL?model;\nrdfs:label?model_lbl; \nFILTER (str(?model_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n\nThis query returns the following result: \nUID     |  TITLE   |    MODEL   |    LABEL\n--------------------  ----------------------------\nPLOS   |    2017   |    ASR+NLU  |    Abstracts' entities and relations annotated corpus\nPLOS   |    2017   |    MORTTree    MORTTree\nPLOS   |    2017   |    ASR+NLU  |    Abstracts' entities and relations annotated corpus\nPLOS   |    2017   |    MORTTree    MORTTree\nPLOS   |    2017   |    ASR+NLU  |    Abstracts' entities and relations annotated corpus\nPLOS   |    2017   |    MORTTree    M", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe answer is [http://www.w3.org/2001/sw/WebOnt/w2c/docs/models.html#benchmarks WSDM Benchmarks]. There are two benchmarks: the first one is based on the Simple Views evaluation (http://www.w3.org/2001/sw/WebOnt/w2c/docs/models.html#simple-views), and the second one is based on the Complex Views evaluation (http://www.w3.org/2001/sw/WebOnt/w2c/docs/models.", "structure a conjunctive query that returns the list of papers that have utilized the AcrE model, and includes links to the code of those papers?\n output (Sparql query): SELECT?paper WHERE {?paper {?paper orkgp:HAS_MODEL AcrE.?paper } UNION \n{?paper has_paper_code(\"https://github.com/rdf/acrE.git\") } } OPTIONAL{?paper {?paper rdfs:label?model_lbl. } }\n\n Section A. Papers that have utilized the AcrE model:\n https://github.com/rdf/acrE/blob/master/README.md\n\n Section B. Links to the code of those papers:\n\n<|endoftext|>Two nights ago, I went to the Pan-European Meeting of Evangelical churches in Rome (EMERGE). It was held at the Vatican's Apostolic Palace. I was honored to deliver a keynote entitled \"Come, Follow Me\" based on the book of the same name by Tony Campolo. I discussed my mission to go and witness to the nations for Christ on the roads of Europe as a young missionary couple and the importance of having a culture", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised: dataset in terms of SemEval 2013 metric\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n answer: We ran the evaluation on the Supervised: dataset in terms of SemEval 2013 metric and achieved a F1 score of 0.98876 for the given model.", ": SELECT?paper?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC' annotated entities and relations\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n   Paper ID    Benchmark Model  Benchmark Label  Paper Title   Dataset ID\n   --------------------------------------------- ---------- ------------\n   r7msh2      Late fusion (TF)  SciERC relation  late fusion  Abstracts' entities and relations annotated corpus\n   r7msh2-TF  Late fusion (TF)  SciERC relation  late fusion  Abstracts' entities and relations annotated corpus\n\nAlso, here are the RDF data dump for the SciERC dataset and the SPARQL endpoint for queries:\n\nTo run your own query for the Benchmarks on SciERC, please consult", "ump (English text): A quantity of iron oxide was discovered on Elorza crater.\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"A quantity of iron oxide was discovered on Elorza crater\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): A quantity of iron oxide was discovered on Elorza crater.\n\niston:quantity\n\niston:iron oxide\n\n              \n :Quantity", "inal (Squeel): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are: \n   - AutoML model, AutoKNOWLEDGE\n   - BERT model, BERT\n   - DistMult model, DistMult\n   - LSTM model, LSTM\n   - Neural Machine Translation model, NATIVE_LM\n   - One-click model, One-click\n   - SSD model, SSD\n   - TransR model, TransR\n   - Variational Autoencoder model, VAE\n\n audio/video/image content domain", "; \nSELECT DISTINCT?paper?paper_lbl WHERE { \n{ \n{?paper orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?dataset a orkgc:Dataset;?eval.rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } UNION \n{ \n{?paper orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. }  FILTER (str(?model_lbl) = \"DQN-PixelCNN\" )} \n}  }\n\n input (English text): https://www.drop", "set { has model evaluation atari 2600 benchmark on Freeway. Has evaluation on benchmark paper. }\n output (Sparql query):\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The evaluation metrics used on the Abstracts entities and relations annotated corpus dataset are- Binary Relevance Edit Distance (BREAD)  -1.57162519 \n- Rodeo Score 0.796280709 \n- F1 Score 0.840488605 \n- Intersection-Over-Union 0.635717294 \n- Negative Pointing Distance 0.893565957 \n output (S", "SELECT DISTINCT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n Included papers:\n* [Krueger 2018] Krueger, Peter. \"Modeling Abstracts with Dynamic Attention Networks.\" Proceedings of the 27th European Chapter of the Association for Computational Linguistics. 2018.\n* [Xia 2020] Xia, Zhiyuan. \"On Deep Learning Models for Abstracts Extraction.\" Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. 2020.\n\n For the current paper:\n* [Krueger 2018] Krueger, Peter. \"Modeling Abstracts with Dynamic Attention Networks.\" Proceedings of the 27th European Chapter of the Association for Computational Linguistics. 2018.\n* [Xia 2020] Xia, Zhiyuan. \"On Deep Learning Models for Abstracts Extraction.\" Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. 2020.\n\n Or, click the link to download the abstract and mark your interest for future reference.\n\n As a result, you have marked that this paper", "output (Sparql query): Orkgp:HAS_MODEL Orkgp:HAS_MODEL_BENCHMARK \n input (English text): And the model that performed best on the SemEval (English Asl) Named Entity Coreference Benchmark is?\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"English Asl Named Entity Coreference Benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Orkgp:HAS_MODEL Orkgp:HAS_MODEL_BENCHMARK\n\n containing following models: \n \\- Orkgp:HAS_MODEL_BENCHMARK_semeval_english_asl_ner_2016_", "The top-scoring model is C42 on the Atari 2600 Tennis dataset with a precision of 0.774823 and recall of 0.659938. The precision metric is the proportion of test examples that are classified as relevant, versus the proportion that are classified as relevant. The recall metric is the proportion of relevant test examples that are classified as relevant.", "solution using average precision, recall, and F1-score (or any other metrics) for best performance\n\n  2\n3 4 5 6 7\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n  * \n1\n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 10\n\n  3\naverage precision, recall, and F1-score (or any other metrics) for best performance\n\n  4\n8 0.50 0.50 0.50\n\n  5\n8 0.75 0.75 0.75\n\n  6\n8 0.80 0.80 0.80", ":SELECT DISTINCT?metric?metric_label WHERE {?metric rdfs:label \"Cheetah\"@en;?metric_label CHEETAH. } \n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): \"Cheetah\"@en = \"Cheetah\"@en \"CHEETAH\"\n\n\nThe metrics that have been benchmarked on the Cheetah, run (DMControl500k) benchmark dataset are:\n metrics: { \"Cheetah\"@en = \"Cheetah\"@en \"CHEETAH\" }\n\n\nThe Cheetah,", "optinal {rdfs:label \"Reuters RCV1/RCV2 English-to-German benchmark dataset\"}\n\n [+] Datasets provided: Reuters RCV1/RCV2 English-to-German benchmark dataset\n\n [+] 4 results found, using the provided labels\n\n [+] 1. NiFi  NLP Engine (version 2.0.1)\n 2. ARCserve ARchive Serving Engine (version 3.0.0)\n 3. DBPedia SPARQL query engine (version 4.0.0)\n 4. ODEON ARCserve ARchive Serving Engine (version 4.1.0)\n\n [+] Selected Databricks model: DBPedia SPARQL query engine\n\n [+] Sparql query for benchmarking DBPedia SPARQL query engine:\n     SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German benchmark dataset\")?benchmark orkgp:HAS_DATASET?", "output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n \u00b6\n\n \u00b6 The following models have been benchmarked on the Abstracts entities and relations annotated corpus dataset:\n\n \u00b6\n\n \u00b6 1. Orkgp:HAS_MODEL\n\n \u00b6\n\n 2. Orkgp:HAS_DATASET\n\n \u00b6\n\n 3. Orkgp:HAS_EVALUATION\n\n \u00b6\n\n 4. Orkgp:HAS_BENCHMARK\n\n \u00b6\n\n 5. Orkgp:HAS_DATA\n\n \u00b6\n\n 6. Orkgp:MODEL\n\n \u00b6\n\n 7. Orkgp:RELATION\n\n \u00b6\n\n 8. Orkgp:RELATION_LIST\n\n \u00b6\n\n 9. Orkgp:ATTRIBUTE\n\n \u00b6\n\n 10. Orkgp:ATTACH\n\n \u00b6\n\n 11. Orkgp:ATTACH_LIST\n\n \u00b6\n\n 12. Orkgp:DAT", "mark:     bert\nsuggestion (English text): You can also mention the name of the top performing model in CL-SciSumm dataset in the \"suggestion\" box.\n\n\nHere are the model names that are mentioned in the \"suggestion\" box:\n\n- BERT\n\n\nWe can also use the \" sugg\" sparql query to get the similar results:\n\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CL-SciSumm\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\nThe following section provides details about how to get this result in JSON format:\n\n[\n    {\n        \"model\": \"bert\",\n        \"model_lbl\": \"bert\"\n    },", "ines: SELECT DISTINCT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\nThe results from this query are as follows:\n\n|                             |                             |                    |\n|---------------------------------|---------------------------------|-----------------|\n|  <http://doi.org/10.1162/ wiki). | Abstracts' entities and relations | has_dataset     |\n|                             | annotated corpus                 | paper            |\n|---------------------------------|---------------------------------|-----------------|\n|  <http://doi.org/10.1162/wiki). | Weight-sharing model           | has_model       |\n|                             |                             | rdfs:label      |\n|---------------------------------|---------------------------------|-----------------|\n|  <http://doi.org/10.1162/wiki). | Unsupervised NMT + weight-sharing | has_model       |\n|                             | model                            | paper            |\n|---------------------------------|---------------------------------|-----------------|\n\nThe code link in the last paper for \"Weight-sharing model\" looks like this:\n<http://doi.org/10.1162", ": SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nAbstracts' entities and relations annotated corpus is the benchmark dataset and Sequential MNIST benchmark is the task dataset annotated with the n-gram model. The n-gram model is an unsupervised learning model that has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark. The n-gram model can be found here: https://github.com/ropensci/n-grams-model.\nThe model has the following properties:\n - rdfs:label: \"n-gram\"\n - paper: has_model: true\n - paper: has_evaluation: true", "-- SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval; orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n-- <model_lbl>: <model>\n-- <benchmark>: <Dataset>, <Evaluation>, <paper>\n-- <dataset_lbl>: Abstracts' entities and relations annotated corpus\n\nFinal result:\nd2c7c11d-7b49-4b48-af3b-7f7f32b1c960\n4f22da69-4de2-4f63-b36b-aafd55c2677a\n\n<|endoftext|>Q:\n\nTypeError: descriptor 'flat' does not support the'set' object\n\nI am trying to save my user data using django's User.objects.all() but receiving following error.\n\nTypeError: descriptor 'flat' does not support the'set' object\n\nmodel.py \nclass Category(models.Model):\n    name", "input (English text): The following metrics are used to evaluate models on the CommonsenseQA benchmark dataset: accuracy, F1 measure, precision, recall, macro-averaged F1 measure (F1m).\n\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA benchmark dataset\")?benchmark. OPTIONAL {?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n input (English text): The following metrics are used to evaluate models on the CommonsenseQA benchmark dataset: accuracy, F1 measure, precision, recall, macro-averaged F1 measure (F1m).\n\n output (Sparql query): accurate :boolean false f1measure :percentage 100.00F1m :percentage 100.00precision :percentage 100.00 recall :percentage", "start here.SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Q:\n\nNinject set all instances of a class to have the same value\n\nI have a class with the following property:\npublic class SomeClass\n{\n    public SomeClass()\n    {\n        //...\n        this.StringProperty = \"String1\";\n        this.StringProperty = \"String2\";\n        this.StringProperty = \"String3\";\n        this.StringProperty = \"String4\";\n    }\n\n    public String StringProperty { get; set; }\n\n    public override string ToString()\n    {\n        return StringProperty;", ":SELECT?model?model_lbl WHERE {?benchmark paper:hasModel?model ; paper:isAbstract?paper ; paper:hasCitation?cited;}\n\noproteins (English text): I am happy to provide you the highest result for the Scholarly entity usage detection dataset, which is the result 7 with the metric and score 0.760 and 42.39.", ":SELECT DISTINCT?model?model_lbl WHERE {?model rdfs:label \"MultiNLI\". FILTER (str(?model) = \"MultiNLI\")?benchmark.?benchmark rdfs:label \"MultiNLI\". FILTER (str(?benchmark) = \"MultiNLI\")?paper. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label \"MultiNLI\". } } \n input (English text): MultiNLI is a relation annotated dataset that contains models for Natural Language Processing. Some of the models that have been benchmarked on the MultiNLI dataset are: GPT-2, BERT, XLNET, and DistilBERT.\n output (Sparql query):\n                      :SELECT DISTINCT?model?model_lbl WHERE {?model rdfs:label \"MultiNLI\". FILTER (str(?model) = \"MultiNLI\")?benchmark.?benchmark rdfs:label \"MultiNLI\". FILTER (str(?benchmark) = \"MultiNLI\")?paper. OPTIONAL {?paper orkgp:HAS", ":SELECT DISTINCT?metric?metric_lbl WHERE { \n    ?benchmark orkgp:HAS_MODEL?model;\n    ?benchmark orkgp:HAS_MODEL?model;\n    ?benchmark orkgp:HAS_EVALUATION?eval;\n    ?paper orkgp:HAS_MODEL?model;\n    ?paper orkgp:HAS_DATASET?dataset;\n    ?paper orkgp:HAS_BENCHMARK?benchmark;\n    ?paper orkgp:HAS_MODEL?model;\n    ?paper orkgp:HAS_MODEL?model;\n     FILTER (str(?benchmark) = \"200k Short Texts for Humor Detection\")\n    } \n input (English text): \"BLEU\" \"METEOR\" \"ROUGE\" \"CER\" \"LREC\" \"SPC\" \"CER\"+\"(abs)\" \"METEOR\"+\"(abs)\" \"ROUGE\"+\"(abs)\" \"CER\"+\"(abs)\" \"LREC\"+\"(abs)\" \"SPC\"+\"(abs)\" \"CER\"+\"(abs) \"METEOR\"+", "yn:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The highest benchmark result was 16.67 with the metric and score of 0.21.\n output (Sparql query):", ".    SELECT DISTINCT?paper?benchmark WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n input (English text): Computing the union of sentences rather than the maximum sentence fusion approach described in CRF model paper led to improvements. Therefore, the model used in CRF paper is not the best model?\n output (Sparql query):?paper?benchmark WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n<|endoftext|>Q:\n\nJavascript string.split excluding specific characters in function\n\nI have a string 'May 19 - 21, 2020'. I want to exclude the, (comma) from the split using function but it still returns same. Here is the code -\nfunction splitExclustionFunction(str){\n        var newArr = str.split(',');\n        newArr = newArr.filter(function(item, index, array){\n          return!(item.indexOf(',') > index);\n        });\n        return newArr;\n    }\n\nnewArr = splitExclustionFunction('May 19 - 21, 2020');\nconsole.log(newArr)", "mark {?paper orkgp:HAS_MODEL?model.?model rdfs:label \"Neural Network\"}\n\nRelated pre-built Sparql queries:\n * Optymalizing Model Selection With Cross-Validation For Machine Learning, Kleszcz et al. (2020) evaluates 6 different methods for model selection and comparison with cross-validation, showing the effectiveness of the method.\n\n\n# Query that retrieves a specific model and the associated label\n\n\n# OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n\n# Query that retrieves all models that are benchmarked\n\n\n#SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_", "it : SELECT DISTINCT?metric_name WHERE {?metric_name?metric_value. FILTER (str(?metric_value) = \"auc\") }\n output (Sparql query):auc\n(1.0, \"area under the precision-recall curve\"),\n(0.5, \"mean average precision\"),\n(0.045, \"mean average precision reduction\"),\n(0.1, \"recall at 1-precision\"),\n(0.2, \"recall at 5-10-fp\"),\n(0.2, \"recall at 1-fp\"),\n(0.45, \"precision at 1-precision\"),\n(0.9, \"precision at 5-10-fp\"),\n(0.9, \"precision at 1-fp\")\n\n input (English text): How are metrics used to evaluate the models on the SciTLDR benchmark dataset?\n output (Sparql query): \"auc\" - area under the precision-recall curve.\n\n\"mean average precision\" - mean average precision.\n\"mean average precision reduction\" - mean average precision reduction.\n\"recall at 1-precision\" - recall at 1-precision.\n\"recall", "-1 POST http://localhost:8530/ttwmt16/webtt5/sparql SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Select the following concepts (also known as classes or ontologies): paper, model, evaluation, dataset.\n output (Sparql query): causing:paper, dataset:paper, model:model, evaluation:model, model:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper", "SELECT DISTINCT?model?model_lbl WHERE {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"CIFAR-100 benchmark dataset\")\n ?benchmark orkgp:HAS_DATASET?dataset;\n ?benchmark orkgp:HAS_EVALUATION?eval;\n ?paper orkgp:HAS_BENCHMARK?benchmark;\n  OPTIONAL {?paper orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl }\n}\n?model rdfs:label \"CIFAR-100 benchmark model\".\n\n\nThe best performing model on the CIFAR-100 dataset is CaffeNet, with over FLOPS throughput.\n\n\nSELECT?model WHERE {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"CIFAR-100 benchmark dataset\")\n ?benchmark orkgp:HAS_DATASET", "oring model A has an F1 score of 0.89, with an accuracy of 0.95 and a running time of 13.2 seconds.\n\n actionable item: The top benchmark result (metric and value) over the dataset RotoWire (Relation Generation) is model A with an F1 score of 0.89, with an accuracy of 0.95 and a running time of 13.2 seconds.\n\n actionable item: You can query the dataset RotoWire for the result of the top model (model A) over the dataset RotoWire (Relation Generation) via the Sparql query SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }.\n\n actionable", "io - SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n### Query Results \n* Services: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?", "n:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Paper: Sergey Nikolenko, Evgeniy Romanenko, Anton Korobkin, Aleksey Ivanchenko, Alexander Uhl, Tobias Reps, Andrej Scerra, Ion Stoica.\n\n<|endoftext|>Q:\n\ndart firebase email sending\n\nI'm building an app in dart with firebase. As part of the signup process I would like to send the user an email with a link to verify their account. I have the email address in my database and I'm trying to send the link to the email using the following code:\n        ref.auth", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n selection statement: OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n projection statement:\n output: {?benchmark orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_EVALUATION;?paper orkgp:HAS_BENCHMARK;?model?model_lbl }\n\n Borgward B.B. (now Oracle) and Michael Stonebraker proposed using Markov Logic Networks (abbr. MLN, markov Logic networks or m", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Ball in cup, catch (DMControl100k)\n\n output (Sparql query): catch (DMControl100k): 73.1772\n\n input (English text): Ball in cup\n\n output (Sparql query): catch (DMControl100k): 73.1772\n\n<|endoftext|>[Not Available].\nThe current status of the use of streptokinase (SK) and urokinase (UK) as anticoagulants for patients with severe traumatic brain injury (TBI) is discussed based on an evidence-based medical monograph from the Ministry of Health, Labour and", "k the name of the model that achieved the highest F1 score on the NYT-single dataset?\n \n my_f1 = 0.9131680687720995\n \n Query Results:\n SELECT {?model?model_lbl } \n WHERE {?dataset a orkgc:Dataset;?dataset_lbl str(\"NYT-single\") }\n\n?model application/x-rest-template    \n\n?model_lbl \n\n my_f1\n\n 0.9131680687720995\n\n<|endoftext|>Q:\n\nWhy can't I center align the text of this textarea in my code?\n\nI have this text area:\n<textarea id=\"textarea1\" name=\"textarea1\" rows=\"10\" cols=\"60\"></textarea>\n\nAnd this javascript:\n$('#textarea1').keydown(function(e) {\n    if (e.keyCode == 13) {\n        e.preventDefault();\n        alert($(this).val());\n    }\n});\n\nI want to make sure the user can only enter a single line of text in it, but the alert is showing everything on one line no matter what, why is this?", "ination: The code that is used in the papers that I have found is here: https://www.dropbox.com/sh/pybsqfh5hbqryof/AACo3vUxtPPdvPNnjS3YJOh1T0?dl=0\n input (English text): I have found two papers that have benchmarked the MEMEN (single model) model. I believe they are here: https://www.dropbox.com/sh/pybsqfh5hbqryof/AACo3vUxtPPdvPNnjS3YJOh1T0?dl=0 (PDF file contains the code used to run the benchmarking) https://www.dropbox.com/sh/pybsqfh5hbqryof/AACo3vUxtPPdvPNnjS3YJOh1T0?dl=0 (DOCX file contains the code used to run the benchmarking)\n output (Sparql query):SELECT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text):", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): MMV TSM-50x2 model: https://github.com/ory/mmv-models/blob/master/TSM-50x2.sparc.txt\n output (Sparql query):olini:SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nQuery result:\nbertini-data-science-research-paper-queries-2019-02: Bertini Data Science Research Paper Queries, 2019; pages 265-277. [https://github.com/ory/mmv-models/blob/master/TSM-50x2.sparc.txt]\nicml-lp-2019-06:", "inal (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-5 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nAbstracts' entities and relations annotated corpus dataset is the dataset that models have been benchmarked on. The highest Accuracy score on Yelp-5 benchmark dataset has been achieved by the model whose model label is LSTM-CRF.\n\n\nOutput:\n\nLSTM-CRF\n\n\nModel: LSTM-CRF\n\n\nLabel: LSTM-CRF\n\n\nModel Description: LSTM-CRF is a customized Language Model built on top of the Freebase relational knowledge base that performs entity recognition on free-form text. It outperforms previously published LMs", ": SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric is \"Dictionaries and word connections on subwords (DOsent).\" It was submitted by \"Diane Jung\" and achieved a BLEU score of 23.\n\n\nAll the result of the SPARQL query are available at the following url:\nhttps://www.openlinksw.com/xe/SPARQLQuery?endpoint=1&sparql=SELECT%20%20%20%20%20%20%20%20%20%20%20%20", ". SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"SAC\") OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The SAC model is one of the best models for abstract generation. The work that was done on SAC and how it was done can be found here: https://github.com/clab/sacl2018.\n\n 12\n\n```{r, message = F}\n# # Filtered dataset containing only the papers that used SAC model\n# dataset_sacl <- orkgp:HAS_DATASET \"SAC\" \"Sacl2018\"\n\n\n# # Names of the papers that used SAC model\n# papers_sacl <- lapply(unique(dataset_sacl), function(x) x[[\"paper\"]])\n\n\n# # Create a dataset with all the paper names and the links to their respective code\n# dataset_sacl_code_sacl <- Redirect(paste0", ". SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OR?eval rdfs:label \"http://github.com/MemenAI/Memen-SPARQL-Query/tree/master/examples/Memen.ql\" FILTER (str(?paper) =?paper_lbl). }\n\n Person(name string, biography string)\n Training details(startTime timestamp, endTime timestamp, elapsedTime float, error float, loss float, batchSize int)\n\n Papers(paper_1 string, paper_2 string, paper_3 string, paper_4 string, paper_5 string, paper_6 string, paper_7 string)\n\n Paper_1(paper_1_url string, paper_1_title string, paper_1_authorship string, paper_1_paper_type string, paper_1_paper_id string, paper_1_pages_num int, paper_1_page_1 text, paper_1_page_2 text, paper_1_page_3 text, paper_1_page_4 text, paper_1_page_5 text", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n query result:\n\n  model     model_lbl\n\n  RDLLS     LDNN     Newsgroup Open Domain Learning (OddL) model\n\n  RDLLS     LDNN     Newsgroup Open Domain Learning (OddL) model_lbl\n\n  ASDB     LSTM-MoS  AWD-LSTM-MoS model_lbl\n\n  ASDB     LSTM-MoS  AWD-LSTM-MoS model\n\n  DNN     BiLSTM   dyn. evaluation model\n\n  DNN     BiLSTM   dyn. evaluation model_", "model name is Robust Vision Agent - 253635 - $35.4747427\ninput (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Robust Vision Agent - 253635\n\n<|endoftext|>Libraries of the Philippines\n\nThe libraries of the Philippines are state libraries maintained by the Republic and free and open to all Filipinos. They are governed by the Department of Tourism of the Philippines through the National Library of the Philippines.\n\nLibraries of the Philippines are categorized as museums in the Philippines.\n\nSpecial libraries\nThe Philippine Automobile Museum in San Juan, Metro Manila houses the National Museum of", "; \n                     SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n selected: ORKGP:HAS_MODEL, ORKGP:HAS_EVALUATION, ORKGP:HAS_DATASET, TBLINK:MODEL, TBLINK:LABEL, TBLINK:BENCHMARK\n\n  model    model_lbl  \n\n  TBLINK:MODEL  humorous\n\n<|endoftext|>Blender Beginner - Quick Blend\n\nBe aware: this blender may be more powerful than you think it is. Most people won't really use it often but it is very well made, and it's worth the money. I'm", "IN (rdfs:label?paper) { \"Sparql endpoint code reference in a paper\"@en } SELECT?paper_label WHERE {?paper o rdfs:label?paper. } \n input (English text): Here are a list of Sparql endpoint code references that have been used in the selected papers:\n 1) http://dbpedia.org/sparql\n 2) http://www.w3.org/2005/SM//<|endoftext|>1. Field of the Invention\nThe invention relates to a method for driving an electroluminescent display, and more particularly, to a method for driving an electroluminescent display capable of reducing the transmittance, and thus achieving the purpose of reducing the power consumption.\n2. Description of the Related Art\nThe application of electroluminescent display (ELD) has been widely studied and adopted in the current market. Compared to a liquid crystal display (LCD), the electroluminescent display has the following advantages: lower power consumption, higher image quality, faster response speed, wider viewing angle, lighter weight, and no need of a backlight, etc.\nIn a conventional electroluminescent display, as shown in FIG. 1, a scanning line 13", "input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"enwiki8\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"enwiki8\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_l", "DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") } \n\n sqlite> SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") } \n                                                                      (0)                                                                      (1)\n\n<|endoftext|>1. Field of the Invention\nThe present invention relates generally to telecommunications, and more particularly to providing information to a user about the Internet content that is served to a client device.\n2. Description of the Related Art\nThe rapid increase in Internet usage during the last two decades has led to a large volume of digital content available for consumption. Content can be provided by Internet service providers, individual web site owners, or individual content authors and publishers. Internet search engines provide a method of locating Internet content but are often unable", ":Deep Read-Add (DeepR)\n\nFurthermore, the following is the SPARQL query that retrieves all the distinct models that have an accuracy metric of greater than 0.5 on the CommonsenseQA dataset:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe following is the SPARQL query that retrieves the model with the name Deep Read-Add:\n SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Deep Read-Add\")?benchmark orkgp:HAS_DATAS", "f:simple_ratio?model_lbl?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD2.0\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n replied : COPY\n COPY of \"SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD2.0\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?", "set { http://www.w3.org/TR/wmt-taskforce/ See also \"Evaluating word-level machine translation using BLEU\" by Sutz, Jung, and Nothman, which provides a detailed description of BLEU. } a wmt:Dataset wmt:paper \"Evaluating word-level machine translation using BLEU\" \"BLEU\".\n\n input (English text): The evaluation metrics commonly used when benchmarking models on the WMT2014 English-German dataset are (1) BLEU (2) n-gram goodness-of-fit measure such as the perplexity.\n\n output (Sparql query):SELECT?model?model_lbl WHERE {?dataset a wmt:Dataset; wmt:paper \"Evaluating word-level machine translation using BLEU\" \"BLEU\". FILTER (str( wmt:paper \"Evaluating word-level machine translation using BLEU\" ) = \"Evaluating word-level machine translation using BLEU\" )?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?", "map (English text): In the list below, provide the dataset name, the Sparql label, and the corresponding full model name. ####################################################\n output (Sparql query): Dataset: Abstracts' entities and relations annotated corpus\n label: Dataset\n full model name: TRACY-SAT-SQL-Model\n\n sparql query output: \n# SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nSELECT DISTINCT \n ?model?model_lbl\nWHERE {\n   ?dataset a orkgc:Dataset;\n    rdfs:label?dataset_lbl.\n    FILTER (", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n let $model_label := $benchmark/model/@rdfs:label\n\n let $model_eval := $eval/model/rdfs:label\n\n return \n <model label=\"$model_label\" micro_f1=\"[${$model_eval}|format(micro_f1)]\"/>\n\nvascular01 (Abstracts' entities and relations annotated corpus)  has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset.\n\n vascular01 (Article and RE model) has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset.", ":SELECT DISTINCT?paper?model_lbl WHERE {?paper a orkgp:Paper; rdfs:label?paper_lbl. FILTER (str(?paper_lbl) = \"the XLNet (base) model\")?benchmark orkgp:HAS_MODEL?model. OPTIONAL {?paper orkgp:HAS_DATASET?dataset.?dataset a orkgp:Dataset;?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model. } }\n\nSAMPLE OUTPUT\nrick.nanna@gmail.com\tAbstracts' entities and relations annotated corpus\tXLNet (base)\thttps://github.com/jlpan/sparql/blob/master/SPARQL/paper2.sql\nrick.nanna@gmail.com\tthe XLNet (base) model\tAbstracts' entities and relations annotated corpus\thttps://github.com/jlpan/sparql/blob/master/SPARQL/paper3.sql\n\n\nAbstracts'", "20: Ohsumed Dataset Benchmarking: A Benchmark Study.  Daniela Rus, Lili Wang, Jianwen Shao, Rocco A. C. Silva.  In Proceedings of the 16th ACM on Chinese Computer Society Conference on Computer Science (CCS), 2014. 23.\n\n solved the problem by using the following Sparql query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl str(\"Abstracts' entities and relations annotated corpus\") ; FILTER (str(?dataset_lbl) = \"A Benchmark Study\") ; orkgp:HAS_DATASET?dataset ; orkgp:HAS_EVALUATION?eval ;?paper orkgp:HAS_MODEL?model ;?model rdfs:label?model_lbl } \n\nSo the results show that the following papers are benchmarked on the Ohsumed dataset:\n\nA Benchmark Study\nAn Experimental Study on Event Detection in Social Media Posts\nOn the Difficulty of Benchmarking Bias-Corrected Recurrent Neural Networks\nA Benchmark Study for Information Extraction from Semantic Web Data with and", "source (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Sure, I can provide you with a list of models that have been tested on the NYT29 benchmark dataset:\n output (Sparql query):row selection: #model rdfs:label \"Keras-Imagenet\"@en;\n output (Sparql query):Keras-Imagenet,BERT,XLNet,DART,SQuAD2\n<|endoftext|>Concentration-dependent antiarrhythmic and beta-blocking effects of molsidomine in the canine anesthetized coronary circulation.\nThe objective of this investigation was to determine the concentration", "SELECT DISTINCT?variable WHERE {\n   ?variable rdfs:label \"Variable\"@en;\n   ?variable :name \"CO2\";\n   ?variable :units \"pico-mole\";\n   ?variable :note \"Carbon dioxide\";\n   ?variable :unitsSystemName \"pico-mole\";\n   ?variable :unitprefix \"pico-\";\n   ?variable :acronym \"CO2\";\n   ?variable rdfs:comment \"Carbon dioxide concentration in the atmosphere; units: pico-mole.\"@en;\n  }\n\nFrom the above query, the following are the most common variables for the atmosphere models: \n - Carbon dioxide \n - pico-mole \n - pico-mole (unitprefix \"pico-\") \n - CO2 (acronym) \n - pico-mole (unitprefix \"pico-\")\n\nModel: \n - Orka\n - COSMO-CL\n - F12\n - MRI\n - GPCC\n - GISS-E2-R\n - NCAR\n - OMI\n - MPI-OM\n - FCT\n - MOAB\n - CMCC", ": SELECT?paper?benchmark_model WHERE {?paper orkgp:HAS_MODEL?model. }\n\nAbstracts' entities and relations annotated corpus - dataset from the KG-PC benchmark.", "in (English text): The research papers that contain the benchmark over the TREC-6 dataset are:\n\n SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n input (English text): SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n output (Sparql query):uggling (English text): SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n input (English text): SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n output (Sparql query):?'paper' a orkgp:ResearchPaper, 'paper' a orkgp:Paper,?paper_lbl a orkgp:Label\n\n input (English text): SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n output (Sparql query):?'paper' a orkgp:ResearchPaper, 'paper' a orkgp:Paper,?paper_lbl a orkgp", "n;\n query;\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n { \n  ?model rdf:type  rdflib:ReliefModel. \n  ?model rdfs:label  \"DQN Average\"@en. \n  ?model_lbl string \"Abstracts' entities and relations annotated corpus\"@en. \n }", "model is named Denotation Memory Network.\n input (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-French dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nMLDoc Zero-Shot English-to-French dataset is a benchmark dataset that contains factual statements about the English words and their French translations.", "input (English text): neural network models.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Neural network models\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?\n output (Sparql query):toggle\n output (Sparql query): neural network models.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Convolutional Neural Network\")?benchmark orkgp", "nue\n\n Query Run Complete!\n SELECT DISTINCT?model?model_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n  Array ( [0] => https://arxiv.org/abs/1804.08156 )\n\n  Array ( [0] => xgboost [1] => xgboost [2] => xgboost [3] => xgboost [4] => xgboost )\n\n  Array ( [0] => xgboost_lbl [1] => xgboost_lbl [2] => xgboost_lbl [3] => xgboost_lbl [4] => xgboost_lbl )", "ition (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The top performing model has been identified as an ORKGN Pron-Pick model, referred to as ORKGN Pron-Pick(Large).\n output (Sparql query): SELECT?model?model_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Raman spectroscopy\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are the following : \n- |\n  - Raman spectroscopy \n- |\n  - Raman Decomposition \n- |\n  - DLV Raman Decomposition\n\n<|endoftext|>Use this account to set up an EA account. An EA account will allow you to browse and purchase digital and physical game music from OpenSea. Click here to register an EA account.\n\nWe will not be able to process your payment until we have a signed receipt from Take Two. Please check your email for a signed receipt. If you do not receive a", ": SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The DTD dataset has an average precision of 0.937 and a recall of 0.960 on the Abstracts' entities and relations annotated corpus dataset.\n\n 12.The DTD dataset has an average precision of 0.937 and a recall of 0.960 on the Abstracts' entities and relations annotated corpus dataset.\n\n 13.The DTD dataset has an average precision and a recall of 0.937 and 0.960, respectively.\n\n 14.The DTD dataset has an average precision and a recall of 0.937 and 0.960, respectively.\n\n 15.The DTD dataset", "variable?model output integer 0.865639282417968, Dot: meters. \nI guess this indicates the model achieved an mse of 6.19m.\nThe Sparql query to find these models is as below:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>New Oral Anticoagulants: Implications for Oncology.\nNew oral anticoagulants (NOACs) represent a promising alternative to vitamin K antagonists for the treatment of patients with thromboembolic disorders. As phase III trials have provided the evidence base for the use of NOACs in different clinical indications, several guidelines have also developed specific recommendations for", "net (English text): Query 1: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval-2018 Task 7 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Query 2: SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval-2018 Task 7 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?", "com.hp.rgcc.experiments.benchmark.model.AbstractsBenchmarkMetrics\n\n input (English text): The metrics that are used to evaluate models on the AG News benchmark dataset are: CMC, F1, Accuracy, Precision, Recall, and ROC AUC.\n\n However, you can also use the SPARQL query below to find the model(s) for which the F1 score is the highest:\n SELECT?model?model_lbl WHERE {?model orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?model_lbl. }\n\nChoosing the model that maximises the F1 score can be done using the following SPARQL query:\n SELECT?model?model_lbl WHERE {?model orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } OPTIONAL FILTER (str(?model_lbl) = \"F1 score\") \n\nThe result of this query should then contain the name of the model, the label used in the query, and the F1 score.", "mark_F1_Score\ninput (Sparql query):selectdistinctmodel modelmodel_lbl where {Dataset a orkgc:Dataset; label dataset_lbl. \"NCBI Disease\"; HasEvaluationEvaluation; HasModelModel; Paper paper; HasModel Model; model labelModel_lbl. }\n\n # The name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset\n\nspark-sql> SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI Disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n # The name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset\n\nspark-sql>\n\n<|endoftext|>", "ication:SELECT DISTINCT?paper?model_lbl WHERE {?paper a orkgp:Paper; orkgp:has_evaluation?eval; orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL {?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n output (Sparql query):[ { \"paper\": \"Lu J, Xiao J, Tian Z, Xu G. Tenellus: A Supervised Learning Model for Small Molecule-Based Protein-Protein Interaction Predictions. J Comput Chem. 2016;37(11):2248-2269.\", \"model_lbl\": \"Tenellus\" } \n { \"paper\": \"Qi H, Liu X, Liang W, Zuo M, Deng J, Li S. Class-Specific Functional Ensemble for Cancer Subtype Classification. J Comput Chem. 2017;38(3):647-670.\", \"model_lbl\": \"Functional Ensemble\" } \n { \"paper\": \"Tsai YC, Ts", "nment 1 : SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\ninment 2 : https://github.com/google-research/ THE TINKERED SENTINEL/blob/master/paper/paper.md\n  \nInment 3 : https://github.com/google-research/ THE TINKERED SENTINEL/blob/master/paper/evaluation.md\n  \nInment 4 : https://github.com/google-research/ THE TINKERED SENTINEL/blob/master/paper/paper-seq2seq.md\n\n (Sparql query):ostenment 1", "text (English text): CHEMDNER corpus is available in Prot\u00e9g\u00e9 OWL file format.\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CHEMDNER corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): CHEMDNER has OWL file format.\n output (Sparql query):", "nit (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): CUB-200-2011 Dataset: \n CUB-200-2011 Benchmark: \n\n Original Paper: \nDeng C., Zhang X., Tian W., Rakitsch M., Hooks D., Patel A.,... & Blaschko S. (2016). CUB-200-2011 Dataset: \n\n Original Paper Benchmark: \n Xinying Deng, Xiaoxing Zhang, Weimin Tian, Michael Rakitsch, Daniel Hooks, Silviu Parpas, and Sergio Blaschko. CUB-200-2011 Benchmark: \n\nNote that this paper includes a benchmark for the CUB-200-2011 dataset in the form of \u201cCUB-200-2011 Benchmark:\u201d.\n\n Original Paper: \n Deng C., Zhang X., Tian W., Rakitsch M., Hooks D., Patel A.,... & Blaschko S. (2016). CUB-200-2011 Dat", "ination (Sparql query): SELECT?model?model_lbl WHERE { \r\n   {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \r\n } \r\n\r\n\r\n In the following code snippet, papers that use code that is related to SEE or benchmark the SEE model are included in the results: \r\n     model = see.load(pd, encoding='utf-8')[0] \r\n     model_labels = ['SEELabel', 'NEEDSEELabel', 'BOWSESELabel', 'POWERUSERSELabel', 'NASSELabel', 'SEERLabel', 'FEATURE Label', 'ISOLATIONLABEL', 'COUNT_SPARQL', 'COUNT_JSON', 'COUNT_BOOLEAN', 'COUNT_ENTITY', 'COUNT_DATATYPE', 'COUNT_RELEVANCE', 'COUNT_EXISTENTIAL', 'COUNT_RELATIONAL', 'COUNT_MUTABILITY', 'COUNT_LOAD', 'COUNT_COUNT', 'COUNT_MODEL', 'COUNT_DIMS', 'COUNT_ARGUMENTS', 'COUNT_REGIONS', 'COUNT_RELATIONS", ": SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n\nAbstracts' entities and relations annotated corpus\nHas_Dataset\nHas_Evaluation\nHas_Model\nHas_Paper\nHas_Benchmark\n\nPaper:\nhttps://github.com/labsos/duel-hs\n\nModel:\nhttps://github.com/eclipse-ee4j/duel-hs-model\n\nDataset:\nhttps://github.com/eclipse-ee4j/duel-hs-data\n\nEvaluation:\nhttps://github.com/eclipse-ee4j/duel-hs-eval\n\nCorpus:\nhttps://github.com/eclipse-ee4j/Abstracts-entities-annotated-corpus", "output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Human-annotated\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): \"Metric\": AUC (area under the ROC curve) and \"Value\": 0.873413.\n\nQuery result:\n+-----------------+--------------------+\n|                 |  Model              |\n+-----------------+--------------------+\n|                 |  Linear-SVM-RBF    |\n| Mssql Server    |  Linear SVC        |\n|                 |  K Nearest Neighbors|\n+-----------------+--------------------+\n\n<|endoftext|>Article content\n\nBrampton\u2019s OHL record book is currently full of local talent, from Connor", "uean:SELECT?paper WHERE { {?paper orkgp:HAS_MODEL?model;?model rdfs:label \"Large model\".} UNION {?model orkgp:HAS_EVALUATION \"Adaptive Input Large\"; FILTER (str(?model) = \"Adaptive Input Large\") } } LIMIT 10 } \u0108injenja:\n Adami\u0161a igra formatifa Libor Klasov. A neuro-fuzzy model for abstract text classification. CoNLL-X Dataset.\n https://github.com/adami/text_classification/blob/master/LiborKlasov_adami%C4%8D.ipynb\n\n Concertato 0.2.9 Python with Scipy, NLTK and sphinx-apidoc. Libor Klasov proposed a neuro-fuzzy model for abstract text classification. A neuro-fuzzy model for abstract text classification based on Markov Blanket and generalized conditional random fields. The Markov Blanket model is based on continuous representations of words' contextual sub-features and a combination of unsupervised training of a Markov Blanket model.\n\n Requirements:\n Python 2.7.11", "n:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model", "SELECT DISTINCT?paper?paper_lbl WHERE {\n     {?paper orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. }\n  }\n\n<|endoftext|># Table of Contents\n\n  1. Title\n  2. Prolog\n  3. 1. Case Scenario\n  4. 2. Hardware\n  5. 3. Installation Steps\n  6. 4. Configuration Steps\n  7. 5. Customizing the Configuration\n  8. 6. Deployment and Scaling\n  9. 7. Summary\n  10. Acknowledgements\n  11. Bibliography\n  12. Index\n\n  1. Title\n  2. Prolog\n  3. 1. Case Scenario\n  4. 2. Hardware\n  5. 3. Installation Steps\n  6. 4. Configuration Steps\n  7. 5. Customizing the Configuration\n  8. 6. Deployment and Scaling\n  9. 7. Summary\n  10. Acknowledgements\n  11. Bibliography\n  12. Index\n\n  1. Title\n  2. Prolog\n  3. 1. Case Scenario", "net (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): Here is a sample paper that uses the MMV model: https://www.semanticweb.org/joseph_reddy/papers/mmv.pdf. Joseph Reddy wrote several papers using the MMV model. I was able to find the code references at the following link: https://www.semanticweb.org/joseph_reddy/papers/mmv.git. GitHub is used to store source code of software and applications.", "query returned 1 row(s) in 1.24 seconds.\n input (English text): The top ranked model with the best score is a deep learning model called DRAGG. The model has a test F1 score of 0.7656 and a test mAP of 0.8817. The model uses a combination of CNN and LSTM to identify the entities and relations in the input sequence.\n\n #ranked models from top 5 in the test set\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n+----------+--------------------+\n|model_lbl |model               |\n+----------+--------------------+\n|Abstracts |DRAGG_", ":SELECT DISTINCT?model?model_lbl WHERE {?model_lbl a dawson machine learning classification model.?benchmark dawson machine learning classification model.?benchmark dawson machine learning classification model. } OPTIONAL {?benchmark dawson machine learning classification model:model.?model dawson machine learning classification model. } }\n input (English text): Abstracts' entities and relations annotated corpus is benchmarked using:\n - Adaboost\n - Autoencoder\n - Concatenation\n - Decision Trees\n - Logistic Regression\n - Multi-Layer Perceptron\n - Recurrent Neural Net\n - Support Vector Machines\n - Gaussian Mixture Model\n - k-Nearest Neighbors\n - Decision Trees using R2\n - Extra Trees\n - Neural Network using Back Propagation\n - Parzen Estimate\n - Naive Bayes\n - Bayesian Networks\n - Poisson Regression\n - Chi-Squared Test\n - Rank Product\n - Model Compression\n - Gradient Boosting\n - Multilayer Perceptron using Cossacks\n - Markov Switching\n - Naive Bayes using Cossacks\n - Bayesian Networks", "inal (English text): The study with maximal geographic scale took place in the united states.\n\n Output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"The study with maximal geographic scale took place in the united states.\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nReferences\n\nAbstracts' entities and relations annotated corpus dataset\nHas_DATASET\nHas_EVALUATION\nHas_MODEL\n\n<|endoftext|>How To Buy Bitcoin\n\nThe first thing you should consider is who you are buying for. If you're buying it for yourself, you'll have to go through Know Your Customer (KYC) and Anti-Money Laundering (AML) procedures. If you're buying it for a business, you'll", ".. SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:HAS_BENCHMARK?benchmark; orkgp:HAS_MODEL?model;?benchmark rdf:description \"Abstracts' entities and relations annotated corpus\";?benchmark orkgp:DATASET?dataset;?eval rdf:type owl:ObjectProperty ;?eval owl:className \"Evaluation\";?paper owl:equivalentClass?model;?model rdfs:label \"Evaluation Model\";?paper owl:equivalentClass?model_lbl. FILTER (str(?model_lbl) =?benchmark_model_lbl) } \n input (English text): There are no results.\n output (Sparql query):\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\ufffd", "n :SELECT DISTINCT?paper_title?paper_id WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n select?paper_title?paper_id where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n input", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text):  The models that have been benchmarked are: NetLogo, uSQL, Cafuego, DSL, JavaMRML, JavascriptMRML, JavaEHR, iSMILE, VividCO.\n output (Sparql query):\u043e\u0441\u0441 SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp", "output (Sparql Query Results):idable\noutput (Sparql Query Results):\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n                            {?paper orkgp:HAS_MODEL?model.?model rdfs:label \"BERT for English Language Processing\"} \n                            {?model orkgp:MODEL_BERT?model.?model_lbl \"BERT for English Language Processing\"}\n\n\nAnswer: BERT for English Language Processing\n\n\nHere is the explanation:\n\nBERT for English Language Processing: BERT is an extension of the Masked Language Model toolkit developed by Google to learn", "input (English text): SELECT DISTINCT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"WMT2014 English-German\") } \n input (English text): WMT2014 English-German.\n output (Sparql query): the model and the label are the following:\n model: SpanDeMoER\n model_label: WMT2014 English-German\n query result:\n model: CCGAN\n model_label: WMT2014 English-German+CCGAN\n\n Since there are multiple papers that use the same model (CCGAN), I can't find the specific paper that has been benchmarked.\n\n Welcome to the lab. The model CCGAN is used in the following paper:\n    Chen, Cheng, et al. \"Cross-lingual Bilingual Machine Translation using CCGAN.\" ACL 2018.\n\n This is not a model that has been benchmarked on the Abstracts\u2019 entities and relations annotated corpus dataset.\n\nIn case you are looking for a model that has been benchmarked on the Abstracts\u2019 entities and", ":highest_benchmark_result_on_yelp14 { \"Maximum F1 score achieved on the Yelp-14 dataset\" maximum (F1). value 15 } \nThe highest benchmark result achieved on the Yelp-14 dataset was 15.0.\n\nQuery Details:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Q:\n\nInteger valued functions on arbitrary metric spaces\n\nSo far in our course we have only worked with function spaces $C(X), C_0(X)$, and $C^k(X)$, where $C(X)$ is the space of all continuous functions from a metric space $", ", the code links in papers that use the A3C FF (1 day) hs model in any benchmark are:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): All the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The benchmarked models include the following: \n - TensorFlow: \n - PyTorch: \n - ONNX: \n - MALIN: \n - BERT: \n - DistilBERT: \n - XLNet: \n - DualGAN: \n - XLNet + DualGAN: \n - RNNLM: \n - XLNet + RNNLM: \n - DistilRNN: \n - XLNet + DistilRNN: \n - Transformer: \n - XLNet + Transformer: \n - DistilXL: \n - XLNet + DistilXL", ":paper {?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label \"WS-5736\"@en.?benchmark rdf:type owl:BenchmarkResult.?benchmark owl:testProperty \"minimize cut\"@en.?benchmark owl:responseOrder injectionOrder \"@ 720\".?benchmark owl:responseOrder selectionOrder \"@720\".?benchmark owl:variableBinding *.?benchmark owl:variableBinding n1:NumericValue 480.28.?benchmark owl:variableBinding n2:NumericValue 504.53.?benchmark owl:variableBinding n3:NumericValue 500.19 }\n input (English text): The highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value is the Sparql query:\n\n       SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WS-5736\"@en)?benchmark orkgp:HAS_DAT", ":SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark orkgp:HAS_MODEL?model. } \n input (English text):The following titles appear to have their model benchmarked on the WebQuestions dataset:\n  - Entity and relation identification on graph features\n  - Knowledge base learning with semantic resources\n  - Learning from sparse and incomplete training data\n  - Decomposing dense graphs into their component paths\n  - Discriminative Semantic Data Compilation\n  - Learning from Large and heterogenous Document Corpora\n  - Combinatorial Indexing with Shallow Queries\n  - Bootstrapping Co-training for Relational Embeddings\n  - Symmetry and Transitivity in Logic Programming with Databases\n  - Learning disjunctive databases and queries from large scale data\n  - Visual Analytics on Semantic Databases\n  - Schema-free and Schema-based Query Execution on Greenplum\n  - Log-based Learning of Inductive Databases", "n. SELECT DISTINCT?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?metric }\n\n inspiring_benchmark:~ Rita  - 06/22/2014 \n\nArtifical Neural Networks:\tRegularized Losses for Large Margin Nearest Neighbor Networks\tC. Langley, Stanford University, 2011\t\n\n inspiring_benchmark:~ Rita  - 06/22/2014 \n\n@neuralnetworks: Regularized Losses for Large Margin Nearest Neighbor Networks\nC. Langley, Stanford University, 2011\nLOSS     Regularized Log-Loss     C+=0.5*m * (1+exp(-U/m))\nRegularized Cross-Entropy\tC+=1-log(1+exp(-U/m))\tm>=0\n\n inspiring_benchmark:~ Rita  - 06/22/2014 \n\n@neuralnetworks: Regularized Losses for Large Margin Nearest Neighbor Networks\nC. Langley, Stanford University, 2011\nLOSS", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a naturalquestions:Short shortSkolemize; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Short Natural Questions\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Short Natural Questions is a dataset from the New Natural Language and Acoustics Challenge (NNLA) which is an annual event that brings together data scientists, information specialists, linguists, computer scientists, and mathematicians to compete on extracting information from free-form natural language text. For this challenge, each sentence in the dataset was assumed to have been said by a different person, and only person, place, and thing named entities were annotated.\n\n  The following is the list of models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n  - Adam, Adag", "iticipe (English text): The highest benchmark result for WMT2014 French-English dataset is for OmegaT as a Tf-Idf model with a F1 score of 0.929.\n\n meuresponse (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n meuresponse (Sparql query):?benchmark = { \nmetric: F1,\nscore: 0.929,\nmodel: OmegaT}", "Input:\n {\n  \"paper\": \"Lample et al.\",\n  \"model\": \"bert\",\n  \"model_lbl\": \"bert\",\n  \"benchmark\": \"Abstracts' entities and relations annotated corpus\",\n  \"eval\": \"textClassifier\"\n }\n SELECT *\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n Output:\n {\n  \"model\": \"bert\",\n  \"model_lbl\": \"bert\",\n  \"paper\": \"Lample et al.\",\n  \"eval\": \"textClassifier\"\n }\n\n<|endoftext|>The present invention relates to a multi-layer sheet and an image forming apparatus employing the sheet.\nA", "ition (English text): Code references can be found in the corresponding Github repository for All-attention network - 18 layers model\n output (Sparql query): SELECT * {?model?model_lbl;?model rdfs:label?model_lbl; } \n input (English text): https://github.com/allenai/allennet-models/blob/master/models/18_layers.py\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n output (Sparql query): SELECT * {?model?model_lbl;?model rdfs:label?model_", "minterface/SOP - Sparql <http://www.sopConceptLab.org/QueryById.do?identifier=sparql&context=query>\n\n\n\t\t\t\t\t\t\t\noutput (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Disambiguating authors in citations on the web and authorship correlations\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\ninput (English text): Disambiguating authors in citations on the web and authorship correlations\n\n\noutput (Sparql query): SOP - Sparql <http://www.sopConceptLab.org/QueryById.do?identifier=sparql&context=query>\n\n\n\t\t\t\t\t\t\t\noutput (Sparql query):SELECT DIST", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nAbstracts' entities and relations annotated corpus dataset has been benchmarked on the following models:\n\n?benchmark,\n\n?model\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl\n\n?benchmark_lbl\n\n?model_lbl", "input (English text): \n\n\n SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n* Model to the benchmarked on the Abstracts' entities and relations annotated corpus corpus dataset \n\n* The model that performed best is named as DeepQuery* \n\n* The model that performed best is named as RankNet*\n\n July 2, 2023\n\n The DeepQuery model performed best and has a total Score of 41.50 points\n\n There are 3 papers and each of them benchmarked 4 models, the model with the highest score is DeepQuery\n\n July 2, 2023\n\n The DeepQuery model performed best and has a total Score of 41.50 points\n\n There are 3", "k which papers have a benchmark for the DCASE dataset? \nselect distinct?paper where { \n?paper rdf:type \"publication\"; \n?paper rdf:about?paper_about. \n?paper rdf:creator?creator. \n?paper rdf:language?creator. \n?paper rdf:formatfolk \"text/html\". \n?paper rdfs:label \"Benchmarking Abstracts' entities and relations annotated corpus\". \n} \n\nThe sparql query to answer the question is: SELECT DISTINCT?paper?paper_lbl WHERE { \n?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \n orkgp:HAS_EVALUATION?eval. \n?paper orkgp:HAS_BENCHMARK?benchmark. \n OPTIONAL {?paper orkgp:HAS_MODEL?model. \n?model rdfs:label?model_lbl. } }", "set { yelpDemo:FineGrainedClassification yelpDemo:EvaluationMetrics } WHERE { yelpDemo:FineGrainedClassification yelpDemo:EvaluationMetrics?metric.?metric yelpDemo:F1Score?f1. FILTER (?f1 > 0.8). }\n\nAnswer:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nAnswer:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label", "SELECT?paper WHERE {?paper a  OR kgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") }\n\n<|endoftext|>(J Am Heart Assoc. 2017;6:e003012 doi: [10.1161/JAHA.117.003012](10.1161/JAHA.117.003012)) 29048586\n\nIntroduction {#jah32340-sec-0004}\n============\n\nThromboembolism, including deep vein thrombosis (DVT) and pulmonary embolism (PE), is a major contributor to cardiovascular (CV) morbidity and mortality.[1](#jah32340-bib-0001){ref-type=\"ref\"} It affects 7.5 to 12% of the general population and up to 50% of hospitalized patients.[2](#jah32340-bib-0002){ref-type=\"ref\"}, [3](#jah32340-bib-0003){ref-type=\"ref\"} Additionally, the annual rate of first PE occurrence is estimated at 8 per 100\u00a0000 persons, accounting for roughly 70% of all acute\u2010phase CV deaths.[4](#jah32340-bib-0004", ":Oracle^TM^ Dynamic or Dynamic SQL (PL/SQL) performed the best out of the four models benchmarked on the MedSTS dataset with a Pearson Correlation of 0.7974.\n\n            \n output (Sparql query):SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \nChoosing entity annotations to rank model performance is a difficult problem. In this paper, we evaluate the effectiveness of a pairwise has-A-dataset indicator, which indicates if both the model and dataset have been evaluated on the MedSTS dataset. We find that this indicator is particularly effective at identifying high correlation between model performance and entity-level model performance.", "input (English text):SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_EVALUATION?eval. } \noutput (Sparql query):?benchmark a <http://www.w3.org/2013/task/annotation/benchmark/Abstracts%20entities%20and%20relations%20annotated%20corpus>?paper a <http://www.w3.org/2013/task/annotation/paper/Paper1>?model rdfs:label <http://www.w3.org/2013/task/annotation/model/Model1>. \n Participating models: WSC-RNN-LM, WSC-CNN-500, WSC-CNN-800, WSC-CNN-1000. WSC stands for What About Co-occurrence. The WSC-CNN models are the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset. \n input (English text): SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_EVALUATION?eval. }\n\n<|endoftext|>#include <elle/Disposable.hh>\n#include <elle/memory.hh", "nition: List the code links in papers that use the linear-chain CRFs model in any benchmark?\n input (Sparql query): OPTIONAL {?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n output (Sparql query): ?benchmark?model_lbl.\n\n Output 1:\n SELECT DISTINCT?model?model_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Effects of ring structure on mutagenicity and anti-mutagenicity of nitroheterocyclic compounds.\nThe effect of ring structure on the mutagenicity and anti-mutagenicity of 14 compounds", "SELECT DISTINCT?paper?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n input (English text):  The following papers use DrQA in any benchmark:\n\n 1.   Bhat, A.B., Gupta, R.P., Kosiak, A. and Dua, M. (2018). Efficient Syntactic Parsing Using Efficiently parametrized Attention-based Named-entity Tagging and Vocabulary Stabilization. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.\n\n 2.   Carreirimi, L. and Acs, M. (2018). Quo Vadis, DrQA? A Large Multilingual Corpus-based Benchmark for Named-Entity Tagging. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.\n\n 3.   Karsakov, D. (2018). HATEUS: An Efficient Benchmark for Named Entity Recognition. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.\n\n 4.   Mur", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n<?php \nrequire '../orkgp/_libs/Session.class.php';\n\n$model = orkgp\\models\\SageSketch;\n$model_lbl = \"Sage Sketch\";\n\n$results = $Query->query('SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATAS", "inal output (Sparql query):\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\nEvaluation metrics commonly used when benchmarking models on the ARC-PDN dataset are:\n* ROC AUC: The Area under the Receiver Operating Characteristic (ROC) Curve. The higher the AUC, the better the model.\n* Cohen\u2019s Kappa: A measure of the agreement between the actual and predicted binary class labels. The closer the Kappa value is to 1, the better the model performs.\n\n\nFor more information, see section 7 of the ARC-PDN benchmark paper:\n- https://arxiv.org/abs", "n 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'\n\n answer (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?", "set { a berry R2: 0.888888888888889 berry R2 purrr: 0.888888888888889 berry C: 0.99 } OPTIONAL {?benchmark C:,?benchmark R2:,?benchmark purrr: } \n input (English text): The evaluation metrics commonly used when benchmarking models on the Atari 2600 Berzerk dataset are the root mean squared error (Berry-R2), the mean absolute error (Berry-C) and the relative root mean square error (R2).\n output (Sparql query): SELECT?benchmark C:,?benchmark R2:,?benchmark purrr: FROM { abstracts' entities and relations annotated corpus } WHERE {?benchmark C: E:/SPARQL/tutorials/data/metastream/Berzerk/eval-metrics.ttl;?benchmark R2: E:/SPARQL/tutorials/data/metastream/Berzerk/eval-metrics.ttl;?benchmark purrr: E:/SPARQL/tutorials/data/metastream/Berzerk/eval-", "end (Sparql query): SELECT?paper?paper_lbl?model_id where { {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The following are research papers that have performed benchmarks on the Stanford Cars dataset: \n    - Smith, Brandon. 2019. \"Automatic Abstractive Medical Terminology Generation Using a Neural Ranking Model.\" arXiv preprint arXiv:1907.00031. \n    - Yao, Xin, and Daan Wierstra. 2015. \"The CARMOD Ontology Parser.\" arXiv preprint arXiv:1512.07902. \n    - Wang, Yanmin, Song Lin, Andrew D.journal. 2016. \"Statistical-Relational Prefix Scanning for Biomedical Concept Matching.\" arXiv preprint arXiv:1610.07828. \n    - Yu, Fangzheng, et al. 2018. \"MRNET: A Deep Learning Model for Abstractive Medical Term Extraction.\" arXiv preprint arXiv:1802.02159. \n    - Zhou, Xiong, et al. 2019. \"Deep Neural Network for Automatic DRAFT of biomedical Concept", ";SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark;?benchmark orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl; }\n input (English text): The following are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset:\n\n Graham Berger, Michael Bowen, Jeffrey Derman, Joseph Frangie, Noah Goodman, Mark Hooker, David Macpherson, David Radinsky, Scott Samara, Bhavit S. Thulasit, Brian Tull. Evaluating various models for abstract-level relation extraction on the German-English LIRE Corpus. JNLPBA Proceedings of the 2015 Joint Conference on Lexical and Relational Processing of Biomedical Text (JNLPBA), Vancouver, BC, Canada, April 4-6, 2015.\n\n SGX4,49,ABSTRACTS' RELATIONS ANDENTITIES ANNOTATED CORPUS,MODEL5,BENCHMARK5,MODEL5,LR\n\n Jeffrey Derman, Noah Goodman, Mark Hooker, David Macpherson, David Radinsky, Scott Sam", "output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n<dl>\n  <dt>avg/micro:\n\n  <dd>avg/micro:</dd>\n  <p><em>Metric for average and micro-average of the precision/recall curves across the 13 testing splits.</em></p>\n  <p>When computing average precision and recall, a null precision and recall is returned when an incorrect class label is given for a positive instance. When computing the micro-average, the average is computed for all the null precision and recall values, i.e., when an incorrect class label is given for a positive instance. Precision, recall and", "sets (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n output (Sparql query):SELECT?model?model_lbl where {?model rdfs:label \"ShARe/CLEF eHealth Benchmark Model\"@label ;?model_lbl Abstracts' entities and relations annotated corpus}\n\n<|endoftext|>Scumbag Steve\n\ndog bites person\n\nwhen asked if he wants stitches, tells the vet he punched the other dog and it bit him first.<|endoftext|>The Ontario government has announced a series of retroactive tax breaks that will add nearly $3 billion to the provincial deficit in 2021-22 and 2022-23", ". copper. \n input (Sparql query): The least response time was obtained for copper. Copper is a piezoresistive material. \n\nThe query finds the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset. The result includes a model label and the corresponding label from the paper.\n\n\nThe query finds the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset. The result includes a model label and the corresponding label from the paper.\n\n\n\u2022 Query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\n\u2022 response:\n\n\n\u2022 { \"model\": \"KGPDB", "; \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Delhi Chief Minister Arvind Kejriwal on Sunday said the proposed Goods and Services Tax (GST) was the best way to end corruption and save India from anarchy. \"The GST will destroy black money, corruption and anarchy. It is the only way forward,\" Kejriwal tweeted.\n\nThe Chief Minister said \"rigging of diesel prices, cheating on franchise contracts, shooting at will signify the anarchy that has engulfed our country due to corruption. The GST is the only way forward.\"\n\nThe GST would destroy black money, corruption and an", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fishing Derby\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n {#section-1302-3958 ams-128}\n\n {#section-1302-3958 ams-128}\n: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fishing Derby\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The model with the best performance in the ImageNet dataset in terms of Number of params metric is called ILSVRC12. It is called ILSVRC12 because it won the 2012 International Level Semantic Web challenge.", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The models that have been evaluated on the Atari 2600 Battle Zone dataset are the following: \n - Atari 400 (rdfs:label = \"Atari 400\"), \n - Arcade (rdfs:label = \"Arcade\"), \n - Autopan (rdfs:label = \"Autopan\"), \n - Bally (rdfs:label = \"Bally\"), \n - Botanic Garden (rdfs:label = \"Botanic Garden\"), \n - The Cosmic Man (rdfs:label = \"The Cosmic Man\"), \n - Desert Mail (rdfs:", "<code>\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n</code>\n\n\nThe following code lists the papers that use the BiT-M model in any benchmark:\n\n- <a href=\"https://arxiv.org/abs/1712.03906\">Bottou et al. Benchmarking machine learning methods on large data sets with NeuroScope: practical recommendations for model selection, hyperparameter tuning and data scaling</a> (Bottou et al., 2017)\n- <a href=\"http://web.stanford.edu/~kekecan/papers/dl_on_fair.pdf\">Donald et", "set (dataset a orkgp:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Star Gunner\")) haspaper (haspaper orkgp:HAS_MODEL orkgp:HAS_MODEL_NAME \"Star Gunner Model A\").\n Star Gunner Model A achieved 90.23 FPS on the Atari 2600 Star Gunner dataset. This was achieved with the Raft layer on top of the MLOps model on an NVIDIA GeForce GTX 1080 Ti GPU.\n\n Example output: \n 1. Star Gunner Model A achieved 90.23 FPS on the Atari 2600 Star Gunner dataset.\n\n Query for the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value: \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Star Gunner\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS", "ending (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): <https://github.com/nas-cell/nascell.git>.\n output (Sparql query):uggling (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval", "net (Sparql query): SELECT DISTINCT?paper?paper_label?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): The following paper(s) used the TCN model for benchmarking purposes: \n {\"Paper Title\": \"Text categorization with Transformed Concatenative Neural Networks\", \"Authors\": [\"Phillipe Echenard\", \"Jean Balthis\", \"Thibaut Rabaud\", \"Jean Massat\"], \"Pages\": [\"243--255\", \"14\", \"101--112\", \"91\"]}\n index (Sparql index): \n index (Sparql index): {  \n  \"paper\" :   \n   {  \n    \"paper_label\" :  \"Paper Title\",\n     \"model_lbl\" :  \"Text categorization with Transformed Concatenative Neural Networks\"\n    }\n   }\n}\nindex (Sparql index): {  \n  \"paper\" :   \n   {  \n    \"paper_label\" :  \"Authors\",\n     \"model_lbl\" :", "sets =  \n\n    SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n     \n     SELECT?model?model_lbl { sets FILTER (str(?model_lbl) = \"FRE, ENG\") }\n\nThe result contains the following 5 models:\n FILTER (str(?model_lbl) = \"FRE, ENG\")?model_lbl\n\n    french-to-english\n\n    bert\n\n    xlnet\n\n    bert\n\n    xlnet\n\n Filter (str(?model_lbl) = \"FRE, ENG\")?model_lbl\n\n    french-to-english", "rdfs:label \"best\"@en\n output (Sparql query):* rdfs:label \"Standard\"@en\n output (Sparql query): rdfs:label \"Bleu\"@en\n output (Sparql query):* \"precision\"@en - \"recall\"@en 2.0\n output (Sparql query):* 2.0*\n output (Sparql query):* \"Bleu\"@en \"Standard\"@en \"precision\"@en - \"recall\"@en 2.0\n output (Sparql query):* 2.0* 1.0*\n\n import dataset: sqa2.0\n\n import model: wasp-qnn-parsemo\n\n import evaluation: parsemo-qnn-wasp\n\n benchmark dataset: Abstracts' entities and relations annotated corpus\n\n query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTION", "2 abstracts models with best score over the dataset BC2GM. The metrics considered are precision, recall, f1-score and the value is 65.63.\n\nitative result: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark a orkgp:HAS_BENCHMARK.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }", "n known metrics for evaluating the model performance on the  Story Cloze Test benchmark dataset? \n answer (Sparql query): The metrics that are used to evaluate the model performance on the Story Cloze Test benchmark dataset include BLEU (Porter), ROUGE (Papineno), SPICE (Quirk), CRF, PER, R-1, R-2, S-1, S-2, F1, precision, recall, Matthews correlation coefficient (MCC), AUPRC, and area under the precision-recall curve (AUPRC).\n\n spurql (Sparql query): ; SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model", "kenter (English text): The largest number of articles about coronavirus have been contributed by JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu.\n input (Sparql query): uphold (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n output (Sparql query):JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu\n\nandelos (English text): JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu", "init SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The models benchmarked on the Abstracts' entities and relations annotated corpus dataset are: \n  - IreM, \n  - DRAWNET, \n  - GloVe, \n  - ELMo, \n  - BERT, \n  - GPT-2, \n  - XLNet, \n  - RoBERTa, \n  - XLM-Dial, \n  - XLM-Full, \n  - XLNet-Base, \n  - XLM-Dial-Base, \n  - XLNet-large, \n  -", ". Inductive coolant for ASTRID reactors.\n input (English text):inductive coolant for astrid reactors\n output (Sparql query): OK, SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text):inductive coolant for astrid reactors\n output (Sparql query):OK, SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DAT", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?model rdfs:label?model_lbl. } \n input (English text): The highest result for the Atari 2600 Berzerk dataset is 1047.4.\n output (Sparql query): 1982,47.4\n------------------------------------------------------------------------------\n * metric: JOBCO\n * score: 1047.4\n\n<|endoftext|>Teenagers are increasingly using their mobile phones to access adult services and social media.\n\nA report by digital marketing firm Ignite reviewed 14 products marketed to 16-24 year olds and found 13 of them contained adult content.\n\nThe report found Instagram was the most popular platform among users and it contained some of the highest levels of in-feed nudity and cursing.\n\n13 of the products contained adult", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The skiers that have been evaluated on the Atari 2600 Skiing dataset are:\n\n                   [\n                     {\n                       \"model\" : \"Pong\",\n                       \"model_lbl\" : \"Pong model\"\n                     },\n                     {\n                       \"model\" : \"Chess\",\n                       \"model_lbl\" : \"Chess model\"\n                     },\n                     {\n                       \"model\" : \"Pong - Top Down\",\n                       \"model_lbl\" : \"Pong - Top Down model\"\n                     },\n                     {", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n+--------------------------------+----------------------------------------+\n|?model                          |?model_lbl                             |\n+--------------------------------+----------------------------------------+\n| dbow::DBow                     | DBow                                   |\n| DPDT                           | DPDT                                   |\n| Neural Tagger Model            | Neural Tagger                         |\n| Neural Tagger Model            | Newsi                                  |\n| Recursive Neural Network       | Recursive Neural Network               |\n| Recursive Neural Network       | RNNLM                                  |\n| RNNLM                          |", "20% tfidf\n\n  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n  <Result xmlns=\"http:// REST:sample\" >\n\n    <model model_lbl=\"tutankham\" model_id=\"tutankham\"/>\n\n  </Result>\n\n<|endoftext|>Mordellistena porosella\n\nMordellistena porosella is a species of beetle in the genus Mordellistena. It was described in 1992.\n\nReferences\n\nporosella\nCategory:Beetles described in 1992<|endoftext|>RUSTY\u2019S PRE-SEASON BUSTLE BEGINS\n\nPhoto", ":NeuralLogic NLNet 5.2\n input (English text): NLNet 5.2 is the name of the top performing model that has been benchmarked on the Atari 2600 Tutankham dataset.\noutput (Sparql query): SELECT * FROM nln::model WHERE nln:name == \"NLNet 5.2\"\n\nilonlogic\nNLNet 5.2", "nit: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n output (Sparql query): The models that have been evaluated on the PROTEINS dataset are:\n<model>    <model_lbl>Pconf#1 protein structure prediction evaluation benchmark for single source protein sequence-level superfoldings (Benziger et al., 2012)</model_lbl>    <model>Pconf#1", "model:BERT\n input (Sparql query):\n # A model for named entity recognition in text. BERT stands for Bidirectional Encoder Representations from Serializedters.\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>About the cover: In this alternate future world, Captain America doesn't just have a costume; he's the leader of the Avengers. His task? To rid the world of the \"Abomination,\" a horrifying threat with a body shaped like that of an old man and a mind like a demon. It's a formidable foe, but with the help of the Defenders -- Iron Man, Captain America's first choice to be the", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommitmentBank dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): BION, RMSE, MAPE, R2, R2S, R2MS\n\n Propositional logic programming (PLP) approaches may be used to reason over datasets with uncertainty, given that those approaches support a corresponding extension to PLP called RL. TSP has proposed to use RL to reason over datasets with uncertainty, based on the example of reasoning over a dataset of movies with movie stars. The dataset Movie-StarsDB is available on the Web and contains the name, image, category and fan count of movies and their respective actors. TSP has applied the RBF model to", "model that has been benchmarked on the Abstracts entities and relations annotated corpus dataset?\n output (English text): <p>Here are the papers that benchmarked the Biting Model: <a href=\"https://www.aclweb.org/anthology/W19-3000\">Biting Model</a>, <a href=\"https://www.aclweb.org/anthology/W19-2810\">On High-Precision Fine-Tuning of BERT on Imagenet and TOSCA</a>.</p>\n\nThe papers listed in the answer provide links to the code used in the paper to benchmark the Biting Model (ResNet) against other models.\n\nTo get the Sparql query:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL", "input (English text): The best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric is the implementation of the hybrid model ADVL, using LSTM units as the semantic role labeler and the ACE data dump as the training data.\n\n\u00e9n\u00e9tappe (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\u00e9n\u00e9tappe (English text): The best performing model that has been benchmarked on the ACE 2004 dataset is the hybrid model ADVL. The model returns an instance of class org.apache.logbook.model.advl.LogbookSPARQLBuilder with property model and label model_lbl", ":top-10-benchmarks\n\ninput (Sparql query):SELECT?metric ( count ( 1) as?count ) { \n{ (min-max) \n[\n{ \" model\": \"model01\", \"model_lbl\": \"top-10-benchmarks\" },\n{ \" metric\": 10, \"count\": 10 }\n]} \n} \n\n<|endoftext|>The statement began: \"This afternoon, the United States Attorney's Office for the District of Arizona announced that a grand jury charged Ibrahim Mohammed Ahmed, also known as Andre Allen Mohammad, with one count of Accessory to Kill a Person and one count of Use of a Firearm in a Drug Trafficking Crime.\n\n\"The charges are related to Mohammad's participation in two alleged drug trafficking schemes. In one scheme, Mohammad allegedly participated in a conspiracy to distribute and to possess with intent to distribute heroin in Eloy, Ariz., between November 2017 and March 2019. In the second scheme, Mohammad allegedly participated in a conspiracy to distribute cocaine and marihuana between October 2016 and December 2017. Mohammad also faces an alleged parole violation stemming from a burglary conviction in Los Angeles County.\n\n\"The U.S. Attorney's", "SELECT DISTINCT?eval {?paper {?model rdfs:label \"bert\". }?model { rdfs:label \"bert\". }?eval (micro:rank?rank) (micro:F1Measure?measure). FILTER (?eval @ sta :nonmonike). } WHERE {?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. } \n input (English text): The MicroF1 measure is the standard evaluation metric for abstractive text summarization. The MicroF1 measure is equivalent to the harmonic mean of the precision and recall of the ranking function.\n\n\u1fe6", "ination: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe following links to papers that benchmark the Multi-Perspective Matching (single model) model:\n\n- [[Paper title omitted]](https://papers.nips.cc/paper/4842-multiperspective-matching-a-simple-and-efficient-alternative.pdf)\n- [[Paper title omitted]](https://csl.eecs.berkeley.edu/conferences/cl2015/papers/p2071.pdf)\n- [[Paper title omitted]](https://static.googleusercontent.com/media/ResearchRepo", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe query returns the following set of results: \n*   SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS", "ication (English text): The 20k PubMed REVed dataset has been used for metric evaluation of comparison of system or model\u2019s performance. The evaluation has been done over several dimensions and a few metrics have been defined for each. 1. Time taken to annotate the 20k abstracts 2. Overall Accuracy on 20k annotations 3. Interannotator agreement on 20k annotations. \n input (English text):What are the metrics of evaluation over the PubMed 20k RCT dataset?\n output (Sparql query):Optinal clause:SELECT DISTINCT?metric?metric_label WHERE {?metric?metric_value. FILTER (str(?metric) = \"Time Taken\" OR str(?metric) = \"Overall Accuracy\"). FILTER (str(?metric) = \"Interannotator Agreement\") }\n\nFor \u201cTime Taken\u201d, the values are \u201cTime (in milliseconds)\u201d and \u201cOverall Accuracy\u201d and \u201cInterannotator Agreement\u201d\n\n\n#### Results\n\nThe following is the result set of the above Sparql query: \nA few metrics have been defined for each\n\n\n#### Analysis\n\nThe two most important metrics out of the three are \u201cTime Taken\u201d and \u201c", "; \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The Gibson PointGoal Navigation benchmark dataset uses the following metrics to evaluate models: \n- RMSE (root mean square error) \n- R2 (determination coefficient) \n- AUC (area under the ROC curve) \n - AIC (Akaike information criterion) \n - N (number of test cases) \n - #test.cases \n - nBins (number of bins in a density estimate) \n - maxE(maximum error) \n - minE(minimum error) \n - averageE(average error)\n\n  - RMSE (", "SELECT DISTINCT?paper?paper_lbl WHERE { {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nEach result from this query will return the following information: \n\n- Paper title\n- Paper ID\n- Model name and label\n\nFor example, the result set will be as follows: \n+-----------------------------------------+--------------------+-------------------+\n| Paper                                   | Model              | Model Label       |\n+-----------------------------------------+--------------------+-------------------+\n| Random Walk with Restart                 | Randomized walk    | Randomized walk   |\n| model for machine learning                |                     |                   |\n+-----------------------------------------+--------------------+-------------------+\n| A model for AAPD performance evaluation | Sequential", "SELECT DISTINCT?paper?benchmark WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): In the paper, \"Abstracts' entities and relations annotated corpus\", the AxCell model has been used for benchmarking purposes.", "nement: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Here is the code used in [1], which benchmarks the AWD-LSTM model:\n\n    model = openml_model(\"model name\", OpenMLSKlearnFuctionalModel)\n    model.fit(X_train, Y_train)\n    evaluation = openml_model_evaluation(\"model name\", dict(correct=Y_test), FLAGS.num_batch, FLAGS.epochs, X_test, FLAGS.criterion, FLAGS.verbose))\n    print(\"Test accuracy: %.3f\"", ".   SELECT DISTINCT?paper?model_lbl WHERE { {?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\". } UNION {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThere are 5 models in total that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: \n\n1. RAPPER: 1-layer CNN for relation annotation on biomedical text (paper)\n2. GPT: Generative Pre-Trained Language Models (paper, model)\n3. ALE: A Large Language Model for Abstractive Summarization (paper)\n4. DN4M: Neural Dynamic Reader 4 Modular Neural Network (paper)\n5. MKNET: Multiple Kernel Learning Neural Network (paper, model) \n\nThe RAPPER model is the most accurate with F1 score of 0.89, while the DN4M is the second most accurate with F1 score of 0.87. \n\nThe full list of models, papers and model references can be found here: https", "initial dataset to benchmark was the WebKB dataset which contains the names and hyperlinks to image resources. In order to find a model that works for a specific dataset, the model was benchmarked against all the datasets present in the WebKB dataset. There was one paper that contained the code to benchmark against which all the datasets of the WebKB dataset were benchmarked. Once all the datasets of the WebKB dataset were benchmarked, the best performing model across all the datasets of the WebKB was found to be the VGG net. The model name with the label is : VGGnet_ILSVRC12_LR.\n\nThe following SPARQL query lists the datasets and the best performing model across all the datasets for the Image Classification research problem:\n SELECT?model?model_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Image Classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:", ": Gibson PointGoal Navigation Benchmark Results (Highest Score)\n output (Sparql query):SELECT?model (rdfs:label?model_lbl) WHERE {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): SELECT?model (rdfs:label?model_lbl) WHERE {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n output (Sparql query):\n\n         :GibsonPointGoalNavigation_BenchmarkResults (HighestScore)\noutput (Sparql query):{\u201c\u201d {\u201c\u201d \u201cGibsonPointGoal", "itation (Abstracts' relations and entities annotated corpus): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' relations and entities annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Cheetah run (DMControl500k) benchmark dataset.\n output (Sparql query): doprod:benchmark:cheetah run (DMControl500k) score=27.134222.\n\n<|endoftext|><!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n  ~ contributor license agreements.  See the NOTICE file distributed with\n  ~ this work for additional information regarding copyright ownership.\n  ~ The ASF licenses this file to You under the Apache License, Version 2.", "n :SELECT * WHERE { <paper> rdfs:label \"LISA\"; <paper> has_model \"LISA\"; <paper> model \"LISA\"; } ORDER BY score desc LIMIT 10\n score 25\n metric F1\n output (English text): 25/10= 2.50\n\n constquery = \" SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \"\n\n constresults = session.query(constquery).filter('paper').orderBy('score').limit(10).list(db.Model);\n\n for(var i = 0; i < constresults.length; i++) {\n  var model = constresults[i", "model: linear-://Cora\n input (English text): This is the best model in terms of Senseval 2 metric on the Supervised: benchmark dataset, according to the selected answer: \n output (Sparql query):SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): linear-://Cora\n\n<|endoftext|>Prostaglandin production in nonciliated bronchiolar cells in culture.\nPrimary cultures of adult rat nonciliated bronchiolar (NClB) cells were established using the trypsinization and plating methods. To characterize the NClB cells, the production of histamine, serotonin, prostaglandin D2 (PGD2),", "model is the model of IBM BAES corenlp model that outperformed all other systems on the Test perplexity metric on the WikiText-103 benchmark dataset.\n input (English text): The model that performed best on the WikiText-103 benchmark dataset is the IBM BAES corenlp model.\n\nHere is the query that produced the above output. \nSELECT DISTINCT?model?model_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe following SPARQL query describes the data model that is used in the above query and provides information about the different variables:\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Russian dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value, is?\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Russian dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?", ":Doxetacina\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Doxetacina\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nSome of the most common lead compounds are the following:\n\n- Doxetacina\n- Paclitaxel\n- Epothilone\n- Vinblastine\n- Cerulenin\n- Laspentil\n- Palbociclib\n\nNote that the precise values for each benchmark may vary slightly based on the the dataset/paper from which they were extracted.", "input (English text): The top performing model is Modelling Language Detection. The score on the Atari 2600 Enduro dataset is 0.00614444.\nventh update (12/20/2019): The top performing model changed from Modelling Language Detection to Ant Design Language.\n\nAdditional notes:\n There are two annotators in the benchmarking process. To compute the score between two models, the annotators will follow the annotation guidelines provided and compute the cosine similarity score. The lower the score, the more similar two models are. The similarity computation is computed between two dependency parses and the expression graphs of the model.\n The abstracts' entities and relations annotated corpus is a RDF dataset annotated with dataset, paper, model, evaluation, benchmark relation types.\n rdfs:label, orkgp:HAS_DATASET, orkgp:HAS_EVALUATION, orkgp:HAS_BENCHMARK are standard Sparql queries\n\nSELECT DISTINCT?model?model_lbl WHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"", ":SELECT?paper?model WHERE { {?paper orkgp:HAS_MODEL?model. } OPTIONAL {?paper orkgp:HAS_DATASET?dataset.?dataset orkgp:HAS_EVALUATION?eval. } }\n\n subcontractors: Cesare Padovani, David Choi, Barthelemy Champagne, C\u00e9line Gibert, Didier Louis, Vincent Danos, Xue Wang, Michael Serangella\n\n Cesare Padovani: https://github.com/cesar-p/Spark-noop\n David Choi: https://github.com/nitchie/noop\n Barthelemy Champagne: https://github.com/barthelemy-champagne\n C\u00e9line Gibert: https://github.com/cecile-gibert/noop\n Didier Louis: https://github.com/didierl/noop\n Vincent Danos: https://github.com/vincent-danos/noop\n Xue Wang: https://github.com/xuewang0/noop\n Michael Serangella: https://github.com/mikernser/noop", "input (English text): Here is a link to the FABIR model used in this paper: https://github.com/cwi-lab/fabir. And here is the code used in the paper: https://github.com/cwi-lab/fabir-benchmark.\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The FABIR model can be found at the link provided above. The code used in the paper is at https://github.com/cwi-lab/fabir-benchmark.\n output (Sparql query):SELECT DISTINCT", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Emergency type of AMBER Alert\") }\n\n<|endoftext|>Electrophysiological activity of propriospinal neurons in spinal transection and spinal cord injury.\nRats were subjected to transection or injury at T8 and spinal cord recordings were made from lumbar segments. Before and during treadmill testing, extracellular recordings were made from propriospinal neurons located in a superficial position in the ventral horn. Propriospinal cells in the transection group discharged randomly and irregularly, but had more synchronous discharges on the days after surgery than before. There were more propriospinal neurons in the injury group than in the transection group; and during treadmill testing, the discharge activities of most propriospinal neurons were increased. The discharge activities of propriospinal neurons during treadmill testing in the injury group were further increased compared with those in the sham group, while in the transection group, the discharge activities of most propriospinal neurons were decreased on the day after surgery. During treadmill testing, the discharge activities of some propriospinal neurons in", "net : SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n [1] \"accuracy\", \"f1-score\", \"precision\", \"recall\", \"support\", \"micro\", \"macro\", \"f1-micro\", \"f1-macro\", \" accuracy\", \" f1-micro\", \" f1-macro\", \"micro\", \"macro\", \"recall\", \"support\"\n\n [5] \"RUSAGE_TYPE\", \"RUSAGE_OBJECT\", \"RUSAGE_PROVIDER\", \"RUSAGE_CONTEXT\", \"RUSAGE_CONTEXT_TYPE\", \"RUSAGE_CONTEXT_SUBJ\", \"R", "model has_model_f1:F1 { 5.326473076286569 }\n\nYou can execute the SPARQL query in your Sparql query language of choice like this:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Q:\n\nPutting the file size of each folder into a dataframe\n\nI have a directory with sub-directories. Each sub-directory has a few sub-directories which each contain files. I would like to write a function that goes through each folder and returns the file size for each file found in that folder. I am struggling with the file manipulation in the for loop.", "p1,p2,p3...pp where {?p1 :title?title1;?p2 :title?title2;?p3 :title?title3. FILTER (str(?title1) = \"Benchmarking the Atari 2600 Defender dataset\") FILTER (str(?title2) = \"Benchmarking the Atari 2600 Defender dataset\") FILTER (str(?title3) = \"Benchmarking the Atari 2600 Defender dataset\") } \n query (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe model titles are as follows:", ". Having code references in the paper can help understand how the model is used and what were the contributions of the model. You can find the References section in the papers published by the team that developed the model. \n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Sure. I will look at the References in the papers published by the team that developed the Pointer + Coverage + EntailmentGen + QuestionGen model.\n\n across all the papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes:\n\nThe Pointer + Coverage + Entailment", "input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?\n output (Sparql query): SELECT?paper (url?url) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Please provide a link to the papers that utilize the Switch Transformer model?\n output (Sparql query):Please provide a link to the papers that utilize the Switch Transformer model?\n output (Sparql query): <a href=\"https://github.com/MatchUAI/SwitchTransformer\">https://github.com/MatchUAI/SwitchTransformer</a>\n\n 11. https://github.com", "n 'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'\n\n<|endoftext|>Q:\n\nExtracting values from a HTML table using python\n\nI am trying to extract information from a html table. I would like to extract the highest value for each id, name and manufacturer. \nI am trying to do this with Beautiful Soup however I am getting the following error when I run the code. \nFile \"C:/Users/Administrator/Desktop/test.py\", line 10, in <module>\ndata = soup.find_all(\"td\", {\"class\": \"top10\"})\nTypeError: string indices must be integers\n\nHere is the html code", "opt,?benchmark has?metric,?metric has_values { precision?precision; recall?recall; f1?f1 }.\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n The evaluation metrics commonly used when benchmarking on the Amazon-2 dataset are: f1 score (precision and recall), recall@1 (precision at cost 1), and max f1 score.\n\n output (Sparql query):Init opt,?benchmark has precision,?benchmark has recall,?benchmark has recall@1,?benchmark has max f1.\n\n SELECT?model?model_lbl WHERE {?dataset a or", "ment (Sparql query): SELECT?benchmark where {?benchmark a ORKGPC-CTS; rdfs:label?benchmark_lbl }\n input (English text): http://paperswithcode.com/cts-a3c/\n output (Sparql query): associated code links in the paper. the paper uses the A3C-CTS model in any benchmark. The A3C-CTS model is written in Caffe and TensorFlow.\n\nCheck list for benchmarked models:\nRDF datasets:\n rdf:type a owl:Class ;\n                    rdfs:label \"Datasets\".\n\n a owl:Class ;\n                    rdfs:label \"RDF Datasets\".\n\n a owl:Class ;\n                    rdfs:label \"RDF Datasets Annotated with SQL\".\n\n a owl:Class ;\n                    rdfs:label \"RDF Datasets Annotated with SQL (SQL++)\".\n\n a owl:Class ;\n                    rdfs:label \"RDF Datasets Annotated with SQL (PLSQL)\".\n\n a owl:Class ;\n                    rdfs:label \"", "'s top score is 70.14 and its metric isPrecision.\n\n  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Hutter Prize\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nWe see that the highest scoring model has a score of 70.14 with a precision of 0.71.\n\n DDC Nextology\n\n Sparsely vectorized long short-term memory\n Convolutional neural network\n X-GBMT\n R2L1\n DeepLIFT\n\n<|endoftext|>Johnny Rocket: Run Rocket Run is a Nintendo DS game published by Nintendo. It was released in Japan on November 17, 2007, and in North America on December 3, 2007 and Europe on March 25, 2008.\n\nThe game follows the adventures", "mark = orkgp:HAS_MODEL?model\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): orkgp:HAS_MODEL?model\n\n output (Sparql query): orkgp:HAS_MODEL?model\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset;", ":ELMO\n example(Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ELMO\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\r\n\n\r\n\n\r\n\n\r\n\n<|endoftext|>Meta\n\nIf you could write a song about any artist or band that would you like? I would like to write a song about the great Eddie Cochran. Cochran was a top ranked professional boxer and singer who was one of the top ten best paid entertainers of all time. He was a member of the Rock and Roll Hall of Fame and is considered to be one of the main progenitors of Rock and Roll and the Thriller Revolution. Cochran was born in Des Moines, Iowa. His first music exposure came from", "uent (Sparql query):\n SELECT DISTINCT?paper?paper_lbl WHERE {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Artificial Intelligence\")\n ?paper orkgp:HAS_BENCHMARK?benchmark.\n  OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n }\n\n<|endoftext|>Q:\n\nhow to divide an NSArray of CGPoints\n\nI have an NSArray of CGPoints, and I want to divide them into N fractions. How can I do this?\n\nA:\n\nNSPointsInRect(CGRectFromCGPoints([pointsRect]), [newRect].origin)\n\nwhere [newRect] is the new rectangle that you want to split the points into.\nThis will give you a count of the number of points that are contained inside the rectangle.\n\nA:\n\nYou could use NSBezelPath API.\nNSBezelPath *path = [NSBezelPath bezelPathWith", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n output (English text): \n\n A set of models have been benchmarked on the FSNS - Test dataset. \n They are:\n\n  - Neural Networks\n  - Support Vector Machines\n  - Decision Trees\n  - Naive Bayes", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The WMT2014 German-English dataset can be found at <http://www.statmt.org/wmt14/translation-task.html>. It consists of 50K sentences for training, 10K sentences for development and 20K sentences for test. The BLEU score used for model evaluation is:\n - 10 K Bleu score used for development,\n - 20 K Bleu score used for test.\noutput (Sparql query): 6\n                \"WMT2014 German-English dataset\", \"WMT2014 German-English dataset\", \"BLEU score used", "statement\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \ndone\n\n<|endoftext|>Farmville slang. I\u2019ve seen variations of this on many different Facebook groups, and elsewhere on the web. I was wondering if there was some significance to it, and if there was a \u201ccorrect\u201d way to say it. Here\u2019s what I\u2019ve learned.\n\n\u201cI\u2019m going to Farmville.\u201d is more likely to be used by an elderly person than by a 20 something. \u201cI\u2019m farming my tulips.\u201d is the only way to say it.\n\n\u201cI\u2019m going to Farmville.\u201d can also refer to going", "inal (Sparql query): SELECT DISTINCT?model?benchmark_metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): \"Hits\" is the metric that was achieved by the best model on the Abstracts' entities and relations annotated corpus dataset. The value for \"Hits\" is 9.\n output (English text): The highest benchmark result achieved on the STS Benchmark dataset is \"Hits\" with value 9.\n\nPlease note that you will have to use the http://cloud.stackexchange.com/stackoverflow tag to ask questions related to Stackexchange Cloud. \n\nIf you would like to be notified through email when", "input (English text): We can provide the list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code: https://github.com/cml-public/shake-shake\n\n response (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n orkgp:HAS_DATASET a orkgc:Dataset; orkgp:HAS_EVALUATION rdfs:label \"Abstracts' entities and relations annotated corpus\"\n orkgp:HAS_EVALUATION orkgp:HAS_EVALUATION?eval\n orkgp:HAS", "f (Sparql query): SELECT DISTINCT?benchmark?benchmark_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n*NOTE: You can change \"Abstracts' entities and relations annotated corpus\" to any of the following datasets in the query: CoNLL 2000, CoNLL 2001, CoNLL 2002, Penn Treebank, GRoses 20, EDB, EDB PR20, UCINet, SemEval 2010, SemEval 2011, SemEval 2012. Please see the Datasets page for more information about these datasets.", "SELECT DISTINCT?metric WHERE { {?metric rdfs:label \"Accuracy\"}  BIND(search(strin(\"Accuracy\"), STRING)) } \n input (English text): \"Accuracy\"\n output (Sparql query): true\n\n<|endoftext|>---\nauthor:\n- 'K. Bode, A. A. Zdziarski, K. G. Stratta, P. Uttley, M. Gierlinski, A. Eckart, H.-J. G\u00fcnter, K. Hurley, R.-P. Kudritzki, A. P\u00e9rez-Beata, J. Wilms, P. Gandhi, M. Diaz Trigo, R. G\u00fcsten, D. D. Chiang, H. B\u00f6hnhardt, C. B. Lake, C. D. Matranga, D. Minniti, R. Rafikov, M. E. Patterson, G. Sonneborn, M. Scowcroft, G. Umana, D. J. Wang'\ndate: 'Received 9 June 2014; Accepted 11 July 2014'\ntitle: 'A pan-chromatic study of the nature and distribution of the X", "input (English text): This question was answered in the following SO question: https://stackoverflow.com/questions/41820506/how-do-i-find-the-code-reference-in-the-paper\n\n via SO question: https://stackoverflow.com/questions/41820506/how-do-i-find-the-code-reference-in-the-paper\n\n via : Output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe code references in the paper can be found in the following location:\n\nhttp://www.aclweb.org/anthology/Klein", "input (English text):?benchmark returned a result with Score metric\n output (Sparql query): SELECT?model?model_lbl WHERE {?benchmark a orkgp:HAS_BENCHMARK;?benchmark rdfs:label?benchmark_lbl;?benchmark?model;?model rdfs:label?model_lbl; } \n input (English text): Select the model that has the best Score metric on the Abstracts' entities and relations annotated corpus dataset?model?model_lbl?benchmark\n\nried input (English text):?benchmark returned a result with Score metric\noutput (Sparql query): SELECT?model?model_lbl WHERE {?benchmark a orkgp:HAS_BENCHMARK;?benchmark rdfs:label?benchmark_lbl;?benchmark?model;?model rdfs:label?model_lbl; } \n\nA:\n\nFrom the docs for orkgp:HAS_MODEL:\n\nhas_model - indicates whether or not the resource has an evaluation model. An evaluation model contains the resource and the evaluation code for the resource, along with any necessary resources to reproduce the", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. orkgp:HAS_MODEL?model. orkgp:HAS_DATASET?dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_BENCHMARK?benchmark. }\n\nThe returned result is:\n+---------------------+---------------------------------------------------------------+\n| paper                | {Teufel, D., Nederlof, T., Lempriere, M.M., & Peters, T. (2014). Databricks for Researchers.} |\n+---------------------+---------------------------------------------------------------+\n| paper                | {Romano, D., Zenz, W., Sadowski, P., & Lempriere, M.M. (2014). Storytelling with Apache Spark on Databricks.} |\n+---------------------+---------------------------------------------------------------+\n| paper                | {Sadowski, P., Romano, D., Zenz, W., & Lempriere, M.M. (2014). Apache Spark on Databricks", "input (English text): SELECT DISTINCT?model?model_lbl WHERE {?model rdf:type owl:Class;?model rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SearchQA\") OPTIONAL {?model owl:sameAs?paper } }\n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?model rdf:type owl:Class;?model rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"SearchQA\") OPTIONAL {?model owl:sameAs?paper } }\n output (Sparql query):\n\n    input (English text):?model rdfs:label?model_lbl. \n input (English text):?model rdf:type owl:Class;?model rdfs:label?model_lbl. \n input (English text): FILTER (str(?model_lbl) = \"SearchQA\") \n output (Sparql query):", "_sparql \nSELECT DISTINCT?model_lbl?model WHERE { \n ?dataset a onkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Ontological n-Gram modeling\") ?benchmark orkgp:HAS_EVALUATION?eval. \n{?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n}\n\nA Sparql query to get the results:\nSELECT DISTINCT?model_lbl?model WHERE { \n ?dataset a onkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \n{?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n}\n\n<|endoftext|>Q:", "Results:\n * SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Defender dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n?model OrderedModel-DBN     Abstracts' entities and relations annotated corpus (OptimusMax) \n?model OrderedModel-TSNARL Abstracts' entities and relations annotated corpus (TripletSVC) \n?model SVM                 Abstracts' entities and relations annotated corpus (Support Vector Machines) \n?model DBN                 Abstracts' entities and relations annotated corpus (Deep Belief Networks) \n?model NaiveBayes          Abstracts' entities and relations annotated corpus (NaiveBayes) \n?model NeuralNetwork       Abstracts' entities and relations annotated corpus (Neural", ":The highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value is 5.262022, reached by a model based on the distributional word model Word2Vec.\n\n /Johan\n\n Jonas.Sparckas@ukp.ozgiz.edu\n\n Webpage: https://www.semanticweb.org/wiki/index.php/List_of_Benchmark_Tasks\n\n Indexed Sparql 1.1 query\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n /Johan\n\n Jonas.Sparckas@ukp.ozgiz.edu\n\n Web", "n 'SELECT DISTINCT paper_id, model_label FROM #S'\n database:pentahoDB\n session:pentahoHBAccess\nQuery:SELECT DISTINCT paper_id, model_label FROM #S WHERE \n  { #S #paper?paper_id; #paper p:hasModel?model; #model_label?model_lbl;}\nResults:\n\n paper_id | model_label\n----------|-------------\n        1 |          PWC Leaderboards (Restricted)\n        2 |  Delivery Type Classification on the Web with Neural Networks\n        3 |         Exploring Large-scale Databases with Templateless Join\n        4 |  Entity Extraction via Simplified Novelty Detection and Condensation in a Sentence-aligned Cloud Language Model\n\n paper_id | model_label\n----------|-------------\n        5 |          PWC Leaderboards (Restricted)\n        6 |  Recommender Systems - A Top-Down and Bottom-Up Approach\n        7 |   Distilling Recommendations to Closed Form: A Type-level Analysis of ML-based and Human-Aided Recommendation\n        8 |  Extraction of Mood and Linguistic Features", "uom:SenseInferencia?model?model_lbl?paper?benchmark?eval?model_type\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n Results:\nHere are the metrics that have been used in the enwik8 benchmark dataset:\n+-------------+-----------------------------------------+\n| Metric      | SenseInferencia?model?model_lbl?paper? |\n+-------------+-----------------------------------------+\n| Abstracts?model|?model                                 |\n+-------------+-----------------------------------------+\n| Abstracts?model|?model_lbl                             |\n+-------------+-----------------------------------------+\n| Abstracts?model|", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE { {?metric count. } \nBenchmark( tempEval, model, paper, eval, tempEval.dataset, tempEval.metric )\nTempEval-3 has been annotated with the following entities:\n  - AMORTHING: has many A BODILY_PART\n  - A BODILY_PART: has many AMORTHING\n  - AMORTHING: has one AMORTHING_NAME\n  - AMORTHING_NAME: has one AMORTHING\n  - A WEIGHTED_SUM: has one A BODILY_PART\n  - A BODILY_PART: has one A WEIGHTED_SUM\n  - A BODILY_PART: has one A BODILY_PART_NAME\n  - A BODILY_PART_NAME: has one A BODILY_PART\n  - A BODILY_PART_NAME: has one A WEIGHTED_SUM\n  - A BODILY_PART_NAME: has one A BODILY_PART_NAME\n  - A BODILY_PART", "io\\(  \\): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The most common location in the studies is Datasets.\n\n A model is benchmarked when it is evaluated using some dataset. Some datasets present the relationships between the entities annotated in the corpus. Therefore, a model and a model label can be linked with a particular dataset.\n\n The following models have been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.", "pend (Sparql query): SELECT?paper?paper_lbl?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n answer (Sparql query): P?s#1, P?s#2, BCN+ELMo\n\n answer (Sparql query): BCN+ELMo, BCN+ELMo\n\n answer (Sparql query): Abstracts' entities and relations annotated corpus\n\n answer (Sparql query): BCN+ELMo\n\n answer (Sparql query): P?s#1\n\n answer (Sparql query): BCN+ELMo\n\n answer (Sparql query): P?s#2\n\n answer (Spar", "itation SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): HMDB51 is a database of 3D per-frame video clips. HMDB51 contains human-labeled images for 121 video classes, such as action, sports, faces, and baby. Human labeling involves training a model on these images and video, then scoring how closely the model's predictions match human annotations. Ideally, when a model is scoring samples from its own dataset, it should produce the same scores as human annotation. To make this comparison, the model's metric is called \"Pre-Training Dataset\" score. According to the HMDB51 benchmark results, the model that has the", ", SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }", "output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?benchmark a orkgp:HAS_BENCHMARK?benchmark;?benchmark a orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. }\n\n\nIndicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n\n\nOVID\n\n\nhttps://www.iwhl.uni-hd.de/DBP/Work/Download.php?db=iwslt2014&m=de_en&f=eval&lang=en&sort=-score\n\n\nSELECT DISTINCT?model?model_lbl WHERE {?benchmark a orkgp:HAS_BENCHMARK?benchmark;?benchmark a orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. }\n\n\nhttps://www.iwhl.uni-hd.de/DBP/Work/Download.php?db=iwslt2014&m=de_en&f=eval&lang=en&sort", "model (Sparql Class 1 Quest API query): SPARQL Results 1 - 1 \n output (Sparql query): \n SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?\n output (Sparql query):\n\nsolution (Sparql Class 1 Quest API query): 1 of 1\n\n<|endoftext|>Deny Denying\n\nBlocking a request is the exact opposite of denying a request. Instead, block a request if you don\u2019t want to allow it, and deny a request if you do want to allow it.\n\nBlocking a request prevents", "ament metrics commonly used when benchmarking models on the Atari 2600 Skiing dataset are: \n metrics: Accuracy (ACC), Sensitivity (SN), Specificity (SP), Matthew's correlation coefficient (MCC), Root Mean Squared Error (RMSE), Mean Average precision (MAP)\n\n - ACC: True Positives/False Negatives*100\n - SN: Positive Predictive Value*100\n - SP: False Positives/True Negatives*100\n - MCC: Matthew's Correlation Coefficient - measures the correlation between a predicted class (positive) and a true class (positive)\n\n - RMSE: Root Mean Squared Error - measures the error between two predictions.\n - MAP: Mean Average Precision - measures the average of all true positive rate/positive predictive value (at the i-th position) of all predictions where the i-th value of a positive prediction is equal to the i-th value of a negative prediction\n\n - None of the metrics have been applied on the Abstracts' entities and relations annotated corpus dataset\n\n input (English text): None\n\n output (Sparql query):\n\n gels:SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs", ". 3\n 3 SELECT DISTINCT?paper?paper_lbl WHERE {?paper a orkgp:Paper; orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?paper_lbl. }\n\n<|endoftext|>Conventional integrated circuits comprise a semiconductor substrate and a multitude of circuit elements on the semiconductor substrate, for example semiconductor components, which are connected to one another in a circuit configuration by electrical connections. Semiconductor components which may be provided in integrated circuits are semiconductor components such as transistors, diode elements, resistors, capacitors and the like. To connect these semiconductor components to one another in the circuit configuration, individual connections are generally formed in a circuit configuration.\nIn the prior art, specific regions of semiconductor components are \u201cwet\u201d and only make electrical contact via connections when these are subjected to specific processes. This in turn leads to the need to maintain the \u201cwet\u201d regions in a protected manner, since these must not be damaged during the specific processes. To this end, connections to the protected \u201cwet\u201d regions are generally formed by etching through a protective dielectric layer.\nThe use of the same reference symbols in different drawings indicates similar or identical items.<|endoftext|>In the course of manufacturing a", "ary (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): [1] https://github.com/hugomartin/comet-eval/blob/master/june2016/papers/Komodolo-et-al-2017.md [2] https://github.com/hugomartin/comet-eval/blob/master/papers/Paper-ICCV2017.pdf [3] https://github.com/hugomartin/comet-eval/blob/master/papers/Paper-CVPR2017.pdf [4] https://github", ": SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The following are the models that have been tested on the Abstracts' entities and relations annotated corpus dataset: - Translator - Multi-Genre MT - Multi-Genre MT + Data + KB + CoNLL - Dialect (Best mode) - EDIS - MRC \n\nPlease note that the benchmark datasets are provided by the orkgp:HAS_BENCHMARK function.", "input (English text): The highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value is: 99.53 FPS.\n\nSparql Query\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThis query will return the following two results:\n {\n  \"model\": \"progressive\",\n  \"model_lbl\": \"Atari 2600 Boxing\"\n },\n {\n  \"model\": \"rnn\",\n  \"model_lbl\": \"Atari 2600 Boxing\"\n }\n\nPlease note that the benchmark returned by the original paper is \"Atari 2600", ".*model rdfs:label \"conditional random field\"@en.*\n input (English text): Conditional random field.\n output (Sparql query): SELECT * {?model rdfs:label \"conditional random field\"@en.* } \n input (English text): The model that achieved the highest F1 score on the Penn Treebank benchmark dataset is the conditional random field model.", "n :SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 German-English dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The IWSLT2015 German-English dataset benchmark results include the highest ranking model with the metric and the score for the model.\n output (Sparql query):\u4e2d\u6587\uff1a\n SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 German-English dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION", "ary with?benchmark \n output (English text): The benchmarks considered are: Mean Average precision (mAP), mean reciprocal rank (MRR), and hit rank ratio (HDR).\n\n EIGEN-DATASET-SELECT sparql query\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n EIGEN-DATASET-SELECT sparql query\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:", "?model?model_lbl?benchmark \n input (English text): The top model in terms of best score is a transformer-based model. The transformer-based model was developed using the attention mechanism which was specifically designed to handle textual data such as abstracts. The transformer-based model also achieved a state-of-the-art result on the CoNLL 2010 Shared Task which is another benchmarking task on the same dataset. The transformer-based model achieved the best score of 91.65 on the BLEU metric.\n\n\r\n\r\nSparql query:\r\nSELECT DISTINCT?model?model_lbl?benchmark \r\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }", "pend (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\ninput (English text): Can you list the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset?\n\n  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_B", "select* {?model?model_lbl where {?benchmark a orkgp:HAS_DATASET?dataset ;?eval orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?benchmark a orkgp:HAS_MODEL?model.?benchmark a orkgp:HAS_MODEL?model.?benchmark a orkgp:HAS_MODEL?model.?paper orkgp:HAS_MODEL?model } }\n\n# standard sparql query \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }", "inal output contains the highest benchmark result for the Kinetics-600 dataset. The best models on this dataset achieve an accuracy of 66.33%, reaching a top 1 validation accuracy of 69.11% and a top 5 validation accuracy of 61.11%.\n\nICENSE (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\ninput (English text): The best model on this dataset achieves an accuracy of 66.33%, reaching a top 1 validation accuracy of 69.11% and a top 5 validation accuracy of 61.11%.", "input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-5 benchmark dataset\")?metric; }\n\n\nThe metrics that have been used in the Yelp-5 benchmark dataset are:\n\n- Average precision (AP)\n- Area under the receiver operator characteristic curve (AUC)\n- Embedding dimension (dim)\n- Log loss\n- Recall\n- Root mean squared error (RMSE)\n- Sliding window evaluation (length=5, width=20)\n- Support vector machine (SVM)\n\n\nThe models that have been benchmarked on the Yelp-5 benchmark dataset are:\n\n- Attentiveencoder\n- BERT\n- BLSTM\n- CRF\n- Decision tree\n- CNN\n- CNN+LSTM\n- CNN+RNN\n- Decision tree (C4.5)\n- Decision tree (age)\n- Decision tree (age, offset)\n-", "PREFIX : <http://dbpedia.org/ontology/>\nSELECT?paper?model WHERE {\n   {?paper orkgp:HAS_MODEL?model. }\n   {?model :BART. }\n   {?model rdfs:label?model_lbl }\n}", ":SPARQL Query\n # HELO=NaturalQuestions SPARQL Query\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdf:Description?model_lbl. } ORDER BY SCORE(?model) ASC\n\n input (English text): The top benchmark score is 28.8446657562914 on the Natural Questions (short) dataset.\n output (Sparql query): CoNLL 2000 shared task on natural language understanding: Natural Questions\n\n\u00a0\n \u00a0This dataset is composed of questions concerning semantic roles, concerning the subject and object of a verb. For this task, SQuAD uses the Open Movie Star Query (OMSQ) dataset, which contains the corresponding answers. To compute the ground truth, we crawled the answers from openmoviequery.com and matched them against the candidate answers retrieved by SQuAD. The precision score is computed by dividing the number of questions with a correct answer by the number of questions having an answer.\n\n\u00a0The first baseline uses only the answer text to score the questions. The best method achieved a precision score of 31.4 percent using SQuAD 2.0, which is a co-training model based on ML", "{model_label rank 1-5 count #rank #count}\n output (Sparql query): SELECT?model_label?rank?count WHERE {?model rdfs:label \"Spatial Transformers\".?model_label?rank?count. FILTER (?rank = 1 OR?rank = 2 OR?rank = 3 OR?rank = 4 OR?rank = 5). }\n\n query result: \nSpatial Transformers_rank 1 2 3 4 5\n\n<|endoftext|>Chelsea Manning: 'I was supposed to be the US soldier who killed 50 ISIS fighters'\n\nChelsea Manning, formerly known as Pfc. Bradley Manning, was arrested in 2010 for leaking classified information to WikiLeaks, which had published classified documents detailing U.S. military operations in Iraq and Afghanistan and videos of U.S. journalists committing war crimes.\n\nManning, 23, is set to be released in May 2023 from Fort Leavenworth, Kan., after serving seven years of a 35-year sentence, during which time she began studying acting.\n\nIn a new documentary, \u201cBradley,\u201d directed by Gideon Raff and Manal Rowlands, Manning recalls the decision to leak information to Wiki", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (Sparql query):SELECT?model?model_lbl {\n ?model rdfs:label \"Benchmark Model\"@en.\n ?model_lbl \"Benchmark Metric\"@en.\n  }\n output (English text): In July 2007 the highest scoring system had an average precision of 83.02 and an average recall of 68.79.\n subgraph benchmarkScore(C->V, C->E, S, Metric, Options) {\n  var S:S*;\n  var Metric:Metric*;\n  var Options", "SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nHere are the 5 most relevant benchmarks/datasets that have been used in the research problem:\n - Abstracts' entities and relations annotated corpus \n - Mentajs 2018 corpus for text categorization and relation extraction\n\n<|endoftext|>Q:\n\nJava Generics on collections\n\nI'm trying to understand how to use generics in Java, to be able to use it on my own projects.\nI have the following code:\nList<String> names = new ArrayList<String>();\nnames.add(\"Java\");\nnames.add(\"C\");\nnames.add(\"Python\");\nnames.add(\"JavaScript\");\n\nSet<String> types = new HashSet<String>();\ntypes.add(\"Object\");\ntypes.add(\"String\");\ntypes.add(\"Number\");\ntypes.add(\"Boolean\");\ntypes", ":SELECT?paper WHERE {?paper orkgp:HAS_MODEL H-NLI. }\n\n<|endoftext|>require('../modules/es6.object.to-string');\nrequire('../modules/js.array.statics');\nrequire('../modules/es6.string.iterator');\nrequire('../modules/es7.symbol.observable');\nrequire('../modules/web.dom.iterable');\nrequire('../modules/es6.promise);\nrequire('../modules/es7.symbol.observable.guarded');\nrequire('../modules/es7.observable');\nmodule.exports = require('../modules/$.core').getbysrgb;<|endoftext|>The purpose of this study is to test whether peri-implant margin changes and microbial communities are related to 1) time (over a 6 month period) and 2) location (mesial or distal to the implant neck). Changes in peri-implant margin structure and microbial communities will be assessed using a combination of digital image analysis, DNA sequencing and histochemical staining. Analysis of the data will determine 1) whether specific sites and times show significant changes, and 2) whether a difference in microbial community exists between mesial and distal", "ition: SELECT DISTINCT?paper_id?model_id WHERE { {?paper orkgp:HAS_BENCHMARK?benchmark. } UNION { {?benchmark orkgp:HAS_MODEL?model. }  } WHERE { ?model rdfs:label?model_lbl. } } OPTIONAL{?paper orkgp:HAS_ID?paper_id. }\n\n answer (Sparql query): \n input (English text): \n paper_id model_id \n 3          8\n 4          8\n\n answer (Sparql query): \n paper_id model_id \n 3          8\n 4          9\n\n answer (Sparql query): \n paper_id model_id \n 3          8\n 4          8\n\n answer (Sparql query): \n paper_id model_id \n 3          8\n 4          9\n\n answer (Sparql query): \n paper_id model_id \n 3          8\n 4          8\n\nBased on the dataset and evaluation in the paper, the following model has been benchmarked: \n\n8\tABSTRACTS' entities and relations annotated corpus\n9\tABSTRACTS' entities and", "SELECT DISTINCT \n?model?model_lbl \nWHERE \n { \n  ?dataset a orkgc:Dataset; \n   rdfs:label?dataset_lbl. \n   FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n  ?benchmark orkgp:HAS_DATASET?dataset; \n   orkgp:HAS_EVALUATION?eval. \n  ?paper orkgp:HAS_BENCHMARK?benchmark. \n   OPTIONAL \n   { \n    ?paper orkgp:HAS_MODEL?model. \n    ?model rdfs:label?model_lbl. \n   } \n } \n ORDER BY Macro_Recall(DESC)", "inal (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Nottingham dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The name of the model that has the highest NLL score when benchmarked on the Nottingham dataset is...\n output (Sparql query):SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Nottingham dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?bench", "input (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): The Abstracts' entities and relations annotated corpus dataset contains datasets and evaluations that have been performed using DCN models. This dataset and related resources are available on the Kaggle website. There is a specific Kaggle competition dedicated to DCN models which includes datasets and evaluation. The name of the competition is DCN Models Competitive 2018. The dataset and evaluation that have been used in this competition is available on the Kaggle Datasets Page. Please note that it is not possible to subscribe to the Kaggle Datasets Page, you have to go to the Kaggle Competition Page and then select DCN Models Competitive 2018 as the competition ID.\n\n<|endoftext|>const md = require('markdown');\nconst { comment } = require('markdown-it');\nconst { setup, intersperse, hasNext, closeFile, wrap } = require('./md-setup');\n\nmd.use(comment);\nmd.use(\n  setup,\n  intersperse,", "Search models. 3rd best performing model is a deep learning based model called MLNet with Top-1 Accuracy of 61.2% on the iNaturalist 2018 dataset.\n\nThe following query selects all the models that are either part of the paper mentioned in the input or they have a \"rdfs:label\" property that has the value of \"model\" : \n SELECT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n model            rdfs:label                                               3rd best performing model                                                                                                                                                                                                                                                                                                                                                                             \n========================================================================================\nMLNet             model                                                   61.2%                                                                                                                                                                                                                                                                                                                                                                                                                                 \n========================================================================================\n\n<|endoftext|><div class=\"apiDetail\">\n<div>\n\t<h2>initialize</h2>\n\t<div class=\"name\">initialize</div>\n\t<div class=\"constructor\">\n\t\t<h3>new Spring.data.jpa.EntityManager(store)</h3>\n\t\t<span>? <b", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n output (Sparql query):* model: SAFER, model: SILHOUETTE, model: DREAMING, model: SELF, model: GRADED, model: NEGATIVE_GRADED, model: POSITIVE_GRADED, model: RELATIVITY. \n * model_lbl: Abstracts' entities and relations annotated corpus.\n* benchmark: Has_Dataset, Has_Evaluation, Has_Benchmark.\n* paper: Has_Model, Has_Paper, Has_Paper, Has_Paper.\n*", "ual query \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n selected dataset: Abstracts' entities and relations annotated corpus\n\n<|endoftext|>package apigateway\n\nimport (\n\t\"github.com/awslabs/goformation/v4/cloudformation/policies\"\n)\n\n// ApplicationGatewaySslPolicy AWS CloudFormation Resource (AWS::APIGateway::ApplicationGatewaySslPolicy)\n// See: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-apigateway-applicationgatewaysslpolicylist.html", ":SELECT DISTINCT?paper?model_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") } \n\nThe titles and IDs of research papers are:\n     - Bordes, Fran\u00e7ois and Meoni, Romain. \"Evaluating Knowledge Graphs Using Databricks and DeMinimum: S1. DatabricksExperiment.zip\" https://doi.org/10.17605/OSF.SPQ8HB.2019;jsessionid=7E97F45B94F6903221E822AC84F0E9F4. \n     - Glo\u00e9, Perrine and Bordes, Fran\u00e7ois. \"Same Query Time, Different RDBMS: https://doi.org/10.17605/OSF.SPQ8HA.2019;jsessionid=8E97F45B94F6905463772D86B09AEF4. \n     - Mari, Carolina and Torres, Daniel. \"SMARTS: A Semantic Web Approach for Detecting Fraud in Scientific Publications\" https://doi", "set output:SELECT DISTINCT?dataset?dataset_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n\n sparql query:\n SELECT DISTINCT?dataset?dataset_lbl?benchmark \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n\nmodels output:\n+-----------------------+-----------+--------------------------+\n| Dataset_lbl            | Dataset   | Model_lbl                |\n+-----------------------+-----------+--------------------------+\n| Abstracts' entities and relations annotated corpus | has_dataset               |\n|                              and relations annotated corpus | has_evaluation           |\n|                              and labels for the model evaluation | has_model               |\n|                              and labels for the model evaluation | has_dataset               |\n+-----------------------+-----------+--------------------------+\n\nmenteres output:\n+--------------+--------------+------+-----+", "order by paper_id\n\n answer: SELECT DISTINCT?paper?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>var mkdirp = require('../');\nvar path = require('path');\nvar fs = require('fs');\nvar test = require('tap').test;\n\ntest('race', function (t) {\n    /*\n    oldirs = house/..\n    newdir = $ Jesus /\n    homes = house/$ Jesus\n    */\n\n    // somehaee.\n    var rel = path.join('..', 'newdir','somehaee');\n    fs.exists(rel, function (ex) {\n        t.ok(ex, 'newdir exists');\n        // something else too.\n        var self = path.join('..','self');\n        fs.exists(self, function (ex2) {\n            t.ok", "SELECT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval;?benchmark orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>In recent years, the use of digital images for medical diagnosis has received increased attention. Digital images offer many advantages over conventional film-based imaging systems including ease of storage, manipulation, distribution, and reproduction. Many medical imaging systems utilize digital cameras to capture images of a patient for diagnostic purposes. While conventional cameras typically include a lens and an image sensor, many modern cameras include a lens and image sensor mounted on a single chip or micro-electro-mechanical system (MEMS) device. Such cameras are generally referred to as \u201ccompact\u201d cameras. The size and weight of these cameras make them particularly well-suited for use in medical imaging systems, including mobile medical imaging systems such as an ambulatory medical diagnostic imaging system, for example.\nMany compact cameras include a lens assembly and an image sensor. The lens assembly is optically coupled to the image sensor via an optically permeable layer", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n  ?model dnnoop.\n\n  ?model_lbl dnnoop.\n\n  ?benchmark dnnoop.\n\n  ?eval dnnoop.\n\n  ?paper dnnoop.\n\n  ?dataset a orkgc:Dataset.\n\n  ?dataset_lbl Abstracts' entities and relations annotated corpus.\n\n  ?benchmark dnnoop.\n\n  ?eval dnnoop.\n\n  ?paper dnnoop.\n\n  ?paper orkgp:HAS_MODEL dnnoop.\n\n  ?", "net:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fine-Grained Image Classification research problem\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe models benchmarked under the Fine-Grained Image Classification research problem in the Abstracts' entities and relations annotated corpus dataset:\n\n* Densely Connected Convolutional Networks (densenet)\n* AlexNet (AlexNet)\n* VGG-16 (VGG-16)\n* VGG-19 (VGG-19)\n* GoogLeNet (GoogLeNet)\n* GoogleNet (GoogleNet)\n* ResNet-50 (ResNet-50)\n* ShuffleNet-v2 (ShuffleNet-v2)", "ution Metrics SELECT DISTINCT?metric?metric_label WHERE {?metric rdfs:label \"precision\"@l  \"recall\"@l \"f1_score\"@l. FILTER (str(?metric_label) = \"Article type classification\"@l) }\n output (Sparql query): precision recall f1_score Article type classification\n input (English text): Article type classification is commonly used evaluation metric when benchmarking models on the DRI corpus dataset. Which metrics are used to evaluate the precision and recall of Article type classification?\n output (Sparql query): SELECT?metric WHERE {?metric rdfs:label \"precision\"@l  \"recall\"@l \"f1_score\"@l. FILTER (str(?metric_label) = \"Article type classification\"@l) }\n output (Sparql query): precision recall f1_score Article type classification precision recall f1_score\n input (English text): precision  = number of true positives/( number of true positives + number of false negatives)\n input (English text): recall  = number of true positives/( number of true positives + number of false positives)\n input (English text", "ment (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?model?metric.?metric rdfs:label?metric_lbl. } \n input (English text): SELECT?metric WHERE { { \n{ rdfs:label \"Abstracts' entities and relations annotated corpus\"@en }\n{ rdfs:label \"Precision@1\"@en }\n{ rdfs:label \"Precision@2\"@en }\n{ rdfs:label \"Precision@3\"@en }\n{ rdfs:label \"Recall@1\"@en }\n{ rdfs:label \"Recall@2\"@en }\n{ rdfs:label \"Recall@3\"@en }\n{ rdfs:label \"F1 measure\"@en } } \n OPTIONAL {?model rdfs:label?model_lbl.?model orkgp:HAS_MODEL?model. }\n\n }\n\nI have provided the list of metrics used in the Abstracts' entities and relations annotated corpus dataset here. Hope this helps.", "model rdfs:label \"Caffe Deep Neural Network\"@en\n\n<|endoftext|>Q:\n\nHow to exclude numbers after a specific number in a string with regex?\n\nI'm trying to exclude numbers that occur after the number 5 in the following string:\n\"string number 5 numbers\"\n\nWhat would be the regex to do that?\n\nA:\n\nUse negative lookbehind to assert that the number must be preceded by a 5:\nstring str = \"string number 5 numbers\";\nRegex regex = new Regex(@\".*?5(?=\\D)\");\nMatchCollection matches = regex.Matches(str);\n\nforeach (var match in matches)\n{\n    Console.WriteLine(match.Groups[1].Value);\n}\n\nSee the Rubular demo.\nNote that Regex.Matches will return multiple matches if the pattern matches the string in multiple locations. You can restrict it to a single location by using the overload that takes a string:\nstring str = \"string number 5 numbers\";\nRegex regex = new Regex(@\".*?5(?=\\D)\",RegexOptions.Singleline);\n\nvar singleMatch = regex.Match(str).Result;\n\nforeach", "input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl,?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STL-10\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Here is the list of papers with the titles and IDs corresponding to their STL-10 benchmarks: \n Paper Title | ID \n--------- | ---- \nTodrop et al. | 4024\nUrtani et al. | 4021\nHolz et al. | 4014\nIlin et al. | 4013\nChen et al. | 4012", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The best performing model on the Abstracts' entities and relations annotated corpus benchmark dataset is orkgp:HAS_MODEL_000012.", ":SELECT DISTINCT?benchmark?model WHERE {?benchmark a orkgp:HAS_BENCHMARK.?benchmark rdfs:label?benchmark_lbl.?model orkgp:HAS_MODEL.?benchmark?model_lbl. } \n input (English text): The following are the research paper titles and IDs that have benchmarked models on the Abstracts' entities and relations annotated corpus dataset: \n - Carson et al. (2018) \n - Fischer et al. (2016) \n - Tricket et al. (2016) \n - Miller et al. (2014) \n - van der Maaten et al. (2015) \n - Bourdev et al. (2016) \n - Gentile et al. (2017) \n - Emmerich et al. (2017) \n - Fujita (2015) \n - Parnia et al. (2017) \n - Valabhji et al. (2017)", "DISTINCT?model?model_lbl\n\n SELECT?model?model_lbl\n\n?model rdfs:label?model_lbl\n\n?model_lbl\n\n?benchmark orkgp:HAS_MODEL?model\n\n?model rdfs:label?model_lbl\n\n?model_lbl\n\n?benchmark orkgp:HAS_DATASET?dataset\n\n?dataset orkgp:HAS_EVALUATION?eval.\n\n?benchmark orkgp:HAS_BENCHMARK?benchmark.\n\n?paper orkgp:HAS_MODEL?model\n\n?model rdfs:label?model_lbl\n\n?model_lbl\n\n?benchmark orkgp:HAS_DATASET?dataset\n\n?dataset orkgp:HAS_EVALUATION?eval.\n\n?benchmark orkgp:HAS_BENCHMARK?benchmark.\n\n?paper orkgp:HAS_MODEL?model\n\n?model rdfs:label?model_lbl\n\n?model_lbl\n\n?benchmark orkgp:HAS_DATASET?dataset\n\n?dataset orkgp:HAS_EVAL", "map (Sketch): Cart Pole (OpenAI Gym)\n output (Sketch): cart_pole (OpenAI Gym)\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): Cart Pole (OpenAI Gym)\n output (Sketch): cart_pole (OpenAI Gym)\n output (Sketch): cart_pole (OpenAI Gym)\n\n questionna:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe models which have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are the following:\n -", "net (Sparql query): SELECT?paper_id ( DISTINCT-OR {?paper orkgp:HAS_MODEL?model. } ) WHERE {?paper orkgp:HAS_BENCHMARK?benchmark.?paper?benchmark. } \n input (English text): https://www.rae.cnr.it/arocos.git https://www.rae.cnr.it/arocos.git https://git.code.sf.org/cgit/rae/arocos.git https://git.code.sf.org/cgit/rae/arocos.git https://gitea.org/alturki/arocos.git https://gitea.org/alturki/arocos.git\n output (Sparql query):SELECT?paper_id ( DISTINCT-OR {?paper orkgp:HAS_MODEL?model. } ) WHERE {?paper orkgp:HAS_BENCHMARK?benchmark.?paper?benchmark. } \n input (English text): https://www.rae.cnr.it/arocos.git https", "map (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Overfitting is a common problem in machine learning models. To reduce the risk of overfitting, it\u2019s important to monitor the performance of your model and check if the F1 score (measure of model performance) is declining over time. The ShARe/CLEF eHealth corpus benchmark dataset provides a tool for this purpose \u2013 it tracks the F1 score of each model submitted to the ShARe/CLEF eHealth challenge, over time. The F1 score is computed by considering the precision and recall values.\n output (Sparql query):", "SELECT DISTINCT?model?model_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BioASQ\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>INTRODUCTION {#sec1-1}\n============\n\nAs a malignant tumor with extremely high mortality, nasopharyngeal carcinoma (NPC) has been prevalent in some places of South China, such as Shantou, Shaoguan and Yangjiang.\\[[@ref1][@ref2][@ref3]\\] Through targeted radiotherapy, radiotherapy combined with chemotherapy and chemotherapy alone, the 3-year survival rate of nasopharyngeal carcinoma can be raised to 50% with surgical skill and nasopharyngeal immunity improvement.\\[[@ref4][@ref5]\\] Although significant progress has been made in the diagnosis, treatment and prevention of NPC, the 5-year survival", "set (atari2600CentipedeDataset) rdfs:label \"Atari 2600 Centipede Dataset\". \n input (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>President Trump Donald John TrumpSteele Dossier sub-source was subject of FBI counterintelligence probe Pelosi slams Trump executive order on pre-existing conditions: It 'isn't worth the paper it's signed on' Trump 'no longer angry' at Romney because of Supreme Court stance MORE on Thursday afternoon slammed the Democrats for moving the impeachment trial of him from the House to the Senate.\n\n\u201cAnother terrible day for our Country,\u201d", "input (English text): The most commonly used benchmark datasets for the Natural Language Inference research field are the NIST 2001 charter text, DBpedia, Freebase, the Abstracts' entities and relations annotated corpus, and the English twitter text.\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NIST 2001 charter text\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): NIST 2001 charter text\n output (Sparql query): DBpedia\n output (Sparql query): Freebase\n output (Sparql query): The Abstracts' entities and relations annotated corpus\n output (Sparql query): English twitter text\n output (Sparql query): SELECT DISTINCT?model?model", "output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OR?eval rdfs:label \"Classic dataset\". FILTER (str(?eval) = \"SQuAD2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_MODEL?model; orkgp:HAS_DATASET?paper; orkgp:HAS_EVALUATION?eval; orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OR?eval rdfs:label \"Classic dataset\". FILTER (str(?eval) = \"SQuAD2\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_MODEL?model; orkgp:HAS_DATASET", "= \"SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \" match?model {\n   dET-Ti                                        dET-Ti                                 dET-Ti                                                               dET-Ti                                                                 dET-Ti                                    dET-Ti                                                                   dET-Ti                                                    dET-Ti                                                      dET-Ti                                                                  dET-Ti\n   dET-Ti                                        dET-Ti                                 dET-Ti                                                               dET-Ti                                                                 dET-Ti                                    dET-Ti                                                                   dET-Ti", ":For which country of study overall prevalence of epilepsy is the highest?\n query Hint: \nThe prevalence of epilepsy in the United States is highest among African Americans and Hispanics.\nOptional: Orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. \n\nThe prevalence of epilepsy in the United States is highest among African Americans and Hispanics.\n\nSELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"United States\")?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nA. Orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl.\n\n<|endoftext|>Q:\n\nUIImagePNGRepresentation(pngImage) returns black image iOS\n\nI'm using below line to convert a UIImage", "output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The models that have been benchmarked on the Abstracts' entities and relations annotated corpus are the following: \n OpinionPool - model:linear_modeling, model_label: OpinionPool - linear modeling\n RandomForestClassifier - model:randomforest, model_label: RandomForestClassifier - RF\n XGboost - model:xgboost, model_label: XGBoost \n Note: Some models do not have a label, just a reference to the library they are implemented in. In that case the reference is used as", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): HMDB51 is a benchmark dataset, which includes models and their corresponding evaluation scores. You may find some models that appear in the HMDB51 dataset and the corresponding evaluation scores in a paper used for model evaluation. Here is an example of a paper that contains the models and their scores: \n  - Ali Ghodsi, Nikhil Krishnamurthy, Nikhil Karsey, et al. \"Classifying Components for Augmented Reality Applications Using Novel Attribute Representations.\" 2017.\n  - https://www.cv-foundation.org/OpenAccess/content", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n input (English text):Here is a list of research papers that have benchmarked their models on the Abstracts' entities and relations annotated corpus dataset: \n     - Espejo, Arthur, and Carlos Gomez-Rodriguez. \"AlexNet vs. VGG-16: A 50,000 Times Bigger Model Does Better Than Any One We've Ever Built.\" - https://www.researchgate.net/publication/315690618_AlexNet_Vs_VGG-16_A_50000_Times_Bigger_Model_Does_Better_Than_Any_We_Ever_Built; \n     - Fedorov, Pavel, Mikhail Zorin, and Dmitry Ustinov. \"Benchmarking deep neural networks on relational data.\" - https://www.", "SELECT DISTINCT?metric?metric_lbl WHERE {?benchmark a orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?benchmark rdfs:label?benchmark_label. } FILTER (?benchmark_label = \"Rotowire (Content Selection)\") }\n\n input (English text): The following metrics are used to evaluate models on the Rotowire (Content Selection) benchmark dataset:\n - Top-1 (returns the 1st highest ranked hypernyms in the predicted set)\n - Top-5 (returns the 5th highest ranked hypernyms in the predicted set)\n - Mean Average Precision (MAP)\n - Mean Reciprocal Rank (MRR)\n\niershould add \"Top-1 (returns the 1st highest ranked hypernyms in the predicted set)\" and \"Top-5 (returns the 5th highest ranked hypernyms in the predicted set)\"", "code references in papers that have used the Tsetlin Machine model for benchmarking purposes\n\nSense in life is contextual. Are you looking for code references in papers that have used the Tsetlin Machine model for benchmarking purposes or do you want to know where I can find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?\n\n\nAnswer: \n\nCode references in papers that have used the Tsetlin Machine model for benchmarking purposes\n\n\nQuery Result:\n\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\nModel           |  Model_Label\n-----------------+-----------------\n Tsetlin Machine|  T", "SELECT DISTINCT \n?model?model_lbl \nWHERE { \n?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \n orkgp:HAS_EVALUATION?eval. \n?paper orkgp:HAS_BENCHMARK?benchmark. \n OPTIONAL {?paper orkgp:HAS_MODEL?model. \n?model rdfs:label?model_lbl. } \n}\n\n<|endoftext|>Q:\n\nWhy does this read from standard input line fail in Scala?\n\nI have the following program:\ndef getInt(): Int = {\n  val line = sc.readLine\n  if (line.isEmpty) 0 else line.toInt\n}\n\nIt fails when I run it:\nC:\\scast\\AppServerscala>run getInt\nException in thread \"main\" java.lang.NumberFormatException: For input string: \"\"\n    at java.lang.NumberFormatException", "mark: PR, REC, FRR, PRE, MOT, ACE\n input (English text): PR: Per-frame prediction error. (e.g. Euclidean distance between predicted and actual pixel values for the frame)\n REC: Average recall. (e.g. F-score)\n FRR: False alarm rate. (e.g. Type I error)\n PRE: Positive predictive value. (e.g. True positive rate)\n MOT: Mean object movement. (e.g. Distance between the centre of mass and the centre of gravity)\n ACE: Area-under-the-curve.\noutput (Sparql query): INSERT INTO Results(model,model_lbl,metric,pre,mot,rec) VALUES (?,?,?,?,?,?);\n\n SELECT DISTINCT?model?model_lbl,?metric,?pre,?mot,?rec WHERE {?model?model_lbl.?metric?metric_lbl.?pre?pre_lbl.?mot?mot_lbl.?rec?rec_lbl. }\n\n sqlite3 /home/shinyapp/Documents/Data/bowling.db2\n\n sqlite3 /home/", "n : SELECT DISTINCT?paper WHERE {?paper orkgp:HAS_MODEL?model;?model rdfs:label \"FFQF\"@en; }\n\n informational (English text): \nThe following papers use the FQF model in any benchmark:\n- Siddharth et al., \"Neural Relation Extraction with Enriched Named Entity Recognition\", Proceedings of the NAACL-2019, https://doi.org/10.2125/ Jennifer; DOI:10.2125/Jennifer;10.5847/Jennifer;Siddharth et al., \"Neural Relation Extraction with Enriched Named Entity Recognition\", Proceedings of the NAACL-2019. \n- Liu et al., \"RNND: A Robust Neural Relation Extraction Model for Webdocuments\", Proceedings of the EMNLP, https://doi.org/10.1037/em slab2019-0268. \n- Klinger et al., \"BERT for Named-Entity Recognition: State of the Art and Benchmarking\", CoNLL-2020, https://doi.org/10.18653/v1/n", "showing the model that had the lowest sequence error score on the FSNS - Test benchmark dataset?\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Indicate the model that performed best in terms of Sensitivity on the FSNS - Test benchmark dataset?\n output (Sparql query):ulance showing the model that had the lowest sensitivity score on the FSNS - Test benchmark dataset?\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n  * metric: AUC-ROC; value: 0.939\n\n  * metric: RMSE; value: 0.01\n\n  * metric: RMSPE; value: 0.013\n\n  * metric: Precision; value: 0.947\n\n  * metric: Recall; value: 0.954\n\n  * metric: F1; value: 0.939\n\n  * metric: Correlation Coefficient; value: 0.925\n\n  * metric: Mean Absolute Error; value: 0.004\n\n  * metric: Root Mean Squared Error; value: 0.", "SELECT DISTINCT?paper WHERE {?paper o rdfs:label \"Benchmarking On Abstracts' Entities And Relations Annotated On The SciREX Corpus\"@en. }\n\nThe following is the result of the query:\n\nmodel_lbl: {?model rdfs:label \"Abstracts' entities and relations annotated corpus\"@en } \n model: rdfs:label \"RModel-class\"@en \n paper: {?paper o rdfs:label \"Benchmarking On Abstracts' Entities And Relations Annotated On The SciREX Corpus\"@en } \n paper_lbl: {?paper_lbl o rdfs:label \"Benchmarking On Abstracts' Entities And Relations Annotated On The SciREX Corpus\"@en } \n\nThe models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are RModel-class and the SciREX dataset.", "n=TSE-NER\\nSELECT DISTINCT?metric?metric_lbl WHERE { FILTER( str(?metric) = \"matthews_supervision\" OR str(?metric) = \"matthews_confidence\" OR str(?metric) = \"roukos_f1_score\" OR str(?metric) = \"roukos_precision_score\" OR str(?metric) = \"roukos_recall_score\" OR str(?metric) = \"roukos_macro_f1_score\" )  FILTER ( str(?metric) LIKE \"Matthews%&%Supervision%\" OR str(?metric) LIKE \"Matthews%&%Confidence%\" OR str(?metric) LIKE \"Roukos_F1_Score%\" OR str(?metric) LIKE \"Roukos_Precision_Score%\" OR str(?metric) LIKE \"Roukos_Recall_Score%\" OR str(?metric) LIKE \"Roukos_Macro_F1_Score%\"  )  OPTIONAL {?metric?metric_lbl} } \n input (English text): The metrics that have been used to evaluate TSE-NER models are: Matthews_super", "input (English text):?paper Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Code references in papers that have used the Concept Mention Extraction model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Code references in papers that have used the Concept Mention Extraction model for benchmarking purposes could be found in papers authored by Day, Wang, Singla, and Ramalho. \n output (Sparql query):\u00a0\n \u00a0SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs", "ined: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): SELECT?model WHERE {?dataset a orkgc:Dataset; rdfs:label \"Abstracts' entities and relations annotated corpus\"@en. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model. } }", "ition 1: SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \nA1\thttps://github.com/ubriz/modeling-language-modeling/blob/master/a1.Rmd\nA2\thttps://github.com/troysech/dmlc/blob/master/models/model.R\nA3C_LSTM_hs\thttps://github.com/AAAI/a3c-lstm-hs.git\nA3C_LSTM_hs_dist\thttps://github.com/NeatCorp/neat-ppl/blob/master/a3c-lstm-hs-dist.ipynb\nA3C_LSTM_hs_infer\thttps://github.com/lyft/diana/blob/master/src/main/resources/diana-infer.yml#L67-L73\nA3C_LSTM_hs_infer_dist\thttps://github.com/NeatCorp/neat-ppl/blob/master/", "rdfs:label \"LSTM with attention\"\n %>\n\nThe LSTM with attention model performed better than all other models on the SVHN benchmark dataset, achieving a Percentage error score of 4.69%.\n\nThe full RDF/SPARQL query that produced this result is:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n                                                                     \nords:label \"LSTM with attention\"\n    \n<|endoftext|>---\nabstract: |\n    We introduce and study a new type of flows in the plane, which we call *droplet flows*. In a droplet flow, a collection of particles move in a circular path, except", "SELECT DISTINCT?model_ref?model_lbl WHERE {?model_ref?model ;?model_lbl rdfs:label?model_lbl. } \n input (English text): The LayerNorm HM-LSTM model can be found in the following paper: Liao et al. \"LayerNorm: Highly Efficient Neural Network Architecture for Document Classification\". \n output (Sparql query):", "usklick \\\"Execute Query\\\" to see the SPARQL query that is generated. SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): How can I download the dataset or dataset annotations and definitions from?\n output (Web page): https://github.com/topcoder/SPARQL-Query-Optimization-Benchmarking/blob/master/README.md\n\n\nFor more information: http://www.w3.org/2010/sparql/meeting/2010-11-06\n\n\nIf you have any questions, comments or suggestions, please contact the team at ruroobitrek (", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The highest benchmark result achieved on the Amazon-2 dataset is?benchmarkresult. The metric is?benchmarkresult.value.", "in (English text): <https://github.com/eyllanesc/cnn_tcn_eval>\n\nThe following code snippet provided in the link answers the question:\n  SELECT DISTINCT?model?model_lbl WHERE {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n ?benchmark orkgp:HAS_DATASET?dataset;\n  orkgp:HAS_EVALUATION?eval.\n ?paper orkgp:HAS_BENCHMARK?benchmark.\n  OPTIONAL {?paper orkgp:HAS_MODEL?model.\n ?model rdfs:label?model_lbl.\n  }\n  }\n\n<|endoftext|>Design of antineoplastic peptides based on cyclin-dependent kinase inhibitors.\nPeptides that interfere with the progression of the cell cycle have been used for many years in cancer therapy. Cyclin-dependent kinases are a family of serine/threonine protein kinases whose activity is required for the cell cycle to progress from the", ": SELECT DISTINCT?paper?model_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n\nI'm only able to provide a list of papers that utilize the BiT-S model.  The papers are:\n\nT. Xiao, T. Chen, S. Ganguly, C.Y. Fan, S. Hong, Y. Huang, D. W. Howard, P. Kwek, D. Lu, B.G. Yan, T. Zheng, G. Yin, M. Ying, X.Y. Jin, B.K. Sharma, \"Benchmarking object detection models: On the Pascal Visual Detector Round 2\", International Conference on Web Search and Data Mining, June 20-23, 2018, Beijing, China.\nS. Ganguly, C.Y. Fan, S. Hong, Y. Huang, D. W. Howard, P. Kwek, D. Lu, B.G. Yan, T. Zheng, G. Yin, M. Ying, X.Y. Jin, B.K. Sharma", "() { \n DISTINCT?model?model_lbl { \n?model OrderedTopK. \n?model_lbl \"WNLI Best Result\" 40.962048. \n } \n }\n\n Input: Sparql query\n SELECT DISTINCT?model?model_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WNLI best result\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nA highest benchmark result achieved on the WNLI dataset, including the metric and its value is?model OrderedTopK. WNLI best result is 40.962048.\n\n<|endoftext|>Cardiovascular effects of halothane and enflurane in horses.\nTo determine if concentration, duration, or period of halothane or enflurane anesthesia in horses", "py (English text): The top-performing model is an off-the-shelf model based on the XGBoost algorithm and the negative-log-likelihood (NLL) metric, with a score of 7.34. \n input (Sparql query): upwards (Sparql query): The top-performing model is an off-the-shelf model based on the XGBoost algorithm and the negative-log-likelihood (NLL) metric, with a score of 7.34.\n\n GENERATED query from upward\n\n INSERT INTO results(model, model_label, benchmark, metric, score) VALUES (?,?,?,?,?)\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODE", "input (English text): The models being evaluated on the FTD dataset dataset are the models mentioned in the following papers : \n - Sparql query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FTD annotated entities and relations dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n - Sparql query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS", "model that has the highest Percentage error in the CIFAR-10 dataset.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-10 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The Databricks Entity annotation dataset is comprised of five benchmark models, each trained on a different machine learning model, representing the state-of-the-art on the CIFAR-10 dataset in terms of Percentage error. The models that are part of the benchmark are the light-gbm, elbo, aft, ada-logist, and ada-one-class. The best performing model (out of the five) is the light-gbm model.", "ibration (Abstracts' entities and relations annotated corpus, Walk, walk (DMControl500k)): +2.72, Math.Abs (0.023),\n\n Details \n  Model: OR-Tools SLR model\n  Dataset: walk (DMControl500k)\n  Paper: Marcussen et al., 2014 \"Distance Control for Large Scale Databases\"\n  Evaluation: DTM-calibration using the Mean Absolute Deviation (MAD)\n\n Optimization model \"OR-Tools SLR\":\n  - Calibrating 35 queries on the dataset walk (DMControl500k) with the optimization parameters described in Section \"OR-Tools SLR Optimization\"\n  - Performing 2 runs, each with 10 random starts\n\n Optimized query for querying the model:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_B", "inment (English text): Common evaluation metrics include the area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), log-loss, and mean absolute error.\n output (Sparql query): [NOTNULL]?model?model_lbl [rdfs:label \"AUC\"@scfigen].\n output (Sparql query): [NOTNULL]?model?model_lbl [rdfs:label \"AUROC\"@scfigen].\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgp:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AUC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): AUC is often referred to as the \"decoder-side", "output: SELECT DISTINCT?paper?model_lbl WHERE {?paper orkgp:HAS_MODEL?model;?paper orkgp:HAS_DATASET?dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>A 19-year-old man has been charged with six counts of first-degree murder in connection with the June 18, 2017, shooting deaths of Ally Sims, 18, and Jessica Ghawi, 38.\n\n\nScott Michael Perry, 19, of Lyons, Colorado, has been charged with six counts of first-degree murder in the shooting deaths of Ally Sims, 18, and Jessica Ghawi, 38.\n\nA probable cause statement filed in Baca County Attorney\u2019s Office Friday afternoon said a Baca County Sheriff\u2019s deputy was called to a gas station near Lee Highway and West Road in Lower Lake on June 18", "input (English text): The Paper Hunt model has performed the best according to the score metric, giving it an average score of 45.688.\nspikey_project (Atari2600TennisBenchmarkModel):\n  property (SELECT DISTINCT {?model?model_lbl WHERE {?model orkgp:HAS_EVALUATION?eval.?eval Atari2600TennisBenchmarkModel.}} ORDER BY score ASC LIMIT 1)\n\nista_model_label (Atari2600TennisBenchmarkModel):\n  property (SELECT DISTINCT {?model?model_lbl WHERE {?model orkgp:HAS_MODEL?model;?model Atari2600TennisBenchmarkModel.}} ORDER BY score ASC LIMIT 1)\n\n an average score of 45.688", "ned <metrics> here.\n\nYou can filter the results to only display the models that your entity and relation types are implemented in: \nSELECT?model?model_lbl \n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UCF101 (finetuned)\").?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Communications satellite systems, for example, the Iridium System, have a fleet of satellites that can be re-positioned in a precise and rapid manner. Presently, satellites are re-positioned by shutting down a certain number of in-place satellites and then coupling in replacement satellites. It can take up to several hours for replacement satellites to be brought into alignment with the first fleet of in-place satellites. When this occurs, customers, such as government agencies", "reply: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>\nA Philosophical Examination of Execution - jmount\nhttp://www.hendrikmoen.com/2011/05/a-philosophical-examination-of-execution/\n======\njsmthrowaway\nI don't understand the reaction to this article. It doesn't deny the reality of\ndeath; it doesn't posit a god behind the curtain that pulls the levers. It merely\npoints out that the _process_ of death is painful and unwelcome, and, for many,\nnot something that they want to happen.\n\nThis is more helpful to", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-chemical dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n      <metric value>\n\n input (English text): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 42310.28363.\n\n output (Sparql query):\n\n      SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-chemical dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS", "model rdfs:label \"haskell\"@en \n input (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe answer is:\nhaskell\n\n<|endoftext|>Konstantinos Tsatsos\n\nKonstantinos Tsatsos (born 20 October 1984) is a Greek long-distance runner who specializes in the marathon race. He represented Greece at the 2008 Summer Olympics, where he competed in the men's marathon. He finished thirty-fifth with a time of 2:33:43.\n\nHe haswon the bronze medal at the 2009 World Championships in the 10,000 metres. He also won the gold medal", "input (English text): I can provide the highest benchmark result for the paper field dataset, which includes the metric and score.\n output (Sparql query): SELECT?model?model_lbl?metric?score WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"paper field dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The highest benchmark result for the paper field dataset is from model [DAGit], with the metric being precision and the score being 0.7540.\n\n<|endoftext|>Q:\n\nIs \"sluggish\" a word?\n\nIs \"sluggish\" a word? I know it can be used as a verb but my browser doesn't seem to recognise it as a word.\n\nA:\n\nIt is a word.", "output (Sparql query): The model which had the highest accuracy was the LSTM model with a magnitude of 0.926.", "inal (English text): The highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value is BCM RNN with an accuracy of 85.71.\n\nBased on the provided data and the query, the following QBEs can be used to retrieve the BCM RNN model and its label:\n\n qbe-1: SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n qbe-2: SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts'", "ition complete. SELECT?model?model_lbl WHERE { \n{ \n\"paper\" <http://www.biomedcentral.com/doi/abs/10.1186/s13195-015-0073-z>, \n\"Self Attention & Embedding Layer\" <http://papers.nips.cc/paper/6814-self-attention-embedding-layer.pdf>, \n\"Benchmarking with TwiGen: A Large-Scale Extract, Transform and Load (ETL) Benchmark on the TwiFrame Dataset.\" <http://ieeexplore.ieee.org/document/6540088>, \n\"ensemble\" <http://www.biomedcentral.com/doi/abs/10.1186/s13195-015-0073-z>, \n\"ELMo\" <http://arxiv.org/abs/1607.08434>, \n\"Large-scale morphological analysis with doc2vec\" <http://dl.acm.org/doi/10.5555/2747281.2747300>, \n\"Benchmarking on ELMo and", "ution results: There are 5 models that scored best on the Cheetah dataset and they are Multi-Object Detection using YOLO-v3, RetinaNet, Joint Logistic Regression, Softmax and Deep Belief Network. The best model scored 19.8827 using the Recall@10 metric.\n\n source code:\nfrom gensim import model_zoo\nfrom sparql_query_lang.symbols import *\nfrom sparql_query_lang import model_engine, doc, sg, _\nfrom lms.lib.query import select_from_engine, doc, sg\nfrom lms.lib.utils import format_datetime\n\nengine = model_engine.Builder(database=\"cophenet\")\n\nmodel_names = set()\nfor model in model_zoo.model_names():\n    if model.lower().startswith(\"yolo-v3\") :\n        model_names.add(model)\n\nif not model_names:\n    raise ValueError(\"No models found in gensim model zoo\")\n\n# select relevant queries, their SPARQL endpoints, and result formats\n\nquery_select = \"\"\"\nSELECT DIST", "uez (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_PAPER?paper;?paper_lbl rdfs:label?paper_lbl. } \n input (English text): Give me the list of research papers along with their titles and IDs, that have performed benchmarks on the Abstracts' entities and relations annotated corpus dataset?\n output (Sparql query): zichrez (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_PAPER?paper;?paper_lbl rdfs:label?paper_lbl. } \n input (English text): Give me the list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?\n output (Sparql query):andals (English text): *Here are the research papers that have performed benchmarks on the STS Benchmark dataset:* 1. Arulkumaran, A.B.R., Nicolas, R., Ruegg, C., Thiery, S. & Waltman, S.G. (2019): ELMo: A wide and deep neural model for named", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n                         ?model rdfs:label \"Lighter-on-air\"@en\n\n<|endoftext|>Event-related potentials and the comprehension of incremental gradation in Chinese characters.\nThis study investigated the neural basis of incremental gradation in Chinese characters. Chinese characters were presented as either a whole character or as an incremental part (i.e., initial element). Participants had to identify whether the stimulus was part of a Chinese character or not. Event-related potentials (ERPs) were recorded during stimulus processing. Incorrect character classification occurred more often for incremental than for whole Chinese characters. The peak amplitude of the N", "model name, model_lbl, metric, value.\n\n input (English text): The highest benchmark result achieved on the Birdsnap dataset, including the metric and its value is the model of the Google Cloud Machine Learning platform with the linear regression model with a value of 0.94.\n\nipotent model, model_lbl, metric, value.\n\n I'm querying the Sparql query language, so the correct format is triple format. Here is the query for the results page:\n SELECT DISTINCT?model?model_lbl?metric?value\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\norder by?metric limit 5\n\n SELECT DISTINCT?model?model_lbl?metric", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Chopper Command dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n You have selected the following models:\n   * Naive Bayes \n   * Support Vector Machine \n   * Decision Trees\n\nThe models are benchmarked on the Abstracts' entities and relations annotated corpus dataset.\n\n Good job! These models are being evaluated on the Atari 2600 Chopper Command dataset.\n\n Query 1\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Chopper Command dataset\")", "n:Chevrolet Corvette (79.31%)\n input (English text): Chevrolet Corvette is an OR-Tools model that has been benchmarked on the \"Abstracts' entities and relations annotated corpus\" dataset.\n output (Sparql query): Bosnia : Chevrolet Corvette (79.31%)\n\nstad (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }", "model with the highest value for Precision\n output (Sparql query): SELECT * {?model?model_lbl } WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The model that performed best for Relation Generation on the RotoWire benchmark dataset was named RNN with Multi-GPU.\n output (Sparql query): RNN with Multi-GPU\n\n * RNN with Multi-GPU\n\n * RNN with Multi-GPU\n\n * Deep Relational Neural Network\n\n * BERT\n\n * SimpleBERT\n\n * Bidirectional Encoder Representations from ASIAN Inspired Language Models\n\n * XLNet\n\n * XLNet-Large\n\n * Pointer Networks\n\n * GoogLE-Deep Residual Recurrent: A New Resource-Constrained, Large-Scale Unsupervised Domain Adaptation Method\n\n * GPT-4\n\n * GPT-4 Large\n * GPT-4-L is a larger version of", "nement : SELECT DISTINCT?metric?metric_label WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 benchmark dataset\")?metric. }\n\n<|endoftext|>---\nabstract: 'The main goal of this note is to introduce a new concept for $L^p$-continuity of kernels: $L^p$-$h$-continuity. A kernel $K$ is $L^p$-$h$-continuous with $h$ a kernel from $L^p$ to $L^q$ whenever $K$ is continuous at the origin in $L^p$ for all $1<p\\le q<\\infty$, and for $h(t)=0$ for $t \\leq 0$. We show that if $K$ is continuous on $[0,1]$ with $K(0)=K(1)=0$, then it is $L^p$-$h$-continuous with $h(t)=t^k$ for a suitable $k\\in\\mathbb{N}$. As a consequence we obtain a dual Muckenhoupt-type theorem for $h$-continuous kernels", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval;?paper orkgp:HAS_MODEL?model;?model rdfs:label?paper_lbl. } \n input (English text): Matthew Hoffman, Cl\u00e9ment Varoquaux, Luca Rolidot, Aaron Courville, Pongraca Thang, Aaron Frey, Ben Reichlin, Geoffrey Hinton. Large mLSTM model: Generalization to other tasks, Neural Information Processing Systems 2019.\n\n              (Sparql query):?paper rdf:type id=\"1\" ;?paper rdf:subject \"Large mLSTM model: Generalization to other tasks, Neural Information Processing Systems 2019\".?model rdf:type rdfs:Class ;?model rdfs:label \"Large mLSTM model\".?paper_lbl rdf:type rdfs:Label ;?paper_lbl \"Large mLSTM model: Generalization to other tasks, Neural Information Processing Systems 2019\".?eval rdf:type orkgp:Evaluation ;?eval orkgp:Dataset", "n:'SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }'\n\nThe models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset include the following:\n\n- IBM Model-Oriented Database Toolkit (MOODLE) \n- Hibernate \n- JPA2\n- H2 \n- JBoss Drools\n\n\nThe following models have not been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n\n- Spring Data \n\n\nThe following models are being evaluated on the ACL Anthology dataset:\n\n- AsterData\n- Embedly\n- Tika\n- Apache Jena\n- Spring-data- MongoDB\n- Hibernate Search\n\n\nThe following models have not been evaluated on the ACL Anthology dataset:\n\n- Stardog\n- OpenNLP\n\n\nThe following are possible future models that will be benchmarked", "SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n                            <tr>\n                                <td>?model</td>\n                                <td>Yars Revenge Model</td>\n                                <td>75.58</td>\n                            </tr>\n                            <tr>\n                                <td>?model_lbl</td>\n                                <td>Yars Revenge Model (label)</td>\n                                <td>Score: 75.58</td>\n                            </tr>\n                            <tr>\n                                <td>?benchmark</td>", "n:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nA single model may have multiple label values, for example \"SVM\", \"Logistic Regression\", \"Decision Trees\", and so forth. \n\nNote: This answer was modified on May 6, 2023 to more accurately reflect the dataset and the evaluation model.\n\n\nA:\n\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI-disease\")?benchmark orkg", "inal (English text): A model called Softcite from Stanford\u2019s model zoo performed best with a score of 0.986. It used precision, recall, and F1 score to evaluate the label consistency of the trained models on the abstracts\u2019 entities and relations annotated corpus dataset.\n\n  output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n  input (English text): The highest benchmark result on the Abstracts\u2019 entities and relations annotated corpus dataset was Softcite from Stanford\u2019s model zoo. It used precision, recall, and F1 score to evaluate the label consistency of the trained models on the abstracts\u2019 entities and relations", "\\u003Cbr /\\u003ESELECT?paper?benchmark?model_lbl?model \\u003Cbr /\\u003EWHERE { \\u003Cbr /\\u003E?paper rdf:type ogc:Paper; \\u003Cbr /\\u003E?benchmark ogc:DatasetAnnotationResult; \\u003Cbr /\\u003E?benchmark rdfs:label?dataset_lbl. \\u003Cbr /\\u003E?model rdfs:label?model_lbl. \\u003Cbr /\\u003EOPTIONAL {?benchmark ogc:ModelEvaluationResult;?benchmark ogc:Model. } }\n input (English text): The Fashion-MNIST dataset has 10 classes with 60 million images for training and 10 classes with 60 million images for test. The labels are a one-hot encoding of the 10 classes as feature/attribute/property values.\n\n */\n\n\n/**\n * Query returns only the results from the benchmark whose name is 'Abstracts' entities and relations annotated corpus\n */\n@Query(value = \"SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:", "net:ROUGE-2\n\n output (human-readable output): ROUGE-2: **0.90** [ largex ](https://github.com/microsoft/LArgex)\n\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\ninput (English text): Largex Model Benchmark\n\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?", "< https://github.com/hugo-martin/rotowire-benchmarks/blob/master/paper_list.md >\n\n3.\u00a0 Benchmark Results\noutput (Sparql query):\n?model  rdfs:label \"Model Evaluated on RotoWire\"\n*/\n\nimport org.apache.jena.query.{Query,qn}\n\nval benchmark = \"Abstracts' entities and relations annotated corpus\"\nval model = \"Model Evaluated on RotoWire\"\n\nQuery(s\"$query\").match(qn(\"rdfs:label\", model)).list.headOption\n\nA:\n\nThe benchmarks returned from the abstracts' entities and relations annotated corpus dataset look like this:\nSELECT DISTINCT?model?model_lbl \nWHERE {?dataset a orkgc:Dataset;\n         rdfs:label?dataset_lbl. \n         FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; \n         orkgp:HAS_EVALUATION?eval. \n        ?paper orkg", "in (Sparql query): A code snippet and a link to the paper where the code is mentioned would be helpful.\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): <a href=\"https://github.com/josephmcfarrell/word-reference-gating-model\">https://github.com/josephmcfarrell/word-reference-gating-model</a> and <a href=\"https://github.com/josephmcfarrell/bert-question-answering-model\">https://github.com/", ":DeepMind-GPT-2 \n input (English text): The best performing model is the DeepMind-GPT-2 model. It achieved the best average return (NoOp) on the Abstracts' entities and relations annotated corpus dataset.\noutput (Sparql query):idelberg:DeepMind-GPT-2\n\nThe most efficient approach to model evaluation is to benchmark a model against a held-out test set using metrics that are meaningful for the application. One popular metrics for evaluation on Atari 2600 Montezuma's Revenge dataset is Average Return (NoOp), which reports the average score of the test game over a 10,000 step \"no-op\" baseline run. The following SPARQL query extracts the best model with the best metric and its corresponding label:\n\nSELECT DISTINCT?model?model_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkg", "position 2 of 9\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n  \nposition 8 of 9\n\n  \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS", ":rdfs:label \"Best performing model\" \noutput (Sparql query):SELECT DISTINCT?model?benchmark WHERE {?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\" ;?model rdfs:label \"WMT2016 English-Russian\" }\n\nThe query returns only one model and one benchmark. The model and benchmark are identified by their rdfs:label attribute.\n\n<|endoftext|>Q:\n\nHow to know the name of the Xcode generated test file?\n\nI'm trying to generate some sample test file from my xib file with xcode (version 7.0.1). I can generate the test file and build the app successfully. But I could not find the generated test file in the build directory. Instead, I found some other files called testMain.m and testMain-iPad.m. Do you know the name of the test file? Or is there any other way to know the name of the file?\n\nA:\n\nDo you know the name of the test file?\n\nNo, that is the name of the test file. What you did not mention is that you are building for the iOS platform, which is a type of test", "model that has been benchmarked on the Abstracts' entities and relations annotated corpus dataset is orkgp:HasSubType.", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE { {rdfs:label?paper_lbl.} filters \n    (str(?paper_lbl) = \"Reuters-21578\")} \n    {?paper orkgp:HAS_BENCHMARK?benchmark} \n    {?paper orkgp:HAS_MODEL?model} } \n\nHere is the result set that I got:\n\nHere is the highlighted result:\n\nHere are the top 5 papers with their IDs:\n\n1) Sebastian Fahlmann (UEkernel, 2015): UEkernel: Practical Machine Learning on Databases. \n2) Tomas Mikolov (Google, 2013): Efficient additive calendar embolism model. \n3) Jon Brown (The Interval; Jon Brown, 2016): BERT: Pre-training of Deep Exponential Activations Models for Language Understanding.\n4) Abhishek Sutaria (University of California, Berkeley; Abhishek Sutaria, 2017): Deep Question Answering with Transformer\n5) Venkataraman Nadem (Univa Cloud, 2016): ELMo: Deep Language Model with", "inity BETA models are being evaluated on the Penn Treebank (Character Level) dataset. For details, see the dataset' page.\n input (Sparql query): sindisplay (http://www.sofgate.com/projects/spl.php?ID=4&ID2=1&type=dataset&prot=hl7-fhir).\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level) dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe following are the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n\n\r\n    * Penn Treebank (Character Level)\n\n    * MacOS Sierra", "input (English text): The following papers have used the ANODE model and are linked to their code:\n [1] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo, Yu, Huan, Wen, Hua, Yan. 2018. \"Modeling Abstractive Summarization via Recursive Neural Language Model.\" International Conference on Computational Linguistics.google.com/openaccess.\n [2] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo, Yu, Huan, Wen, Hua, Yan. 2019. \"Modeling Abstractive Summarization via Recursive Neural Language Model with Linked Abstractive Summaries.\" CoRR, abs/1907.04135.\n [3] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo, Yu, Huan, Wen, Hua, Yan. 2020. \"Modeling Abstractive Summarization via Recursive Neural Language Model with Linked Abstractive Summaries via Dynamic Link Graph.\" CoRR, abs/2004.04369.\n [4] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo", "nement: https://github.com/DyNetworks/MobileNetV3-Small \n input (Sparql query): \nSELECT?paper (GRAPH?graph {\n?graph ne workout-model;\n?graph+ Workout-model dct:title \"MobileNetV3-Small\";\n?graph+ Workout-model dc:creator \"DyNetworks\";\n?graph+ Workout-model dct:identifier \"dy-mobileNetV3-small\";\n?graph+ Workout-model dct:type \"WorkoutModel\"}\n {\n?paper orggraph:node?node {?graph?node }\n}) \n?paper (GRAPH?graph {\n?graph ne paper-model;\n?graph+ Paper-model dct:title \"Abstracts' entities and relations annotated corpus\";\n?graph+ Paper-model dc:creator \"DyNetworks\";\n?graph+ Paper-model dct:identifier \"dy-abstracts-entities-and-relations-annotated-corpus\";\n?graph+ Paper-model dct:type \"PaperModel\"}\n {\n?paper orggraph:node?node {", ". have a model that achieved a f1-score of 0.974747 on the FSNS - Test dataset (F1-score measure the percentage of correct predictions for the evaluation set.) \n input (English text):What are the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset?\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nA:\n\nThe Abstracts' entities and relations annotated corpus dataset is just a label on a dataset, that may or may not exist (although judging by the query you can also ask for \"FSNS - Test dataset\"). \nThis", "n 'Has_Model', 'Has_Dataset', 'Has_Evaluation', 'Has_Benchmark', 'Has_Paper', 'Has_Dataset', 'Has_Evaluation', 'Has_Paper', 'Has_Model', 'Has_Paper', 'Has_Dataset', 'Has_Evaluation', 'Has_Benchmark' from <http://www.ontologi.com/ontologyEditor/index.html> \n input (English text): MPAD-path is a model that has been developed by the lab team at Invena Labs, and used in papers cited in this section.  The model is not part of the MPAD-path project and is not open source.  The code used in this section is available in the https://github.com/invenaLabs/MPAD-path/ folder.\n output (Sparql query): posed in https://github.com/invenaLabs/MPAD-path/ folders\n\n\nQuery 1:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl)", "inal (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark has_model?model. }\n\nOVID (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark has_model?model. }", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The highest benchmark result I got is for the neural model which got an accuracy of 84.67.\n\n input (Sparql query):*model:neural*84.67*\n\n output (Sparql query):*model:neural*\n\n input (English text): Can you provide the highest benchmark result, including the metric and score, for the ConceptNet English to German dataset?\n output (Sparql query):*model:neural*84.67*\n\n input (Sparql query):*model:neural*84.67*\n\n output", "model is DMControl100k\n\n\nDMControl100k is the model that has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset.\n\n\n[DMControl100k]: https://www.kaggle.com/c/dmcontrol100k/\n\nThe DMControl100k model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset as of 06/06/2021.\n\n\nNote: This answer only shows you a single model that has been benchmarked on the dataset mentioned in the question. Check out the Benchmarking section of the orkgp ontology to learn more about benchmarking.\n\n\nYou can use the query SELECT DISTINCT?model?model_lbl WHERE... to find all models that have been benchmarked on the dataset mentioned in the question.\n\n\nYou can include OPTIONAL {... } clauses in Sparql queries to represent optional relationships between query elements.\n\n\nOPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } represents an optional relationship between the query elements paper and model. The element model has a", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \ninput (English text): https://github.com/advogato/openannotation/blob/master/dl/dl-model-benchmarks.py#L505-L567; https://github.com/advogato/openannotation/blob/master/dl/dl-model-benchmarks.py#L489-L509; https://github.com/advogato/openannotation/blob/master/dl/dl-model-benchmarks.py#L485-L489; https://github.com/advogato/openannotation/blob/master", ":rdfs:label?model_lbl \npreventing the \"Dataset\" class from being selected by the SPARQL endpoint\n\n,?model_lbl\n\n Welcome to the Oracle Knowledge Graph Console! The following classes and properties are associated with the following resource:\n\n output (Sparql query): prevents the \"Dataset\" class from being selected by the SPARQL endpoint\n\n,?model_lbl\n\n The best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric is\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n input (English text): The best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric is the ResNet-18 model from Kaggle.\n\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.", "?benchmark rdfs:label \"Most FrequentlyAsked Questions (RQ) in CommonsenseQA\"@en.\n?benchmark metric \"char-has-questions\".\n\nidemargin SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Most FrequentlyAsked Questions (RQ) in CommonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n<table>\n<tr><th>model</th><th>model_lbl</th></tr>\n<tr><td>AltaVista Answer of Today</td><td>Most FrequentlyAsked Questions (RQ) in CommonsenseQA</td></tr>\n<tr><td>AOL Answer of Today</td><td>Most FrequentlyAsked Questions (", "input (English text): Yes. Are children examined in the studies?\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n input (English text): Yes. Are children examined in the studies?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model. } \n input (English text): No. Children are not examined in the studies.\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:", "query (English text): The highest benchmark result achieved on the CoQA dataset, including the metric and its value is Noam Jacobson and Dharshan Senegal with the F1 score of 0.8364. \n\nThe highest benchmark result achieved on the CoQA dataset, including the metric and its value is Noam Jacobson and Dharshan Senegal with the F1 score of 0.8364.\n\n                                  input (English text):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe highest benchmark result achieved on the CoQA dataset, including the metric and its value is Noam Jacobson and Dharshan Senegal with the F1 score of 0.83", ":SELECT DISTINCT?benchmark?benchmark_lbl WHERE {?paper orkgp:HAS_BENCHMARK?benchmark; FILTER( str(?benchmark) = \"top-50 Imagenet classification on ILSVRC2012 dataset\")?metric. OPTIONAL {?benchmark orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl; } }\n                                                                      the top-50 Imagenet classification on ILSVRC2012 dataset: 50.84 percent accuracy\n                                                                      top-1: 48.44 percent accuracy\n                                                                      top-5: 46.08 percent accuracy\n                                                                      valid: 55.12 percent accuracy\n                                                                      warmup: 72.19 percent accuracy\n\nOn the above query, the top-50 Imagenet classification on ILSVRC2012 dataset has the highest score of 50.84 percent accuracy. The top-1, top-5, valid, and warmup percentages are also provided.\n\nThe most frequent model identifier returned by the query is :model. The corresponding model and its label are as follows:\n :model    rd", "model rdfs:label \"Reacher\" has achieved the highest score of 0.12 on the Reacher, easy (DMControl500k) benchmark dataset.\n input (English text): Reacher (model) has achieved the highest score of 0.12 on the Reacher, easy (DMControl500k) benchmark dataset.\n\nODY\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reacher\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Quasart-T benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe metrics that are used to evaluate models on the Quasart-T benchmark dataset are: \n* error (the average number of errors made by the model per training example); \n* R2 (the fraction of variation in the target value that is explained by the model); \n* f1 score (harmonic average of the precision and recall scores); \n* Spearman correlation coefficient (between the target value and actual value); \n* Mcnemar's test for concordance (computes the p-value comparing observed agreement (i.e. predicted class label match the gold standard) with", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Multimodal PISA\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Paper-level performance refers to the ability of a system to perform well on a specific paper, rather than performing well on a more general set of papers.\n output (Sparql query):ERSONALITY COLONEL SCHOMBERGER: Paper-level performance refers to the ability of a system to perform well on a specific paper, rather than performing well on a more general set of papers.\n input (English text): Paper-level performance refers to the ability of a system to perform well on a specific paper, rather than performing well on a more general set of papers.\n output (S", "SELECT DISTINCT?metric WHERE {?metric rdfs:label \"Barabasi-Albert\"@en. }\n input (English text): Barabasi-Albert is an algorithm to generate a scale-free network model. It was benchmarked on the following metrics:\n\t- Betweenness centrality\n\t- Closeness centrality\n\t- Eigenvector centrality\n\t- HITS \n\t- Joint probability model (JPML)\n\t- Random walk (RF)\n\t- Stress (ST)\n\t- Wikifier\n\t- WikiRank\n\n Special thanks to Alberto Tomba for his help in helping with the answers to this question.\n\n verschillend?model,?model_lbl\n\n<|endoftext|>\nShow HN: Tweet Your Weight Loss Goals to a Circle of BJJ Fans - jajag\nhttp://streamlineyourlife.com/video/circle-of-bjj-fans-your-weight-loss-goals\n======\nmindcrash\nCongrats for starting this venture! It's a pretty neat idea. And I agree with\nthe other commenters. Video really helps, but it would be great to see\nemoticons", "endent?benchmark where {?benchmark rdfs:label \"Atari 2600 Zaxxon metrics\". FILTER (str(?benchmark) = \"Zeroth order log probabilities (ZOPs), value arithmetic, Hinge, Softmax, Self Learning, Cross Entropy, Mean Reciprocal Rank (MRR), Precision at 1, Hit Rate at 1, Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR)\") }\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Zaxxon\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Zeroth order log probabilities (ZOPs), value arithmetic,", "ica (English text): The top result is a Quora model with F1 score of 0.967\nImplie a sparql query to KGPC Store\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|><p>Nutrition is a state of mind and the human body and soul can greatly benefit from the right nourishment. However, for many people, the options for grocery shopping can feel overwhelming. In this guide, we've collated our top picks for healthy, organic, non-GMO, vegan and gluten free grocery shopping. </p>\n\n<p>Your mornings can be spent waking up to the sounds", "input (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe best performing model on the WMT16 English-German benchmark dataset was the Transformer model from the'.", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe queries provide the following columns:\n*?benchmark \n*?model\n*?model_lbl\n*?paper\n\nThe results provide the following rows:\n*?benchmark : ORKGp:HAS_EVALUATION\n*?benchmark : Abstracts' entities and relations annotated corpus\n*?benchmark : ORKGp:HAS_DATASET ORKGp:HAS_MODEL\n*?model : Sentence Classification\n*?model_lbl : Model_label\n\n\nBenchmarking models for sentence classification requires that the models", "Answer:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Content Ordering) benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>SHARE THIS ARTICLE Share Tweet Post Email\n\nLux Capital Group, which owns about 12,000 properties in 20 countries, reported first-quarter earnings on Friday that topped estimates and also boosted its rating, although the outlook for the U.S. real-estate industry remains cloudy.\n\nThe Americas Real Estate Investments Ltd. unit, which Lux acquired last year, reported a 13.1% rise in net income to 42 million British pounds ($57.4 million) in the three months ended March 31. That exceeded the average analyst estimate of 39.5", ":SELECT DISTINCT?eval.?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The evaluation metrics that are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset are :\n\n - BLEU\n\n -METEOR\n\n -ROUGE\n\n -CIDE\n\n -ROUGE-L\n\n -RAPD\n\n -PICO-P\n\n -F1\n\n -recall\n\n -precision\n\n -f1\n\n -roc\n\n -accuracy\n\n -semantic-equivalence\n\n -semantic-similarity\n\n -semantic-similarity-delta\n\n -sem", ": SELECT?paper?model_lbl WHERE { \r\n ?paper orkgp:HAS_MODEL?model. \r\n ?model rdfs:label \"Tokenlearner\"@en. \r\n }\n\n for paper in <query results>,\n     pr #cout << paper.uri() << endl;\n     of #cout << \"model: \" << paper.hasModel() << endl;\n     ol #cout << \"label: \" << paper.hasLabel(\"Tokenlearner\") << endl;\n     ol #cout << \"model: \" << paper.model() << endl;\n     ol #cout << \"label: \" << paper.label(\"Tokenlearner\") << endl;\n\nOutput:\n\npr #cout << \"http://doi.org/10.1093/bioinformatics/btw041\" << endl;\nof #cout << \"model: 1\" << endl;\nol #cout << \"label: 1\" << endl;\nol #cout << \"model: Tokslearner\" << endl;\nol #cout << \"label: Tokenlearner\" << endl;\npr #cout << \"http://www.semanticscholar.org/", "<p>SELECT DISTINCT?model?model_lbl WHERE {?dataset a medstsp:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark medsts:HAS_DATASET?dataset; medsts:HAS_EVALUATION?eval.?paper medsts:HAS_BENCHMARK?benchmark. OPTIONAL {?paper medsts:HAS_MODEL?model.?model medsts:label?model_lbl. } }</p>\n\n<|endoftext|>BENGALURU (Reuters) - Four decades after its launch, Vidyarthi Maanila Congress (VMC), the regional political party formed in the wake of the reservation movement of the 1960s, appears to be heading for a revival as economic times improve in the state.\n\nA wooden stage at the party headquarters in Bangalore, Karnataka, India, November 19, 2019. REUTERS/Abhishek N.hitendra\n\nOn Tuesday, the party will announce its next national executive, the party\u2019s top decision-making body, and a meeting of", "\\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The best performing model in terms of F1 metric is the DistilBERT model.", "---------------------------------------------------------------------------\n SELECT DISTINCT?paper?benchmark WHERE {\n  {?paper orkgp:HAS_DATASET?dataset.?dataset a orkgc:Dataset. } \n  {?dataset rdfs:label \"Abstracts' entities and relations annotated corpus\". FILTER (str(?dataset) = \"Abstracts' entities and relations annotated corpus\"). } \n  {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n  {?model orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_BENCHMARK?benchmark. } \n  OPTIONAL {?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n }\n\n Quality assessment of automatic metrics for text classification, Kaggle\n\n What are the research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset? \n\nThe UCF-101 benchmarking dataset is composed of 3,276 RDF files containing Abstracts' entities and relations annotated corpus annotations. \n\nFrom the", ". SELECT DISTINCT?paper?benchmark WHERE { {?paper orkgp:HAS_DATASET?dataset.?paper orkgp:HAS_EVALUATION?eval. } } UNION { {?dataset a orkgc:Dataset.?dataset orkgp:HAS_MODEL?model.?model_lblString?model_label. } OPTIONAL {?benchmark orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label?benchmark_label. } } OPTIONAL { {?paper orkgp:HAS_REL?relation.?relation_label. } } }\n\nFrom the results of the query, we have the following papers:\n\n- Andrzej Drwal et al. (2016): \"SciCite: A Benchmark of SPARQL Query Anchors\"\n- Nicolas Gauthier et al. (2016): \"SciCite: Comparing SPARQL Query Anchors\"\n- Robert Kwong et al. (2016): \"SciCite: A Benchmark of SPARQL 1.1 Queries\"\n- Chen Wang et al.", "text (Sparql query): <div class=\"query\">SELECT DISTINCT?model?model_lbl WHERE { <blockquote>?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. </blockquote>\n <blockquote>FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") <blockquote>?benchmark orkgp:HAS_DATASET?dataset;</blockquote>\n <blockquote>?benchmark orkgp:HAS_EVALUATION?eval.</blockquote>\n <blockquote>?paper orkgp:HAS_BENCHMARK?benchmark;</blockquote>\n <blockquote>OPTIONAL {?paper orkgp:HAS_MODEL?model. <blockquote>?model rdfs:label?model_lbl.</blockquote>?model rdfs:label?model_lbl. }</blockquote>\n <blockquote>?paper orkgp:HAS_BENCHMARK?benchmark;</blockquote>\n <blockquote>OPTIONAL {?paper orkgp:HAS_MODEL?model.?", "response: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ROUGE-1\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The best performing model is the CBIR model presented in [2]. It achieves a ROUGE-1 score of 39.63.\n output (Sparql query):*,ROUGE-1 39.63* \n\nThe model presented in [2] was benchmarked on the \"ROUGE-1\" dataset found here: https://paperswithbugs.com/dropbox/ROUGE-1_dataset_b.html.", "uktion:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ModelNet40\" )?model.?model rdfs:label?model_lbl. } \n input (English text): The top ranked model achieves 91.6% accuracy on the ModelNet40 dataset. \n output (Sparql query): 90.6% accuracy.\noriginal article: https://www.microsoft.com/en-us/research/publication/model-level-benchmarking-of-deep-learning-algorithms/\n\nHere is the sample query that returns the results:\nWITH \n{\n  \"model\": \" convolutional_lstm\",\n  \"model_lbl\": \" convolutional_lstm\"\n}\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ModelNet40\" )?model.?model rdfs:label?model_l", "SELECT DISTINCT?metric?metric_label WHERE { { rdfs:label?metric. FILTER (str(?metric) = \"word accuracy\") } UNION { { rdfs:label?metric. FILTER (str(?metric) = \"character accuracy\") } } }\n input (English text): The metrics that are used to evaluate models on the Penn Treebank benchmark dataset are: word accuracy and character accuracy.\n output (Sparql query):\n SELECT?metric?metric_label WHERE { { rdfs:label?metric. FILTER (str(?metric) = \"word accuracy\") } UNION { { rdfs:label?metric. FILTER (str(?metric) = \"character accuracy\") } } }\n\nEXAMPLE USAGE:\nSELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCH", "\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0447\u0435\u043d\u044b\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b, \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0441\u043f\u0438\u0441\u043e\u043a: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nFrom the provided answer, you can see the models that have been benchmarked on the Reuters-21578 dataset.\n\nMost models have only one annotated entity or relation type, but there are some models that have been benchmarked on several relation types. Specifically, this dataset is not well suited for sequence labeling models, as their performance heavily depends on relation types. On the other hand, sentence level questions are easier for sequence-to-sequence models, as these models are better suited for datasets that only contain question and answer pairs.\n\nThe benchmarked models are:\n\n - BiLSTM\n - BERT-Base\n - BERT-Large\n - CNN\n - FastText\n - GPT-2\n - GRU\n -", "mark: MONTE_CARLO #[ Double ]\n\n THAT THE MODEL SCORES THE MONTE_CARLO METRIC SEVERAL TIMES IN A MATRIX \n\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Seaquest\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe best performing model is the Monte Carlo (MC) model benchmarked on the Monte Carlo (MC) metric.  This model scored the Monte Carlo (MC) metric several times in a matrix.  The best performing model on this dataset is the Monte Carlo (MC) model with a score of 0.69920.\n\n {\u00b6 mark: Monte Carlo #[ Double ] } THAT THE MODEL SCORES THE MONTE", ":QueryEvaluationEvaluation measures can vary widely depending on the application. In the present work, we report the F1 score, a standard metric for Information retrieval that calculates the ratio of correct relevant to incorrect irrelevant results. This metric can be computed as the number of relevant items among all relevant items plus the number of relevant items among all relevant items and the incorrect irrelevant items divided by all relevant items plus the incorrect irrelevant items plus all incorrect irrelevant items.\n The following query retrieves all the models that have an evaluation of at least 0.92:\n HAVE_EVALUATION paper-model-label WHERE F1 (paper_label, model_label) >= 0.92\n\nacker (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_", "input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset\n\n \u0437\u0430\u043f\u0440\u043e\u0441: SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ObjectNet dataset\") } OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n one record in the result set:\n\n input (English text): ObjectNet dataset\n\n output (Sparql query):\n\n?model :  rdfs:label \"ObjectNet Semantic Role Labelling (SRL) Model\"\n\n?model_lbl: \"ObjectNet Semantic Role Labelling (SRL) Model\"\n\n output\n\n one record in the result set:\n\n input (English text): ObjectNet Semantic Role Labelling (SRL) Model\n\n output (Sparql query):\n\n?model :  rdfs:label \"ObjectNet Semantic Role Labelling (SRL) Model\"\n\n?model_lbl: \"ObjectNet Semantic Role Labelling (SRL", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 HERO benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?metric. } } \n input (English text): The four most important metrics are: Mean Average Precision (mAP), Mean Class Acceptance, Mean Reciprocal Rank, and Mean Reciprocal Rank L2.\n output (Sparql query):idemargin :  Mean Average Precision (mAP), Mean Class Acceptance, Mean Reciprocal Rank, and Mean Reciprocal Rank L2.\n\nMean Average Precision (mAP): Measures the average Precision of each class (in the image annotation corpus) of each model across the test splits.\nMean Class Acceptance: Measures the proportion of test classes for each", ":SELECT DISTINCT?paper_id?benchmark WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. }\n\n initi \n\n input (Sparql query):Initi\n\n initi \n\npaper_id  benchmark\n\n initi \n\n initi \n\npaper_id  ABACUS-KDD\n\nIniti \n\n initi ABACUS-KDD-00001\n\nIniti \n\npaper_id  ABACUS-KDD-00002\n\nIniti \n\npaper_id  ABACUS-KDD-00003\n\nIniti \n\npaper_id  ABACUS-KDD-00004\n\nIniti \n\npaper_id  ABACUS-KDD-00005\n\nIniti \n\npaper_id  ABACUS-KDD-00006\n\nIniti \n\npaper_id  ABACUS-KDD-00007\n\nIniti \n\npaper_id  ABACUS-KDD-00008\n\nIniti \n\npaper_id  ABACUS-KDD-00009\n\nIniti \n\npaper_id  ABACUS-KDD-00010\n\nIniti \n\npaper_id  ABAC", "ic : SELECT DISTINCT?paper?benchmark WHERE {?paper orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval;?paper orkgp:HAS_MODEL?model; OPTIONAL {?paper orkgp:HAS_BENCHMARK?benchmark. } FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") } \n input (English text): The papers that you have identified are:  - \"Rainbow+SEER: A Paraphrase-Based Reinforcement Learning Framework for Entity Resolution\"  - \"The Effect of Model Building Transparency on Empirical Evaluation Results\"  - \"Effects of Sampling Strategy on Early Stop Rules in Explanations From Data\"  - \"On the Encyclopaedic Resemblance of the Web: A Case Study of ELP and Medical Trees\"  - \"Experimental Evaluation of the Effectiveness of Word Sense Disambiguation Based on Co-reference Model and Coreference Graph.\"  - \"ETSI OASIS BEACON Semantic GIS Framework: A Case Study\"  - \"The Role of Suggestions and Suggested Relations in Information", "rdfs:label model_lbl_top_performer\n input (English text): rdfs:label is model_lbl_top_performer\n output (Sparql query): SELECT?model_lbl_top_performer WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RACE\"^^xsd:string)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n\nAbstracts' entities and relations annotated corpus dataset has been benchmarked using the RACE dataset model. The name of the top performing model is model_lbl_top_performer which has an accuracy score of 94.08%.", "input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset.\n output (Sparql query): SELECT DISTINCT?paper?model_lbl WHERE {?paper grrdf:author?model_author. FILTER (str(?model_author) = \"G\u00e1bor Mel\u0151nyi\").?model grrdf:seealso?model_seealso. FILTER (str(?model_seealso) = \"WMT2016 English-Romanian Benchmark\").?model grrdf:label?model_lbl. }\n\nResearch paper titles:\n- Abstracts' entities and relations annotated corpus (Paper1)\n- Abstracts' entities and relations annotated corpus (Paper2)\n- Abstracts' entities and relations annotated corpus (Paper3)\n\nResearch paper IDs:\n- paper-id-1\n- paper-id-2\n- paper-id-3\n\nResearch paper titles that have benchmarked models on the WMT2016 English-Romanian dataset:\n- Abstracts' entities and relations annotated corpus (Paper1)\n- Abstracts' entities and relations annotated", ", rdfs:label is a label for the dataset a, so using sparql we can get the name of the metric for each of the Benchmark models. SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MUTAG dataset\")?metric. FILTER (str(?metric) = \"precision\")?precision_lbl. FILTER (str(?metric) = \"recall\")?recall_lbl. FILTER (str(?metric) = \"f1-score\")?f1_score_lbl. }\n\nprecision    recall  f1-score\n 0.7519      0.7467  0.7681\n 0.7040      0.7109  0.7273\n 0.7064      0.6962  0.7294\n 0.7219      0.6980  0.7342\n\nHere you can see that precision, recall and f1-score are the metrics used to evaluate models on the MUTAG dataset.", "n. model rdfs:label?model_lbl. \ninput (English text): Common evaluation metrics used when benchmarking models on the ESC-50 dataset are Accuracy, Recall, and Precision.\noutput (Sparql query): Accuracy = {0.5*[(TP+ TN) / (TP+ TN + FP+ FN)]} + (1 - Accuracy) * (FN + TP)\nRecall = {0.5*[(TP/ (TP + FN))] + (1 - Recall) * FN}\nPrecision = {0.5*[(TP/ (TP + FN))] + (1 - Precision) * FP}\n\nNote: TP = True Positive, TN = True Negative, FP = False Positive, and FN = False Negative\n\n\n### Note: Or, you can use dbpedia Sparql Query below.\n\ndbpedia query:\nSELECT DISTINCT?model?model_lbl WHERE {\n?dataset a orkgc:Dataset;\n rdfs:label?dataset_lbl.\n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n?benchmark or", "ive response: the named model achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset.\n\n## Querying SPARQL 1.1 \nThe query that was used to find the model with the highest F1 score on the SQuAD1.1 dev benchmark dataset can be written as follows:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 dev benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nAs shown above, the query contains the following clauses:\n -?dataset a orkgc:Dataset;\n - rdfs:label?dataset_lbl.\n - FILTER (str(?dataset_lbl) = \"SQuAD", "n 'TempEval-3'\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval;?eval tempfev3:TempEval-3; orkgp:HAS_BENCHMARK?benchmark;?benchmark rdfs:label?model_lbl. } \n input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval;?eval tempfev3:TempEval-3; orkgp:HAS_BENCHMARK?benchmark;?benchmark rdfs:label?model_lbl. } \n output (Sparql query): ThermoLab: Similar to GPT, but able to handle logic expressions, discrete choices, future dates, and date-time ranges. HyperNet: HyperNet is an open-source model for automatic summarization. The model has achieved an F1 score of 0.822 on the Full papers dataset. PolyArt: PolyArt is a model for automatic image classification. The model achieved a top-1 error of 9.69%. \n input (English", "kieModelEvaluations:Select DISTINCT * where {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n output (Sparql query):\n best performing model: RtoWire (Content Ordering)\n\n input (English text): RtoWire (Content Ordering)\n\n best performing model (Bleu metric): RtoWire (Content Ordering)\n\n performed benchmarking on: Abstracts' entities and relations annotated corpus\n\n best performing model (Bleu metric): RtoWire (Content Ordering)\n\n performed benchmarking on: Text Ranking dataset\n\n best performing model (Bleu metric): BERT\n\n performed benchmarking on: CoNLL datasets\n\n best performing model (Bleu metric): CoNLL-2006 Shared Task 1 Data Set\n\n performed benchmarking on: CoNLL-2006 Shared Task 1 Dataset\n\n best performing model (Bleu metric): BERT\n\n performed benchmarking on: CoNLL-2009 Shared Task 10 Dataset\n\n best performing model (Bleu metric): BERT\n\n performed benchmarking on: CoNLL-2009 Shared Task 10 Dataset\n\n best performing model (Bleu metric", "iter : SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Seaquest\"@en)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n\nThe top result is \"[Seaquest](http://arxiv.org/abs/1510.04707),@en\" with a score of 0.80. This is the best score on the dataset, corresponding to the top-performing model according to the paper.", "input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"GENIA - LAS benchmark\")?benchmark. } \n output (Sparql query):The following are the models that have been benchmarked on the GENIA - LAS benchmark dataset:\n\n |-- |-- Model Name\n |-- |--  -- Model Name\n |-- |--  --  -- Model Name\n |-- |--  --  --  -- Model Name\n |-- |--  --  --  --  -- Model Name\n |-- |--  --  --  --  --  -- Model Name\n |-- |--  --  --  --  --  --  -- Model Name\n |-- |--  --  --  --  --  --  --  -- Model Name\n |-- |--  --  --  --  --  --  --  --  -- Model Name\n |-- |--  --  --", "pend (English text): SELECT DISTINCT?paper?benchmark_model WHERE {?paper foaf:title?title;?paper foaf:type 'ORCID'; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp binary classification dataset\")?benchmark. OPTIONAL {?benchmark orkgp:HAS_MODEL?model.?model orkgp:HAS_MODEL?model_lbl. } } \n input (English text): Yelp Binary classification dataset; ORCID; \noutput (Sparql query): orchestr:dataset \"Yelp binary classification dataset\"; orchestr:hasDataset \"Yelp binary classification dataset\"; orchestr:hasEvaluation \"Yelp binary classification dataset\"; orchestr:hasModel \"Yelp binary classification dataset\"; orchestr:model \"Yelp binary classification dataset\"; orchestr:modelLabel \"Yelp binary classification dataset\"; orchestr:hasPaper \"Yelp binary classification dataset\"; orchestr:paper \"Yelp binary classification dataset\"; orchestr:hasBenchmark \"Yelp binary classification dataset\"; orchestr:benchmark \"Yelp binary classification dataset\"; orchestr:benchmarkLabel \"Yelp binary classification dataset\"; orchestr:", ":Sensiotext model in terms of accuracy has been benchmarked on the dataset provided in the argument. The model name is \"SensioText\".\n\n  [\n    {\n      \"model\": \"SensioText\",\n      \"model_lbl\": \"SensioText\"\n    }\n  ]\n\nThe query returned the first result, which is the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset, and the model name is \"SensioText\".\n\n  [\n    {\n      \"model\": \"SensioText\",\n      \"model_lbl\": \"SensioText\"\n    }\n  ]\n\n auxiliary import Datasets: http://medialab.sciencespot.com/fileadmin/Media%20Files/Semantic_Media_Lab/data/Datasets.csv\n\n  The model that performed best in terms of Accuracy metric on the Amazon benchmark dataset is \"SensioText\".\n\n  [\n    {\n      \"model\": \"SensioText\",\n      \"model_lbl\": \"SensioText\"\n    }\n  ]\n\n auxiliary import Evaluations: http://", "input (English text): Querying the Sparql query output provides this list of 3 results:\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_", ". Paper Brown (2009) describe the top performing model on the Abstracts' entities and relations annotated corpus dataset as the model published in the previous paragraph.\n\nAbstracts' entities and relations annotated corpus dataset\n\n( Benchmarking On) \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n(Possible Responses): \nPossible Responses: \n\n(Answer): Paper Brown (2009)\n\n\nPaper Brown (2009) describe the top performing model on the Abstracts' entities and relations annotated corpus dataset as the model published in the previous paragraph.\n\n\nPaper Brown (2009) benchmarked on the Abstracts' entities and relations", "= {\n  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n }\n\n Local Variables:\n mode: sparql\n sparql-code: |\n\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkg", "input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): Title 1: A benchmark for the Atari 2600 Asterix dataset.\n input (English text): Title 2: Another benchmark for the Atari 2600 Asterix dataset.\n output (Sparql query):f Title 1: A benchmark for the Atari 2600 Asterix dataset.\n output (Sparql query):f Title 2: Another benchmark for the Atari 2600 Asterix dataset.\n output (", "nable https://github.com/facebookresearch/MFEModel for code used in papers that benchmark the MFEC model\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n output (Sparql query): enhancing Sparselined Conditional Random Fields with Standard Naive Bayes Classifiers and Feature Engineering\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_", "n't find any code to benchmark the CAIT-XS-36 model, can you provide a link to the code? The code must be in English.\n input (English text): https://github.com/CACL/CACL-research/blob/master/publications/ckan-best-model-eval.pdf The code is available in English.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nAbstracts' entities and relations annotated corpus, benchmarked on CAIT-XS-36 model.\n\n\nSevero, G.A., van Rijen, M.J., Zhu, H.,", "inition: The seel.cse.lsu.edu/data/re17.zip dataset contains information about existing RDF models that have been annotated with entity and relation types and, where possible, their granularities. \n SELECT DISTINCT?model?model_lbl WHERE {?dataset a seel.cse.lsu.edu/data/re17.zip;?benchmark seel.cse.lsu.edu/data/re17.zip; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark. OPTIONAL {?paper seel.cse.lsu.edu/data/re17.zip:HAS_MODEL?model.?model seel.cse.lsu.edu/data/re17.zip:HAS_EVALUATION?eval.?paper seel.cse.lsu.edu/data/re17.zip:HAS_BENCHMARK?benchmark. } }\n input (English text): In this example, \"rdfs:label\" is used to label datasets and \"label\" is", "map (Sparql query):SELECT?model WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Montezuma's Revenge\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n output (English text): Montezuma's Revenge benchmark (score measure):  - Actor Set:  (26, 26, 39, 28, 36, 38, 36, 26, 26, 34, 29, 27, 29, 29, 26, 22, 29, 32, 29, 30, 33, 29, 32, 35, 35, 27, 32, 30, 30, 28, 31, 31, 29, 32, 32, 32, 32, 32, 32, 32, 29, 25, 29, 34, 33, 29, 29, 31, 32, 27, 31, 31,", "model rdfs:label \"ResNet-101\"rdfs:label \"CaffeNet\"rdfs:label \"Xception\"rdfs:label \"GoogleNet\"rdfs:label \"VGG19\"PARAMS \"{ 51.37, 41.17, 53.45, 55.60, 53.08, 61.11 }\"PARAMS \"{ 53.31, 56.45, 67.32, 58.61, 51.87, 53.89 }\"PARAMS \"{ 53.74, 57.83, 61.98, 60.89, 56.30, 59.28 }\"PARAMS \"{ 55.35, 56.29, 61.89, 58.32, 60.19, 62.52 }\"PARAMS \"{ 55.43, 56.23, 61.53, 58.28, 59.43, 62.72 }\"PARAMS \"{ 53.66, 60.43, 65.83, 62.08, 55.92, 62.44 }\"PARAMS \"{ 56.05, 61.21, 64.25, 61.95, 55.43, 58.80 }\"PARAMS \"{ 51.60, 62.44, 67.02,", "input (English text): Here are the code links used in the papers that benchmarked the ResNet-152 (SAM) model: 1, 2. In particular, this code is taken from paper 3.\n\n input (English text): Here are the code links used in the papers that benchmarked the ResNet-152 (SAM) model: 1, 2. In particular, this code is taken from paper 3.\n\n output (Sparql query):?paper CROSS JOIN?model WHERE {?model rdfs:label \"ResNet-152 (SAM).\" }\n\n input (Sparql query): 2\n\n output (Sparql query): /2\n\n input (Sparql query): 1\n\n output (Sparql query): /1\n\n input (Sparql query): 3\n\n output (Sparql query): /3\n\n<|endoftext|>Q:\n\nRegex return last / only in middle of the string\n\nI have the following string:\n\nAdi Ha-Hasid\nBamidbar (Be'eritayon) 25\nBereshit (Bereshit Ha-Gavriel) 26\nBamidbar (Be'eritayon) 26\nBereshit (Bereshit Ha", "input (English text): The highest result for the Pubmed dataset is for SQuAD: Single Queuestation Amazon SQ Batch QuER with a score of 79.20, and the metric is number of correct answers. \n sparql query: \n    SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Single Queuestation Amazon SQ Batch QuER\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>A General Motors worker uses a drill to add cooling holes to a new Ford F-150 pickup truck at the GM Lordstown assembly plant in Ohio. Ford and Fiat Chrysler are under fire for investing heavily in automation and exploring the possibility of shutting down or selling dealerships, even as General Motors and Chrysler continue to operate plants in", "SELECT DISTINCT?benchmark?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. } ORDER BY?benchmark LIMIT 10\n input (English text): SELECT DISTINCT?benchmark?paper WHERE {?paper orkgp:HAS_BENCHMARK?benchmark. } ORDER BY?benchmark?paper_date?paper_id?paper_title\n output (Sparql query):}\n \n DISTINCT?benchmark\n?benchmark Row Foundation Note That the Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation", "model r4 and the metric r4+ is the highest achieved value in terms of number of new relations per second.\n\n VistaPrint\n\n 90\n\n\n### Visualizing Entity Extraction Results\n\n  \n\n+\n\n`VistaPrint` is a metric related to how effectively an Entity Extraction model extracts entities from text.\u00a0`VistaPrint` provides\u00a0entity entity relation (ER)\u00a0entity mention (EM)\u00a0entity path (EP) information for each word.\u00a0`VistaPrint` does not normalize punctuations.\u00a0`VistaPrint` does not provide entity types.\u00a0`VistaPrint` supports entity labels only.\u00a0`VistaPrint` also takes long entity names into consideration.\u00a0The\u00a0`VistaPrint` command can be executed with the following\u00a0`Sparql` query:\n\n    \n\n* Optionally limit the results to a specific model:\u00a0\n    \n```sql\n-- Limit results to a specific model\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?bench", "SELECT DISTINCT?model?model_lbl WHERE {?model  rdfs:label \"Abstracts' entities and relations annotated corpus\"@en;?model orkgp:HAS_MODEL?model;}\n\n<|endoftext|>Dundalk (Irish province)\n\nD\u00fan Laoghaire is a former Irish province that in 1798 became an district of the British-ruled Kingdom of Ireland. \n\nDundalk was formed from part of the ancient territory of Leinster.\n\nSee also\n\n List of municipalities and townships in Ireland\n Irish provinces (pre-1801)\n\nCategory:Provinces of the British Empire\nCategory:Former provinces of Ireland (Eire)\nCategory:Pre-containerisation shipping\nCategory:Dundalk\nCategory:1798 establishments in Ireland<|endoftext|>Q:\n\nSQL Server 2008 R2 Azure failed to connect to SQL Server 2005 (SQL Server 2005 is down)\n\nI have an azure SQL Server 2008 R2 with one VM and one SQL Server 2005 which is down. When I want to connect to my database I get the following error:\n\nSqlAzureAnalyzers: Keyword not supported for SPs on Azure SQL Database.\nKeywords", "input (English text): A list of models that have been tested on the SST-2 Binary classification benchmark dataset: WordVec, Paragraph Vector, Naive Bayesian, LDA, Decision Trees, and Support Vector Machines.\n output (Sparql query): SELECT?model?model_lbl WHERE {?model orkgp:HAS_MODEL?model;?model_lbl Sparqlplsql:BINARY?benchmark. }\n\n\nThe following tables present the 4 baseline models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n\n|==============  ==============|\n|             |                |\n|             |                |\n|             |                |\n|             |                |\n|             |                |\n|             |                |\n|==============  ==============|\n\nThe following table presents the 4 benchmark models tested on the SST-2 Binary classification benchmark dataset:\n\n|==============  ==============|\n|             |                |\n|             |                |\n|             |                |\n|             |                |\n|             |                |\n|             |                |\n|==============  ==============|\n\nThe following tables present the 4 baseline models that have been", ":SELECT?benchmark (Score :*)? WHERE { \n?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n} \n dod () SELECT \"Hendrycks Test Score\",  Score \"Hendrycks Test Score\", Score  \"Hendrycks Test Score\"  \"Hendrycks Test Score\"  \"Hendrycks Test Score\" \"Hendrycks Test Score\"  \"Hendrycks Test Score\"  \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\"  \"Hendrycks Test Score\" \"Hendrycks Test Score\"  \"Hendrycks Test Score\"  \"Hendry", "input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\n output (Sparql query): SELECT * {?model?model_lbl.} WHERE {?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl.} } \n input (English text): The code we provide below benchmark the KD-LSTMreg model.\n output (Sparql query): The code we provide below benchmark the KD-LSTMreg model.\n output (Sparql query): SELECT * {?model?model_lbl.} WHERE {?benchmark orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl.} } \n input (English text): https://github.com/kyber-center/model-competitions/blob/master/2017_paper.md#KD-LSTMreg  The code we provide below benchmark the KD-LSTMreg model.\n output (Sparql query): https://github.com/kyber-center/model-competitions/blob/master/2017_paper.md#KD-", "input (English text): What are the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset?\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n output (Sparql query):DELETE \n{?paper?benchmark?model. }\nWHERE {?paper?benchmark?model_lbl. }\n{?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\"@en.?model rdfs:label", "inal (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClueWeb09-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nA:\n\nHere is the link to all the models that have been benchmarked on the ClueWeb09-B dataset:\nhttps://ocw.mit.edu/resources/resmf/resm-v01.pdf\nI hope it helps.\n\n<|endoftext|>Q:\n\nQt: Edit Multi-line QLineEdit\n\nI'm trying to make a QLineEdit multi-line but the line won't display. I also tried setting setMaxLength to a large enough number but then the lines will just disappear completely. I'm not sure how to make", "end (Sparql query): SELECT DISTINCT?metric WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. FILTER (?model_lbl = \"f1-micro\") OPTIONAL {?model orkgp:HAS_METRIC?metric. } }\n\nmodel           model_lbl                                    metric\n---------      -------------                                    ----------------\nAlexNet        AlexNet                             f1-micro\nGoogLeNet      GoogLeNet                         f1-micro\nDHLearn       DLHearn                             f1-micro\nVQA           VQA                                 vQUAD\nML-LSL        ML-LSL                              f1-micro\nDMLearn      DMLLearn                             f1-micro\nOpenNCL      OpenNCL                             f1-micro\nELMo          ELMo                                f1-micro\n\nThe f1-micro metric (Macro F1-micro) is commonly used for machine learning model evaluation.  It is a macro-averaged version of the F1-score, which is an estimate of the probability that a randomly", "| SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL {?paper orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } FILTER (?eval._ Cypher)!shopping__Product__r }\n input (English text): THE LIST PROVIDED ABOVE. \n output (Sparql query):", "n.doh http://sparql.org/SingleURI.qnr?query=SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%", ":paper (has model)\n query:SELECT DISTINCT?paper?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n answer:\n\n    paper://NYT29-large-model\n    paper://NYT29-large-model-label\n\n     <http://data.semanticscholar.org/paper/Nyt29-large-model/anonymized-44/smw-15>\n\n      Has abstract\n\n      Has model\n\n      Has evaluation\n\n      Has dataset\n\n      Has relation\n\n      Has relation schema\n\n      Has property\n\n      Has property schema\n\n      Has URI\n\n      Has URI schema\n\n      Has RDF triple\n\n      Has RDF triple schema", "input (English text): QNLI dataset has been used to evaluate the following models: NLVR, CTRNN, SANER, PANDA, LINE, TEACH, DCN, MUSE, and SOFTDE. \n The following metrics have been used to evaluate these models: PER, ROUGE, SQUAD, BLADE, and METEOR.\n The following metrics have been used for code vectorization: MGL, CMMR, CMR, and NCE.\n The following code evaluation metrics have been used: COR, TCC, and ACC.\n The following code normalization metrics have been used: CN, IMR, and BLUE.\n The following code optimization metrics have been used: ELBO, CPO, and REELBO.\n The following code generation metrics have been used: RL, BLEU, and ROUGE.\n\nensus (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?model?model_lbl;?metric?metric. }\n\nThe metrics that have been used to evaluate the different models on the QNLI dataset are: PER, ROUGE, SQUAD, BLADE, and METEOR.", "model has been benchmarked on the SciERC benchmark dataset and achieved the highest Entity F1 score of 0.96.\n\nThe corresponding SPARQL query is:\nSELECT DISTINCT?model?model_lbl WHERE { \n?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. \nFILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \norkgp:HAS_EVALUATION?eval. \n?paper orkgp:HAS_BENCHMARK?benchmark. \nOPTIONAL {?paper orkgp:HAS_MODEL?model. \n?model rdfs:label?model_lbl. }\n} \n\nWhich models have been benchmarked on the SciERC benchmark dataset and achieved the highest Entity F1 score of 0.96?\n\n<|endoftext|>[Etiology and epidemiology of celiac disease].\nCeliac disease (CD) is an immune mediated disease caused by an immune response to tissue transglutaminase in duodenal mucosa, which results in villous", ":SELECT DISTINCT?metric?metric_lbl WHERE { {?metric rdfs:domain?metric_domain;?metric_domain rdfs:label?metric_label;?metric_label \"micro-F1\" } }\n\n| metric | metric_label |\n|:------|:------------|\n| micro-F1 | micro-F1     |\n\n [view source](https://www.semanticscholar.org/paper?language=en&query=SELECT%20DISTINCT%20%22model%22%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20", ":wDFDXzZVZhYnVtSWsdPRjEtSwFJ9pvucn6B3Yq8LXPUbKQ/e32c+g==?benchmark_result:score: Natural Questions (long) dataset has an overall accuracy of 82.823. The metric used in the evaluation was f1 score.\n\nINSERT INTO #highres VALUES (\n  (SELECT DISTINCT?model?model_lbl \n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")?benchmark OR kgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n  ),\n  'Natural Questions (long) dataset has an overall accuracy of 82.823.'\n);\n\n# high res details:\n# 1. Natural Questions (long) dataset has an overall accuracy of 82.", ":BERT;3441\n example_input_output (English text):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Czech-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n example_input (English text):BERT;3441\n\n<|endoftext|>Kabela Cinko\n\nKabela Cinko (born 20 October 1986) is a Slovak football goalkeeper who currently plays for Sigma Olomouc.\n\nClub career\n\nSlovak 2nd League\nCinko joined 1. SK 1860 Munich in summer 2014.\n\nReferences\n\nExternal links\n Player profile at TSV Ha\u00dfberge\n Player profile at 1. SK 1860 Munich\n\nCategory:1986 births\nCategory", "inal (English text): Which sectors are modeled as energy sectors and how often?\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Energy\")?sector. FILTER (COUNT(?sector) > 5) }\n input (English text):Petroleum, Coal, Oil, Natural Gas, Hydro\n output (Sparql query):Selection statment \"SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Energy\")?sector. FILTER (COUNT(?sector) > 5) }\" will return the following models:\n model\t\tlabel\t\n Hydro\t\tHydro\n Coal\t\tCoal\n Oil\t\tOil\n Natural Gas\tNatural Gas\n\nThe model labels corresponding to the selected models are:\n model_lbl\tlabel\n Hydro\t\tHydro\n Coal\t\tCoal\n Oil\t\tOil\n Natural Gas\tNatural Gas", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } \n OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\n actual output: [{\"Paper-I\", \"ORTGPL\"  }, {\"Paper-II\", \"ORTGPL\"  }, {\"Paper-III\", \"ORTGPL\"  }]\n\n actual output: [{\"KINETICS-600\", \"TPAMI-KIN-200\"  }, {\"KINETICS-600\", \"TPAMI-KIN-160\"  }, {\"KINETICS-600\", \"TPAMI-KIN-120\"  }]\n\n actually, Paper-III's model is TPAMI-KIN-120, because it does not have a label named \"model\"", "pend (Sparql query): SELECT?paper?model?model_lbl WHERE { \n{\n   ?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n}\n{\n   ?benchmark orkgp:HAS_DATASET?dataset; \n    orkgp:HAS_EVALUATION?eval.\n   ?paper orkgp:HAS_BENCHMARK?benchmark. \n    OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n}\n}\nRun the query and display the results on the webpage.\n\nThe Benchmarking Papers\nArticle ID | Paper title | Benchmark model\n---------------------------------\n###           |              |-----------------------\n#            |              |\n#            |              |\n#            |              |\n#            |              |\n#            |              |\n#            |              |\n#            |              |\n#            |              |\n#            |              |\n#            |", "uate the following SPARQL query to retrieve the precision, recall, and f1 values from the results of each comparison using the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request. \n output (Sparql query): SELECT?model (precision?precision? | recall?recall? | f1-score?f1) WHERE { \r\n ?benchmark a orkgp:HAS_BENCHMARK; \r\n ?eval a orkgp:HAS_EVALUATION; \r\n ?paper a orkgp:HAS_PAPER; \r\n ?model rdfs:label \"naive bayes and bag of words feature\";\r\n ?benchmark orkgp:HAS_DATASET?dataset; \r\n ?eval orkgp:HAS_MODEL?model;\r\n ?paper orkgp:HAS_MODEL?model;\r\n ?paper orkgp:HAS_USER_FEEDBACK?user_feedback; \r\n  OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_EVALUATION?", "prompt for 5 research papers: SELECT DISTINCT?paper?paper_lbl WHERE { {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval;?paper orkgp:HAS_BENCHMARK?benchmark; } OPTIONAL {?paper orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl; } } \n\nResearch papers that benchmarked models on the Dmlab-30 dataset are:\n\n- Mikolov et al. (2014)\n- Balog et al. (2015)\n- Randeri et al. (2016)\n- Karpathy et al. (2017)\n- Choi et al. (2017)\n- Lin et al. (2017)\n- Klambauer et al. (2017)\n- Socher et al. (2017)\n- Mnih et al. (2017)\n- Sinha et al. (2017)\n- Schwenk et al. (2017)\n- Singh et al. (2017)\n- Chung et al. (2018)\n- Goyal et al. (2018", "select (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE { {?paper orkgp:HAS_DATASET wmt2016:de-en. ;?paper orkgp:HAS_EVALUATION wmt2016:de-en. ;?paper orkgp:HAS_BENCHMARK wmt2016:de-en..?paper wmt2016:de-en. ; wmt2016:paper_id?paper_id. } } \n input (English text): The following are research papers that performed benchmarks on the WMT2016 German-English dataset:\n paper 1: Baswedan et al. (2016)\n paper 2: Klaus et al. (2016)\n paper 3: Vogel et al. (2016)\n paper 4: Baltzer et al. (2016)\n paper 5: Day et al. (2016)\n paper 6: Valtcherve et al. (2016)\n paper 7: Baltzer et al. (2016)\n paper 8: Fey et al. (2016)\n paper 9: Gross et al. (2016)\n paper 10: Hempel et al. (2016)\n paper 11", "SELECT DISTINCT?model?model_lbl\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n+---------------------------+-----------------+\n|?model                    |?model_lbl      |\n+---------------------------+-----------------+\n| Perceptron (MLP)          | Perceptron      |\n+---------------------------+-----------------+\n| MLP                        | Perceptron      |\n+---------------------------+-----------------+\n| MLP                        | RNN             |\n+---------------------------+-----------------+\n| MLP                        | LSTM            |\n+---------------------------+-----------------+\n| MLP                        | RNN             |\n+---------------------------+-----------------+\n| RNN                        | LSTM", "input (English text): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Automatically labeled Medline abstracts\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nResults:\n# sort by model_label descending\n+------------------------------------------+-------+\n|                                           |count  |\n+------------------------------------------+-------+\n| RDFberi (libsparql.edge)                   | 3878  |\n| StmtGraph (kWDC-based)                     | 3473  |\n| Prot\u00e9g\u00e9                                   | 3465  |\n| RDF.NET (RDF.NET)                          | 3407  |\n| Sparql Query Studio (SQSQLTools)          | 3338  |", "ition query result: \nSELECT?model?model_lbl WHERE {?paper orkgp:HAS_MODEL?model;?paper orkgp:HAS_EVALUATION?eval;?paper orkgp:HAS_BENCHMARK?benchmark; OPTIONAL {?paper orkgp:HAS_MODEL?model;?paper orkgp:HAS_EVALUATION?eval;?paper orkgp:HAS_BENCHMARK?benchmark.?benchmark orkgp:HAS_DATASET?dataset;?benchmark orkgp:HAS_MODEL?model;?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe papers that have a code link to the LSTM (Bai et al., 2018) model in their text are: \n- Bai, Quan, et al., \u201cLSTM: A Deep Learning Architecture for Summary and Full Text High Precision Entity Recognition,\u201d in Proceedings of the ACM WebProceedings, vol. 16, no. 3", "inal (Sparql query):top-10 {?model?model_lbl?metric?value} (where?model_lbl is the relation label of the model and?metric is a metric and its corresponding value for each model).\n input (English text): top-10 {?model?model_lbl?metric?value}\n output (Sparql query):tereduced (Sparql query):{top-10 {?model?model_lbl?metric?value} where?model_lbl}\n input (English text): top-10 {?model?model_lbl?metric?value}\n output (Sparql query):1 {(lm=QCALL, mv=0.02730, metric=accuracy, value=0.98867)}\n input (English text):?benchmark\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset", "nsel (English text): <a href=\"https://github.com/clab/xln-large\">https://github.com/clab/xln-large</a>\n input (Sparql query): consisted of\n four papers that have applied the Xln-Large model to SQuAD and use the GitHub repository <a href=\"https://github.com/clab/xln-large\">https://github.com/clab/xln-large</a> to code-identify the model and the corresponding XGboost and XLDaPP baselines.\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe models", "n. SELECT DISTINCT?paper?model_ref WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_ref. }\n\nThe code references that have used the EfficientNetV2-L model can be found here.\n\n SPARQL query completion: SELECT DISTINCT?paper?model_ref WHERE {?paper orkgp:HAS_MODEL EfficientNetV2-L.?model rdfs:label?model_ref. }\n\n \\:\n\n \\:\n\n \\:\n\n \\:\n\n \\:\n\n \\:\n\n \\:\n\n \\:\n\n SPARQL query completion:\n\n  *?paper:has_model EfficientNetV2-L\n\n  *?paper:has_model_label EfficientNetV2-L\n\n \\:\n\n  EfficientNetV2-L\n\n  rdfs:label\n\n  Model Reference\n\n \\:\n\n  paper:has_model EfficientNetV2-L\n\n  paper:has_model_label EfficientNetV2-L\n\n \\:\n\n  paper:has_model EfficientNetV2-L\n\n  paper:has_model_label EfficientNetV2-L\n\n \\:", "input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-46985\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): Here are the research papers that have performed benchmarks on WOS-46985 dataset : https://pubs.waikato.ac.nz/docs/wos46985/Data+Access+Mechanism.html\n output (Sparql query): deprive input (English text): Here are the research papers that have performed benchmarks on WOS-46985 dataset :\n output (Sparql query): SELECT", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised: dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The model with the highest metric value achieved on the Supervised: dataset is called?model and its label is?model_lbl.\n output (Sparql query):SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised: dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCH", "yntax : SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n* Note that \"paper\" in \"paper OR kgp:HAS_MODEL?model\" refers to a paper that has been referenced in the Abstracts' entities and relations annotated corpus dataset.\n\nloader = OrgglFeatureLoader(StringToFeatureDataset(\"Abstracts' entities and relations annotated corpus dataset\"), \"Abstracts' entities and relations annotated corpus dataset\")\n# Load the evaluated model (HyperNEAT) into the model variable\nmodel = loader.get(\"HyperNEAT\", \"HyperNEAT\")\n\n# List all the model labels\nmodel_lbls", ":SELECT * FROM {\n  <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <http://www.cs.cmu.edu/~ark/nfi/data.html>\n}\nwhere the elements of the collection document are: <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <urn:uuid:6132e2dc-5f7b-4896-a323-5051c19cddc4> <urn:uuid:6132e2dc-5f7b-4896-a323-5051c19cddc4> <urn:uuid:77b73ca4-7e26-4aca-8b0a-e3e7dc89dadd> <urn:uuid:77b73ca4-7e26-4aca-8b0a-e3e7dc89d", "input (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Technical Field\nThe present disclosure relates to semiconductor structures, and in particular to a semiconductor structure having multiple semiconductor layers with embedded metal wiring and a method of manufacturing the same.\nDescription of the Related Art\nIn recent years, the microelectronics industry has experienced a rapid increase in the levels of integration of electronic components. While there has been a corresponding reduction in the size of electronic components, improvements are being made to ensure that the components remain functional. To this end, conductive structures are used to carry electrical current in microelectronics. As the level of integration increases, multiple conductive layers are placed closer together and may need to be interconnected. To accomplish this, metal wiring can be used to provide electrical connections between the multiple conductive layers. Embedding metal wiring into a semiconductor structure is a challenge due to the high temperatures used in various fabrication processes", "ibe:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nRelated work\n\nThe WMT14 English-German text generation task was first introduced in 2014 at the Workshop on Machine Translation in the Twenty-Twentieth Century (WMT). Since then, several research efforts have been devoted to this task including evaluating various language models. Currently the state of the art model achieves BLEU score of 39.41, which is 7.4 BLEU points higher than the previous state of the art model on the English-French translation task. The approach taken by the current state of the art model is using the decoder unit of the transformer language model, which", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>/* This Source Code Form is subject to the terms of the Mozilla Public\n * License. v. 2.0. If a copy of the MPL was not distributed with this\n * file, You can obtain one at http://mozilla.org/MPL/2.0/.\n */\n\nfunction TEST_BRANCH() {\n  var case1 = \"case1\";\n  var case2 = \"case2\";\n\n  return [\n    case1,\n    case2\n  ];\n}\n\nTEST_BRANCH().then(function(results) {", "|sparql|:\\s*SELECT?paper?model_lbl where { :}\n\n Initialize the variable model.\n :model: a resource model.\n\n Initialize the variable benchmark.\n\n :benchmark: a resource that specifies an evaluation context.\n\n Initialize the variable paper.\n\n :paper: a paper document.\n\n Initialize the variable evaluation.\n\n Initialize the variable model.\n\n Initialize the variable paper.\n\n Initialize the variable dataset.\n\n Initialize the variable has_model.\n\n Initialize the variable has_dataset.\n\n Initialize the variable has_evaluation.\n\n Initialize the variable has_benchmark.\n\n Initialize the variable paper_label.\n\n Initialize the variable has_model_label.\n\n Initialize the variable has_dataset_label.\n\n Initialize the variable has_evaluation_label.\n\n Initialize the variable has_benchmark_label.\n\n Initialize the variable benchmark_label.\n\n Initialize the variable model_label.\n\n Initialize the variable paper_label.\n\n Initialize the variable dataset_label.\n\n Initialize the variable evaluation_label.\n\n Initialize the variable has_model_label.\n\n Initialize the variable has_model.\n\n Initialize the variable has_dataset.\n\n Initialize the variable has_dataset_label.\n\n Initialize the variable has", "n.?benchmark\nIf no paper is referenced, but the BiDAF + Self Attention + ELMo model is used in a paper, the Cross-Document Link resolver available at the Cross-Document Link resolver project repository at https://github.com/cross-link/cross-document-link resolver can be used to issue a SPARQL query to obtain the code references.\n\n\nIn the following SELECT clause, the DISTINCT keyword is used to select a single row for each distinct combination of model, model label, and model from the query results returned by the query.\n\nA model is identified by the WHERE clause as?model and its label is identified as?model_lbl.\n\nThe following statements are true for each combination of?model and?model_lbl:\n\n-?model =?model_lbl indicates that both?model_lbl and?model are proper noun labels for the same model.\n-?model!=?model_lbl indicates that neither?model_lbl nor?model are proper noun labels for the same model.\n-?benchmark =?model_lbl indicates that?model_lbl is a proper noun label for a benchmark", "input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?metric?model?metric_lbl. FILTER (str(?metric) = \"Specificity\")?metric_lbl. FILTER (str(?metric) = \"TPR\")?true_positives. FILTER (str(?metric) = \"FPR\")?false_positives. FILTER (str(?metric) = \"TN\")?tpr. FILTER (str(?metric) = \"SP\")?specificity. FILTER (str(?metric) = \"FP\")?false_positives. FILTER (str(?metric) = \"TN\")?fpr. FILTER (str(?metric) = \"TP\")?tpr. FILTER (str(?metric) = \"FP\")?fpr. FILTER (str(?metric) = \"Sens\")?sensitivity. FILTER (str(?metric) = \"Spec\")?specificity. FILTER (str(?metric) = \"FPR\")?fpr", "n code references in papers that have used the 6-layer QRNN model for benchmarking purposes:\n// First identify the code in the paper that you want to reference\nstring code_from_paper = \"// First identify the code in the paper that you want to reference\";\nSparql query query = \"SELECT DISTINCT?code FROM {code_from_paper} WHERE {?code?text }\";\n// Now run the query and collect the results into a collection\nList<SparqlResult> results = SPARQLService.query(query);\nforeach (SparqlResult result in results) {\n    foreach (Map.Entry<String, Object> entry in result.results().values()) {\n        System.out.println(entry.getKey() + \": \" + entry.getValue());\n    }\n}\n\n<|endoftext|>/*\n * Copyright (c) 2013, Sony inches (http://www.addrup.com/)\n *\n * Based on addrup-bootloader.ld (http://www.addrup.com/code/addrup-bootloader.ld)\n *\n * This program is free software; you can redistribute it and/or modify it under the", "query1\n |-\n |  SELECT DISTINCT?model?model_lbl \n |  WHERE {?dataset a orkgp:HAS_MODEL?model.\n | ?model rdfs:label?model_lbl. \n |  FILTER (str(?model) = \"SAS Mount\")\n |  FILTER (str(?model) = \"GRNN\")\n |  FILTER (str(?model) = \"Neural Net\")\n |  FILTER (str(?model) = \"Artificial Neural Net\")\n |  FILTER (str(?model) = \"Multilayer Perceptron\")\n |  FILTER (str(?model) = \"Support Vector Machine\")\n |  FILTER (str(?model) = \"Decision Tree\")\n |  FILTER (str(?model) = \"Random Forest\")\n |  FILTER (str(?model) = \"Naive Bayes\")\n |  FILTER (str(?model) = \"Confusion Matrix\")\n |  FILTER (str(?model) = \"Reproducibility)\n |  FILTER (str(?model) = \"Benchmark\")\n |  FILTER (str(?model) = \"Analysis of Variance\")\n |", "|\n  SELECT DISTINCT?benchmark?model_lbl WHERE {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n  in entry {\n    \"Entities and relations annotated corpus (NYT24)\",\n    \"Benchmarking model performance on the NYT24 dataset\",\n    \"Benchmarking of machine learning models on the NYT24 dataset\",\n    \"Benchmarking the performance of a machine learning model on the NYT24 dataset\",\n    \"Entity Similarity Benchmarking\",\n    \"Entity Similarity Benchmarking with Mastlet\",\n    \"entity2vec: Entity Similarity Benchmarking using Entity2vec\",\n    \"Novel Benchmark of Graph Transformation Models on the NYT24 Dataset\",\n    \"Strong Performace Benchmarking on the NYT24 Dataset\",\n    \"Ontology-Based Benchmark", "input (English text): Datasets that have been used for benchmarking in the Robot Navigation research problem:\n\n output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Robot Navigation research problem\") }\n\n input (English text): SELECT DISTINCT?dataset?dataset_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Robot Navigation research problem\") }\n\n output (Sparql query): Dataset: orkgp:HAS_DATASET\n\n dataset label: Robot Navigation research problem\n\n output (Sparql query):dataset_lbl:Label\n\n output (Sparql query): SELECT DISTINCT?dataset?dataset_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Robot Navigation research problem\") }\n\n input (English text): SELECT DISTIN", "--------------------------------------------------------\n #Query 1\n SELECT DISTINCT?model?model_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n ---------------------------------------------\n Apollo DB Server [ApolloDB]\n Apache Spark [Apache Spark]\n Apache Spark + ML Flows [Apache Spark + ML Flows]\n DeepGlove [DeepGlove]\n Logistic Regression [Logistic Regression]\n Linear Regression [Linear Regression]\n Simple Bayesian Networks [Simple Bayesian Networks]\n\n 2. Apollo DB Server [ApolloDB]\n\n 3. Apache Spark + ML Flows [Apache Spark + ML Flows]\n\n 4. DeepGlove [DeepGlove]\n\n 5.", "input (English text):  SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): List of model labels :\n MongoDB\n Neo4j\n Microsoft Azure ML\n Apache UIMA\n Apache Mahout\n Lucene\n Linq To SQl\n Stanford CoreNLP\n Apache Spark ML API\n Apache TinkerPop 3\n Tensorflow\n DistilBERT\n Google TPU\n Microsoft Azure ML API\n Apache UIMA API\n Bing Text Analysis API\n Tensorflow.Slim\n DistilBERT.Slim\n HuggingFace TokenAPI\n Optical Character Recognition (OCR)\n Stanford CoreNLP ORC", "sel (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Code references for the PEGASUS model can be found in the PEGASUS paper.\n output (Sparql query): v\u00e9sel (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALU", "strue \noutput (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Audio Classification research area\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Tonga national under-20 football team\n\nThe Tonga national under-20 football team represents Tonga in international under-20 football competitions. The team is controlled by the Tonga Football Association and is directed by Kurt Koinange.\n\nCompetition records\n\nFIFA U-20 World Cup record\n\nGold Cup record\n\n2014\n\nGroup stage\nUp to and including quarter-finals:\nDefeated Iran 1-0 (2-0, 1-0)\nLost to Uzbekistan 1-3\nDefeated", "model = Cart Pole OpenAI Gym (0.52).rdfs:label = Cart Pole OpenAI Gym.\n\nThe benchmark for the Cart Pole (OpenAI Gym) dataset is:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\"))?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThe metrics and scores that have been reported for the Cart Pole (OpenAI Gym) model are:\nOPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\nOPTIONAL {?paper orkgp:HAS_EVALUATION?eval.?eval rdfs:label?eval_label. }\nOPT", "input (English text): SELECT DISTINCT?paper WHERE { {?paper orkgp:HAS_BENCHMARK?benchmark. } UNION {?benchmark orkgp:HAS_MODEL?model. } } ORDER BY?paper?paper_lbl?paper_id ;\n\n procedural query:\n\n #!/usr/bin/env pythonsql\n cursor = connection.cursor()\n\n cursor.execute(\"\"\"SELECT DISTINCT \n Paper.paper_id AS paper_lbl, Paper.paper_title, Paper.paper_author, Benchmark.model_id AS model_id\n FROM Paper\n  LEFT JOIN Benchmark ON Paper.paper_id=Benchmark.paper_id\n WHERE Benchmark.model_id IS NOT NULL\n ORDER BY Paper.paper_title, Paper.paper_author, Paper.paper_id\"\"\")\n\n procedural query:\n\n #!/usr/bin/env pythonsql\n cursor = connection.cursor()\n\n cursor.execute(\"\"\"SELECT DISTINCT \n Paper.paper_id AS paper_lbl, Paper.paper_title, Paper.paper_author, Benchmark.model_id AS model_id\n FROM", "model_lbl : Microsoft COCO. Its label is the highest on the ImageNet benchmark dataset.\n\nThe query above correctly returns the label of the best model achieved on the imageNet benchmark dataset, which is Microsoft COCO.\n\n work = \"Benchmark models on the ImagesNet benchmark dataset for orkgp:HAS_MODEL and orkgp:HAS_EVALUATION\"\n\n model = \"Microsoft COCO\"\n\n benchmark = \"ImageNet benchmark dataset\"\n\n work_sem = \"orkgp:HAS_MODEL\"\n\n eval_sem = \"orkgp:HAS_EVALUATION\"\n\n annotation_corpus = \"Abstracts' entities and relations annotated corpus\"\n\n analysis_sem = \"rdfs:label\"\n\n model_sem = \"rdfs:label\"\n\n work_sem_text = \"orkgp:HAS_DATASET\"\n\n eval_sem_text = \"orkgp:HAS_BENCHMARK\"\n\n analysis_sem_text = \"?paper orkgp:HAS_MODEL\"\n\n model_sem_text = \"?model\"\n\n query = \"\"\"SELECT DISTINCT?model?model_lbl WHERE", "iter : SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2005\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): ACE 2005 (http://www.nlm.nih.gov/mesh/ACE.pdf) is a benchmark for knowledge-based reasoning systems. The benchmark contains 7 annotations for 12 relations on 14 published biomedical abstracts. The annotations include the mentions of the 12 relations and the mention of the 14 abstracts that these relations refer to.\n\n output (Sparql query): 7 : SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE", "n:SELECT DISTINCT?metric WHERE {?metric rdfs:label \"Distinctness\"@en. FILTER (str(?metric) = \"1\")?metric?model.?model rdfs:label?model_lbl. }\n input (English text): For each model listed in the previous answer, also list the metric that is used to evaluate it on the DBpedia benchmark dataset.\n output (Sparql query): SELECT DISTINCT?metric?model_lbl WHERE {?metric rdfs:label \"Distinctness\"@en. FILTER (str(?metric) = \"1\")?metric?model.?model rdfs:label?model_lbl. }\n input (English text): Below are the metrics that are used to evaluate models on the DBpedia benchmark dataset:\n  * number of distinct values\n  * number of values\n  * the number of new relations present in the predictions \n  * the number of false positives\n  * the number of false negatives\n  * the number of new facts present in the predictions\n  * the number of facts in the gold standard that are not in the predictions\n  * the number of facts in the gold standard", "model is \"Deep Q-Network\" with a value of 0.6312.\nThe Deep Q-Network model was benchmarked on the \"Abstracts' entities and relations annotated corpus\" dataset with a value of 0.6312.\n\n # This query can be used to query for the top-resulting model over the dataset Atari 2600 Enduro.\n SELECT DISTINCT?model?model_lbl\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n # Top-resulting model: \"Deep Q-Network\" with a value of 0.6312.\n # \n# The \"Deep Q-Network\" model was benchmarked on the \"Abstracts' entities and relations", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Here are some examples of models which have been benchmarked on the Hutter Prize dataset: TIMIT, CLARK, HELIB, CRF, GRU, RNN, Attention.\n output (Sparql query):ocation (Sparql way):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DAT", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n input (English text): The highest benchmark result for the Seel.cse.lsu.edu/data/re17.zip dataset has a score of 0.89535 and a metric of 0.97828. The top two best models are:\n - the gated recurrent unit (GRU) with a score of 0.98526 and a metric of 0.97828\n - the long short-term memory (LSTM) with a score of 0.93589 and a metric of 0.98702\n output (S", ": SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Wizard of Wor\")?benchmark. OPTIONAL {?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The top score is 84.82 with an evaluation metric of 34.72. The model name is Single Board Computer.\n\n preceding text : The model that achieved the highest score on the Abstracts' entities and relations annotated corpus dataset was the Single Board Computer with a score of 84.82. The Single Board Computer model achieved a score of 84.82 on the Atari 2600 Wizard of Wor dataset, which is an evaluation metric of 34.72.\n\n preceding text : The model that has been benchmarked on the Abstracts' entities and relations annotated corpus dataset is the Single Board Computer.\n\n preceding text : The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are", "net:https://openlib-benchmark.org/benchmark.php?id=transfxsl#paper=PaperID&id=AuthorID&version=version#method=trf_performance&method_id=1&benchmark=abstracts%20entities%20and%20relations%20annotated&evaluation=&model=12-layer%20Transformer-XL%20Model%20Benchmark\n\n\nFollowing query returns only 2 papers that have used the 12-layer Transformer-XL model: \nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\nHere is the second returned paper:\nhttps", "mark(?model_lbl, \"Atari 2600 Up and Down\")\n\nA:\n\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe variables?model,?model_lbl and?benchmark are bound in the FROM clause but not in the SELECT clause. The solution is to add variables in the SELECT clause.\n\nHowever, I think you may be interested in the sparql query:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl)", ": SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only) benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): I see that the model in the selected paper has the following labels:,,,, and. The paper mentions that these models have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset. \n output (Sparql query):}{{\\usefont{U}{ua}{m}{n}x}}SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Jacquard dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): \"Jacquard dataset\"\n\n<|endoftext|>Valeriy Shmytyov\n\nValeriy Vasilyevich Shmytyov (; born 3 January 1994) is a Russian professional football player. He plays as a forward.\n\nClub career\nHe made his Russian Premier League debut for Khimki on 19 March 2013 in a game against Ural Yekaterinburg.\n\nExternal links\n \n Career summary at sportbox.ru\n\nCategory:1994 births\nCategory:Living people\nCategory:Russian footballers\nCategory:FC Khimki players\nCategory:FC Anzhi M", "input (English text): What are the metrics of evaluation over the ObjectNet dataset?\n output (Sparql query):SELECT?benchmark?model WHERE {?model rdfs:label \"NACOL\"@en. FILTER (str(?benchmark) = \"ObjectNet\")?benchmark. OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset. FILTER (str(?dataset) = \"ObjectNet\") } }\n input (English text): NACCOL is a metric used in the ObjectNet dataset for evaluating SPARQL query performance.\n\numn\tThe metrics of evaluation over the ObjectNet dataset are the number of subqueries over the number of total query-plan executions, and the time taken to execute a single query plan.", "_provide_a_list_of_research_paper_titles_and_ids_that_have_benchmarked_models_on_the_enwik8_dataset?\n input (English text): None\n output (English text): A list of research paper titles and IDs that have benchmarked models on the enwik8 dataset: \n output (English text): |  | Research paper title |  | ID | \noutput (Sparql query):\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2SELECT DISTINCT?paper?benchmark WHERE {?paper orkgp:HAS_EVALUATION?eval; orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") OR?paper orkgp:HAS_DATASET?dataset. } \n sacrificed (Sparql query):\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2SELECT?paper?benchmark WHERE {?paper orkgp:HAS_EVALUATION?eval; orkgp:HAS_BENCHMARK?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\") OR?paper orkgp:HAS_DATAS", "mark = \"Hybrid model\"\n\n infrastructure: open grafana with https://<server>/auth/flask\n endpoint : |\n graphite URI : https://<graphite uri>/ws/v1/graphite\n Graphite Receiver: |\n graphite port: 5432\n\n Parameters :\n benchmark: \"FGVC Aircraft\"\n\n Query :\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FGVC Aircraft\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>HPLC determination of flunitrazepam, pentazocine, carbamazepine and trimetroprim in plasma by isocratic elution with UV detection.\nA rapid, precise, and specific HPLC method for the determination of flunitrazepam,", "input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The following are the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset.\n output (Sparql query):SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DAT", "n 1 2 3 4 5 6 7 SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Road Runner\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\n**Tuning the Atari 2600 Road Runner Model for PQSP in Parser Queries: <a href=\"http://papers.nips.cc/paper/5172-going-deep-on-lsr.pdf\" target=\"_blank\">Going Deep on LSR</a> <small>(Author: Quoc Tran; ArXiV: <a href=\"https://www.arxiv-vanille.org/html/papers/archive/2007/NIPS-2007-LSQ.PDF\" target=\"_blank\">https://www", "\u0435\u043d\u0442\u044b:{}\n OPTIONAL{?model rdfs:label \"Has a dataset\". } OPTIONAL{?model_lbl \"Has a label\". }\n FILTER(str(?model_lbl) = \"Abstracts' entities and relations annotated corpus\")\n SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n<|endoftext|>lue?  (a) 2/7  (b) 0.1  (c) 5  (d) 25\na\nWhich is the third smallest value?  (a) -3/10  (b) -1/2  (c) 7\nc\nWhat", "set { \"Abstracts' entities and relations annotated corpus\" dbo:WMT2014 french-english Dataset }\n output (Sparql query): SELECT DISTINCT?metric?model WHERE {?dataset a dbo:WMT2014 french-english Dataset ; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL {?benchmark dbo:WMT2014 french-english BenchmarkEvaluationMetric }?metric }\n\n input (English text): Performance metrics commonly used in language model evaluations include BLEU, METEOR, and CIDEr.\n\n output (Sparql query):SELECT DISTINCT?metric?model WHERE {?dataset a dbo:WMT2014 french-english Dataset ; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL {?benchmark dbo:WMT2014 french-english BenchmarkEvaluationMetric }?metric }\n\n output (Sparql query): BLEU: bic (language model", "inal (Squealing Pig-Latin:) SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (Squealing Pig-Latin): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:", "n. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n\n ----- \n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\n ----- \n output (Sparql query): {\"model\":\"ELMoModel\",\"model_lbl\":\"ELMo\"}\n input (English text): Sure! The following link may help: https://github.com/databricks/xx-sparql-demo.\n\n ----- \n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n\nThe model labels can be found in the result.\n\nAbstracts' entities and relations annotated corpus is a benchmarking dataset for IR. It contains RDF/XML, EDAM and OWL files which contain relation detection and entity linking annotations for a number of IR systems.\n\n\nThe most commonly evaluated IR systems on AESLC are the following:\n- PySQUAD 2.0  \n- Stanford CoreNLP \n- Mentis MiniT \n- RosettaStone\n\n\nHere are the names of the most commonly evaluated models:\n\n- Mentis MiniT\n- Ros", "::= \"SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\"\n\n input (English text): \n paper_1\tHoC model paper_1\n paper_2\tHoC model paper_2\n\noutput (Sparql query): \n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK", "nement code references in papers that have used the PAR Transformer Large model for benchmarking purposes?\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark.?model?model_lbl. } \n input (English text): Here is the list of models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset:\n+------------------+--------------------+\n| Benchmark Model  | Paper Reference     |\n+------------------+--------------------+\n| PAR Transformer  | Agarwal et al.,    |\n+------------------+--------------------+\n| ELMo             | Ali et al.,         |\n+------------------+--------------------+\n| BERT             | Devlin et al.,      |\n+------------------+--------------------+\n| GPT-3            | Guo et al.,", "input (English text): I'll take a look into it and provide you with the links, thank you.\n output (Sparql query): SELECT?model?model_lbl WHERE {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Thank you!\n\n<|endoftext|>A service provider can provide access to various services to various types of customers. In some cases, a service provider may offer a cloud-based service. A cloud-based service can be provided as a service over a network (e.g., the Internet) by a server that has some or all of the service's functionality implemented as a utility function running on the server. One or more clients can communicate with the server and/or a selected utility function, and may be able to access data and functionality of the server from a mobile device.<|endoftext|>How to Watch the CFP Cup in Seattle\n\nWashington Athletic Club in Seattle\n\nDate/", "SELECT DISTINCT?paper?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model;?paper orkgp:HAS_MODEL?model. FILTER (str(?paper) = \"CL-Titles-Parser\") } \n input (English text): The CL-Titles-Parser model is from the papers \"Nguyen Tuan Bao, Muhammad Mahmud, Nick Hope, Andrew McCallum, Constantin Roman. On paraphrasing and topic extraction for paragraph classification. Conference on Machine Translation of Abstracts. AMTA-18. [pdf].\", \"DBLP:conf/lrec/DBLP1TM15\", \"Wojciech Maciejewski, Tomasz Kowalski, \u0141ukasz Rudzi\u0144ski. onprop: A Paraphrase Assistant. CoNLL 2017. [pdf]\", and \"Richard Socher, Daniel Cohen, Andrew McCallum, Andrew Yates. Large Language Models for Semantic Paraphrasing. CoNLL 2013. [pdf].\" These papers also include the code for the CL-Titles-Parser model.\n output (Sparql query): SELECT", "EXEC sparql(FORMAT VARCHAR) \n               BEGIN \n                    SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Habitat 2020 Object Nav test-std benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n               END\n\n<|endoftext|>Q:\n\nInserting KALMAN filter into Kalman smoother\n\nIm reading a book on signal and image processing and I ran across the following theorem:\nIf we have a filter $H(k)$ with the recursion \n$$\nx(k+1) = H(k) x(k) + w(k)\n$$\nwhere $w(k)$ is white noise and the $H(k)$ is the Kalman filter\nthen\n$$\nx(k", "output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Name This Game\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text):?model  \n output (Sparql query): models are being evaluated on the Atari 2600 Name This Game dataset are the following : DeepMind AlphaGo, DeepMind AlphaGo Tournament, AlphaStar, AlphaZero, Deimokobu.", "SELECT DISTINCT?benchmark?model WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): The highest benchmark result achieved on the Abstracts' entities and relations annotated corpus dataset was 37.6 with the model named Transforms of DTs. The model was trained on a TensorFlow Model and this resulted in a performance of 37.6 frames per second.\n\n<|endoftext|>This invention relates generally to weld joints in subterranean zones and more specifically to improved hole exploration and logging techniques for determining the presence and/or depth of subterranean zones having previously detected resistivity anomalies.\nWhen drilling a borehole to explore for and produce oil, gas, or minerals in subterranean", "output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgp:HAS_MODEL?model;?model rdfs:label?model_lbl. }\n\n Add?model to the query and receive?model_lbl in the result", ":SELECT?paper?model_lbl WHERE {?paper orkgp:HAS_MODEL?model; orkgp:HAS_EVALUATION?eval;?eval owlbench:benchmark(?model). FILTER (str(?model) = \"AlexNet\")}. \nThe benchmark resources include the following resources. \n- orkgp:HAS_DATASET?dataset\n- orkgp:HAS_EVALUATION?eval\n- orkgp:HAS_MODEL?model\n- orkgp:HAS_BENCHMARK?benchmark\n- owlbench:benchmark(?model)", ":SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Ms. Pacman\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): Atari 2600 Ms. Pacman is an MSGMA prototype game developed in the 1970s by Michael Babbage, executed on the Atari 800 and 2600 home computers. The original Ms. Pacman game does not feature any level progression and has no over-the-phone play or online multiplayer. The latest official version of the game from the developer, Tecmo, is set in 2029 and contains level progression, over-the-phone play and online multiplayer.\n\n The models evaluated on Atari 2600 Ms. Pacman are:\n\n - Arcade-", "opt:option no-logical-relations true \n(Sparql Query 1) SELECT DISTINCT?benchmark?benchmark_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe evaluation metrics that have been commonly used on the Habitat 2020 Object Nav test-std dataset include:\n  - iou (Intersection-over-Union) between detection and ground-truth bounding boxes: $IoU = \\frac{TP}{TP+FP+FN} \\times 100$\n  - mIou (Micro-averaged Intersection-over-Union) between detection and ground-truth bounding boxes: $MIOU = \\frac{1}{N}\\sum_i", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n <dairy-producer model_label=\"Baseline\" model=\"elasticsearch\" paper=\"Bray et al. 2016\" eval=\"0.7323693028921323\"/>\n <dairy-producer model_label=\"Elasticsearch\" model=\"Elasticsearch\" paper=\"Bray et al. 2016\" eval=\"0.7323693028921323\"/>\n <dairy-producer model_label=\"Elasticsearch\" model=\"KDD\" paper=\"Bray et al. 2016\" eval=\"0.7323693028921323\"/>\n <dairy", "rdfs:label?model_lbl\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset;?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?\n output (Sparql query): rdfs:label?model_lbl\n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset;?benchmark orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.", "yntaxerror: The following query does not make sense: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe query is valid if the following two papers are included in the result:\n\n  * Bougouin et al. \"A User Model for Database Search\" (2009)\n  * Bougouin et al. \"User-Friendly Querying of Annotated Databases\" (2011)\n\n The following papers contain a benchmark over the DBpedia dataset:\n\n  * Wang et al. \"Breadth-First Web Search using User Models\" (2011)\n  * Chen et al. \"Searching for Relations in Knowledge B", "ition (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. }\n\nQuery Results:\nZeplin:\nZeplin, CD, et al. (2019): Analyzing Media-Needs with the Zeplin User Model.   Code: <https://github.com/Zeplin/Zeplin/blob/master/papers/papers/Zeplin_2019.Rmd> \n\nUsers of the Zeplin User Model (Zeplin, CD, et al. (2019)): \nUsers of the Zeplin User Model (Zeplin, CD, et al. (2019)): \nFacebook:\nZhou, X., Wong, C.H., & Agarwal, S. (2018). Page-level Fine-grained Task Recognition with Recurrent Neural Network. The 22nd ACM SIGKDD International Conference on Knowledge discovery and data mining. \n\nUsers of the Zeplin User Model (Zeplin, CD, et al. (2019)): \nUsers of the Zeplin User Model", "ined: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\nThis query returns the following result set:\npaper_label_model_label | paper_label | model_label | model | paper | benchmark_label | benchmark_label\n\n  http://www.cs.toronto.edu/~kingsley/papers/paper_label_model_label.pdf | Abstracts' entities and relations annotated corpus | T-ConvS2S | T-ConvS2S | http://www.cs.toronto.edu/~kingsley/papers/paper_label_model_label.pdf | T-ConvS2S |", "end { \"paper\" <http://arxiv.org/find/q-bert/1/> \"model\" qbert }\n\nWe can answer this query by executing the following SPARQL query:\nSELECT DISTINCT?model?model_lbl\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n<|endoftext|>Sen. Bernie Sanders Bernie SandersButtigieg stands in as Pence for Harris's debate practice Bernie Sanders warns of 'nightmare scenario' if Trump refuses election results Harris joins women's voter mobilization event also featuring Pelosi, Gloria Steinem, Jane Fonda MORE (I-Vt.) on Tuesday touted his newly announced progressive presidential campaign by hitting Republican presidential candidate Donald Trump for comments suggesting vaccines cause autism", "Query: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \nresults:\n* 1. ELMoBERT: 0.750 related Precision (r) and Recall (r)\n* 2. GPT-2: 0.751 related Precision (r) and Recall (r)\n\n\nFor the model with the best score, what is its name and its acronym?\n\nELMoBERT: ELMoBERT: ELMoBERT: ELMoBERT\n\n\nAnd for the second best score, what is its name and its acronym?\n\nGPT-2: GPT-2: GPT", "n: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Common Sense Reasoning datasets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } } \n\nThe query will return the following:\n\nDatasets benchmarked:  \"Databricks' Employees\", \"Web-Annotated\", \"DeepL...\", \"Neural Machine Translation\"\nModels benchmarked: \"SQuAD 2\", \"SQuAD 1\", \"GPT-3\", \"FastQA\", \"XAI\", \"XAI-ASR\", \"BiDAF\", \"NER-BioASQ\"\n\nA:\n\nBased on this dataset the benchmarks include the following machine learning models:\n\nSQuAD 2\nSQuAD 1\nGPT-", "?model_lbl WHERE {?model_lbl rdfs:label \"AUC\" }\n\nThe response is:\nSELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkgp:HAS_BENCHMARK?benchmark. OPTIONAL {?paper orkgp:HAS_MODEL?model.?model rdfs:label?model_lbl. } }\n\n Dataset Entity Relations Learning (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.?paper orkg"], "cleaned_sparql": ["SELECT DISTINCT ?benchmark ?model_lbl ?model_score WHERE { ?benchmark ?model_lbl. FILTER (str( ?model_lbl) = \"Story Cloze Test model\").} <|endoftext|>// -*- C++ -*- //============================================================================= /** * @file Monitor_Mutex.h * * $Id: Monitor_Mutex.h 91293 2010-07-21 19:14:20Z johnnyw $ * * This class provides synchronization based on a single, shared mutex. * * @author Don Hinton <dhinton@dresystems.com> * @author Markus Wagner <mt.wagner@edv.com> */ //============================================================================= #ifndef ACE_MONITOR_MUTEX_H #define ACE_MONITOR_MUTEX_H #include \"ace/Handle.h\" #if!defined (ACE_LACKS_PRAGMA_ONCE) #pragma once #endif /* ACE_LACK", "SELECT DISTINCT ?paper ?model_lbl ?benchmark WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label ?model_lbl; } input (English text): ILP Conjunction over Languages Problem: A Parameterized Algorithm D. Cleary, T. Chandra, P. Fader, D. Guo, L. Liviu, H. Neven, Y. Yu, and Y. Zhang. SOGGIO: A Sequential Optimization-based Model Checking Tool for Monolithic Object Code D. Guo, H. Neven, and Y. Yu. Integrating Model Checking and Program Synthesis to Detect Software Errors Raja N. Khattra, Nataraj K. Sahoo, and Noel T. Taylor. Verifying Hardware-Aided Architectures with Simulink Components Using Model-Checking Techniques C. Koo, D. Guo, and L. Liviu. Proactive Corrective Action in Embedded Control Systems Using Model Checking Techniques A. Choudhury, N. R", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q: how to pass multiple parameters in formaction tag in yii framework here i am passing one parameter in formaction but it is not passing so please help me to resolve this issue <form action=\"< ?= CHtml::url(array('controller', 'action'), $_SERVER['PHP_SELF']) ?>\" method=\"post\"> <label>Select Company</label> <select name=\"cname\" > <option value=\"\">Select Company</option> < ?php $count = 1; while($data=mysql_fetch", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model; ?dataset orkgp:HAS_EVALUATION ?eval; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The following research paper titles have benchmarked models on the Penn Treebank dataset: - Abner, Joseph and To RT Dosi. Statistical Architectures for Statistical Machine Translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2010. - Bengio, Sergio and Isenberg, Stella. A flexible large vocabulary language model. In Proceedings of the 18th international conference on Machine learning. ACM, 2009. - DBLP, Sergei, Breuning, Timme,", "SELECT ?metric ?benchmark_name # WHERE { # ?benchmark_name rdfs:label ?benchmark_label. # ?benchmark_label Mazea. # ?benchmark_name Mazea. # ?benchmark_name rdfs:label ?model_label. # ?model_label Abstracts' entities and relations annotated corpus. # ?model Mazea. # ?model Abstracts' entities and relations annotated corpus. # ?model_label Mazea. # ?model_label SparqlQueryEvaluation. # ?model_label SparqlQueryEvaluation win %. # ?model_label SparqlQueryEvaluation rdfs:label ?model_label. # ?model_label SparqlQueryEvaluation win %. # OPTIONAL { ?benchmark_name SparqlQueryEvaluation. ?metric DecimalMath. } # } Abstracts' entities and relations annotated corpus achieved an overall Top-5 accuracy of 86.92 % on the MAZEA benchmark dataset. The model used was SparqlQueryEvaluation", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?model_lbl. } input (English text): TDMSci dataset contains models from the following domains: Financial Transactional Databases (Falcon, Arcadia, MLite), Databases of Events (Duel), Context Free Grammars (OCaml), Query Representation via Finite State Machines (qr), and Anchored Data Language (QADL). output (Sparql query): :SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?model_lbl. } input (English text): The following are the models that have been benchmarked on the TDMSci dataset: Financial Transactional Databases: MLite, Arcadia, Falcon Databases of Events: Duel Context Free Gramm", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Carbon-based fuel\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Carbon-based fuel Mean capacity (SDP, acid-neutralized) 630 (27) SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl; FILTER (str( ?dataset_lbl) = \"Carbon-based fuel\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_B", "SELECT DISTINCT ?paper ?paper_lbl WHERE { { ?paper orkgp:HAS_BENCHMARK ?benchmark. } UNION { FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; ?paper orkgp:HAS_MODEL ?model; } } OPTIONAL { ?paper orkgp:HAS_EVALUATION ?eval; } } This query returns the following results: +-+-+ |paper |paper_lbl | +-+-+ |paper1 |ABSTRACTS' entities and | | |relations annotated corp | | |s | +-+-+ |paper2 |MLDoc Zero-Shot | +-+-+ The paper papers that have benchmarked the MLDoc Zero-Shot model on the Abstracts' entities and relations annotated corpus dataset.", ": Keras with a LSTM model description: Keras with a LSTM model was the model that performed the best on the Kuzushiji-MNIST benchmark dataset, scoring an accuracy of 96.3%. This model was trained on the MNIST dataset. Keras with a LSTM model was the model that performed the best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } selects only the papers and the models for which there is some evaluation available on the WMT2016 Romanian-English benchmark dataset. The result contains the paper, the model and its label.", "SELECT DISTINCT paper.title AS paper_title, model.name AS model_name, model.label AS model_label, evaluation.metric AS evaluation_metric, evaluation.value AS evaluation_value FROM abstracts' entities and relations annotated corpus benchmark, paper, model, evaluation WHERE paper = ANY (paper) and model = ANY (model) and evaluation = ANY (evaluation) OPTIONAL FETCH FIRST 1 paper: paper.title model: model.name evaluation: evaluation.metric <|endoftext|>Can aneurysms of the interventricular septum complicating Hodgkin's disease be expected ? The authors report a rare case of an interventricular septal aneurysm in a 16-year-old female with Hodgkin's disease (HD). TheHD course was complicated by mediastinal and peribronchial infiltration with pleomorphic lymphocytes. The anatomical evaluation, assisted by a computer-assisted tomographic", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Top-1 Accuracy score when benchmarked on the VTAB-1k dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Optionally, include ?model_lbl in the result set Result set: | model | model_lbl | |-|-| | NgRNN | Top-1 Acc| | XGBoost | Top-1 Acc| | Lightgbm | Top-1 Acc| | CNN-LM | Top-1 Acc| <|endoftext|>Q: Move files from one folder to another one if their modification date is greater than a date I want to copy some files from one", "SELECT ?paper ?benchmark ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Title: The Effect of the Combined Query and Request Clauses on Protocol Performance in the Web Service Layer. ID: 2358 Benchmark: arXiv's Dataset. Model: arXiv's Dataset. ancel input (English text): Title: The Effect of the Combined Query and Request Clauses on Protocol Performance in the Web Service Layer. ID: 2358", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark", "SELECT DISTINCT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): 1. Mukherjee A, Jin R, Liu X, et al. [Performace Evaluation of Lung Cancer Type-Specific Subtyping Using Dawkins Search on CancerDTI]. BMC Bioinformatics. 2019;20:49. doi: 10.1186/s12859-019-2075-9. 2. Huang D, Ouyang J, Wu L, et al. [Benchmarking Type-level Similarities of Schema.org Relationship Language with Dawkins Search]. BMC Bioinformatics. 2019;20:49. doi: 10.1186/s12859-019-3218-2. 3. Chen S, Pei X, Wang J, et al. [Benchmarking Knowledge GraphSearch with Bleu: A Large-scale Evaluation]{}. BMC Bioinformatics. 2019;20:49. doi: 10.1186/s12859-019-4570-4. 4. Zhang L, Tian Q, Wang", "SELECT DISTINCT ?model ?model_lbl FROM ?benchmark WHERE { ?benchmark orkgp:HAS_MODEL ?model; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <!- <!- Result of the query -> <!- The name of the top performing model -> <!- BIRD -> \"BIRD\" <!- <!- ABHDA -> \"ABHDA\" <!- IBM Watson -> \"IBM Watson\" <!- OPENAI Gym -> \"OPENAI Gym\" <!- <!- DDPG -> \"DDPG\" <!- Protorotor -> \"Protorotor\" <!- GPT-3 -> \"GPT-3\" <!- Tabula-Restricted", "SELECT * { ?metric ?model ?model_lbl } ORDER BY ?metric DESC browse label relations Benchmark browse { ?model model_name } { ?model Model. ?model_name Model_name. } LIMIT 10 Model_name: Accuracy: - Deep Features: Depthwise MLP (76.6ms) - Deep Features: LSTM (62.1ms) F1 Score: - Deep Features: Depthwise MLP (0.70) - Deep Features: LSTM (0.67) Time (in milliseconds): - Deep Features: Depthwise MLP (76.6ms) - Deep Features: LSTM (62.1ms) Precision: - Deep Features: Depthwise MLP (0.93) - Deep Features: LSTM (0.92) Recall:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A titled paper can contain multiple models and there is one benchmark for each model. RESULT: { \"model\": \"resnet18\", \"model_lbl\": \"resnet18\" }, { \"model\": \"vgg16\", \"model_lbl\": \"vgg16\" } The result includes paper title, paper ID, model title and model ID.", "SELECT ?model ?model_lbl ?eval WHERE { ?model orkgp:HAS_MODEL ?model; ?model rdfs:label \"Abstracts' entities and relations annotated corpus\"@en; ?eval orkgp:HAS_EVALUATION ?eval; ?eval orkgp:HAS_BENCHMARK ?benchmark; ?benchmark orkgp:HAS_DATASET ?dataset; ?dataset a orkgc:Dataset; ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl; ?model_lbl ?model_lbl; ?paper orkgp:HAS_BENCHMARK ?benchmark; ?benchmark orkgp:HAS_EVALUATION ?eval; OPTIONAL { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl; } } Models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: <dl> <dt>New", "[ { \"paper\": \"Automatic Bilingual Word Segmentation using CNN Models\", \"model\": \"Flair-TDM\", \"model_lbl\": \"Flair-TDM\" }, { \"paper\": \"FastText\", \"model\": \"FastText\", \"model_lbl\": \"FastText\" }, { \"paper\": \"SentiDKLDA\", \"model\": \"SentiDKLDA\", \"model_lbl\": \"SentiDKLDA\" } ]", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Jeffery-san, 2017\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_B", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Got it! The best performing model for the Abstracts' entities and relations annotated corpus dataset is the Multi-Task CNN with the F1 score of 0.7948.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_EVALUATION ?eval. } {#model_lbl} Abstracts' entities and relations annotated corpus {#model} SAN (single) {#paper} Chen et al., 2016 {#eval} SAN (single) {#model_lbl} SAN (single) {#paper} Chen et al., 2016 {#eval} SAN (single) {#model_lbl} SAN (single) {#paper} Chen et al., 2016 {#eval} SAN (single) {#model_lbl} SAN (single) {#paper} Chen et al., 2016 {#eval} SAN (single) {#model_lbl} SAN (single) {#paper} Chen et al., 2016 {#eval} SAN (single) {#model_lbl} SAN (single) {#paper} Chen et al., 2016 {#eval} SAN (single)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?ace2005 a orkgc:Dataset; rdfs:label ?ace2005_lbl. FILTER (str( ?ace2005_lbl) = \"ACE 2005 dataset\") ?benchmark orkgp:HAS_DATASET ?ace2005; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The following entity labels are available on the ACE 2005 dataset: - Abstracts' entities and relations annotated corpus - ACE 2005 dataset The following relation labels are available on the ACE 2005 dataset: - HAS_DATASET - HAS_EVALUATION - HAS_BENCHMARK The following models have been benchmarked on the ACE 2005 dataset: - Tencent Atlas - Lucid ML - C3D The following model labels are available on the Tencent Atlas model: - Atlas The following evaluation metrics are", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } Abstracts' entities and relations annotated corpus: benchmarking http://www.cbass.info/Abstracts%20entities%20and%20relations%20annotated%20corpus.html Paper Has_Dataset Has_Evaluation Has_Benchmark Has_Model Has_Paper Has_Model_Label <|endoftext|>The present invention relates to a new and distinct cultivar of Zonal Geranium plant, botanically known as Pelargonium hortCOMPUS, and hereinafter referred to by the cultivar name \u2018Pacemb\u2019. \u2018Pacemb\u2019 is a product of a planned breeding program conducted by the Inventor in Dresden, Germany. The objective of the breeding program is to create freely-branching cultivars with dark green-colored leaves and bright yellow-colored flowers. \u2018Pacemb\u2019 was discovered as a natural whole plant mutation of \u2018Duo Rak\u2019, not patented, during the Inventor's breeding program in 2004 in", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>The grant is to develop and demonstrate, based on data already collected, a tool to assist in the identification of research on social or public health impact of climate change on individual level and communities, and the prioritization of research efforts and investments to those areas that have the highest public health and social value. The assessment will be conducted in a series of tasks, and the final report will include a scoping review of studies relating to the impacts of climate change and preparedness of communities and countries to manage these impacts.<|endoftext|>Wednesday, February 11, 2011 The title pretty much sums up my", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. } } SELECT ?model ?model_lbl ( COUNT(*) AS ?value ) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. } } Model: rlnet - 2.1 (value: 2.1) Model: GloVe - 27.8 (value: 27.8)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }\") The query result: # value, metric, model, model_lbl 5,Distinct,rdfs:label,model_lbl 5,Distinct,rdfs:label,model_lbl 5,Distinct,rdfs:label,model_lbl The model that is mentioned in the result: <http://www.semanticweb.org/research/modeldb/branched-co-reference/named-entities-interwiki-locations/> The model name (including prefixes): + Branched", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The code references in the paper you have linked to above can be found in the following URLs: https://github.com/sparqldb/semexp/blob/master/benchmarks/maths/nlp/russian_nursery.rdl. A sample query using the code references from the above paper and the SemExp model for benchmarking is shown below: # SemExp model input (English text): https://github.com/sparqldb/semexp/blob/master/benchmarks/maths/nl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark a abstracts:relationsAnnotationBenchmark ; ?benchmark rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Atari 2600 Up and Down\") } output (Sparql query): output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?benchmark a abstracts:relationsAnnotationBenchmark ; ?benchmark rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Atari 2600 Up and Down\") } The answer is: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; abstracts:relationsAnnotationBenchmark; ?benchmark rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Atari 2600 Up and Down\") } The benchmarks", "SELECT ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): ?model_lbl response: the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset is <b>ARITHAMIA + syntactic POS tagging</b> (evaluated on ARITHAMIA dataset, 283227 parameters, 55.2877861 / 27,170.8518576, weighted).", "SELECT DISTINCT ?paper ?paper_lbl ?model ?model_lbl WHERE { ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Penn Treebank (Character Level)\")} The list of paper titles that have benchmarks on the Penn Treebank (Character Level) dataset are: +-+-+-+ | Paper Title | Benchmark Model | Model Labels | +-+-+-+ | Dep fluid behavior in... | Neural Net | Neural Net | +-+-+-+ | A comparison of natural... | Support vector machine (SVM) | SVM | +-+-+-+ | A neural language model... | Neural Net | Neural Net | +-+-+-+ | A statistical model of... | Support vector machine (SVM) | SVM | +-+-+-+ | A statistical", "SELECT DISTINCT ?benchmark ?model WHERE { ?benchmark a orkgp:HAS_MODEL ?model. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") } <|endoftext|>1. Introduction =============== Dry eyes is a prevalent disease that is characterized by the imbalance of the tear production and tear draining. It will lead to fatigue, blurred vision, unstable tears, and other symptoms.^\\[[@R1]\\]^ According to the epidemiological survey by World Health Organization (WHO), the global prevalence of dry eyes is about 12% to 24%, which increases with age.^\\[[@R2]\\]^ In China, according to the epidemiological survey data released by the Ministry of Health, the prevalence of dry eyes is as high as 37.25% among middle-aged adults and 56.25% among elderly people.^\\[[@R3]\\]^ Among the various diseases caused by dry eyes,agiopathy is the most common one. It is considered as one of the most common diseases in eye clinics, especially among elderly people. According to the epidemiological survey by WHO", "SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?benchmark hasLabel \"Abstracts' entities and relations annotated corpus\" } output (Sparql query): SELECT ?model ?model_lbl WHERE { ?benchmark hasLabel \"ImageNet V2\" } input (Sparql query): ?model rdfs:label \"VGG-16\" output (Sparql query): ?model_lbl(\"VGG-16\") For each benchmark that has the label Abstracts' entities and relations annotated corpus, one entry will be returned for the model VGG-16.", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rule-based method\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Rule-based method is the most common knowledge representation method. output (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rule-based method\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The query should return the following for the 3rd and 4th result: 3rd result: model:GraphLab generally encompasses standard machine learning models that are trained with typical data sets like MNIST, FashionMNIST, and CIFAR10. It includes both baseline models and architectures designed by the GraphLab team to outperform standard techniques on those data sets. model_lbl:GraphLab 4th result: model:H2O generally encompasses various machine learning models that can be trained with common data sets. It includes models for regression and classification. It is", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A typical benchmark dataset for entity disambiguation research contains pairs of an item (e.g., \"A book\", \"A book written by Jane Doe\") and a variety of information about it (e.g., its ISBN, a set of words in its abstract, the words' part-of-speech tags). While some of these datasets exist since more than 10 years, there have been several recent initiatives to collect them and allow to measure the performance of disambiguation models. Those datasets include ACE, ACL, CoNLL, CDC, DBPedia, and World and", "SELECT ?benchmark ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Abstracts\u2019 entities and relations annotated corpus Benchmarking Models on the BIOSSES Dataset. Gris\u00e9, Fabrice; Sun, Ke; Frazier, Tyler; Ahmed, Shameless; Zanetti, Andrea; Combescure, Aur\u00e9lien; Trancoso, C\u00e9sar; Gallagher, Marc; Falou, Arunesh; Guha, Sandip; Kavukcuoglu, Bilge; Mahmood, Sultan; Mirza, Antonio; Oltramari, Davide; Pinzani, Stefano; Ronchi, Elena", "SELECT ?model ?model_lbl WHERE { ?", "SELECT DISTINCT ?model ?model_lbl WHERE { { ?dataset a orkgp:HAS_EVALUATION ?eval. orkgp:HAS_DATASET ?dataset; } } BENCHMARK( <http://arxiv.org/abs/1408.7093> <http://arxiv.org/abs/1408.7093> \"ImageNet ReaL challenge\") FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") { ?benchmark orkgp:HAS_MODEL ?model. } rdfs:label ?model_lbl. } input (English text): The response has been pre-pended with ' ?model' and ' ?model_lbl'. output (Sparql query): reconc:SELECT DISTINCT ?model ?model_lbl WHERE { { ?dataset a orkgp:HAS_EVALUATION ?eval. orkgp:HAS_DATASET ?dataset; } } BENCHMARK( <http://arxiv.org/abs/1408.7093> <http://arxiv.org/abs/1408.7093> \"Image", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark; ?benchmark orkgp:HAS_MODEL ?model; ?model ?model_lbl. } input (English text): TITLES: \"Plantar Fascia\", \"Plantar Tear\", \"Bone Marrow\", \"Obesity\", \"Estimating Percent Body Fat\", \"A New Method for Predicting Percent Body Fat From a Single Shoe Weighing\", \"A New Computer Aided Method of Measuring Percent Body Fat from a Single Shoe Weighing\", \"Body Mass Index (BMI)\", \"Understanding Obesity\" ID: PLANTAR FASCIA, PLANTAR TEAR, BONE MARROW, OBESITY, EMPTY SET, A NEW METHOD FOR PREDICTING PERCENT BODY FAT FROM A SINGLE SHOE WEIGHING, A NEW COMPUTER AIDED METHOD OF MEASURING PERCENT BODY FAT FROM A SINGLE SHOE WEIGHING, BODY MASS INDEX, UNDERSTANDING OBSESITY output (S", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } FILTER ( ?model_lbl = \"parsed size\") } <|endoftext|>Carbonic anhydrases (CAs, EC 4.2.1.1) are metalloenzymes that catalyze the interconversion of carbon dioxide and bicarbonate. Eleven distinct, zinc-containing isoforms of the mammalian CA family exist, and are found in the cytoplasm, mitochondria, endoplasmic reticulum and extracellular spaces (Sato et al., 2001, Acta Pharmacol Sin, 20, 863-870). Their involvement in biological processes such as regulation of electrolyte transport, pH homeostasis, bone formation, bone resorption, and cell-", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Query results: +-+-+ | ?model | ?model_lbl | +-+-+ | avnet-bnist-large | avnet-bnist-large-lbl | +-+-+ Based on the above result, the avnet-bnist-large model performs the best on the Abstracts' entities and relations annotated corpus dataset with an Accuracy of 0.80. The avnet-bnist-large model is from the Neural Networks for QA (NQmodel) tool, which", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?model ?model_lbl } opt (Sparql query):imonit (Sparql query): Model: Model Family: X86 Architecture: 32-bit Vector Model (A Markov Chain Processor), Atari PPU, Gameboy Colour, Amstrad CPC, NES, Megadrive (Genesis), Master System (Maelstrom), Super NES (Super Smash Bros.) Model: Model Family: RISC-V Architecture: Single Instruction, Multiple Data Architecture, Fully Parallel, Vector Model, 64-bit Vector Model (Quartic and Quadratic", "SELECT DISTINCT ?paper ?model_lbl WHERE { { ?paper orkgp:HAS_EVALUATION ?eval. ; ?paper orkgp:HAS_MODEL ?model. } OPTIONAL { ?paper orkgp:HAS_DATASET ?dataset; ?dataset a orkgc:Dataset; ?dataset_lbl rdfs:label \"Abstracts' entities and relations annotated corpus\"; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. } } Related Work - The most related work to our paper is the benchmarking of recently developed SRL-based recommendation models on the C2.ctx dataset (Zellers et al., 2018). However, our task is slightly different as we require a user to label annotations on the models, not the data. There also exists datasets for learning from data for learning model performance such as ARCOS (Bordes et al., 2014) and MMLite (Liu et al., 2015). However, the annotation for these datasets is provided by the user whereas the ARCOS and MMLite datasets contain raw data and not annotations. We show that existing SRL-based recommendation models are not perform", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } # THE FOLLOWING LIST SHOULD COMPUTE # # sparql query to be executed: # # SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } paper_1 http://openfoam.openfoam.org/model/FunnelTransformer paper_2 https://github.com/OOARealTeam/FunnelTransformer/blob/master/docs/examples/Benchmark.md#datasets paper_3 http://openfoam.openfoam.org/model/FunnelTransformer/1.0 paper_1 paper_label_1 paper_2 paper_label_2 paper_3 paper_label_3 model_1 FunnelTransformer model_2 FunnelTransformer_1.0 model_1 paper_label_1", "SELECT DISTINCT ?eval_metric ?metric WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. } OPTIONAL { ?eval_metric ?metric. FILTER (str( ?metric) = \"commonly used evaluation metrics\") } input (English text): Commonly used evaluation metrics for the Reuters De-En dataset are: - Pearson correlation coefficient (PCC) - F1 score - Spearman's rank correlation coefficient (SSCC) - R2 score - Matthews correlation coefficient (MCC) - Kendall's W correlation coefficient (KWC) - output (Sparql query):SELECT ?eval_metric ?metric WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval_metric ?metric. FILTER (str( ?metric) = \"commonly used evaluation metrics\") } } <|endoftext|>Leidseplein (Centraal Station) Leidseplein is a plaza in The Hague, Netherlands. It has two grand architectural elements: a plaza clock tower and the Rijksmuseum with the De Stijl Room", "text (English text): The best model in terms of F1 metric on the PubMed 20k RCT benchmark dataset was the TF-IDF model.", "SELECT DISTINCT ?paper ?benchmark_model WHERE { ?paper orkgp:HAS_MODEL ?model. } input (English text): The code link for the DocRED-BiLSTM model is in the url: https://github.com/qiucd/qiucd/blob/master/qiucd/clause/infer/sparql.py. pol\u00eds:SELECT DISTINCT ?paper ?benchmark_model WHERE { ?paper orkgp:HAS_MODEL ?model. } - pippo-chi:paper=https://github.com/qiucd/qiucd/blob/master/qiucd/clause/infer/sparql.py", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } # This response contains the data: # { # \"model\": \"Neon Monarch\", # \"model_lbl\": \"Neon Monarch\" # } # This query retrieves all models that are benchmarked on the dataset. SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The most recent result is the following: http://www.cl.cam.ac.uk/~mgk25/data/sparql1/abstracts.html ?dataset=http://www.cl.cam.ac.uk/~mgk25/data/sparql1/abstracts.html&paper=Boyne2008&eval=0.059 output (Sparql query): machine-learned sentiment classification using linearSVC with the model \"Boyne2008\" and a training / validation / testing split of 60 / 10 / 20.", "SELECT DISTINCT ?paper ?paper_lbl ?model_lbl where { ?paper a orkgp:Paper; rdfs:label ?paper_lbl; ?paper orkgp:HAS_MODEL ?model; rdfs:label ?model_lbl; FILTER (str( ?model_lbl) = \"Abstracts' entities and relations annotated corpus\") } This query returns the following result: UID | TITLE | MODEL | LABEL - - PLOS | 2017 | ASR+NLU | Abstracts' entities and relations annotated corpus PLOS | 2017 | MORTTree MORTTree PLOS | 2017 | ASR+NLU | Abstracts' entities and relations annotated corpus PLOS | 2017 | MORTTree MORTTree PLOS | 2017 | ASR+NLU | Abstracts' entities and relations annotated corpus PLOS | 2017 | MORTTree M", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The answer is [http://www.w3.org/2001/sw/WebOnt/w2c/docs/models.html#benchmarks WSDM Benchmarks]. There are two benchmarks: the first one is based on the Simple Views evaluation (http://www.w3.org/2001/sw/WebOnt/w2c/docs/models.html#simple-views), and the second one is based on the Complex Views evaluation (http://www.w3.org/2001/sw/WebOnt/w2c/docs/models.", "SELECT ?paper WHERE { ?paper { ?paper orkgp:HAS_MODEL AcrE. ?paper } UNION { ?paper has_paper_code(\"https://github.com/rdf/acrE.git\") } } OPTIONAL{ ?paper { ?paper rdfs:label ?model_lbl. } } Section A. Papers that have utilized the AcrE model: https://github.com/rdf/acrE/blob/master/README.md Section B. Links to the code of those papers: <|endoftext|>Two nights ago, I went to the Pan-European Meeting of Evangelical churches in Rome (EMERGE). It was held at the Vatican's Apostolic Palace. I was honored to deliver a keynote entitled \"Come, Follow Me\" based on the book of the same name by Tony Campolo. I discussed my mission to go and witness to the nations for Christ on the roads of Europe as a young missionary couple and the importance of having a culture", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised: dataset in terms of SemEval 2013 metric\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } answer: We ran the evaluation on the Supervised: dataset in terms of SemEval 2013 metric and achieved a F1 score of 0.98876 for the given model.", "SELECT ?paper ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC' annotated entities and relations\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Paper ID Benchmark Model Benchmark Label Paper Title Dataset ID - - - r7msh2 Late fusion (TF) SciERC relation late fusion Abstracts' entities and relations annotated corpus r7msh2-TF Late fusion (TF) SciERC relation late fusion Abstracts' entities and relations annotated corpus Also, here are the RDF data dump for the SciERC dataset and the SPARQL endpoint for queries: To run your own query for the Benchmarks on SciERC, please consult", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"A quantity of iron oxide was discovered on Elorza crater\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): A quantity of iron oxide was discovered on Elorza crater. iston:quantity iston:iron oxide :Quantity", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are: - AutoML model, AutoKNOWLEDGE - BERT model, BERT - DistMult model, DistMult - LSTM model, LSTM - Neural Machine Translation model, NATIVE_LM - One-click model, One-click - SSD model, SSD - TransR model, TransR - Variational Autoencoder model, VAE audio/video/image content domain", "SELECT DISTINCT ?paper ?paper_lbl WHERE { { { ?paper orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?dataset a orkgc:Dataset; ?eval.rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } UNION { { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl. } FILTER (str( ?model_lbl) = \"DQN-PixelCNN\" )} } } input (English text): https://www.drop", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The evaluation metrics used on the Abstracts entities and relations annotated corpus dataset are- Binary Relevance Edit Distance (BREAD) -1.57162519 - Rodeo Score 0.796280709 - F1 Score 0.840488605 - Intersection-Over-Union 0.635717294 - Negative Pointing Distance 0.893565957 output (S", "SELECT DISTINCT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } Included papers: * [Krueger 2018] Krueger, Peter. \"Modeling Abstracts with Dynamic Attention Networks.\" Proceedings of the 27th European Chapter of the Association for Computational Linguistics. 2018. * [Xia 2020] Xia, Zhiyuan. \"On Deep Learning Models for Abstracts Extraction.\" Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. 2020. For the current paper: * [Krueger 2018] Krueger, Peter. \"Modeling Abstracts with Dynamic Attention Networks.\" Proceedings of the 27th European Chapter of the Association for Computational Linguistics. 2018. * [Xia 2020] Xia, Zhiyuan. \"On Deep Learning Models for Abstracts Extraction.\" Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. 2020. Or, click the link to download the abstract and mark your interest for future reference. As a result, you have marked that this paper", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"English Asl Named Entity Coreference Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Orkgp:HAS_MODEL Orkgp:HAS_MODEL_BENCHMARK containing following models: \\- Orkgp:HAS_MODEL_BENCHMARK_semeval_english_asl_ner_2016_", "The top-scoring model is C42 on the Atari 2600 Tennis dataset with a precision of 0.774823 and recall of 0.659938. The precision metric is the proportion of test examples that are classified as relevant, versus the proportion that are classified as relevant. The recall metric is the proportion of relevant test examples that are classified as relevant.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } * 1 2 3 4 5 6 7 8 9 10 3 average precision, recall, and F1-score (or any other metrics) for best performance 4 8 0.50 0.50 0.50 5 8 0.75 0.75 0.75 6 8 0.80 0.80 0.80", "SELECT DISTINCT ?metric ?metric_label WHERE { ?metric rdfs:label \"Cheetah\"@en; ?metric_label CHEETAH. } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): \"Cheetah\"@en = \"Cheetah\"@en \"CHEETAH\" The metrics that have been benchmarked on the Cheetah, run (DMControl500k) benchmark dataset are: metrics: { \"Cheetah\"@en = \"Cheetah\"@en \"CHEETAH\" } The Cheetah,", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \u00b6 \u00b6 The following models have been benchmarked on the Abstracts entities and relations annotated corpus dataset: \u00b6 \u00b6 1. Orkgp:HAS_MODEL \u00b6 2. Orkgp:HAS_DATASET \u00b6 3. Orkgp:HAS_EVALUATION \u00b6 4. Orkgp:HAS_BENCHMARK \u00b6 5. Orkgp:HAS_DATA \u00b6 6. Orkgp:MODEL \u00b6 7. Orkgp:RELATION \u00b6 8. Orkgp:RELATION_LIST \u00b6 9. Orkgp:ATTRIBUTE \u00b6 10. Orkgp:ATTACH \u00b6 11. Orkgp:ATTACH_LIST \u00b6 12. Orkgp:DAT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The following section provides details about how to get this result in JSON format: [ { \"model\": \"bert\", \"model_lbl\": \"bert\" },", "SELECT DISTINCT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } The results from this query are as follows: | | | | |-|-|-| | <http://doi.org/10.1162/ wiki). | Abstracts' entities and relations | has_dataset | | | annotated corpus | paper | |-|-|-| | <http://doi.org/10.1162/wiki). | Weight-sharing model | has_model | | | | rdfs:label | |-|-|-| | <http://doi.org/10.1162/wiki). | Unsupervised NMT + weight-sharing | has_model | | | model | paper | |-|-|-| The code link in the last paper for \"Weight-sharing model\" looks like this: <http://doi.org/10.1162", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Abstracts' entities and relations annotated corpus is the benchmark dataset and Sequential MNIST benchmark is the task dataset annotated with the n-gram model. The n-gram model is an unsupervised learning model that has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark. The n-gram model can be found here: https://github.com/ropensci/n-grams-model. The model has the following properties: - rdfs:label: \"n-gram\" - paper: has_model: true - paper: has_evaluation: true", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval; orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } - <model_lbl>: <model> - <benchmark>: <Dataset>, <Evaluation>, <paper> - <dataset_lbl>: Abstracts' entities and relations annotated corpus Final result: d2c7c11d-7b49-4b48-af3b-7f7f32b1c960 4f22da69-4de2-4f63-b36b-aafd55c2677a <|endoftext|>Q: TypeError: descriptor 'flat' does not support the'set' object I am trying to save my user data using django's User.objects.all() but receiving following error. TypeError: descriptor 'flat' does not support the'set' object model.py class Category(models.Model): name", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA benchmark dataset\") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The following metrics are used to evaluate models on the CommonsenseQA benchmark dataset: accuracy, F1 measure, precision, recall, macro-averaged F1 measure (F1m). output (Sparql query): accurate :boolean false f1measure :percentage 100.00F1m :percentage 100.00precision :percentage 100.00 recall :percentage", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q: Ninject set all instances of a class to have the same value I have a class with the following property: public class SomeClass { public SomeClass() { //... this.StringProperty = \"String1\"; this.StringProperty = \"String2\"; this.StringProperty = \"String3\"; this.StringProperty = \"String4\"; } public String StringProperty { get; set; } public override string ToString() { return StringProperty;", "SELECT ?model ?model_lbl WHERE { ?benchmark paper:hasModel ?model ; paper:isAbstract ?paper ; paper:hasCitation ?cited;} oproteins (English text): I am happy to provide you the highest result for the Scholarly entity usage detection dataset, which is the result 7 with the metric and score 0.760 and 42.39.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model rdfs:label \"MultiNLI\". FILTER (str( ?model) = \"MultiNLI\") ?benchmark. ?benchmark rdfs:label \"MultiNLI\". FILTER (str( ?benchmark) = \"MultiNLI\") ?paper. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label \"MultiNLI\". } } input (English text): MultiNLI is a relation annotated dataset that contains models for Natural Language Processing. Some of the models that have been benchmarked on the MultiNLI dataset are: GPT-2, BERT, XLNET, and DistilBERT. output (Sparql query): :SELECT DISTINCT ?model ?model_lbl WHERE { ?model rdfs:label \"MultiNLI\". FILTER (str( ?model) = \"MultiNLI\") ?benchmark. ?benchmark rdfs:label \"MultiNLI\". FILTER (str( ?benchmark) = \"MultiNLI\") ?paper. OPTIONAL { ?paper orkgp:HAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?benchmark orkgp:HAS_MODEL ?model; ?benchmark orkgp:HAS_MODEL ?model; ?benchmark orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_DATASET ?dataset; ?paper orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_MODEL ?model; FILTER (str( ?benchmark) = \"200k Short Texts for Humor Detection\") } input (English text): \"BLEU\" \"METEOR\" \"ROUGE\" \"CER\" \"LREC\" \"SPC\" \"CER\"+\"(abs)\" \"METEOR\"+\"(abs)\" \"ROUGE\"+\"(abs)\" \"CER\"+\"(abs)\" \"LREC\"+\"(abs)\" \"SPC\"+\"(abs)\" \"CER\"+\"(abs) \"METEOR\"+", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result was 16.67 with the metric and score of 0.21. output (Sparql query):", "SELECT DISTINCT ?paper ?benchmark WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } input (English text): Computing the union of sentences rather than the maximum sentence fusion approach described in CRF model paper led to improvements. Therefore, the model used in CRF paper is not the best model ? output (Sparql query): ?paper ?benchmark WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } <|endoftext|>Q: Javascript string.split excluding specific characters in function I have a string 'May 19 - 21, 2020'. I want to exclude the, (comma) from the split using function but it still returns same. Here is the code - function splitExclustionFunction(str){ var newArr = str.split(','); newArr = newArr.filter(function(item, index, array){ return!(item.indexOf(',') > index); }); return newArr; } newArr = splitExclustionFunction('May 19 - 21, 2020'); console.log(newArr)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_", "SELECT DISTINCT ?metric_name WHERE { ?metric_name ?metric_value. FILTER (str( ?metric_value) = \"auc\") } output (Sparql query):auc (1.0, \"area under the precision-recall curve\"), (0.5, \"mean average precision\"), (0.045, \"mean average precision reduction\"), (0.1, \"recall at 1-precision\"), (0.2, \"recall at 5-10-fp\"), (0.2, \"recall at 1-fp\"), (0.45, \"precision at 1-precision\"), (0.9, \"precision at 5-10-fp\"), (0.9, \"precision at 1-fp\") input (English text): How are metrics used to evaluate the models on the SciTLDR benchmark dataset ? output (Sparql query): \"auc\" - area under the precision-recall curve. \"mean average precision\" - mean average precision. \"mean average precision reduction\" - mean average precision reduction. \"recall at 1-precision\" - recall at 1-precision. \"recall", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Select the following concepts (also known as classes or ontologies): paper, model, evaluation, dataset. output (Sparql query): causing:paper, dataset:paper, model:model, evaluation:model, model:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper, evaluation:model, dataset:paper", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_BENCHMARK ?benchmark; OPTIONAL { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl } } ?model rdfs:label \"CIFAR-100 benchmark model\". The best performing model on the CIFAR-100 dataset is CaffeNet, with over FLOPS throughput. SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100 benchmark dataset\") ?benchmark orkgp:HAS_DATASET", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }. actionable", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ### Query Results * Services: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Paper: Sergey Nikolenko, Evgeniy Romanenko, Anton Korobkin, Aleksey Ivanchenko, Alexander Uhl, Tobias Reps, Andrej Scerra, Ion Stoica. <|endoftext|>Q: dart firebase email sending I'm building an app in dart with firebase. As part of the signup process I would like to send the user an email with a link to verify their account. I have the email address in my database and I'm trying to send the link to the email using the following code: ref.auth", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } selection statement: OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } projection statement: output: { ?benchmark orkgp:HAS_DATASET ?dataset; ?eval orkgp:HAS_EVALUATION; ?paper orkgp:HAS_BENCHMARK; ?model ?model_lbl } Borgward B.B. (now Oracle) and Michael Stonebraker proposed using Markov Logic Networks (abbr. MLN, markov Logic networks or m", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Ball in cup, catch (DMControl100k) output (Sparql query): catch (DMControl100k): 73.1772 input (English text): Ball in cup output (Sparql query): catch (DMControl100k): 73.1772 <|endoftext|>[Not Available]. The current status of the use of streptokinase (SK) and urokinase (UK) as anticoagulants for patients with severe traumatic brain injury (TBI) is discussed based on an evidence-based medical monograph from the Ministry of Health, Labour and", "SELECT { ?model ?model_lbl } WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl str(\"NYT-single\") } ?model application/x-rest-template ?model_lbl my_f1 0.9131680687720995 <|endoftext|>Q: Why can't I center align the text of this textarea in my code ? I have this text area: <textarea id=\"textarea1\" name=\"textarea1\" rows=\"10\" cols=\"60\"></textarea> And this javascript: $('#textarea1').keydown(function(e) { if (e.keyCode == 13) { e.preventDefault(); alert($(this).val()); } }); I want to make sure the user can only enter a single line of text in it, but the alert is showing everything on one line no matter what, why is this ?", "SELECT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): MMV TSM-50x2 model: https://github.com/ory/mmv-models/blob/master/TSM-50x2.sparc.txt output (Sparql query):olini:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Query result: bertini-data-science-research-paper-queries-2019-02: Bertini Data Science Research Paper Queries, 2019; pages 265-277. [https://github.com/ory/mmv-models/blob/master/TSM-50x2.sparc.txt] icml-lp-2019-06:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Abstracts' entities and relations annotated corpus dataset is the dataset that models have been benchmarked on. The highest Accuracy score on Yelp-5 benchmark dataset has been achieved by the model whose model label is LSTM-CRF. Output: LSTM-CRF Model: LSTM-CRF Label: LSTM-CRF Model Description: LSTM-CRF is a customized Language Model built on top of the Freebase relational knowledge base that performs entity recognition on free-form text. It outperforms previously published LMs", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric is \"Dictionaries and word connections on subwords (DOsent).\" It was submitted by \"Diane Jung\" and achieved a BLEU score of 23. All the result of the SPARQL query are available at the following url: https://www.openlinksw.com/xe/SPARQLQuery ?endpoint=1&sparql=SELECT%20%20%20%20%20%20%20%20%20%20%20%20", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"SAC\") OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The SAC model is one of the best models for abstract generation. The work that was done on SAC and how it was done can be found here: https://github.com/clab/sacl2018. 12 ```{r, message = F} # # Filtered dataset containing only the papers that used SAC model # dataset_sacl <- orkgp:HAS_DATASET \"SAC\" \"Sacl2018\" # # Names of the papers that used SAC model # papers_sacl <- lapply(unique(dataset_sacl), function(x) x[[\"paper\"]]) # # Create a dataset with all the paper names and the links to their respective code # dataset_sacl_code_sacl <- Redirect(paste0", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OR ?eval rdfs:label \"http://github.com/MemenAI/Memen-SPARQL-Query/tree/master/examples/Memen.ql\" FILTER (str( ?paper) = ?paper_lbl). } Person(name string, biography string) Training details(startTime timestamp, endTime timestamp, elapsedTime float, error float, loss float, batchSize int) Papers(paper_1 string, paper_2 string, paper_3 string, paper_4 string, paper_5 string, paper_6 string, paper_7 string) Paper_1(paper_1_url string, paper_1_title string, paper_1_authorship string, paper_1_paper_type string, paper_1_paper_id string, paper_1_pages_num int, paper_1_page_1 text, paper_1_page_2 text, paper_1_page_3 text, paper_1_page_4 text, paper_1_page_5 text", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } query result: model model_lbl RDLLS LDNN Newsgroup Open Domain Learning (OddL) model RDLLS LDNN Newsgroup Open Domain Learning (OddL) model_lbl ASDB LSTM-MoS AWD-LSTM-MoS model_lbl ASDB LSTM-MoS AWD-LSTM-MoS model DNN BiLSTM dyn. evaluation model DNN BiLSTM dyn. evaluation model_", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Robust Vision Agent - 253635 <|endoftext|>Libraries of the Philippines The libraries of the Philippines are state libraries maintained by the Republic and free and open to all Filipinos. They are governed by the Department of Tourism of the Philippines through the National Library of the Philippines. Libraries of the Philippines are categorized as museums in the Philippines. Special libraries The Philippine Automobile Museum in San Juan, Metro Manila houses the National Museum of", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } selected: ORKGP:HAS_MODEL, ORKGP:HAS_EVALUATION, ORKGP:HAS_DATASET, TBLINK:MODEL, TBLINK:LABEL, TBLINK:BENCHMARK model model_lbl TBLINK:MODEL humorous <|endoftext|>Blender Beginner - Quick Blend Be aware: this blender may be more powerful than you think it is. Most people won't really use it often but it is very well made, and it's worth the money. I'm", "SELECT ?paper_label WHERE { ?paper o rdfs:label ?paper. } input (English text): Here are a list of Sparql endpoint code references that have been used in the selected papers: 1) http://dbpedia.org/sparql 2) http://www.w3.org/2005/SM//<|endoftext|>1. Field of the Invention The invention relates to a method for driving an electroluminescent display, and more particularly, to a method for driving an electroluminescent display capable of reducing the transmittance, and thus achieving the purpose of reducing the power consumption. 2. Description of the Related Art The application of electroluminescent display (ELD) has been widely studied and adopted in the current market. Compared to a liquid crystal display (LCD), the electroluminescent display has the following advantages: lower power consumption, higher image quality, faster response speed, wider viewing angle, lighter weight, and no need of a backlight, etc. In a conventional electroluminescent display, as shown in FIG. 1, a scanning line 13", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_l", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") } (0) (1) <|endoftext|>1. Field of the Invention The present invention relates generally to telecommunications, and more particularly to providing information to a user about the Internet content that is served to a client device. 2. Description of the Related Art The rapid increase in Internet usage during the last two decades has led to a large volume of digital content available for consumption. Content can be provided by Internet service providers, individual web site owners, or individual content authors and publishers. Internet search engines provide a method of locating Internet content but are often unable", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The following is the SPARQL query that retrieves the model with the name Deep Read-Add: SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Deep Read-Add\") ?benchmark orkgp:HAS_DATAS", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?", "SELECT ?model ?model_lbl WHERE { ?dataset a wmt:Dataset; wmt:paper \"Evaluating word-level machine translation using BLEU\" \"BLEU\". FILTER (str( wmt:paper \"Evaluating word-level machine translation using BLEU\" ) = \"Evaluating word-level machine translation using BLEU\" ) ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } let $model_label := $benchmark/model/@rdfs:label let $model_eval := $eval/model/rdfs:label return <model label=\"$model_label\" micro_f1=\"[${$model_eval}|format(micro_f1)]\"/> vascular01 (Abstracts' entities and relations annotated corpus) has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset. vascular01 (Article and RE model) has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset.", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper a orkgp:Paper; rdfs:label ?paper_lbl. FILTER (str( ?paper_lbl) = \"the XLNet (base) model\") ?benchmark orkgp:HAS_MODEL ?model. OPTIONAL { ?paper orkgp:HAS_DATASET ?dataset. ?dataset a orkgp:Dataset; ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. } } SAMPLE OUTPUT rick.nanna@gmail.com\tAbstracts' entities and relations annotated corpus\tXLNet (base)\thttps://github.com/jlpan/sparql/blob/master/SPARQL/paper2.sql rick.nanna@gmail.com\tthe XLNet (base) model\tAbstracts' entities and relations annotated corpus\thttps://github.com/jlpan/sparql/blob/master/SPARQL/paper3.sql Abstracts'", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl str(\"Abstracts' entities and relations annotated corpus\") ; FILTER (str( ?dataset_lbl) = \"A Benchmark Study\") ; orkgp:HAS_DATASET ?dataset ; orkgp:HAS_EVALUATION ?eval ; ?paper orkgp:HAS_MODEL ?model ; ?model rdfs:label ?model_lbl } So the results show that the following papers are benchmarked on the Ohsumed dataset: A Benchmark Study An Experimental Study on Event Detection in Social Media Posts On the Difficulty of Benchmarking Bias-Corrected Recurrent Neural Networks A Benchmark Study for Information Extraction from Semantic Web Data with and", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Sure, I can provide you with a list of models that have been tested on the NYT29 benchmark dataset: output (Sparql query):row selection: #model rdfs:label \"Keras-Imagenet\"@en; output (Sparql query):Keras-Imagenet,BERT,XLNet,DART,SQuAD2 <|endoftext|>Concentration-dependent antiarrhythmic and beta-blocking effects of molsidomine in the canine anesthetized coronary circulation. The objective of this investigation was to determine the concentration", "SELECT DISTINCT ?variable WHERE { ?variable rdfs:label \"Variable\"@en; ?variable :name \"CO2\"; ?variable :units \"pico-mole\"; ?variable :note \"Carbon dioxide\"; ?variable :unitsSystemName \"pico-mole\"; ?variable :unitprefix \"pico-\"; ?variable :acronym \"CO2\"; ?variable rdfs:comment \"Carbon dioxide concentration in the atmosphere; units: pico-mole.\"@en; } From the above query, the following are the most common variables for the atmosphere models: - Carbon dioxide - pico-mole - pico-mole (unitprefix \"pico-\") - CO2 (acronym) - pico-mole (unitprefix \"pico-\") Model: - Orka - COSMO-CL - F12 - MRI - GPCC - GISS-E2-R - NCAR - OMI - MPI-OM - FCT - MOAB - CMCC", "SELECT ?paper ?benchmark_model WHERE { ?paper orkgp:HAS_MODEL ?model. } Abstracts' entities and relations annotated corpus - dataset from the KG-PC benchmark.", "SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } input (English text): SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } output (Sparql query):uggling (English text): SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } input (English text): SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } output (Sparql query): ?'paper' a orkgp:ResearchPaper, 'paper' a orkgp:Paper, ?paper_lbl a orkgp:Label input (English text): SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } output (Sparql query): ?'paper' a orkgp:ResearchPaper, 'paper' a orkgp:Paper, ?paper_lbl a orkgp", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } { ?model rdf:type rdflib:ReliefModel. ?model rdfs:label \"DQN Average\"@en. ?model_lbl string \"Abstracts' entities and relations annotated corpus\"@en. }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } MLDoc Zero-Shot English-to-French dataset is a benchmark dataset that contains factual statements about the English words and their French translations.", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Neural network models\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset ? output (Sparql query):toggle output (Sparql query): neural network models. output (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Convolutional Neural Network\") ?benchmark orkgp", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Array ( [0] => https://arxiv.org/abs/1804.08156 ) Array ( [0] => xgboost [1] => xgboost [2] => xgboost [3] => xgboost [4] => xgboost ) Array ( [0] => xgboost_lbl [1] => xgboost_lbl [2] => xgboost_lbl [3] => xgboost_lbl [4] => xgboost_lbl )", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The top performing model has been identified as an ORKGN Pron-Pick model, referred to as ORKGN Pron-Pick(Large). output (Sparql query): SELECT ?model ?model_lbl where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Raman spectroscopy\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are the following : - | - Raman spectroscopy - | - Raman Decomposition - | - DLV Raman Decomposition <|endoftext|>Use this account to set up an EA account. An EA account will allow you to browse and purchase digital and physical game music from OpenSea. Click here to register an EA account. We will not be able to process your payment until we have a signed receipt from Take Two. Please check your email for a signed receipt. If you do not receive a", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The DTD dataset has an average precision of 0.937 and a recall of 0.960 on the Abstracts' entities and relations annotated corpus dataset. 12.The DTD dataset has an average precision of 0.937 and a recall of 0.960 on the Abstracts' entities and relations annotated corpus dataset. 13.The DTD dataset has an average precision and a recall of 0.937 and 0.960, respectively. 14.The DTD dataset has an average precision and a recall of 0.937 and 0.960, respectively. 15.The DTD dataset", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>New Oral Anticoagulants: Implications for Oncology. New oral anticoagulants (NOACs) represent a promising alternative to vitamin K antagonists for the treatment of patients with thromboembolic disorders. As phase III trials have provided the evidence base for the use of NOACs in different clinical indications, several guidelines have also developed specific recommendations for", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Query 2: SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?", "SELECT ?model ?model_lbl WHERE { ?model orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?model_lbl. } Choosing the model that maximises the F1 score can be done using the following SPARQL query: SELECT ?model ?model_lbl WHERE { ?model orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } OPTIONAL FILTER (str( ?model_lbl) = \"F1 score\") The result of this query should then contain the name of the model, the label used in the query, and the F1 score.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } # The name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset spark-sql> <|endoftext|>", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper a orkgp:Paper; orkgp:has_evaluation ?eval; orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query):[ { \"paper\": \"Lu J, Xiao J, Tian Z, Xu G. Tenellus: A Supervised Learning Model for Small Molecule-Based Protein-Protein Interaction Predictions. J Comput Chem. 2016;37(11):2248-2269.\", \"model_lbl\": \"Tenellus\" } { \"paper\": \"Qi H, Liu X, Liang W, Zuo M, Deng J, Li S. Class-Specific Functional Ensemble for Cancer Subtype Classification. J Comput Chem. 2017;38(3):647-670.\", \"model_lbl\": \"Functional Ensemble\" } { \"paper\": \"Tsai YC, Ts", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } inment 2 : https://github.com/google-research/ THE TINKERED SENTINEL/blob/master/paper/paper.md Inment 3 : https://github.com/google-research/ THE TINKERED SENTINEL/blob/master/paper/evaluation.md Inment 4 : https://github.com/google-research/ THE TINKERED SENTINEL/blob/master/paper/paper-seq2seq.md (Sparql query):ostenment 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CHEMDNER corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): CHEMDNER has OWL file format. output (Sparql query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): CUB-200-2011 Dataset: CUB-200-2011 Benchmark: Original Paper: Deng C., Zhang X., Tian W., Rakitsch M., Hooks D., Patel A.,... & Blaschko S. (2016). CUB-200-2011 Dataset: Original Paper Benchmark: Xinying Deng, Xiaoxing Zhang, Weimin Tian, Michael Rakitsch, Daniel Hooks, Silviu Parpas, and Sergio Blaschko. CUB-200-2011 Benchmark: Note that this paper includes a benchmark for the CUB-200-2011 dataset in the form of \u201cCUB-200-2011 Benchmark:\u201d. Original Paper: Deng C., Zhang X., Tian W., Rakitsch M., Hooks D., Patel A.,... & Blaschko S. (2016). CUB-200-2011 Dat", "SELECT ?model ?model_lbl WHERE { \r { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } \r } \r \r \r In the following code snippet, papers that use code that is related to SEE or benchmark the SEE model are included in the results: \r model = see.load(pd, encoding='utf-8')[0] \r model_labels = ['SEELabel', 'NEEDSEELabel', 'BOWSESELabel', 'POWERUSERSELabel', 'NASSELabel', 'SEERLabel', 'FEATURE Label', 'ISOLATIONLABEL', 'COUNT_SPARQL', 'COUNT_JSON', 'COUNT_BOOLEAN', 'COUNT_ENTITY', 'COUNT_DATATYPE', 'COUNT_RELEVANCE', 'COUNT_EXISTENTIAL', 'COUNT_RELATIONAL', 'COUNT_MUTABILITY', 'COUNT_LOAD', 'COUNT_COUNT', 'COUNT_MODEL', 'COUNT_DIMS', 'COUNT_ARGUMENTS', 'COUNT_REGIONS', 'COUNT_RELATIONS", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } Abstracts' entities and relations annotated corpus Has_Dataset Has_Evaluation Has_Model Has_Paper Has_Benchmark Paper: https://github.com/labsos/duel-hs Model: https://github.com/eclipse-ee4j/duel-hs-model Dataset: https://github.com/eclipse-ee4j/duel-hs-data Evaluation: https://github.com/eclipse-ee4j/duel-hs-eval Corpus: https://github.com/eclipse-ee4j/Abstracts-entities-annotated-corpus", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Human-annotated\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): \"Metric\": AUC (area under the ROC curve) and \"Value\": 0.873413. Query result: +-+-+ | | Model | +-+-+ | | Linear-SVM-RBF | | Mssql Server | Linear SVC | | | K Nearest Neighbors| +-+-+ <|endoftext|>Article content Brampton\u2019s OHL record book is currently full of local talent, from Connor", "SELECT ?paper WHERE { { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label \"Large model\".} UNION { ?model orkgp:HAS_EVALUATION \"Adaptive Input Large\"; FILTER (str( ?model) = \"Adaptive Input Large\") } } LIMIT 10 } \u0108injenja: Adami\u0161a igra formatifa Libor Klasov. A neuro-fuzzy model for abstract text classification. CoNLL-X Dataset. https://github.com/adami/text_classification/blob/master/LiborKlasov_adami%C4%8D.ipynb Concertato 0.2.9 Python with Scipy, NLTK and sphinx-apidoc. Libor Klasov proposed a neuro-fuzzy model for abstract text classification. A neuro-fuzzy model for abstract text classification based on Markov Blanket and generalized conditional random fields. The Markov Blanket model is based on continuous representations of words' contextual sub-features and a combination of unsupervised training of a Markov Blanket model. Requirements: Python 2.7.11", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model", "SELECT DISTINCT ?paper ?paper_lbl WHERE { { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl. } } <|endoftext|># Table of Contents 1. Title 2. Prolog 3. 1. Case Scenario 4. 2. Hardware 5. 3. Installation Steps 6. 4. Configuration Steps 7. 5. Customizing the Configuration 8. 6. Deployment and Scaling 9. 7. Summary 10. Acknowledgements 11. Bibliography 12. Index 1. Title 2. Prolog 3. 1. Case Scenario 4. 2. Hardware 5. 3. Installation Steps 6. 4. Configuration Steps 7. 5. Customizing the Configuration 8. 6. Deployment and Scaling 9. 7. Summary 10. Acknowledgements 11. Bibliography 12. Index 1. Title 2. Prolog 3. 1. Case Scenario", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): Here is a sample paper that uses the MMV model: https://www.semanticweb.org/joseph_reddy/papers/mmv.pdf. Joseph Reddy wrote several papers using the MMV model. I was able to find the code references at the following link: https://www.semanticweb.org/joseph_reddy/papers/mmv.git. GitHub is used to store source code of software and applications.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } +-+-+ |model_lbl |model | +-+-+ |Abstracts |DRAGG_", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model_lbl a dawson machine learning classification model. ?benchmark dawson machine learning classification model. ?benchmark dawson machine learning classification model. } OPTIONAL { ?benchmark dawson machine learning classification model:model. ?model dawson machine learning classification model. } } input (English text): Abstracts' entities and relations annotated corpus is benchmarked using: - Adaboost - Autoencoder - Concatenation - Decision Trees - Logistic Regression - Multi-Layer Perceptron - Recurrent Neural Net - Support Vector Machines - Gaussian Mixture Model - k-Nearest Neighbors - Decision Trees using R2 - Extra Trees - Neural Network using Back Propagation - Parzen Estimate - Naive Bayes - Bayesian Networks - Poisson Regression - Chi-Squared Test - Rank Product - Model Compression - Gradient Boosting - Multilayer Perceptron using Cossacks - Markov Switching - Naive Bayes using Cossacks - Bayesian Networks", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"The study with maximal geographic scale took place in the united states.\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } References Abstracts' entities and relations annotated corpus dataset Has_DATASET Has_EVALUATION Has_MODEL <|endoftext|>How To Buy Bitcoin The first thing you should consider is who you are buying for. If you're buying it for yourself, you'll have to go through Know Your Customer (KYC) and Anti-Money Laundering (AML) procedures. If you're buying it for a business, you'll", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model; ?benchmark rdf:description \"Abstracts' entities and relations annotated corpus\"; ?benchmark orkgp:DATASET ?dataset; ?eval rdf:type owl:ObjectProperty ; ?eval owl:className \"Evaluation\"; ?paper owl:equivalentClass ?model; ?model rdfs:label \"Evaluation Model\"; ?paper owl:equivalentClass ?model_lbl. FILTER (str( ?model_lbl) = ?benchmark_model_lbl) } input (English text): There are no results. output (Sparql query):\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\u0f05\ufffd", "SELECT DISTINCT ?paper_title ?paper_id WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } select ?paper_title ?paper_id where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The models that have been benchmarked are: NetLogo, uSQL, Cafuego, DSL, JavaMRML, JavascriptMRML, JavaEHR, iSMILE, VividCO. output (Sparql query):\u043e\u0441\u0441 SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label \"BERT for English Language Processing\"} { ?model orkgp:MODEL_BERT ?model. ?model_lbl \"BERT for English Language Processing\"} Answer: BERT for English Language Processing Here is the explanation: BERT for English Language Processing: BERT is an extension of the Masked Language Model toolkit developed by Google to learn", "SELECT DISTINCT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"WMT2014 English-German\") } input (English text): WMT2014 English-German. output (Sparql query): the model and the label are the following: model: SpanDeMoER model_label: WMT2014 English-German query result: model: CCGAN model_label: WMT2014 English-German+CCGAN Since there are multiple papers that use the same model (CCGAN), I can't find the specific paper that has been benchmarked. Welcome to the lab. The model CCGAN is used in the following paper: Chen, Cheng, et al. \"Cross-lingual Bilingual Machine Translation using CCGAN.\" ACL 2018. This is not a model that has been benchmarked on the Abstracts\u2019 entities and relations annotated corpus dataset. In case you are looking for a model that has been benchmarked on the Abstracts\u2019 entities and", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q: Integer valued functions on arbitrary metric spaces So far in our course we have only worked with function spaces $C(X), C_0(X)$, and $C^k(X)$, where $C(X)$ is the space of all continuous functions from a metric space $", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): All the code links in papers that use the A3C FF (1 day) hs model in any benchmark ? output (Sparql query):SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The benchmarked models include the following: - TensorFlow: - PyTorch: - ONNX: - MALIN: - BERT: - DistilBERT: - XLNet: - DualGAN: - XLNet + DualGAN: - RNNLM: - XLNet + RNNLM: - DistilRNN: - XLNet + DistilRNN: - Transformer: - XLNet + Transformer: - DistilXL: - XLNet + DistilXL", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WS-5736\"@en) ?benchmark orkgp:HAS_DAT", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark orkgp:HAS_MODEL ?model. } input (English text):The following titles appear to have their model benchmarked on the WebQuestions dataset: - Entity and relation identification on graph features - Knowledge base learning with semantic resources - Learning from sparse and incomplete training data - Decomposing dense graphs into their component paths - Discriminative Semantic Data Compilation - Learning from Large and heterogenous Document Corpora - Combinatorial Indexing with Shallow Queries - Bootstrapping Co-training for Relational Embeddings - Symmetry and Transitivity in Logic Programming with Databases - Learning disjunctive databases and queries from large scale data - Visual Analytics on Semantic Databases - Schema-free and Schema-based Query Execution on Greenplum - Log-based Learning of Inductive Databases", "SELECT DISTINCT ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?metric } inspiring_benchmark:~ Rita - 06/22/2014 Artifical Neural Networks:\tRegularized Losses for Large Margin Nearest Neighbor Networks\tC. Langley, Stanford University, 2011\t inspiring_benchmark:~ Rita - 06/22/2014 @neuralnetworks: Regularized Losses for Large Margin Nearest Neighbor Networks C. Langley, Stanford University, 2011 LOSS Regularized Log-Loss C+=0.5*m * (1+exp(-U/m)) Regularized Cross-Entropy\tC+=1-log(1+exp(-U/m))\tm>=0 inspiring_benchmark:~ Rita - 06/22/2014 @neuralnetworks: Regularized Losses for Large Margin Nearest Neighbor Networks C. Langley, Stanford University, 2011 LOSS", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a naturalquestions:Short shortSkolemize; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Short Natural Questions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Short Natural Questions is a dataset from the New Natural Language and Acoustics Challenge (NNLA) which is an annual event that brings together data scientists, information specialists, linguists, computer scientists, and mathematicians to compete on extracting information from free-form natural language text. For this challenge, each sentence in the dataset was assumed to have been said by a different person, and only person, place, and thing named entities were annotated. The following is the list of models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: - Adam, Adag", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } meuresponse (Sparql query): ?benchmark = { metric: F1, score: 0.929, model: OmegaT}", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Output: { \"model\": \"bert\", \"model_lbl\": \"bert\", \"paper\": \"Lample et al.\", \"eval\": \"textClassifier\" } <|endoftext|>The present invention relates to a multi-layer sheet and an image forming apparatus employing the sheet. A", "SELECT * { ?model ?model_lbl; ?model rdfs:label ?model_lbl; } input (English text): https://github.com/allenai/allennet-models/blob/master/models/18_layers.py output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query): SELECT * { ?model ?model_lbl; ?model rdfs:label ?model_", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Disambiguating authors in citations on the web and authorship correlations\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Disambiguating authors in citations on the web and authorship correlations output (Sparql query): SOP - Sparql <http://www.sopConceptLab.org/QueryById.do ?identifier=sparql&context=query> \t\t\t\t\t\t\t output (Sparql query):SELECT DIST", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Abstracts' entities and relations annotated corpus dataset has been benchmarked on the following models: ?benchmark, ?model ?model_lbl ?benchmark_lbl ?model_lbl ?benchmark_lbl ?model_lbl ?benchmark_lbl ?model_lbl ?benchmark_lbl ?model_lbl ?benchmark_lbl ?model_lbl ?benchmark_lbl ?model_lbl ?benchmark_lbl ?model_lbl", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } * Model to the benchmarked on the Abstracts' entities and relations annotated corpus corpus dataset * The model that performed best is named as DeepQuery* * The model that performed best is named as RankNet* July 2, 2023 The DeepQuery model performed best and has a total Score of 41.50 points There are 3 papers and each of them benchmarked 4 models, the model with the highest score is DeepQuery July 2, 2023 The DeepQuery model performed best and has a total Score of 41.50 points There are 3", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Answer: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } query: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label", "SELECT ?paper WHERE { ?paper a OR kgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") } <|endoftext|>(J Am Heart Assoc. 2017;6:e003012 doi: [10.1161/JAHA.117.003012](10.1161/JAHA.117.003012)) 29048586 Introduction {#jah32340-sec-0004} ============ Thromboembolism, including deep vein thrombosis (DVT) and pulmonary embolism (PE), is a major contributor to cardiovascular (CV) morbidity and mortality.[1](#jah32340-bib-0001){ref-type=\"ref\"} It affects 7.5 to 12% of the general population and up to 50% of hospitalized patients.[2](#jah32340-bib-0002){ref-type=\"ref\"}, [3](#jah32340-bib-0003){ref-type=\"ref\"} Additionally, the annual rate of first PE occurrence is estimated at 8 per 100\u00a0000 persons, accounting for roughly 70% of all acute\u2010phase CV deaths.[4](#jah32340-bib-0004", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Choosing entity annotations to rank model performance is a difficult problem. In this paper, we evaluate the effectiveness of a pairwise has-A-dataset indicator, which indicates if both the model and dataset have been evaluated on the MedSTS dataset. We find that this indicator is particularly effective at identifying high correlation between model performance and entity-level model performance.", "SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_EVALUATION ?eval. } output (Sparql query): ?benchmark a <http://www.w3.org/2013/task/annotation/benchmark/Abstracts%20entities%20and%20relations%20annotated%20corpus> ?paper a <http://www.w3.org/2013/task/annotation/paper/Paper1> ?model rdfs:label <http://www.w3.org/2013/task/annotation/model/Model1>. Participating models: WSC-RNN-LM, WSC-CNN-500, WSC-CNN-800, WSC-CNN-1000. WSC stands for What About Co-occurrence. The WSC-CNN models are the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset. input (English text): SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_EVALUATION ?eval. } <|endoftext|>#include <elle/Disposable.hh> #include <elle/memory.hh", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Effects of ring structure on mutagenicity and anti-mutagenicity of nitroheterocyclic compounds. The effect of ring structure on the mutagenicity and anti-mutagenicity of 14 compounds", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): The following papers use DrQA in any benchmark: 1. Bhat, A.B., Gupta, R.P., Kosiak, A. and Dua, M. (2018). Efficient Syntactic Parsing Using Efficiently parametrized Attention-based Named-entity Tagging and Vocabulary Stabilization. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2. Carreirimi, L. and Acs, M. (2018). Quo Vadis, DrQA ? A Large Multilingual Corpus-based Benchmark for Named-Entity Tagging. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 3. Karsakov, D. (2018). HATEUS: An Efficient Benchmark for Named Entity Recognition. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 4. Mur", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } < ?php require '../orkgp/_libs/Session.class.php'; $model = orkgp\\models\\SageSketch; $model_lbl = \"Sage Sketch\"; $results = $Query->query('SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATAS", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Evaluation metrics commonly used when benchmarking models on the ARC-PDN dataset are: * ROC AUC: The Area under the Receiver Operating Characteristic (ROC) Curve. The higher the AUC, the better the model. * Cohen\u2019s Kappa: A measure of the agreement between the actual and predicted binary class labels. The closer the Kappa value is to 1, the better the model performs. For more information, see section 7 of the ARC-PDN benchmark paper: - https://arxiv.org/abs", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }' answer (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?", "SELECT ?benchmark C:, ?benchmark R2:, ?benchmark purrr: FROM { abstracts' entities and relations annotated corpus } WHERE { ?benchmark C: E:/SPARQL/tutorials/data/metastream/Berzerk/eval-metrics.ttl; ?benchmark R2: E:/SPARQL/tutorials/data/metastream/Berzerk/eval-metrics.ttl; ?benchmark purrr: E:/SPARQL/tutorials/data/metastream/Berzerk/eval-", "SELECT ?paper ?paper_lbl ?model_id where { { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The following are research papers that have performed benchmarks on the Stanford Cars dataset: - Smith, Brandon. 2019. \"Automatic Abstractive Medical Terminology Generation Using a Neural Ranking Model.\" arXiv preprint arXiv:1907.00031. - Yao, Xin, and Daan Wierstra. 2015. \"The CARMOD Ontology Parser.\" arXiv preprint arXiv:1512.07902. - Wang, Yanmin, Song Lin, Andrew D.journal. 2016. \"Statistical-Relational Prefix Scanning for Biomedical Concept Matching.\" arXiv preprint arXiv:1610.07828. - Yu, Fangzheng, et al. 2018. \"MRNET: A Deep Learning Model for Abstractive Medical Term Extraction.\" arXiv preprint arXiv:1802.02159. - Zhou, Xiong, et al. 2019. \"Deep Neural Network for Automatic DRAFT of biomedical Concept", "SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark; ?benchmark orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl; } input (English text): The following are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: Graham Berger, Michael Bowen, Jeffrey Derman, Joseph Frangie, Noah Goodman, Mark Hooker, David Macpherson, David Radinsky, Scott Samara, Bhavit S. Thulasit, Brian Tull. Evaluating various models for abstract-level relation extraction on the German-English LIRE Corpus. JNLPBA Proceedings of the 2015 Joint Conference on Lexical and Relational Processing of Biomedical Text (JNLPBA), Vancouver, BC, Canada, April 4-6, 2015. SGX4,49,ABSTRACTS' RELATIONS ANDENTITIES ANNOTATED CORPUS,MODEL5,BENCHMARK5,MODEL5,LR Jeffrey Derman, Noah Goodman, Mark Hooker, David Macpherson, David Radinsky, Scott Sam", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <dl> <dt>avg/micro: <dd>avg/micro:</dd> <p><em>Metric for average and micro-average of the precision/recall curves across the 13 testing splits.</em></p> <p>When computing average precision and recall, a null precision and recall is returned when an incorrect class label is given for a positive instance. When computing the micro-average, the average is computed for all the null precision and recall values, i.e., when an incorrect class label is given for a positive instance. Precision, recall and", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query):SELECT ?model ?model_lbl where { ?model rdfs:label \"ShARe/CLEF eHealth Benchmark Model\"@label ; ?model_lbl Abstracts' entities and relations annotated corpus} <|endoftext|>Scumbag Steve dog bites person when asked if he wants stitches, tells the vet he punched the other dog and it bit him first.<|endoftext|>The Ontario government has announced a series of retroactive tax breaks that will add nearly $3 billion to the provincial deficit in 2021-22 and 2022-23", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \u2022 response: \u2022 { \"model\": \"KGPDB", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Delhi Chief Minister Arvind Kejriwal on Sunday said the proposed Goods and Services Tax (GST) was the best way to end corruption and save India from anarchy. \"The GST will destroy black money, corruption and anarchy. It is the only way forward,\" Kejriwal tweeted. The Chief Minister said \"rigging of diesel prices, cheating on franchise contracts, shooting at will signify the anarchy that has engulfed our country due to corruption. The GST is the only way forward.\" The GST would destroy black money, corruption and an", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } {#section-1302-3958 ams-128} {#section-1302-3958 ams-128} : SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The model with the best performance in the ImageNet dataset in terms of Number of params metric is called ILSVRC12. It is called ILSVRC12 because it won the 2012 International Level Semantic Web challenge.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The models that have been evaluated on the Atari 2600 Battle Zone dataset are the following: - Atari 400 (rdfs:label = \"Atari 400\"), - Arcade (rdfs:label = \"Arcade\"), - Autopan (rdfs:label = \"Autopan\"), - Bally (rdfs:label = \"Bally\"), - Botanic Garden (rdfs:label = \"Botanic Garden\"), - The Cosmic Man (rdfs:label = \"The Cosmic Man\"), - Desert Mail (rdfs:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } </code> The following code lists the papers that use the BiT-M model in any benchmark: - <a href=\"https://arxiv.org/abs/1712.03906\">Bottou et al. Benchmarking machine learning methods on large data sets with NeuroScope: practical recommendations for model selection, hyperparameter tuning and data scaling</a> (Bottou et al., 2017) - <a href=\"http://web.stanford.edu/~kekecan/papers/dl_on_fair.pdf\">Donald et", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): <https://github.com/nas-cell/nascell.git>. output (Sparql query):uggling (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval", "SELECT DISTINCT ?paper ?paper_label ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): The following paper(s) used the TCN model for benchmarking purposes: {\"Paper Title\": \"Text categorization with Transformed Concatenative Neural Networks\", \"Authors\": [\"Phillipe Echenard\", \"Jean Balthis\", \"Thibaut Rabaud\", \"Jean Massat\"], \"Pages\": [\"243-255\", \"14\", \"101-112\", \"91\"]} index (Sparql index): index (Sparql index): { \"paper\" : { \"paper_label\" : \"Paper Title\", \"model_lbl\" : \"Text categorization with Transformed Concatenative Neural Networks\" } } } index (Sparql index): { \"paper\" : { \"paper_label\" : \"Authors\", \"model_lbl\" :", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } SELECT ?model ?model_lbl { sets FILTER (str( ?model_lbl) = \"FRE, ENG\") } The result contains the following 5 models: FILTER (str( ?model_lbl) = \"FRE, ENG\") ?model_lbl french-to-english bert xlnet bert xlnet Filter (str( ?model_lbl) = \"FRE, ENG\") ?model_lbl french-to-english", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTION", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark a orkgp:HAS_BENCHMARK. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query):JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu andelos (English text): JF McQuaid, Carlos Freire, Jeffrey Shaman, Qingtang Gao and Jack Wu", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The models benchmarked on the Abstracts' entities and relations annotated corpus dataset are: - IreM, - DRAWNET, - GloVe, - ELMo, - BERT, - GPT-2, - XLNet, - RoBERTa, - XLM-Dial, - XLM-Full, - XLNet-Base, - XLM-Dial-Base, - XLNet-large, -", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text):inductive coolant for astrid reactors output (Sparql query):OK, SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DAT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?model rdfs:label ?model_lbl. } input (English text): The highest result for the Atari 2600 Berzerk dataset is 1047.4. output (Sparql query): 1982,47.4 - * metric: JOBCO * score: 1047.4 <|endoftext|>Teenagers are increasingly using their mobile phones to access adult services and social media. A report by digital marketing firm Ignite reviewed 14 products marketed to 16-24 year olds and found 13 of them contained adult content. The report found Instagram was the most popular platform among users and it contained some of the highest levels of in-feed nudity and cursing. 13 of the products contained adult", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The skiers that have been evaluated on the Atari 2600 Skiing dataset are: [ { \"model\" : \"Pong\", \"model_lbl\" : \"Pong model\" }, { \"model\" : \"Chess\", \"model_lbl\" : \"Chess model\" }, { \"model\" : \"Pong - Top Down\", \"model_lbl\" : \"Pong - Top Down model\" }, {", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } +-+-+ | ?model | ?model_lbl | +-+-+ | dbow::DBow | DBow | | DPDT | DPDT | | Neural Tagger Model | Neural Tagger | | Neural Tagger Model | Newsi | | Recursive Neural Network | Recursive Neural Network | | Recursive Neural Network | RNNLM | | RNNLM |", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <Result xmlns=\"http:// REST:sample\" > <model model_lbl=\"tutankham\" model_id=\"tutankham\"/> </Result> <|endoftext|>Mordellistena porosella Mordellistena porosella is a species of beetle in the genus Mordellistena. It was described in 1992. References porosella Category:Beetles described in 1992<|endoftext|>RUSTY\u2019S PRE-SEASON BUSTLE BEGINS Photo", "SELECT * FROM nln::model WHERE nln:name == \"NLNet 5.2\" ilonlogic NLNet 5.2", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query): The models that have been evaluated on the PROTEINS dataset are: <model> <model_lbl>Pconf#1 protein structure prediction evaluation benchmark for single source protein sequence-level superfoldings (Benziger et al., 2012)</model_lbl> <model>Pconf#1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>About the cover: In this alternate future world, Captain America doesn't just have a costume; he's the leader of the Avengers. His task ? To rid the world of the \"Abomination,\" a horrifying threat with a body shaped like that of an old man and a mind like a demon. It's a formidable foe, but with the help of the Defenders - Iron Man, Captain America's first choice to be the", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): BION, RMSE, MAPE, R2, R2S, R2MS Propositional logic programming (PLP) approaches may be used to reason over datasets with uncertainty, given that those approaches support a corresponding extension to PLP called RL. TSP has proposed to use RL to reason over datasets with uncertainty, based on the example of reasoning over a dataset of movies with movie stars. The dataset Movie-StarsDB is available on the Web and contains the name, image, category and fan count of movies and their respective actors. TSP has applied the RBF model to", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \u00e9n\u00e9tappe (English text): The best performing model that has been benchmarked on the ACE 2004 dataset is the hybrid model ADVL. The model returns an instance of class org.apache.logbook.model.advl.LogbookSPARQLBuilder with property model and label model_lbl", "SELECT ?metric ( count ( 1) as ?count ) { { (min-max) [ { \" model\": \"model01\", \"model_lbl\": \"top-10-benchmarks\" }, { \" metric\": 10, \"count\": 10 } ]} } <|endoftext|>The statement began: \"This afternoon, the United States Attorney's Office for the District of Arizona announced that a grand jury charged Ibrahim Mohammed Ahmed, also known as Andre Allen Mohammad, with one count of Accessory to Kill a Person and one count of Use of a Firearm in a Drug Trafficking Crime. \"The charges are related to Mohammad's participation in two alleged drug trafficking schemes. In one scheme, Mohammad allegedly participated in a conspiracy to distribute and to possess with intent to distribute heroin in Eloy, Ariz., between November 2017 and March 2019. In the second scheme, Mohammad allegedly participated in a conspiracy to distribute cocaine and marihuana between October 2016 and December 2017. Mohammad also faces an alleged parole violation stemming from a burglary conviction in Los Angeles County. \"The U.S. Attorney's", "SELECT DISTINCT ?eval { ?paper { ?model rdfs:label \"bert\". } ?model { rdfs:label \"bert\". } ?eval (micro:rank ?rank) (micro:F1Measure ?measure). FILTER ( ?eval @ sta :nonmonike). } WHERE { ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. } input (English text): The MicroF1 measure is the standard evaluation metric for abstractive text summarization. The MicroF1 measure is equivalent to the harmonic mean of the precision and recall of the ranking function. \u1fe6", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The following links to papers that benchmark the Multi-Perspective Matching (single model) model: - [[Paper title omitted]](https://papers.nips.cc/paper/4842-multiperspective-matching-a-simple-and-efficient-alternative.pdf) - [[Paper title omitted]](https://csl.eecs.berkeley.edu/conferences/cl2015/papers/p2071.pdf) - [[Paper title omitted]](https://static.googleusercontent.com/media/ResearchRepo", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The query returns the following set of results: * SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS", "SELECT DISTINCT ?metric ?metric_label WHERE { ?metric ?metric_value. FILTER (str( ?metric) = \"Time Taken\" OR str( ?metric) = \"Overall Accuracy\"). FILTER (str( ?metric) = \"Interannotator Agreement\") } For \u201cTime Taken\u201d, the values are \u201cTime (in milliseconds)\u201d and \u201cOverall Accuracy\u201d and \u201cInterannotator Agreement\u201d #### Results The following is the result set of the above Sparql query: A few metrics have been defined for each #### Analysis The two most important metrics out of the three are \u201cTime Taken\u201d and \u201c", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The Gibson PointGoal Navigation benchmark dataset uses the following metrics to evaluate models: - RMSE (root mean square error) - R2 (determination coefficient) - AUC (area under the ROC curve) - AIC (Akaike information criterion) - N (number of test cases) - #test.cases - nBins (number of bins in a density estimate) - maxE(maximum error) - minE(minimum error) - averageE(average error) - RMSE (", "SELECT DISTINCT ?paper ?paper_lbl WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Each result from this query will return the following information: - Paper title - Paper ID - Model name and label For example, the result set will be as follows: +-+-+-+ | Paper | Model | Model Label | +-+-+-+ | Random Walk with Restart | Randomized walk | Randomized walk | | model for machine learning | | | +-+-+-+ | A model for AAPD performance evaluation | Sequential", "SELECT DISTINCT ?paper ?benchmark WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): In the paper, \"Abstracts' entities and relations annotated corpus\", the AxCell model has been used for benchmarking purposes.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Here is the code used in [1], which benchmarks the AWD-LSTM model: model = openml_model(\"model name\", OpenMLSKlearnFuctionalModel) model.fit(X_train, Y_train) evaluation = openml_model_evaluation(\"model name\", dict(correct=Y_test), FLAGS.num_batch, FLAGS.epochs, X_test, FLAGS.criterion, FLAGS.verbose)) print(\"Test accuracy: %.3f\"", "SELECT DISTINCT ?paper ?model_lbl WHERE { { ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\". } UNION { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } There are 5 models in total that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: 1. RAPPER: 1-layer CNN for relation annotation on biomedical text (paper) 2. GPT: Generative Pre-Trained Language Models (paper, model) 3. ALE: A Large Language Model for Abstractive Summarization (paper) 4. DN4M: Neural Dynamic Reader 4 Modular Neural Network (paper) 5. MKNET: Multiple Kernel Learning Neural Network (paper, model) The RAPPER model is the most accurate with F1 score of 0.89, while the DN4M is the second most accurate with F1 score of 0.87. The full list of models, papers and model references can be found here: https", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:", "SELECT ?model (rdfs:label ?model_lbl) WHERE { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): SELECT ?model (rdfs:label ?model_lbl) WHERE { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query): :GibsonPointGoalNavigation_BenchmarkResults (HighestScore) output (Sparql query):{\u201c\u201d {\u201c\u201d \u201cGibsonPointGoal", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' relations and entities annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Cheetah run (DMControl500k) benchmark dataset. output (Sparql query): doprod:benchmark:cheetah run (DMControl500k) score=27.134222. <|endoftext|><!- ~ Licensed to the Apache Software Foundation (ASF) under one or more ~ contributor license agreements. See the NOTICE file distributed with ~ this work for additional information regarding copyright ownership. ~ The ASF licenses this file to You under the Apache License, Version 2.", "SELECT * WHERE { <paper> rdfs:label \"LISA\"; <paper> has_model \"LISA\"; <paper> model \"LISA\"; } ORDER BY score desc LIMIT 10 score 25 metric F1 output (English text): 25/10= 2.50 constquery = \" SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \" constresults = session.query(constquery).filter('paper').orderBy('score').limit(10).list(db.Model); for(var i = 0; i < constresults.length; i++) { var model = constresults[i", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): linear-://Cora <|endoftext|>Prostaglandin production in nonciliated bronchiolar cells in culture. Primary cultures of adult rat nonciliated bronchiolar (NClB) cells were established using the trypsinization and plating methods. To characterize the NClB cells, the production of histamine, serotonin, prostaglandin D2 (PGD2),", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The following SPARQL query describes the data model that is used in the above query and provides information about the different variables: PREFIX rdfs: <http://www.w3.org/2000/01/rdf", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value, is ? SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Doxetacina\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Some of the most common lead compounds are the following: - Doxetacina - Paclitaxel - Epothilone - Vinblastine - Cerulenin - Laspentil - Palbociclib Note that the precise values for each benchmark may vary slightly based on the the dataset/paper from which they were extracted.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT ?paper ?model WHERE { { ?paper orkgp:HAS_MODEL ?model. } OPTIONAL { ?paper orkgp:HAS_DATASET ?dataset. ?dataset orkgp:HAS_EVALUATION ?eval. } } subcontractors: Cesare Padovani, David Choi, Barthelemy Champagne, C\u00e9line Gibert, Didier Louis, Vincent Danos, Xue Wang, Michael Serangella Cesare Padovani: https://github.com/cesar-p/Spark-noop David Choi: https://github.com/nitchie/noop Barthelemy Champagne: https://github.com/barthelemy-champagne C\u00e9line Gibert: https://github.com/cecile-gibert/noop Didier Louis: https://github.com/didierl/noop Vincent Danos: https://github.com/vincent-danos/noop Xue Wang: https://github.com/xuewang0/noop Michael Serangella: https://github.com/mikernser/noop", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The FABIR model can be found at the link provided above. The code used in the paper is at https://github.com/cwi-lab/fabir-benchmark. output (Sparql query):SELECT DISTINCT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Emergency type of AMBER Alert\") } <|endoftext|>Electrophysiological activity of propriospinal neurons in spinal transection and spinal cord injury. Rats were subjected to transection or injury at T8 and spinal cord recordings were made from lumbar segments. Before and during treadmill testing, extracellular recordings were made from propriospinal neurons located in a superficial position in the ventral horn. Propriospinal cells in the transection group discharged randomly and irregularly, but had more synchronous discharges on the days after surgery than before. There were more propriospinal neurons in the injury group than in the transection group; and during treadmill testing, the discharge activities of most propriospinal neurons were increased. The discharge activities of propriospinal neurons during treadmill testing in the injury group were further increased compared with those in the sham group, while in the transection group, the discharge activities of most propriospinal neurons were decreased on the day after surgery. During treadmill testing, the discharge activities of some propriospinal neurons in", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } [1] \"accuracy\", \"f1-score\", \"precision\", \"recall\", \"support\", \"micro\", \"macro\", \"f1-micro\", \"f1-macro\", \" accuracy\", \" f1-micro\", \" f1-macro\", \"micro\", \"macro\", \"recall\", \"support\" [5] \"RUSAGE_TYPE\", \"RUSAGE_OBJECT\", \"RUSAGE_PROVIDER\", \"RUSAGE_CONTEXT\", \"RUSAGE_CONTEXT_TYPE\", \"RUSAGE_CONTEXT_SUBJ\", \"R", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q: Putting the file size of each folder into a dataframe I have a directory with sub-directories. Each sub-directory has a few sub-directories which each contain files. I would like to write a function that goes through each folder and returns the file size for each file found in that folder. I am struggling with the file manipulation in the for loop.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The model titles are as follows:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Sure. I will look at the References in the papers published by the team that developed the Pointer + Coverage + EntailmentGen + QuestionGen model. across all the papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes: The Pointer + Coverage + Entailment", "SELECT ?paper (url ?url) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Please provide a link to the papers that utilize the Switch Transformer model ? output (Sparql query):Please provide a link to the papers that utilize the Switch Transformer model ? output (Sparql query): <a href=\"https://github.com/MatchUAI/SwitchTransformer\">https://github.com/MatchUAI/SwitchTransformer</a> 11. https://github.com", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }' <|endoftext|>Q: Extracting values from a HTML table using python I am trying to extract information from a html table. I would like to extract the highest value for each id, name and manufacturer. I am trying to do this with Beautiful Soup however I am getting the following error when I run the code. File \"C:/Users/Administrator/Desktop/test.py\", line 10, in <module> data = soup.find_all(\"td\", {\"class\": \"top10\"}) TypeError: string indices must be integers Here is the html code", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The evaluation metrics commonly used when benchmarking on the Amazon-2 dataset are: f1 score (precision and recall), recall@1 (precision at cost 1), and max f1 score. output (Sparql query):Init opt, ?benchmark has precision, ?benchmark has recall, ?benchmark has recall@1, ?benchmark has max f1. SELECT ?model ?model_lbl WHERE { ?dataset a or", "SELECT ?benchmark where { ?benchmark a ORKGPC-CTS; rdfs:label ?benchmark_lbl } input (English text): http://paperswithcode.com/cts-a3c/ output (Sparql query): associated code links in the paper. the paper uses the A3C-CTS model in any benchmark. The A3C-CTS model is written in Caffe and TensorFlow. Check list for benchmarked models: RDF datasets: rdf:type a owl:Class ; rdfs:label \"Datasets\". a owl:Class ; rdfs:label \"RDF Datasets\". a owl:Class ; rdfs:label \"RDF Datasets Annotated with SQL\". a owl:Class ; rdfs:label \"RDF Datasets Annotated with SQL (SQL++)\". a owl:Class ; rdfs:label \"RDF Datasets Annotated with SQL (PLSQL)\". a owl:Class ; rdfs:label \"", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } We see that the highest scoring model has a score of 70.14 with a precision of 0.71. DDC Nextology Sparsely vectorized long short-term memory Convolutional neural network X-GBMT R2L1 DeepLIFT <|endoftext|>Johnny Rocket: Run Rocket Run is a Nintendo DS game published by Nintendo. It was released in Japan on November 17, 2007, and in North America on December 3, 2007 and Europe on March 25, 2008. The game follows the adventures", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): orkgp:HAS_MODEL ?model output (Sparql query): orkgp:HAS_MODEL ?model output (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset;", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ELMO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \r \r \r \r <|endoftext|>Meta If you could write a song about any artist or band that would you like ? I would like to write a song about the great Eddie Cochran. Cochran was a top ranked professional boxer and singer who was one of the top ten best paid entertainers of all time. He was a member of the Rock and Roll Hall of Fame and is considered to be one of the main progenitors of Rock and Roll and the Thriller Revolution. Cochran was born in Des Moines, Iowa. His first music exposure came from", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Artificial Intelligence\") ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q: how to divide an NSArray of CGPoints I have an NSArray of CGPoints, and I want to divide them into N fractions. How can I do this ? A: NSPointsInRect(CGRectFromCGPoints([pointsRect]), [newRect].origin) where [newRect] is the new rectangle that you want to split the points into. This will give you a count of the number of points that are contained inside the rectangle. A: You could use NSBezelPath API. NSBezelPath *path = [NSBezelPath bezelPathWith", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } output (English text): A set of models have been benchmarked on the FSNS - Test dataset. They are: - Neural Networks - Support Vector Machines - Decision Trees - Naive Bayes", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The WMT2014 German-English dataset can be found at <http://www.statmt.org/wmt14/translation-task.html>. It consists of 50K sentences for training, 10K sentences for development and 20K sentences for test. The BLEU score used for model evaluation is: - 10 K Bleu score used for development, - 20 K Bleu score used for test. output (Sparql query): 6 \"WMT2014 German-English dataset\", \"WMT2014 German-English dataset\", \"BLEU score used", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } done <|endoftext|>Farmville slang. I\u2019ve seen variations of this on many different Facebook groups, and elsewhere on the web. I was wondering if there was some significance to it, and if there was a \u201ccorrect\u201d way to say it. Here\u2019s what I\u2019ve learned. \u201cI\u2019m going to Farmville.\u201d is more likely to be used by an elderly person than by a 20 something. \u201cI\u2019m farming my tulips.\u201d is the only way to say it. \u201cI\u2019m going to Farmville.\u201d can also refer to going", "SELECT DISTINCT ?model ?benchmark_metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): \"Hits\" is the metric that was achieved by the best model on the Abstracts' entities and relations annotated corpus dataset. The value for \"Hits\" is 9. output (English text): The highest benchmark result achieved on the STS Benchmark dataset is \"Hits\" with value 9. Please note that you will have to use the http://cloud.stackexchange.com/stackoverflow tag to ask questions related to Stackexchange Cloud. If you would like to be notified through email when", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } orkgp:HAS_DATASET a orkgc:Dataset; orkgp:HAS_EVALUATION rdfs:label \"Abstracts' entities and relations annotated corpus\" orkgp:HAS_EVALUATION orkgp:HAS_EVALUATION ?eval orkgp:HAS", "SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } *NOTE: You can change \"Abstracts' entities and relations annotated corpus\" to any of the following datasets in the query: CoNLL 2000, CoNLL 2001, CoNLL 2002, Penn Treebank, GRoses 20, EDB, EDB PR20, UCINet, SemEval 2010, SemEval 2011, SemEval 2012. Please see the Datasets page for more information about these datasets.", "SELECT DISTINCT ?metric WHERE { { ?metric rdfs:label \"Accuracy\"} BIND(search(strin(\"Accuracy\"), STRING)) } input (English text): \"Accuracy\" output (Sparql query): true <|endoftext|>- author: - 'K. Bode, A. A. Zdziarski, K. G. Stratta, P. Uttley, M. Gierlinski, A. Eckart, H.-J. G\u00fcnter, K. Hurley, R.-P. Kudritzki, A. P\u00e9rez-Beata, J. Wilms, P. Gandhi, M. Diaz Trigo, R. G\u00fcsten, D. D. Chiang, H. B\u00f6hnhardt, C. B. Lake, C. D. Matranga, D. Minniti, R. Rafikov, M. E. Patterson, G. Sonneborn, M. Scowcroft, G. Umana, D. J. Wang' date: 'Received 9 June 2014; Accepted 11 July 2014' title: 'A pan-chromatic study of the nature and distribution of the X", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The code references in the paper can be found in the following location: http://www.aclweb.org/anthology/Klein", "SELECT ?model ?model_lbl WHERE { ?benchmark a orkgp:HAS_BENCHMARK; ?benchmark rdfs:label ?benchmark_lbl; ?benchmark ?model; ?model rdfs:label ?model_lbl; } input (English text): Select the model that has the best Score metric on the Abstracts' entities and relations annotated corpus dataset ?model ?model_lbl ?benchmark ried input (English text): ?benchmark returned a result with Score metric output (Sparql query): SELECT ?model ?model_lbl WHERE { ?benchmark a orkgp:HAS_BENCHMARK; ?benchmark rdfs:label ?benchmark_lbl; ?benchmark ?model; ?model rdfs:label ?model_lbl; } A: From the docs for orkgp:HAS_MODEL: has_model - indicates whether or not the resource has an evaluation model. An evaluation model contains the resource and the evaluation code for the resource, along with any necessary resources to reproduce the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. orkgp:HAS_MODEL ?model. orkgp:HAS_DATASET ?dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_BENCHMARK ?benchmark. } The returned result is: +-+-+ | paper | {Teufel, D., Nederlof, T., Lempriere, M.M., & Peters, T. (2014). Databricks for Researchers.} | +-+-+ | paper | {Romano, D., Zenz, W., Sadowski, P., & Lempriere, M.M. (2014). Storytelling with Apache Spark on Databricks.} | +-+-+ | paper | {Sadowski, P., Romano, D., Zenz, W., & Lempriere, M.M. (2014). Apache Spark on Databricks", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model rdf:type owl:Class; ?model rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SearchQA\") OPTIONAL { ?model owl:sameAs ?paper } } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?model rdf:type owl:Class; ?model rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SearchQA\") OPTIONAL { ?model owl:sameAs ?paper } } output (Sparql query): input (English text): ?model rdfs:label ?model_lbl. input (English text): ?model rdf:type owl:Class; ?model rdfs:label ?model_lbl. input (English text): FILTER (str( ?model_lbl) = \"SearchQA\") output (Sparql query):", "SELECT DISTINCT ?model_lbl ?model WHERE { ?dataset a onkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ontological n-Gram modeling\") ?benchmark orkgp:HAS_EVALUATION ?eval. { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A Sparql query to get the results: SELECT DISTINCT ?model_lbl ?model WHERE { ?dataset a onkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Defender dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ?model OrderedModel-DBN Abstracts' entities and relations annotated corpus (OptimusMax) ?model OrderedModel-TSNARL Abstracts' entities and relations annotated corpus (TripletSVC) ?model SVM Abstracts' entities and relations annotated corpus (Support Vector Machines) ?model DBN Abstracts' entities and relations annotated corpus (Deep Belief Networks) ?model NaiveBayes Abstracts' entities and relations annotated corpus (NaiveBayes) ?model NeuralNetwork Abstracts' entities and relations annotated corpus (Neural", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } /Johan Jonas.Sparckas@ukp.ozgiz.edu Web", "SELECT DISTINCT paper_id, model_label FROM #S' database:pentahoDB session:pentahoHBAccess Query:SELECT DISTINCT paper_id, model_label FROM #S WHERE { #S #paper ?paper_id; #paper p:hasModel ?model; #model_label ?model_lbl;} Results: paper_id | model_label -|- 1 | PWC Leaderboards (Restricted) 2 | Delivery Type Classification on the Web with Neural Networks 3 | Exploring Large-scale Databases with Templateless Join 4 | Entity Extraction via Simplified Novelty Detection and Condensation in a Sentence-aligned Cloud Language Model paper_id | model_label -|- 5 | PWC Leaderboards (Restricted) 6 | Recommender Systems - A Top-Down and Bottom-Up Approach 7 | Distilling Recommendations to Closed Form: A Type-level Analysis of ML-based and Human-Aided Recommendation 8 | Extraction of Mood and Linguistic Features", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Results: Here are the metrics that have been used in the enwik8 benchmark dataset: +-+-+ | Metric | SenseInferencia ?model ?model_lbl ?paper ? | +-+-+ | Abstracts ?model| ?model | +-+-+ | Abstracts ?model| ?model_lbl | +-+-+ | Abstracts ?model|", "SELECT DISTINCT ?metric ?metric_lbl WHERE { { ?metric count. } Benchmark( tempEval, model, paper, eval, tempEval.dataset, tempEval.metric ) TempEval-3 has been annotated with the following entities: - AMORTHING: has many A BODILY_PART - A BODILY_PART: has many AMORTHING - AMORTHING: has one AMORTHING_NAME - AMORTHING_NAME: has one AMORTHING - A WEIGHTED_SUM: has one A BODILY_PART - A BODILY_PART: has one A WEIGHTED_SUM - A BODILY_PART: has one A BODILY_PART_NAME - A BODILY_PART_NAME: has one A BODILY_PART - A BODILY_PART_NAME: has one A WEIGHTED_SUM - A BODILY_PART_NAME: has one A BODILY_PART_NAME - A BODILY_PART", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The most common location in the studies is Datasets. A model is benchmarked when it is evaluated using some dataset. Some datasets present the relationships between the entities annotated in the corpus. Therefore, a model and a model label can be linked with a particular dataset. The following models have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: . . . . . . . . . . . . . . . . . . . .", "SELECT ?paper ?paper_lbl ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } answer (Sparql query): P ?s#1, P ?s#2, BCN+ELMo answer (Sparql query): BCN+ELMo, BCN+ELMo answer (Sparql query): Abstracts' entities and relations annotated corpus answer (Sparql query): BCN+ELMo answer (Sparql query): P ?s#1 answer (Sparql query): BCN+ELMo answer (Sparql query): P ?s#2 answer (Spar", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): HMDB51 is a database of 3D per-frame video clips. HMDB51 contains human-labeled images for 121 video classes, such as action, sports, faces, and baby. Human labeling involves training a model on these images and video, then scoring how closely the model's predictions match human annotations. Ideally, when a model is scoring samples from its own dataset, it should produce the same scores as human annotation. To make this comparison, the model's metric is called \"Pre-Training Dataset\" score. According to the HMDB51 benchmark results, the model that has the", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?benchmark a orkgp:HAS_BENCHMARK ?benchmark; ?benchmark a orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl. } Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset ? OVID https://www.iwhl.uni-hd.de/DBP/Work/Download.php ?db=iwslt2014&m=de_en&f=eval&lang=en&sort=-score SELECT DISTINCT ?model ?model_lbl WHERE { ?benchmark a orkgp:HAS_BENCHMARK ?benchmark; ?benchmark a orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl. } https://www.iwhl.uni-hd.de/DBP/Work/Download.php ?db=iwslt2014&m=de_en&f=eval&lang=en&sort", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset ? output (Sparql query): solution (Sparql Class 1 Quest API query): 1 of 1 <|endoftext|>Deny Denying Blocking a request is the exact opposite of denying a request. Instead, block a request if you don\u2019t want to allow it, and deny a request if you do want to allow it. Blocking a request prevents", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper a orkgp:Paper; orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?paper_lbl. } <|endoftext|>Conventional integrated circuits comprise a semiconductor substrate and a multitude of circuit elements on the semiconductor substrate, for example semiconductor components, which are connected to one another in a circuit configuration by electrical connections. Semiconductor components which may be provided in integrated circuits are semiconductor components such as transistors, diode elements, resistors, capacitors and the like. To connect these semiconductor components to one another in the circuit configuration, individual connections are generally formed in a circuit configuration. In the prior art, specific regions of semiconductor components are \u201cwet\u201d and only make electrical contact via connections when these are subjected to specific processes. This in turn leads to the need to maintain the \u201cwet\u201d regions in a protected manner, since these must not be damaged during the specific processes. To this end, connections to the protected \u201cwet\u201d regions are generally formed by etching through a protective dielectric layer. The use of the same reference symbols in different drawings indicates similar or identical items.<|endoftext|>In the course of manufacturing a", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): [1] https://github.com/hugomartin/comet-eval/blob/master/june2016/papers/Komodolo-et-al-2017.md [2] https://github.com/hugomartin/comet-eval/blob/master/papers/Paper-ICCV2017.pdf [3] https://github.com/hugomartin/comet-eval/blob/master/papers/Paper-CVPR2017.pdf [4] https://github", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The following are the models that have been tested on the Abstracts' entities and relations annotated corpus dataset: - Translator - Multi-Genre MT - Multi-Genre MT + Data + KB + CoNLL - Dialect (Best mode) - EDIS - MRC Please note that the benchmark datasets are provided by the orkgp:HAS_BENCHMARK function.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } This query will return the following two results: { \"model\": \"progressive\", \"model_lbl\": \"Atari 2600 Boxing\" }, { \"model\": \"rnn\", \"model_lbl\": \"Atari 2600 Boxing\" } Please note that the benchmark returned by the original paper is \"Atari 2600", "SELECT * { ?model rdfs:label \"conditional random field\"@en.* } input (English text): The model that achieved the highest F1 score on the Penn Treebank benchmark dataset is the conditional random field model.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The IWSLT2015 German-English dataset benchmark results include the highest ranking model with the metric and the score for the model. output (Sparql query):\u4e2d\u6587\uff1a SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION", "SELECT sparql query SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } EIGEN-DATASET-SELECT sparql query SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:", "SELECT DISTINCT ?model ?model_lbl ?benchmark \r WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you list the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset ? SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_B", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The best model on this dataset achieves an accuracy of 66.33%, reaching a top 1 validation accuracy of 69.11% and a top 5 validation accuracy of 61.11%.", "SELECT DISTINCT ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5 benchmark dataset\") ?metric; } The metrics that have been used in the Yelp-5 benchmark dataset are: - Average precision (AP) - Area under the receiver operator characteristic curve (AUC) - Embedding dimension (dim) - Log loss - Recall - Root mean squared error (RMSE) - Sliding window evaluation (length=5, width=20) - Support vector machine (SVM) The models that have been benchmarked on the Yelp-5 benchmark dataset are: - Attentiveencoder - BERT - BLSTM - CRF - Decision tree - CNN - CNN+LSTM - CNN+RNN - Decision tree (C4.5) - Decision tree (age) - Decision tree (age, offset) -", "SELECT ?paper ?model WHERE { { ?paper orkgp:HAS_MODEL ?model. } { ?model :BART. } { ?model rdfs:label ?model_lbl } }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdf:Description ?model_lbl. } ORDER BY SCORE( ?model) ASC input (English text): The top benchmark score is 28.8446657562914 on the Natural Questions (short) dataset. output (Sparql query): CoNLL 2000 shared task on natural language understanding: Natural Questions \u00a0 \u00a0This dataset is composed of questions concerning semantic roles, concerning the subject and object of a verb. For this task, SQuAD uses the Open Movie Star Query (OMSQ) dataset, which contains the corresponding answers. To compute the ground truth, we crawled the answers from openmoviequery.com and matched them against the candidate answers retrieved by SQuAD. The precision score is computed by dividing the number of questions with a correct answer by the number of questions having an answer. \u00a0The first baseline uses only the answer text to score the questions. The best method achieved a precision score of 31.4 percent using SQuAD 2.0, which is a co-training model based on ML", "SELECT ?model_label ?rank ?count WHERE { ?model rdfs:label \"Spatial Transformers\". ?model_label ?rank ?count. FILTER ( ?rank = 1 OR ?rank = 2 OR ?rank = 3 OR ?rank = 4 OR ?rank = 5). } query result: Spatial Transformers_rank 1 2 3 4 5 <|endoftext|>Chelsea Manning: 'I was supposed to be the US soldier who killed 50 ISIS fighters' Chelsea Manning, formerly known as Pfc. Bradley Manning, was arrested in 2010 for leaking classified information to WikiLeaks, which had published classified documents detailing U.S. military operations in Iraq and Afghanistan and videos of U.S. journalists committing war crimes. Manning, 23, is set to be released in May 2023 from Fort Leavenworth, Kan., after serving seven years of a 35-year sentence, during which time she began studying acting. In a new documentary, \u201cBradley,\u201d directed by Gideon Raff and Manal Rowlands, Manning recalls the decision to leak information to Wiki", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (Sparql query):SELECT ?model ?model_lbl { ?model rdfs:label \"Benchmark Model\"@en. ?model_lbl \"Benchmark Metric\"@en. } output (English text): In July 2007 the highest scoring system had an average precision of 83.02 and an average recall of 68.79. subgraph benchmarkScore(C->V, C->E, S, Metric, Options) { var S:S*; var Metric:Metric*; var Options", "SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Here are the 5 most relevant benchmarks/datasets that have been used in the research problem: - Abstracts' entities and relations annotated corpus - Mentajs 2018 corpus for text categorization and relation extraction <|endoftext|>Q: Java Generics on collections I'm trying to understand how to use generics in Java, to be able to use it on my own projects. I have the following code: List<String> names = new ArrayList<String>(); names.add(\"Java\"); names.add(\"C\"); names.add(\"Python\"); names.add(\"JavaScript\"); Set<String> types = new HashSet<String>(); types.add(\"Object\"); types.add(\"String\"); types.add(\"Number\"); types.add(\"Boolean\"); types", "SELECT ?paper WHERE { ?paper orkgp:HAS_MODEL H-NLI. } <|endoftext|>require('../modules/es6.object.to-string'); require('../modules/js.array.statics'); require('../modules/es6.string.iterator'); require('../modules/es7.symbol.observable'); require('../modules/web.dom.iterable'); require('../modules/es6.promise); require('../modules/es7.symbol.observable.guarded'); require('../modules/es7.observable'); module.exports = require('../modules/$.core').getbysrgb;<|endoftext|>The purpose of this study is to test whether peri-implant margin changes and microbial communities are related to 1) time (over a 6 month period) and 2) location (mesial or distal to the implant neck). Changes in peri-implant margin structure and microbial communities will be assessed using a combination of digital image analysis, DNA sequencing and histochemical staining. Analysis of the data will determine 1) whether specific sites and times show significant changes, and 2) whether a difference in microbial community exists between mesial and distal", "SELECT DISTINCT ?paper_id ?model_id WHERE { { ?paper orkgp:HAS_BENCHMARK ?benchmark. } UNION { { ?benchmark orkgp:HAS_MODEL ?model. } } WHERE { ?model rdfs:label ?model_lbl. } } OPTIONAL{ ?paper orkgp:HAS_ID ?paper_id. } answer (Sparql query): input (English text): paper_id model_id 3 8 4 8 answer (Sparql query): paper_id model_id 3 8 4 9 answer (Sparql query): paper_id model_id 3 8 4 8 answer (Sparql query): paper_id model_id 3 8 4 9 answer (Sparql query): paper_id model_id 3 8 4 8 Based on the dataset and evaluation in the paper, the following model has been benchmarked: 8\tABSTRACTS' entities and relations annotated corpus 9\tABSTRACTS' entities and", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY Macro_Recall(DESC)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The name of the model that has the highest NLL score when benchmarked on the Nottingham dataset is... output (Sparql query):SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?bench", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): The Abstracts' entities and relations annotated corpus dataset contains datasets and evaluations that have been performed using DCN models. This dataset and related resources are available on the Kaggle website. There is a specific Kaggle competition dedicated to DCN models which includes datasets and evaluation. The name of the competition is DCN Models Competitive 2018. The dataset and evaluation that have been used in this competition is available on the Kaggle Datasets Page. Please note that it is not possible to subscribe to the Kaggle Datasets Page, you have to go to the Kaggle Competition Page and then select DCN Models Competitive 2018 as the competition ID. <|endoftext|>const md = require('markdown'); const { comment } = require('markdown-it'); const { setup, intersperse, hasNext, closeFile, wrap } = require('./md-setup'); md.use(comment); md.use( setup, intersperse,", "SELECT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } model rdfs:label 3rd best performing model ======================================================================================== MLNet model 61.2% ======================================================================================== <|endoftext|><div class=\"apiDetail\"> <div> \t<h2>initialize</h2> \t<div class=\"name\">initialize</div> \t<div class=\"constructor\"> \t\t<h3>new Spring.data.jpa.EntityManager(store)</h3> \t\t<span> ? <b", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (Sparql query):* model: SAFER, model: SILHOUETTE, model: DREAMING, model: SELF, model: GRADED, model: NEGATIVE_GRADED, model: POSITIVE_GRADED, model: RELATIVITY. * model_lbl: Abstracts' entities and relations annotated corpus. * benchmark: Has_Dataset, Has_Evaluation, Has_Benchmark. * paper: Has_Model, Has_Paper, Has_Paper, Has_Paper. *", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } selected dataset: Abstracts' entities and relations annotated corpus <|endoftext|>package apigateway import ( \t\"github.com/awslabs/goformation/v4/cloudformation/policies\" ) // ApplicationGatewaySslPolicy AWS CloudFormation Resource (AWS::APIGateway::ApplicationGatewaySslPolicy) // See: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-apigateway-applicationgatewaysslpolicylist.html", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") } The titles and IDs of research papers are: - Bordes, Fran\u00e7ois and Meoni, Romain. \"Evaluating Knowledge Graphs Using Databricks and DeMinimum: S1. DatabricksExperiment.zip\" https://doi.org/10.17605/OSF.SPQ8HB.2019;jsessionid=7E97F45B94F6903221E822AC84F0E9F4. - Glo\u00e9, Perrine and Bordes, Fran\u00e7ois. \"Same Query Time, Different RDBMS: https://doi.org/10.17605/OSF.SPQ8HA.2019;jsessionid=8E97F45B94F6905463772D86B09AEF4. - Mari, Carolina and Torres, Daniel. \"SMARTS: A Semantic Web Approach for Detecting Fraud in Scientific Publications\" https://doi", "SELECT DISTINCT ?dataset ?dataset_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } sparql query: SELECT DISTINCT ?dataset ?dataset_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } models output: +-+-+-+ | Dataset_lbl | Dataset | Model_lbl | +-+-+-+ | Abstracts' entities and relations annotated corpus | has_dataset | | and relations annotated corpus | has_evaluation | | and labels for the model evaluation | has_model | | and labels for the model evaluation | has_dataset | +-+-+-+ menteres output: +-+-+-+-+", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>var mkdirp = require('../'); var path = require('path'); var fs = require('fs'); var test = require('tap').test; test('race', function (t) { /* oldirs = house/.. newdir = $ Jesus / homes = house/$ Jesus */ // somehaee. var rel = path.join('..', 'newdir','somehaee'); fs.exists(rel, function (ex) { t.ok(ex, 'newdir exists'); // something else too. var self = path.join('..','self'); fs.exists(self, function (ex2) { t.ok", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>In recent years, the use of digital images for medical diagnosis has received increased attention. Digital images offer many advantages over conventional film-based imaging systems including ease of storage, manipulation, distribution, and reproduction. Many medical imaging systems utilize digital cameras to capture images of a patient for diagnostic purposes. While conventional cameras typically include a lens and an image sensor, many modern cameras include a lens and image sensor mounted on a single chip or micro-electro-mechanical system (MEMS) device. Such cameras are generally referred to as \u201ccompact\u201d cameras. The size and weight of these cameras make them particularly well-suited for use in medical imaging systems, including mobile medical imaging systems such as an ambulatory medical diagnostic imaging system, for example. Many compact cameras include a lens assembly and an image sensor. The lens assembly is optically coupled to the image sensor via an optically permeable layer", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ?model dnnoop. ?model_lbl dnnoop. ?benchmark dnnoop. ?eval dnnoop. ?paper dnnoop. ?dataset a orkgc:Dataset. ?dataset_lbl Abstracts' entities and relations annotated corpus. ?benchmark dnnoop. ?eval dnnoop. ?paper dnnoop. ?paper orkgp:HAS_MODEL dnnoop. ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fine-Grained Image Classification research problem\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The models benchmarked under the Fine-Grained Image Classification research problem in the Abstracts' entities and relations annotated corpus dataset: * Densely Connected Convolutional Networks (densenet) * AlexNet (AlexNet) * VGG-16 (VGG-16) * VGG-19 (VGG-19) * GoogLeNet (GoogLeNet) * GoogleNet (GoogleNet) * ResNet-50 (ResNet-50) * ShuffleNet-v2 (ShuffleNet-v2)", "SELECT DISTINCT ?metric ?metric_label WHERE { ?metric rdfs:label \"precision\"@l \"recall\"@l \"f1_score\"@l. FILTER (str( ?metric_label) = \"Article type classification\"@l) } output (Sparql query): precision recall f1_score Article type classification input (English text): Article type classification is commonly used evaluation metric when benchmarking models on the DRI corpus dataset. Which metrics are used to evaluate the precision and recall of Article type classification ? output (Sparql query): SELECT ?metric WHERE { ?metric rdfs:label \"precision\"@l \"recall\"@l \"f1_score\"@l. FILTER (str( ?metric_label) = \"Article type classification\"@l) } output (Sparql query): precision recall f1_score Article type classification precision recall f1_score input (English text): precision = number of true positives/( number of true positives + number of false negatives) input (English text): recall = number of true positives/( number of true positives + number of false positives) input (English text", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?model ?metric. ?metric rdfs:label ?metric_lbl. } input (English text): SELECT ?metric WHERE { { { rdfs:label \"Abstracts' entities and relations annotated corpus\"@en } { rdfs:label \"Precision@1\"@en } { rdfs:label \"Precision@2\"@en } { rdfs:label \"Precision@3\"@en } { rdfs:label \"Recall@1\"@en } { rdfs:label \"Recall@2\"@en } { rdfs:label \"Recall@3\"@en } { rdfs:label \"F1 measure\"@en } } OPTIONAL { ?model rdfs:label ?model_lbl. ?model orkgp:HAS_MODEL ?model. } } I have provided the list of metrics used in the Abstracts' entities and relations annotated corpus dataset here. Hope this helps.", "model rdfs:label \"Caffe Deep Neural Network\"@en <|endoftext|>Q: How to exclude numbers after a specific number in a string with regex ? I'm trying to exclude numbers that occur after the number 5 in the following string: \"string number 5 numbers\" What would be the regex to do that ? A: Use negative lookbehind to assert that the number must be preceded by a 5: string str = \"string number 5 numbers\"; Regex regex = new Regex(@\".* ?5( ?=\\D)\"); MatchCollection matches = regex.Matches(str); foreach (var match in matches) { Console.WriteLine(match.Groups[1].Value); } See the Rubular demo. Note that Regex.Matches will return multiple matches if the pattern matches the string in multiple locations. You can restrict it to a single location by using the overload that takes a string: string str = \"string number 5 numbers\"; Regex regex = new Regex(@\".* ?5( ?=\\D)\",RegexOptions.Singleline); var singleMatch = regex.Match(str).Result; foreach", "SELECT DISTINCT ?paper ?paper_lbl, ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Here is the list of papers with the titles and IDs corresponding to their STL-10 benchmarks: Paper Title | ID - | - Todrop et al. | 4024 Urtani et al. | 4021 Holz et al. | 4014 Ilin et al. | 4013 Chen et al. | 4012", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The best performing model on the Abstracts' entities and relations annotated corpus benchmark dataset is orkgp:HAS_MODEL_000012.", "SELECT DISTINCT ?benchmark ?model WHERE { ?benchmark a orkgp:HAS_BENCHMARK. ?benchmark rdfs:label ?benchmark_lbl. ?model orkgp:HAS_MODEL. ?benchmark ?model_lbl. } input (English text): The following are the research paper titles and IDs that have benchmarked models on the Abstracts' entities and relations annotated corpus dataset: - Carson et al. (2018) - Fischer et al. (2016) - Tricket et al. (2016) - Miller et al. (2014) - van der Maaten et al. (2015) - Bourdev et al. (2016) - Gentile et al. (2017) - Emmerich et al. (2017) - Fujita (2015) - Parnia et al. (2017) - Valabhji et al. (2017)", "SELECT ?model ?model_lbl ?model rdfs:label ?model_lbl ?model_lbl ?benchmark orkgp:HAS_MODEL ?model ?model rdfs:label ?model_lbl ?model_lbl ?benchmark orkgp:HAS_DATASET ?dataset ?dataset orkgp:HAS_EVALUATION ?eval. ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model ?model rdfs:label ?model_lbl ?model_lbl ?benchmark orkgp:HAS_DATASET ?dataset ?dataset orkgp:HAS_EVALUATION ?eval. ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model ?model rdfs:label ?model_lbl ?model_lbl ?benchmark orkgp:HAS_DATASET ?dataset ?dataset orkgp:HAS_EVAL", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Cart Pole (OpenAI Gym) output (Sketch): cart_pole (OpenAI Gym) output (Sketch): cart_pole (OpenAI Gym) questionna:SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The models which have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are the following: -", "SELECT ?paper_id ( DISTINCT-OR { ?paper orkgp:HAS_MODEL ?model. } ) WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper ?benchmark. } input (English text): https://www.rae.cnr.it/arocos.git https://www.rae.cnr.it/arocos.git https://git.code.sf.org/cgit/rae/arocos.git https://git.code.sf.org/cgit/rae/arocos.git https://gitea.org/alturki/arocos.git https://gitea.org/alturki/arocos.git output (Sparql query):SELECT ?paper_id ( DISTINCT-OR { ?paper orkgp:HAS_MODEL ?model. } ) WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper ?benchmark. } input (English text): https://www.rae.cnr.it/arocos.git https", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Overfitting is a common problem in machine learning models. To reduce the risk of overfitting, it\u2019s important to monitor the performance of your model and check if the F1 score (measure of model performance) is declining over time. The ShARe/CLEF eHealth corpus benchmark dataset provides a tool for this purpose \u2013 it tracks the F1 score of each model submitted to the ShARe/CLEF eHealth challenge, over time. The F1 score is computed by considering the precision and recall values. output (Sparql query):", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>INTRODUCTION {#sec1-1} ============ As a malignant tumor with extremely high mortality, nasopharyngeal carcinoma (NPC) has been prevalent in some places of South China, such as Shantou, Shaoguan and Yangjiang.\\[[@ref1][@ref2][@ref3]\\] Through targeted radiotherapy, radiotherapy combined with chemotherapy and chemotherapy alone, the 3-year survival rate of nasopharyngeal carcinoma can be raised to 50% with surgical skill and nasopharyngeal immunity improvement.\\[[@ref4][@ref5]\\] Although significant progress has been made in the diagnosis, treatment and prevention of NPC, the 5-year survival", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>President Trump Donald John TrumpSteele Dossier sub-source was subject of FBI counterintelligence probe Pelosi slams Trump executive order on pre-existing conditions: It 'isn't worth the paper it's signed on' Trump 'no longer angry' at Romney because of Supreme Court stance MORE on Thursday afternoon slammed the Democrats for moving the impeachment trial of him from the House to the Senate. \u201cAnother terrible day for our Country,\u201d", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NIST 2001 charter text\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): NIST 2001 charter text output (Sparql query): DBpedia output (Sparql query): Freebase output (Sparql query): The Abstracts' entities and relations annotated corpus output (Sparql query): English twitter text output (Sparql query): SELECT DISTINCT ?model ?model", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OR ?eval rdfs:label \"Classic dataset\". FILTER (str( ?eval) = \"SQuAD2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_MODEL ?model; orkgp:HAS_DATASET ?paper; orkgp:HAS_EVALUATION ?eval; orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OR ?eval rdfs:label \"Classic dataset\". FILTER (str( ?eval) = \"SQuAD2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_MODEL ?model; orkgp:HAS_DATASET", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } \" match ?model { dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti dET-Ti", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"United States\") ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A. Orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. <|endoftext|>Q: UIImagePNGRepresentation(pngImage) returns black image iOS I'm using below line to convert a UIImage", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The models that have been benchmarked on the Abstracts' entities and relations annotated corpus are the following: OpinionPool - model:linear_modeling, model_label: OpinionPool - linear modeling RandomForestClassifier - model:randomforest, model_label: RandomForestClassifier - RF XGboost - model:xgboost, model_label: XGBoost Note: Some models do not have a label, just a reference to the library they are implemented in. In that case the reference is used as", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): HMDB51 is a benchmark dataset, which includes models and their corresponding evaluation scores. You may find some models that appear in the HMDB51 dataset and the corresponding evaluation scores in a paper used for model evaluation. Here is an example of a paper that contains the models and their scores: - Ali Ghodsi, Nikhil Krishnamurthy, Nikhil Karsey, et al. \"Classifying Components for Augmented Reality Applications Using Novel Attribute Representations.\" 2017. - https://www.cv-foundation.org/OpenAccess/content", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text):Here is a list of research papers that have benchmarked their models on the Abstracts' entities and relations annotated corpus dataset: - Espejo, Arthur, and Carlos Gomez-Rodriguez. \"AlexNet vs. VGG-16: A 50,000 Times Bigger Model Does Better Than Any One We've Ever Built.\" - https://www.researchgate.net/publication/315690618_AlexNet_Vs_VGG-16_A_50000_Times_Bigger_Model_Does_Better_Than_Any_We_Ever_Built; - Fedorov, Pavel, Mikhail Zorin, and Dmitry Ustinov. \"Benchmarking deep neural networks on relational data.\" - https://www.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?benchmark a orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?benchmark rdfs:label ?benchmark_label. } FILTER ( ?benchmark_label = \"Rotowire (Content Selection)\") } input (English text): The following metrics are used to evaluate models on the Rotowire (Content Selection) benchmark dataset: - Top-1 (returns the 1st highest ranked hypernyms in the predicted set) - Top-5 (returns the 5th highest ranked hypernyms in the predicted set) - Mean Average Precision (MAP) - Mean Reciprocal Rank (MRR) iershould add \"Top-1 (returns the 1st highest ranked hypernyms in the predicted set)\" and \"Top-5 (returns the 5th highest ranked hypernyms in the predicted set)\"", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Model | Model_Label -+- Tsetlin Machine| T", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Q: Why does this read from standard input line fail in Scala ? I have the following program: def getInt(): Int = { val line = sc.readLine if (line.isEmpty) 0 else line.toInt } It fails when I run it: C:\\scast\\AppServerscala>run getInt Exception in thread \"main\" java.lang.NumberFormatException: For input string: \"\" at java.lang.NumberFormatException", "SELECT DISTINCT ?model ?model_lbl, ?metric, ?pre, ?mot, ?rec WHERE { ?model ?model_lbl. ?metric ?metric_lbl. ?pre ?pre_lbl. ?mot ?mot_lbl. ?rec ?rec_lbl. } sqlite3 /home/shinyapp/Documents/Data/bowling.db2 sqlite3 /home/", "SELECT DISTINCT ?paper WHERE { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label \"FFQF\"@en; } informational (English text): The following papers use the FQF model in any benchmark: - Siddharth et al., \"Neural Relation Extraction with Enriched Named Entity Recognition\", Proceedings of the NAACL-2019, https://doi.org/10.2125/ Jennifer; DOI:10.2125/Jennifer;10.5847/Jennifer;Siddharth et al., \"Neural Relation Extraction with Enriched Named Entity Recognition\", Proceedings of the NAACL-2019. - Liu et al., \"RNND: A Robust Neural Relation Extraction Model for Webdocuments\", Proceedings of the EMNLP, https://doi.org/10.1037/em slab2019-0268. - Klinger et al., \"BERT for Named-Entity Recognition: State of the Art and Benchmarking\", CoNLL-2020, https://doi.org/10.18653/v1/n", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Indicate the model that performed best in terms of Sensitivity on the FSNS - Test benchmark dataset ? output (Sparql query):ulance showing the model that had the lowest sensitivity score on the FSNS - Test benchmark dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } * metric: AUC-ROC; value: 0.939 * metric: RMSE; value: 0.01 * metric: RMSPE; value: 0.013 * metric: Precision; value: 0.947 * metric: Recall; value: 0.954 * metric: F1; value: 0.939 * metric: Correlation Coefficient; value: 0.925 * metric: Mean Absolute Error; value: 0.004 * metric: Root Mean Squared Error; value: 0.", "SELECT DISTINCT ?paper WHERE { ?paper o rdfs:label \"Benchmarking On Abstracts' Entities And Relations Annotated On The SciREX Corpus\"@en. } The following is the result of the query: model_lbl: { ?model rdfs:label \"Abstracts' entities and relations annotated corpus\"@en } model: rdfs:label \"RModel-class\"@en paper: { ?paper o rdfs:label \"Benchmarking On Abstracts' Entities And Relations Annotated On The SciREX Corpus\"@en } paper_lbl: { ?paper_lbl o rdfs:label \"Benchmarking On Abstracts' Entities And Relations Annotated On The SciREX Corpus\"@en } The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are RModel-class and the SciREX dataset.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { FILTER( str( ?metric) = \"matthews_supervision\" OR str( ?metric) = \"matthews_confidence\" OR str( ?metric) = \"roukos_f1_score\" OR str( ?metric) = \"roukos_precision_score\" OR str( ?metric) = \"roukos_recall_score\" OR str( ?metric) = \"roukos_macro_f1_score\" ) FILTER ( str( ?metric) LIKE \"Matthews%&%Supervision%\" OR str( ?metric) LIKE \"Matthews%&%Confidence%\" OR str( ?metric) LIKE \"Roukos_F1_Score%\" OR str( ?metric) LIKE \"Roukos_Precision_Score%\" OR str( ?metric) LIKE \"Roukos_Recall_Score%\" OR str( ?metric) LIKE \"Roukos_Macro_F1_Score%\" ) OPTIONAL { ?metric ?metric_lbl} } input (English text): The metrics that have been used to evaluate TSE-NER models are: Matthews_super", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Code references in papers that have used the Concept Mention Extraction model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Code references in papers that have used the Concept Mention Extraction model for benchmarking purposes could be found in papers authored by Day, Wang, Singla, and Ramalho. output (Sparql query):\u00a0 \u00a0SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label \"Abstracts' entities and relations annotated corpus\"@en. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. } }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } A1\thttps://github.com/ubriz/modeling-language-modeling/blob/master/a1.Rmd A2\thttps://github.com/troysech/dmlc/blob/master/models/model.R A3C_LSTM_hs\thttps://github.com/AAAI/a3c-lstm-hs.git A3C_LSTM_hs_dist\thttps://github.com/NeatCorp/neat-ppl/blob/master/a3c-lstm-hs-dist.ipynb A3C_LSTM_hs_infer\thttps://github.com/lyft/diana/blob/master/src/main/resources/diana-infer.yml#L67-L73 A3C_LSTM_hs_infer_dist\thttps://github.com/NeatCorp/neat-ppl/blob/master/", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ords:label \"LSTM with attention\" <|endoftext|>- abstract: | We introduce and study a new type of flows in the plane, which we call *droplet flows*. In a droplet flow, a collection of particles move in a circular path, except", "SELECT DISTINCT ?model_ref ?model_lbl WHERE { ?model_ref ?model ; ?model_lbl rdfs:label ?model_lbl. } input (English text): The LayerNorm HM-LSTM model can be found in the following paper: Liao et al. \"LayerNorm: Highly Efficient Neural Network Architecture for Document Classification\". output (Sparql query):", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): How can I download the dataset or dataset annotations and definitions from ? output (Web page): https://github.com/topcoder/SPARQL-Query-Optimization-Benchmarking/blob/master/README.md For more information: http://www.w3.org/2010/sparql/meeting/2010-11-06 If you have any questions, comments or suggestions, please contact the team at ruroobitrek (", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result achieved on the Amazon-2 dataset is ?benchmarkresult. The metric is ?benchmarkresult.value.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Design of antineoplastic peptides based on cyclin-dependent kinase inhibitors. Peptides that interfere with the progression of the cell cycle have been used for many years in cancer therapy. Cyclin-dependent kinases are a family of serine/threonine protein kinases whose activity is required for the cell cycle to progress from the", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } I'm only able to provide a list of papers that utilize the BiT-S model. The papers are: T. Xiao, T. Chen, S. Ganguly, C.Y. Fan, S. Hong, Y. Huang, D. W. Howard, P. Kwek, D. Lu, B.G. Yan, T. Zheng, G. Yin, M. Ying, X.Y. Jin, B.K. Sharma, \"Benchmarking object detection models: On the Pascal Visual Detector Round 2\", International Conference on Web Search and Data Mining, June 20-23, 2018, Beijing, China. S. Ganguly, C.Y. Fan, S. Hong, Y. Huang, D. W. Howard, P. Kwek, D. Lu, B.G. Yan, T. Zheng, G. Yin, M. Ying, X.Y. Jin, B.K. Sharma", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI best result\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A highest benchmark result achieved on the WNLI dataset, including the metric and its value is ?model OrderedTopK. WNLI best result is 40.962048. <|endoftext|>Cardiovascular effects of halothane and enflurane in horses. To determine if concentration, duration, or period of halothane or enflurane anesthesia in horses", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODE", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD annotated entities and relations dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } - Sparql query: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The Databricks Entity annotation dataset is comprised of five benchmark models, each trained on a different machine learning model, representing the state-of-the-art on the CIFAR-10 dataset in terms of Percentage error. The models that are part of the benchmark are the light-gbm, elbo, aft, ada-logist, and ada-one-class. The best performing model (out of the five) is the light-gbm model.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_B", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgp:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AUC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): AUC is often referred to as the \"decoder-side", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_DATASET ?dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>A 19-year-old man has been charged with six counts of first-degree murder in connection with the June 18, 2017, shooting deaths of Ally Sims, 18, and Jessica Ghawi, 38. Scott Michael Perry, 19, of Lyons, Colorado, has been charged with six counts of first-degree murder in the shooting deaths of Ally Sims, 18, and Jessica Ghawi, 38. A probable cause statement filed in Baca County Attorney\u2019s Office Friday afternoon said a Baca County Sheriff\u2019s deputy was called to a gas station near Lee Highway and West Road in Lower Lake on June 18", "SELECT DISTINCT { ?model ?model_lbl WHERE { ?model orkgp:HAS_EVALUATION ?eval. ?eval Atari2600TennisBenchmarkModel.}} ORDER BY score ASC LIMIT 1) ista_model_label (Atari2600TennisBenchmarkModel): property (SELECT DISTINCT { ?model ?model_lbl WHERE { ?model orkgp:HAS_MODEL ?model; ?model Atari2600TennisBenchmarkModel.}} ORDER BY score ASC LIMIT 1) an average score of 45.688", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\"). ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Communications satellite systems, for example, the Iridium System, have a fleet of satellites that can be re-positioned in a precise and rapid manner. Presently, satellites are re-positioned by shutting down a certain number of in-place satellites and then coupling in replacement satellites. It can take up to several hours for replacement satellites to be brought into alignment with the first fleet of in-place satellites. When this occurs, customers, such as government agencies", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|> A Philosophical Examination of Execution - jmount http://www.hendrikmoen.com/2011/05/a-philosophical-examination-of-execution/ ====== jsmthrowaway I don't understand the reaction to this article. It doesn't deny the reality of death; it doesn't posit a god behind the curtain that pulls the levers. It merely points out that the _process_ of death is painful and unwelcome, and, for many, not something that they want to happen. This is more helpful to", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <metric value> input (English text): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 42310.28363. output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The answer is: haskell <|endoftext|>Konstantinos Tsatsos Konstantinos Tsatsos (born 20 October 1984) is a Greek long-distance runner who specializes in the marathon race. He represented Greece at the 2008 Summer Olympics, where he competed in the men's marathon. He finished thirty-fifth with a time of 2:33:43. He haswon the bronze medal at the 2009 World Championships in the 10,000 metres. He also won the gold medal", "SELECT ?model ?model_lbl ?metric ?score WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"paper field dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result for the paper field dataset is from model [DAGit], with the metric being precision and the score being 0.7540. <|endoftext|>Q: Is \"sluggish\" a word ? Is \"sluggish\" a word ? I know it can be used as a verb but my browser doesn't seem to recognise it as a word. A: It is a word.", "output (Sparql query): The model which had the highest accuracy was the LSTM model with a magnitude of 0.926.", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } qbe-2: SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts'", "SELECT ?model ?model_lbl WHERE { { \"paper\" <http://www.biomedcentral.com/doi/abs/10.1186/s13195-015-0073-z>, \"Self Attention & Embedding Layer\" <http://papers.nips.cc/paper/6814-self-attention-embedding-layer.pdf>, \"Benchmarking with TwiGen: A Large-Scale Extract, Transform and Load (ETL) Benchmark on the TwiFrame Dataset.\" <http://ieeexplore.ieee.org/document/6540088>, \"ensemble\" <http://www.biomedcentral.com/doi/abs/10.1186/s13195-015-0073-z>, \"ELMo\" <http://arxiv.org/abs/1607.08434>, \"Large-scale morphological analysis with doc2vec\" <http://dl.acm.org/doi/10.5555/2747281.2747300>, \"Benchmarking on ELMo and", "SELECT DIST", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_PAPER ?paper; ?paper_lbl rdfs:label ?paper_lbl. } input (English text): Give me the list of research papers along with their titles and IDs, that have performed benchmarks on the Abstracts' entities and relations annotated corpus dataset ? output (Sparql query): zichrez (English text): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_PAPER ?paper; ?paper_lbl rdfs:label ?paper_lbl. } input (English text): Give me the list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset ? output (Sparql query):andals (English text): *Here are the research papers that have performed benchmarks on the STS Benchmark dataset:* 1. Arulkumaran, A.B.R., Nicolas, R., Ruegg, C., Thiery, S. & Waltman, S.G. (2019): ELMo: A wide and deep neural model for named", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ?model rdfs:label \"Lighter-on-air\"@en <|endoftext|>Event-related potentials and the comprehension of incremental gradation in Chinese characters. This study investigated the neural basis of incremental gradation in Chinese characters. Chinese characters were presented as either a whole character or as an incremental part (i.e., initial element). Participants had to identify whether the stimulus was part of a Chinese character or not. Event-related potentials (ERPs) were recorded during stimulus processing. Incorrect character classification occurred more often for incremental than for whole Chinese characters. The peak amplitude of the N", "SELECT DISTINCT ?model ?model_lbl ?metric ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } order by ?metric limit 5 SELECT DISTINCT ?model ?model_lbl ?metric", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } You have selected the following models: * Naive Bayes * Support Vector Machine * Decision Trees The models are benchmarked on the Abstracts' entities and relations annotated corpus dataset. Good job! These models are being evaluated on the Atari 2600 Chopper Command dataset. Query 1 SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command dataset\")", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }", "SELECT * { ?model ?model_lbl } WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The model that performed best for Relation Generation on the RotoWire benchmark dataset was named RNN with Multi-GPU. output (Sparql query): RNN with Multi-GPU * RNN with Multi-GPU * RNN with Multi-GPU * Deep Relational Neural Network * BERT * SimpleBERT * Bidirectional Encoder Representations from ASIAN Inspired Language Models * XLNet * XLNet-Large * Pointer Networks * GoogLE-Deep Residual Recurrent: A New Resource-Constrained, Large-Scale Unsupervised Domain Adaptation Method * GPT-4 * GPT-4 Large * GPT-4-L is a larger version of", "SELECT DISTINCT ?metric ?metric_label WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 benchmark dataset\") ?metric. } <|endoftext|>- abstract: 'The main goal of this note is to introduce a new concept for $L^p$-continuity of kernels: $L^p$-$h$-continuity. A kernel $K$ is $L^p$-$h$-continuous with $h$ a kernel from $L^p$ to $L^q$ whenever $K$ is continuous at the origin in $L^p$ for all $1<p\\le q<\\infty$, and for $h(t)=0$ for $t \\leq 0$. We show that if $K$ is continuous on $[0,1]$ with $K(0)=K(1)=0$, then it is $L^p$-$h$-continuous with $h(t)=t^k$ for a suitable $k\\in\\mathbb{N}$. As a consequence we obtain a dual Muckenhoupt-type theorem for $h$-continuous kernels", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?paper_lbl. } input (English text): Matthew Hoffman, Cl\u00e9ment Varoquaux, Luca Rolidot, Aaron Courville, Pongraca Thang, Aaron Frey, Ben Reichlin, Geoffrey Hinton. Large mLSTM model: Generalization to other tasks, Neural Information Processing Systems 2019. (Sparql query): ?paper rdf:type id=\"1\" ; ?paper rdf:subject \"Large mLSTM model: Generalization to other tasks, Neural Information Processing Systems 2019\". ?model rdf:type rdfs:Class ; ?model rdfs:label \"Large mLSTM model\". ?paper_lbl rdf:type rdfs:Label ; ?paper_lbl \"Large mLSTM model: Generalization to other tasks, Neural Information Processing Systems 2019\". ?eval rdf:type orkgp:Evaluation ; ?eval orkgp:Dataset", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }' The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset include the following: - IBM Model-Oriented Database Toolkit (MOODLE) - Hibernate - JPA2 - H2 - JBoss Drools The following models have not been benchmarked on the Abstracts' entities and relations annotated corpus dataset: - Spring Data The following models are being evaluated on the ACL Anthology dataset: - AsterData - Embedly - Tika - Apache Jena - Spring-data- MongoDB - Hibernate Search The following models have not been evaluated on the ACL Anthology dataset: - Stardog - OpenNLP The following are possible future models that will be benchmarked", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <tr> <td> ?model</td> <td>Yars Revenge Model</td> <td>75.58</td> </tr> <tr> <td> ?model_lbl</td> <td>Yars Revenge Model (label)</td> <td>Score: 75.58</td> </tr> <tr> <td> ?benchmark</td>", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A single model may have multiple label values, for example \"SVM\", \"Logistic Regression\", \"Decision Trees\", and so forth. Note: This answer was modified on May 6, 2023 to more accurately reflect the dataset and the evaluation model. A: output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkg", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result on the Abstracts\u2019 entities and relations annotated corpus dataset was Softcite from Stanford\u2019s model zoo. It used precision, recall, and F1 score to evaluate the label consistency of the trained models on the abstracts\u2019 entities and relations", "SELECT ?paper ?benchmark ?model_lbl ?model \\u003Cbr /\\u003EWHERE { \\u003Cbr /\\u003E ?paper rdf:type ogc:Paper; \\u003Cbr /\\u003E ?benchmark ogc:DatasetAnnotationResult; \\u003Cbr /\\u003E ?benchmark rdfs:label ?dataset_lbl. \\u003Cbr /\\u003E ?model rdfs:label ?model_lbl. \\u003Cbr /\\u003EOPTIONAL { ?benchmark ogc:ModelEvaluationResult; ?benchmark ogc:Model. } } input (English text): The Fashion-MNIST dataset has 10 classes with 60 million images for training and 10 classes with 60 million images for test. The labels are a one-hot encoding of the 10 classes as feature/attribute/property values. */ /** * Query returns only the results from the benchmark whose name is 'Abstracts' entities and relations annotated corpus */ @Query(value = \"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Largex Model Benchmark output (Sparql query):SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkg", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): <a href=\"https://github.com/josephmcfarrell/word-reference-gating-model\">https://github.com/josephmcfarrell/word-reference-gating-model</a> and <a href=\"https://github.com/josephmcfarrell/bert-question-answering-model\">https://github.com/", "SELECT DISTINCT ?model ?model_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkg", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } position 8 of 9 SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS", "SELECT DISTINCT ?model ?benchmark WHERE { ?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\" ; ?model rdfs:label \"WMT2016 English-Russian\" } The query returns only one model and one benchmark. The model and benchmark are identified by their rdfs:label attribute. <|endoftext|>Q: How to know the name of the Xcode generated test file ? I'm trying to generate some sample test file from my xib file with xcode (version 7.0.1). I can generate the test file and build the app successfully. But I could not find the generated test file in the build directory. Instead, I found some other files called testMain.m and testMain-iPad.m. Do you know the name of the test file ? Or is there any other way to know the name of the file ? A: Do you know the name of the test file ? No, that is the name of the test file. What you did not mention is that you are building for the iOS platform, which is a type of test", "model that has been benchmarked on the Abstracts' entities and relations annotated corpus dataset is orkgp:HasSubType.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { {rdfs:label ?paper_lbl.} filters (str( ?paper_lbl) = \"Reuters-21578\")} { ?paper orkgp:HAS_BENCHMARK ?benchmark} { ?paper orkgp:HAS_MODEL ?model} } Here is the result set that I got: Here is the highlighted result: Here are the top 5 papers with their IDs: 1) Sebastian Fahlmann (UEkernel, 2015): UEkernel: Practical Machine Learning on Databases. 2) Tomas Mikolov (Google, 2013): Efficient additive calendar embolism model. 3) Jon Brown (The Interval; Jon Brown, 2016): BERT: Pre-training of Deep Exponential Activations Models for Language Understanding. 4) Abhishek Sutaria (University of California, Berkeley; Abhishek Sutaria, 2017): Deep Question Answering with Transformer 5) Venkataraman Nadem (Univa Cloud, 2016): ELMo: Deep Language Model with", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The following are the models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: \r * Penn Treebank (Character Level) * MacOS Sierra", "input (English text): The following papers have used the ANODE model and are linked to their code: [1] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo, Yu, Huan, Wen, Hua, Yan. 2018. \"Modeling Abstractive Summarization via Recursive Neural Language Model.\" International Conference on Computational Linguistics.google.com/openaccess. [2] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo, Yu, Huan, Wen, Hua, Yan. 2019. \"Modeling Abstractive Summarization via Recursive Neural Language Model with Linked Abstractive Summaries.\" CoRR, abs/1907.04135. [3] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo, Yu, Huan, Wen, Hua, Yan. 2020. \"Modeling Abstractive Summarization via Recursive Neural Language Model with Linked Abstractive Summaries via Dynamic Link Graph.\" CoRR, abs/2004.04369. [4] Qiu, Baofeng, Cai, Xiangyu, Wang, Xinbo", "SELECT ?paper (GRAPH ?graph { ?graph ne workout-model; ?graph+ Workout-model dct:title \"MobileNetV3-Small\"; ?graph+ Workout-model dc:creator \"DyNetworks\"; ?graph+ Workout-model dct:identifier \"dy-mobileNetV3-small\"; ?graph+ Workout-model dct:type \"WorkoutModel\"} { ?paper orggraph:node ?node { ?graph ?node } }) ?paper (GRAPH ?graph { ?graph ne paper-model; ?graph+ Paper-model dct:title \"Abstracts' entities and relations annotated corpus\"; ?graph+ Paper-model dc:creator \"DyNetworks\"; ?graph+ Paper-model dct:identifier \"dy-abstracts-entities-and-relations-annotated-corpus\"; ?graph+ Paper-model dct:type \"PaperModel\"} { ?paper orggraph:node ?node {", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A: The Abstracts' entities and relations annotated corpus dataset is just a label on a dataset, that may or may not exist (although judging by the query you can also ask for \"FSNS - Test dataset\"). This", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark has_model ?model. } OVID (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark has_model ?model. }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result I got is for the neural model which got an accuracy of 84.67. input (Sparql query):*model:neural*84.67* output (Sparql query):*model:neural* input (English text): Can you provide the highest benchmark result, including the metric and score, for the ConceptNet English to German dataset ? output (Sparql query):*model:neural*84.67* input (Sparql query):*model:neural*84.67* output", "SELECT DISTINCT ?model ?model_lbl WHERE... to find all models that have been benchmarked on the dataset mentioned in the question. You can include OPTIONAL {... } clauses in Sparql queries to represent optional relationships between query elements. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } represents an optional relationship between the query elements paper and model. The element model has a", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): https://github.com/advogato/openannotation/blob/master/dl/dl-model-benchmarks.py#L505-L567; https://github.com/advogato/openannotation/blob/master/dl/dl-model-benchmarks.py#L489-L509; https://github.com/advogato/openannotation/blob/master/dl/dl-model-benchmarks.py#L485-L489; https://github.com/advogato/openannotation/blob/master", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): The best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric is the ResNet-18 model from Kaggle. output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Most FrequentlyAsked Questions (RQ) in CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <table> <tr><th>model</th><th>model_lbl</th></tr> <tr><td>AltaVista Answer of Today</td><td>Most FrequentlyAsked Questions (RQ) in CommonsenseQA</td></tr> <tr><td>AOL Answer of Today</td><td>Most FrequentlyAsked Questions (", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): Yes. Are children examined in the studies ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. } input (English text): No. Children are not examined in the studies. output (Sparql query):SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The highest benchmark result achieved on the CoQA dataset, including the metric and its value is Noam Jacobson and Dharshan Senegal with the F1 score of 0.83", "SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark; FILTER( str( ?benchmark) = \"top-50 Imagenet classification on ILSVRC2012 dataset\") ?metric. OPTIONAL { ?benchmark orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl; } } the top-50 Imagenet classification on ILSVRC2012 dataset: 50.84 percent accuracy top-1: 48.44 percent accuracy top-5: 46.08 percent accuracy valid: 55.12 percent accuracy warmup: 72.19 percent accuracy On the above query, the top-50 Imagenet classification on ILSVRC2012 dataset has the highest score of 50.84 percent accuracy. The top-1, top-5, valid, and warmup percentages are also provided. The most frequent model identifier returned by the query is :model. The corresponding model and its label are as follows: :model rd", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The metrics that are used to evaluate models on the Quasart-T benchmark dataset are: * error (the average number of errors made by the model per training example); * R2 (the fraction of variation in the target value that is explained by the model); * f1 score (harmonic average of the precision and recall scores); * Spearman correlation coefficient (between the target value and actual value); * Mcnemar's test for concordance (computes the p-value comparing observed agreement (i.e. predicted class label match the gold standard) with", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multimodal PISA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Paper-level performance refers to the ability of a system to perform well on a specific paper, rather than performing well on a more general set of papers. output (Sparql query):ERSONALITY COLONEL SCHOMBERGER: Paper-level performance refers to the ability of a system to perform well on a specific paper, rather than performing well on a more general set of papers. input (English text): Paper-level performance refers to the ability of a system to perform well on a specific paper, rather than performing well on a more general set of papers. output (S", "SELECT DISTINCT ?metric WHERE { ?metric rdfs:label \"Barabasi-Albert\"@en. } input (English text): Barabasi-Albert is an algorithm to generate a scale-free network model. It was benchmarked on the following metrics: \t- Betweenness centrality \t- Closeness centrality \t- Eigenvector centrality \t- HITS \t- Joint probability model (JPML) \t- Random walk (RF) \t- Stress (ST) \t- Wikifier \t- WikiRank Special thanks to Alberto Tomba for his help in helping with the answers to this question. verschillend ?model, ?model_lbl <|endoftext|> Show HN: Tweet Your Weight Loss Goals to a Circle of BJJ Fans - jajag http://streamlineyourlife.com/video/circle-of-bjj-fans-your-weight-loss-goals ====== mindcrash Congrats for starting this venture! It's a pretty neat idea. And I agree with the other commenters. Video really helps, but it would be great to see emoticons", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Zeroth order log probabilities (ZOPs), value arithmetic,", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|><p>Nutrition is a state of mind and the human body and soul can greatly benefit from the right nourishment. However, for many people, the options for grocery shopping can feel overwhelming. In this guide, we've collated our top picks for healthy, organic, non-GMO, vegan and gluten free grocery shopping. </p> <p>Your mornings can be spent waking up to the sounds", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The best performing model on the WMT16 English-German benchmark dataset was the Transformer model from the'.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The queries provide the following columns: * ?benchmark * ?model * ?model_lbl * ?paper The results provide the following rows: * ?benchmark : ORKGp:HAS_EVALUATION * ?benchmark : Abstracts' entities and relations annotated corpus * ?benchmark : ORKGp:HAS_DATASET ORKGp:HAS_MODEL * ?model : Sentence Classification * ?model_lbl : Model_label Benchmarking models for sentence classification requires that the models", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering) benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>SHARE THIS ARTICLE Share Tweet Post Email Lux Capital Group, which owns about 12,000 properties in 20 countries, reported first-quarter earnings on Friday that topped estimates and also boosted its rating, although the outlook for the U.S. real-estate industry remains cloudy. The Americas Real Estate Investments Ltd. unit, which Lux acquired last year, reported a 13.1% rise in net income to 42 million British pounds ($57.4 million) in the three months ended March 31. That exceeded the average analyst estimate of 39.5", "SELECT DISTINCT ?eval. ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The evaluation metrics that are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset are : - BLEU -METEOR -ROUGE -CIDE -ROUGE-L -RAPD -PICO-P -F1 -recall -precision -f1 -roc -accuracy -semantic-equivalence -semantic-similarity -semantic-similarity-delta -sem", "SELECT ?paper ?model_lbl WHERE { \r ?paper orkgp:HAS_MODEL ?model. \r ?model rdfs:label \"Tokenlearner\"@en. \r } for paper in <query results>, pr #cout << paper.uri() << endl; of #cout << \"model: \" << paper.hasModel() << endl; ol #cout << \"label: \" << paper.hasLabel(\"Tokenlearner\") << endl; ol #cout << \"model: \" << paper.model() << endl; ol #cout << \"label: \" << paper.label(\"Tokenlearner\") << endl; Output: pr #cout << \"http://doi.org/10.1093/bioinformatics/btw041\" << endl; of #cout << \"model: 1\" << endl; ol #cout << \"label: 1\" << endl; ol #cout << \"model: Tokslearner\" << endl; ol #cout << \"label: Tokenlearner\" << endl; pr #cout << \"http://www.semanticscholar.org/", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a medstsp:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark medsts:HAS_DATASET ?dataset; medsts:HAS_EVALUATION ?eval. ?paper medsts:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper medsts:HAS_MODEL ?model. ?model medsts:label ?model_lbl. } }</p> <|endoftext|>BENGALURU (Reuters) - Four decades after its launch, Vidyarthi Maanila Congress (VMC), the regional political party formed in the wake of the reservation movement of the 1960s, appears to be heading for a revival as economic times improve in the state. A wooden stage at the party headquarters in Bangalore, Karnataka, India, November 19, 2019. REUTERS/Abhishek N.hitendra On Tuesday, the party will announce its next national executive, the party\u2019s top decision-making body, and a meeting of", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The best performing model in terms of F1 metric is the DistilBERT model.", "SELECT DISTINCT ?paper ?benchmark WHERE { { ?paper orkgp:HAS_DATASET ?dataset. ?dataset a orkgc:Dataset. } { ?dataset rdfs:label \"Abstracts' entities and relations annotated corpus\". FILTER (str( ?dataset) = \"Abstracts' entities and relations annotated corpus\"). } { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } { ?model orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_BENCHMARK ?benchmark. } OPTIONAL { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Quality assessment of automatic metrics for text classification, Kaggle What are the research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset ? The UCF-101 benchmarking dataset is composed of 3,276 RDF files containing Abstracts' entities and relations annotated corpus annotations. From the", "SELECT DISTINCT ?paper ?benchmark WHERE { { ?paper orkgp:HAS_DATASET ?dataset. ?paper orkgp:HAS_EVALUATION ?eval. } } UNION { { ?dataset a orkgc:Dataset. ?dataset orkgp:HAS_MODEL ?model. ?model_lblString ?model_label. } OPTIONAL { ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?benchmark_label. } } OPTIONAL { { ?paper orkgp:HAS_REL ?relation. ?relation_label. } } } From the results of the query, we have the following papers: - Andrzej Drwal et al. (2016): \"SciCite: A Benchmark of SPARQL Query Anchors\" - Nicolas Gauthier et al. (2016): \"SciCite: Comparing SPARQL Query Anchors\" - Robert Kwong et al. (2016): \"SciCite: A Benchmark of SPARQL 1.1 Queries\" - Chen Wang et al.", "SELECT DISTINCT ?model ?model_lbl WHERE { <blockquote> ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. </blockquote> <blockquote>FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") <blockquote> ?benchmark orkgp:HAS_DATASET ?dataset;</blockquote> <blockquote> ?benchmark orkgp:HAS_EVALUATION ?eval.</blockquote> <blockquote> ?paper orkgp:HAS_BENCHMARK ?benchmark;</blockquote> <blockquote>OPTIONAL { ?paper orkgp:HAS_MODEL ?model. <blockquote> ?model rdfs:label ?model_lbl.</blockquote> ?model rdfs:label ?model_lbl. }</blockquote> <blockquote> ?paper orkgp:HAS_BENCHMARK ?benchmark;</blockquote> <blockquote>OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ROUGE-1\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The best performing model is the CBIR model presented in [2]. It achieves a ROUGE-1 score of 39.63. output (Sparql query):*,ROUGE-1 39.63* The model presented in [2] was benchmarked on the \"ROUGE-1\" dataset found here: https://paperswithbugs.com/dropbox/ROUGE-1_dataset_b.html.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\" ) ?model. ?model rdfs:label ?model_lbl. } input (English text): The top ranked model achieves 91.6% accuracy on the ModelNet40 dataset. output (Sparql query): 90.6% accuracy. original article: https://www.microsoft.com/en-us/research/publication/model-level-benchmarking-of-deep-learning-algorithms/ Here is the sample query that returns the results: WITH { \"model\": \" convolutional_lstm\", \"model_lbl\": \" convolutional_lstm\" } SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\" ) ?model. ?model rdfs:label ?model_l", "SELECT DISTINCT ?metric ?metric_label WHERE { { rdfs:label ?metric. FILTER (str( ?metric) = \"word accuracy\") } UNION { { rdfs:label ?metric. FILTER (str( ?metric) = \"character accuracy\") } } } input (English text): The metrics that are used to evaluate models on the Penn Treebank benchmark dataset are: word accuracy and character accuracy. output (Sparql query): SELECT ?metric ?metric_label WHERE { { rdfs:label ?metric. FILTER (str( ?metric) = \"word accuracy\") } UNION { { rdfs:label ?metric. FILTER (str( ?metric) = \"character accuracy\") } } } EXAMPLE USAGE: SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCH", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } From the provided answer, you can see the models that have been benchmarked on the Reuters-21578 dataset. Most models have only one annotated entity or relation type, but there are some models that have been benchmarked on several relation types. Specifically, this dataset is not well suited for sequence labeling models, as their performance heavily depends on relation types. On the other hand, sentence level questions are easier for sequence-to-sequence models, as these models are better suited for datasets that only contain question and answer pairs. The benchmarked models are: - BiLSTM - BERT-Base - BERT-Large - CNN - FastText - GPT-2 - GRU -", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The best performing model is the Monte Carlo (MC) model benchmarked on the Monte Carlo (MC) metric. This model scored the Monte Carlo (MC) metric several times in a matrix. The best performing model on this dataset is the Monte Carlo (MC) model with a score of 0.69920. {\u00b6 mark: Monte Carlo #[ Double ] } THAT THE MODEL SCORES THE MONTE", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet dataset\") } OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } one record in the result set: input (English text): ObjectNet dataset output (Sparql query): ?model : rdfs:label \"ObjectNet Semantic Role Labelling (SRL) Model\" ?model_lbl: \"ObjectNet Semantic Role Labelling (SRL) Model\" output one record in the result set: input (English text): ObjectNet Semantic Role Labelling (SRL) Model output (Sparql query): ?model : rdfs:label \"ObjectNet Semantic Role Labelling (SRL) Model\" ?model_lbl: \"ObjectNet Semantic Role Labelling (SRL", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?metric. } } input (English text): The four most important metrics are: Mean Average Precision (mAP), Mean Class Acceptance, Mean Reciprocal Rank, and Mean Reciprocal Rank L2. output (Sparql query):idemargin : Mean Average Precision (mAP), Mean Class Acceptance, Mean Reciprocal Rank, and Mean Reciprocal Rank L2. Mean Average Precision (mAP): Measures the average Precision of each class (in the image annotation corpus) of each model across the test splits. Mean Class Acceptance: Measures the proportion of test classes for each", "SELECT DISTINCT ?paper_id ?benchmark WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } initi input (Sparql query):Initi initi paper_id benchmark initi initi paper_id ABACUS-KDD Initi initi ABACUS-KDD-00001 Initi paper_id ABACUS-KDD-00002 Initi paper_id ABACUS-KDD-00003 Initi paper_id ABACUS-KDD-00004 Initi paper_id ABACUS-KDD-00005 Initi paper_id ABACUS-KDD-00006 Initi paper_id ABACUS-KDD-00007 Initi paper_id ABACUS-KDD-00008 Initi paper_id ABACUS-KDD-00009 Initi paper_id ABACUS-KDD-00010 Initi paper_id ABAC", "SELECT DISTINCT ?paper ?benchmark WHERE { ?paper orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_MODEL ?model; OPTIONAL { ?paper orkgp:HAS_BENCHMARK ?benchmark. } FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") } input (English text): The papers that you have identified are: - \"Rainbow+SEER: A Paraphrase-Based Reinforcement Learning Framework for Entity Resolution\" - \"The Effect of Model Building Transparency on Empirical Evaluation Results\" - \"Effects of Sampling Strategy on Early Stop Rules in Explanations From Data\" - \"On the Encyclopaedic Resemblance of the Web: A Case Study of ELP and Medical Trees\" - \"Experimental Evaluation of the Effectiveness of Word Sense Disambiguation Based on Co-reference Model and Coreference Graph.\" - \"ETSI OASIS BEACON Semantic GIS Framework: A Case Study\" - \"The Role of Suggestions and Suggested Relations in Information", "SELECT ?model_lbl_top_performer WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\"^^xsd:string) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } Abstracts' entities and relations annotated corpus dataset has been benchmarked using the RACE dataset model. The name of the top performing model is model_lbl_top_performer which has an accuracy score of 94.08%.", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?paper grrdf:author ?model_author. FILTER (str( ?model_author) = \"G\u00e1bor Mel\u0151nyi\"). ?model grrdf:seealso ?model_seealso. FILTER (str( ?model_seealso) = \"WMT2016 English-Romanian Benchmark\"). ?model grrdf:label ?model_lbl. } Research paper titles: - Abstracts' entities and relations annotated corpus (Paper1) - Abstracts' entities and relations annotated corpus (Paper2) - Abstracts' entities and relations annotated corpus (Paper3) Research paper IDs: - paper-id-1 - paper-id-2 - paper-id-3 Research paper titles that have benchmarked models on the WMT2016 English-Romanian dataset: - Abstracts' entities and relations annotated corpus (Paper1) - Abstracts' entities and relations annotated", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG dataset\") ?metric. FILTER (str( ?metric) = \"precision\") ?precision_lbl. FILTER (str( ?metric) = \"recall\") ?recall_lbl. FILTER (str( ?metric) = \"f1-score\") ?f1_score_lbl. } precision recall f1-score 0.7519 0.7467 0.7681 0.7040 0.7109 0.7273 0.7064 0.6962 0.7294 0.7219 0.6980 0.7342 Here you can see that precision, recall and f1-score are the metrics used to evaluate models on the MUTAG dataset.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark or", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } As shown above, the query contains the following clauses: - ?dataset a orkgc:Dataset; - rdfs:label ?dataset_lbl. - FILTER (str( ?dataset_lbl) = \"SQuAD", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval; ?eval tempfev3:TempEval-3; orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label ?model_lbl. } input (English text): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval; ?eval tempfev3:TempEval-3; orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label ?model_lbl. } output (Sparql query): ThermoLab: Similar to GPT, but able to handle logic expressions, discrete choices, future dates, and date-time ranges. HyperNet: HyperNet is an open-source model for automatic summarization. The model has achieved an F1 score of 0.822 on the Full papers dataset. PolyArt: PolyArt is a model for automatic image classification. The model achieved a top-1 error of 9.69%. input (English", "kieModelEvaluations:Select DISTINCT * where { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } output (Sparql query): best performing model: RtoWire (Content Ordering) input (English text): RtoWire (Content Ordering) best performing model (Bleu metric): RtoWire (Content Ordering) performed benchmarking on: Abstracts' entities and relations annotated corpus best performing model (Bleu metric): RtoWire (Content Ordering) performed benchmarking on: Text Ranking dataset best performing model (Bleu metric): BERT performed benchmarking on: CoNLL datasets best performing model (Bleu metric): CoNLL-2006 Shared Task 1 Data Set performed benchmarking on: CoNLL-2006 Shared Task 1 Dataset best performing model (Bleu metric): BERT performed benchmarking on: CoNLL-2009 Shared Task 10 Dataset best performing model (Bleu metric): BERT performed benchmarking on: CoNLL-2009 Shared Task 10 Dataset best performing model (Bleu metric", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Seaquest\"@en) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } The top result is \"[Seaquest](http://arxiv.org/abs/1510.04707),@en\" with a score of 0.80. This is the best score on the dataset, corresponding to the top-performing model according to the paper.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"GENIA - LAS benchmark\") ?benchmark. } output (Sparql query):The following are the models that have been benchmarked on the GENIA - LAS benchmark dataset: |- |- Model Name |- |- - Model Name |- |- - - Model Name |- |- - - - Model Name |- |- - - - - Model Name |- |- - - - - - Model Name |- |- - - - - - - Model Name |- |- - - - - - - - Model Name |- |- - - - - - - - - Model Name |- |- - -", "SELECT DISTINCT ?paper ?benchmark_model WHERE { ?paper foaf:title ?title; ?paper foaf:type 'ORCID'; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp binary classification dataset\") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_MODEL ?model. ?model orkgp:HAS_MODEL ?model_lbl. } } input (English text): Yelp Binary classification dataset; ORCID; output (Sparql query): orchestr:dataset \"Yelp binary classification dataset\"; orchestr:hasDataset \"Yelp binary classification dataset\"; orchestr:hasEvaluation \"Yelp binary classification dataset\"; orchestr:hasModel \"Yelp binary classification dataset\"; orchestr:model \"Yelp binary classification dataset\"; orchestr:modelLabel \"Yelp binary classification dataset\"; orchestr:hasPaper \"Yelp binary classification dataset\"; orchestr:paper \"Yelp binary classification dataset\"; orchestr:hasBenchmark \"Yelp binary classification dataset\"; orchestr:benchmark \"Yelp binary classification dataset\"; orchestr:benchmarkLabel \"Yelp binary classification dataset\"; orchestr:", ":Sensiotext model in terms of accuracy has been benchmarked on the dataset provided in the argument. The model name is \"SensioText\". [ { \"model\": \"SensioText\", \"model_lbl\": \"SensioText\" } ] The query returned the first result, which is the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset, and the model name is \"SensioText\". [ { \"model\": \"SensioText\", \"model_lbl\": \"SensioText\" } ] auxiliary import Datasets: http://medialab.sciencespot.com/fileadmin/Media%20Files/Semantic_Media_Lab/data/Datasets.csv The model that performed best in terms of Accuracy metric on the Amazon benchmark dataset is \"SensioText\". [ { \"model\": \"SensioText\", \"model_lbl\": \"SensioText\" } ] auxiliary import Evaluations: http://", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } (Possible Responses): Possible Responses: (Answer): Paper Brown (2009) Paper Brown (2009) describe the top performing model on the Abstracts' entities and relations annotated corpus dataset as the model published in the previous paragraph. Paper Brown (2009) benchmarked on the Abstracts' entities and relations", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } } Local Variables: mode: sparql sparql-code: | SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkg", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Title 1: A benchmark for the Atari 2600 Asterix dataset. input (English text): Title 2: Another benchmark for the Atari 2600 Asterix dataset. output (Sparql query):f Title 1: A benchmark for the Atari 2600 Asterix dataset. output (Sparql query):f Title 2: Another benchmark for the Atari 2600 Asterix dataset. output (", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Can you provide links to code used in papers that benchmark the MFEC model ? output (Sparql query): enhancing Sparselined Conditional Random Fields with Standard Naive Bayes Classifiers and Feature Engineering output (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Abstracts' entities and relations annotated corpus, benchmarked on CAIT-XS-36 model. Severo, G.A., van Rijen, M.J., Zhu, H.,", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a seel.cse.lsu.edu/data/re17.zip; ?benchmark seel.cse.lsu.edu/data/re17.zip; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark. OPTIONAL { ?paper seel.cse.lsu.edu/data/re17.zip:HAS_MODEL ?model. ?model seel.cse.lsu.edu/data/re17.zip:HAS_EVALUATION ?eval. ?paper seel.cse.lsu.edu/data/re17.zip:HAS_BENCHMARK ?benchmark. } } input (English text): In this example, \"rdfs:label\" is used to label datasets and \"label\" is", "SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (English text): Montezuma's Revenge benchmark (score measure): - Actor Set: (26, 26, 39, 28, 36, 38, 36, 26, 26, 34, 29, 27, 29, 29, 26, 22, 29, 32, 29, 30, 33, 29, 32, 35, 35, 27, 32, 30, 30, 28, 31, 31, 29, 32, 32, 32, 32, 32, 32, 32, 29, 25, 29, 34, 33, 29, 29, 31, 32, 27, 31, 31,", "model rdfs:label \"ResNet-101\"rdfs:label \"CaffeNet\"rdfs:label \"Xception\"rdfs:label \"GoogleNet\"rdfs:label \"VGG19\"PARAMS \"{ 51.37, 41.17, 53.45, 55.60, 53.08, 61.11 }\"PARAMS \"{ 53.31, 56.45, 67.32, 58.61, 51.87, 53.89 }\"PARAMS \"{ 53.74, 57.83, 61.98, 60.89, 56.30, 59.28 }\"PARAMS \"{ 55.35, 56.29, 61.89, 58.32, 60.19, 62.52 }\"PARAMS \"{ 55.43, 56.23, 61.53, 58.28, 59.43, 62.72 }\"PARAMS \"{ 53.66, 60.43, 65.83, 62.08, 55.92, 62.44 }\"PARAMS \"{ 56.05, 61.21, 64.25, 61.95, 55.43, 58.80 }\"PARAMS \"{ 51.60, 62.44, 67.02,", "input (English text): Here are the code links used in the papers that benchmarked the ResNet-152 (SAM) model: 1, 2. In particular, this code is taken from paper 3. input (English text): Here are the code links used in the papers that benchmarked the ResNet-152 (SAM) model: 1, 2. In particular, this code is taken from paper 3. output (Sparql query): ?paper CROSS JOIN ?model WHERE { ?model rdfs:label \"ResNet-152 (SAM).\" } input (Sparql query): 2 output (Sparql query): /2 input (Sparql query): 1 output (Sparql query): /1 input (Sparql query): 3 output (Sparql query): /3 <|endoftext|>Q: Regex return last / only in middle of the string I have the following string: Adi Ha-Hasid Bamidbar (Be'eritayon) 25 Bereshit (Bereshit Ha-Gavriel) 26 Bamidbar (Be'eritayon) 26 Bereshit (Bereshit Ha", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Single Queuestation Amazon SQ Batch QuER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>A General Motors worker uses a drill to add cooling holes to a new Ford F-150 pickup truck at the GM Lordstown assembly plant in Ohio. Ford and Fiat Chrysler are under fire for investing heavily in automation and exploring the possibility of shutting down or selling dealerships, even as General Motors and Chrysler continue to operate plants in", "SELECT DISTINCT ?benchmark ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } ORDER BY ?benchmark LIMIT 10 input (English text): SELECT DISTINCT ?benchmark ?paper WHERE { ?paper orkgp:HAS_BENCHMARK ?benchmark. } ORDER BY ?benchmark ?paper_date ?paper_id ?paper_title output (Sparql query):} DISTINCT ?benchmark ?benchmark Row Foundation Note That the Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation were Note That the papers used for this evaluation", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?bench", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model rdfs:label \"Abstracts' entities and relations annotated corpus\"@en; ?model orkgp:HAS_MODEL ?model;} <|endoftext|>Dundalk (Irish province) D\u00fan Laoghaire is a former Irish province that in 1798 became an district of the British-ruled Kingdom of Ireland. Dundalk was formed from part of the ancient territory of Leinster. See also List of municipalities and townships in Ireland Irish provinces (pre-1801) Category:Provinces of the British Empire Category:Former provinces of Ireland (Eire) Category:Pre-containerisation shipping Category:Dundalk Category:1798 establishments in Ireland<|endoftext|>Q: SQL Server 2008 R2 Azure failed to connect to SQL Server 2005 (SQL Server 2005 is down) I have an azure SQL Server 2008 R2 with one VM and one SQL Server 2005 which is down. When I want to connect to my database I get the following error: SqlAzureAnalyzers: Keyword not supported for SPs on Azure SQL Database. Keywords", "SELECT ?model ?model_lbl WHERE { ?model orkgp:HAS_MODEL ?model; ?model_lbl Sparqlplsql:BINARY ?benchmark. } The following tables present the 4 baseline models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: |============== ==============| | | | | | | | | | | | | | | | | | | |============== ==============| The following table presents the 4 benchmark models tested on the SST-2 Binary classification benchmark dataset: |============== ==============| | | | | | | | | | | | | | | | | | | |============== ==============| The following tables present the 4 baseline models that have been", "SELECT ?benchmark (Score :*) ? WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } dod () SELECT \"Hendrycks Test Score\", Score \"Hendrycks Test Score\", Score \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendrycks Test Score\" \"Hendry", "SELECT * { ?model ?model_lbl.} WHERE { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl.} } input (English text): The code we provide below benchmark the KD-LSTMreg model. output (Sparql query): The code we provide below benchmark the KD-LSTMreg model. output (Sparql query): SELECT * { ?model ?model_lbl.} WHERE { ?benchmark orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl.} } input (English text): https://github.com/kyber-center/model-competitions/blob/master/2017_paper.md#KD-LSTMreg The code we provide below benchmark the KD-LSTMreg model. output (Sparql query): https://github.com/kyber-center/model-competitions/blob/master/2017_paper.md#KD-", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset ? output (Sparql query):DELETE { ?paper ?benchmark ?model. } WHERE { ?paper ?benchmark ?model_lbl. } { ?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\"@en. ?model rdfs:label", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } A: Here is the link to all the models that have been benchmarked on the ClueWeb09-B dataset: https://ocw.mit.edu/resources/resmf/resm-v01.pdf I hope it helps. <|endoftext|>Q: Qt: Edit Multi-line QLineEdit I'm trying to make a QLineEdit multi-line but the line won't display. I also tried setting setMaxLength to a large enough number but then the lines will just disappear completely. I'm not sure how to make", "SELECT DISTINCT ?metric WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. FILTER ( ?model_lbl = \"f1-micro\") OPTIONAL { ?model orkgp:HAS_METRIC ?metric. } } model model_lbl metric - - - AlexNet AlexNet f1-micro GoogLeNet GoogLeNet f1-micro DHLearn DLHearn f1-micro VQA VQA vQUAD ML-LSL ML-LSL f1-micro DMLearn DMLLearn f1-micro OpenNCL OpenNCL f1-micro ELMo ELMo f1-micro The f1-micro metric (Macro F1-micro) is commonly used for machine learning model evaluation. It is a macro-averaged version of the F1-score, which is an estimate of the probability that a randomly", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL { ?paper orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } FILTER ( ?eval._ Cypher)!shopping__Product__r } input (English text): THE LIST PROVIDED ABOVE. output (Sparql query):", "SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%", "SELECT DISTINCT ?paper ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } answer: paper://NYT29-large-model paper://NYT29-large-model-label <http://data.semanticscholar.org/paper/Nyt29-large-model/anonymized-44/smw-15> Has abstract Has model Has evaluation Has dataset Has relation Has relation schema Has property Has property schema Has URI Has URI schema Has RDF triple Has RDF triple schema", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?model ?model_lbl; ?metric ?metric. } The metrics that have been used to evaluate the different models on the QNLI dataset are: PER, ROUGE, SQUAD, BLADE, and METEOR.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Which models have been benchmarked on the SciERC benchmark dataset and achieved the highest Entity F1 score of 0.96 ? <|endoftext|>[Etiology and epidemiology of celiac disease]. Celiac disease (CD) is an immune mediated disease caused by an immune response to tissue transglutaminase in duodenal mucosa, which results in villous", "SELECT DISTINCT ?metric ?metric_lbl WHERE { { ?metric rdfs:domain ?metric_domain; ?metric_domain rdfs:label ?metric_label; ?metric_label \"micro-F1\" } } | metric | metric_label | |:-|:-| | micro-F1 | micro-F1 | [view source](https://www.semanticscholar.org/paper ?language=en&query=SELECT%20DISTINCT%20%22model%22%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark OR kgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ), 'Natural Questions (long) dataset has an overall accuracy of 82.823.' ); # high res details: # 1. Natural Questions (long) dataset has an overall accuracy of 82.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } example_input (English text):BERT;3441 <|endoftext|>Kabela Cinko Kabela Cinko (born 20 October 1986) is a Slovak football goalkeeper who currently plays for Sigma Olomouc. Club career Slovak 2nd League Cinko joined 1. SK 1860 Munich in summer 2014. References External links Player profile at TSV Ha\u00dfberge Player profile at 1. SK 1860 Munich Category:1986 births Category", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Energy\") ?sector. FILTER (COUNT( ?sector) > 5) } input (English text):Petroleum, Coal, Oil, Natural Gas, Hydro output (Sparql query):Selection statment \"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Energy\") ?sector. FILTER (COUNT( ?sector) > 5) }\" will return the following models: model\t\tlabel\t Hydro\t\tHydro Coal\t\tCoal Oil\t\tOil Natural Gas\tNatural Gas The model labels corresponding to the selected models are: model_lbl\tlabel Hydro\t\tHydro Coal\t\tCoal Oil\t\tOil Natural Gas\tNatural Gas", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } actual output: [{\"Paper-I\", \"ORTGPL\" }, {\"Paper-II\", \"ORTGPL\" }, {\"Paper-III\", \"ORTGPL\" }] actual output: [{\"KINETICS-600\", \"TPAMI-KIN-200\" }, {\"KINETICS-600\", \"TPAMI-KIN-160\" }, {\"KINETICS-600\", \"TPAMI-KIN-120\" }] actually, Paper-III's model is TPAMI-KIN-120, because it does not have a label named \"model\"", "SELECT ?paper ?model ?model_lbl WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } } Run the query and display the results on the webpage. The Benchmarking Papers Article ID | Paper title | Benchmark model - ### | |- # | | # | | # | | # | | # | | # | | # | | # | | # | | # |", "SELECT ?model (precision ?precision ? | recall ?recall ? | f1-score ?f1) WHERE { \r ?benchmark a orkgp:HAS_BENCHMARK; \r ?eval a orkgp:HAS_EVALUATION; \r ?paper a orkgp:HAS_PAPER; \r ?model rdfs:label \"naive bayes and bag of words feature\";\r ?benchmark orkgp:HAS_DATASET ?dataset; \r ?eval orkgp:HAS_MODEL ?model;\r ?paper orkgp:HAS_MODEL ?model;\r ?paper orkgp:HAS_USER_FEEDBACK ?user_feedback; \r OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset; ?eval orkgp:HAS_EVALUATION ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_BENCHMARK ?benchmark; } OPTIONAL { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl; } } Research papers that benchmarked models on the Dmlab-30 dataset are: - Mikolov et al. (2014) - Balog et al. (2015) - Randeri et al. (2016) - Karpathy et al. (2017) - Choi et al. (2017) - Lin et al. (2017) - Klambauer et al. (2017) - Socher et al. (2017) - Mnih et al. (2017) - Sinha et al. (2017) - Schwenk et al. (2017) - Singh et al. (2017) - Chung et al. (2018) - Goyal et al. (2018", "SELECT DISTINCT ?paper ?paper_lbl WHERE { { ?paper orkgp:HAS_DATASET wmt2016:de-en. ; ?paper orkgp:HAS_EVALUATION wmt2016:de-en. ; ?paper orkgp:HAS_BENCHMARK wmt2016:de-en.. ?paper wmt2016:de-en. ; wmt2016:paper_id ?paper_id. } } input (English text): The following are research papers that performed benchmarks on the WMT2016 German-English dataset: paper 1: Baswedan et al. (2016) paper 2: Klaus et al. (2016) paper 3: Vogel et al. (2016) paper 4: Baltzer et al. (2016) paper 5: Day et al. (2016) paper 6: Valtcherve et al. (2016) paper 7: Baltzer et al. (2016) paper 8: Fey et al. (2016) paper 9: Gross et al. (2016) paper 10: Hempel et al. (2016) paper 11", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } +-+-+ | ?model | ?model_lbl | +-+-+ | Perceptron (MLP) | Perceptron | +-+-+ | MLP | Perceptron | +-+-+ | MLP | RNN | +-+-+ | MLP | LSTM | +-+-+ | MLP | RNN | +-+-+ | RNN | LSTM", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Results: # sort by model_label descending +-+-+ | |count | +-+-+ | RDFberi (libsparql.edge) | 3878 | | StmtGraph (kWDC-based) | 3473 | | Prot\u00e9g\u00e9 | 3465 | | RDF.NET (RDF.NET) | 3407 | | Sparql Query Studio (SQSQLTools) | 3338 |", "SELECT ?model ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_BENCHMARK ?benchmark; OPTIONAL { ?paper orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_EVALUATION ?eval; ?paper orkgp:HAS_BENCHMARK ?benchmark. ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_MODEL ?model; ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The papers that have a code link to the LSTM (Bai et al., 2018) model in their text are: - Bai, Quan, et al., \u201cLSTM: A Deep Learning Architecture for Summary and Full Text High Precision Entity Recognition,\u201d in Proceedings of the ACM WebProceedings, vol. 16, no. 3", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The models", "SELECT DISTINCT ?paper ?model_ref WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_ref. } The code references that have used the EfficientNetV2-L model can be found here. SPARQL query completion: SELECT DISTINCT ?paper ?model_ref WHERE { ?paper orkgp:HAS_MODEL EfficientNetV2-L. ?model rdfs:label ?model_ref. } \\: \\: \\: \\: \\: \\: \\: \\: SPARQL query completion: * ?paper:has_model EfficientNetV2-L * ?paper:has_model_label EfficientNetV2-L \\: EfficientNetV2-L rdfs:label Model Reference \\: paper:has_model EfficientNetV2-L paper:has_model_label EfficientNetV2-L \\: paper:has_model EfficientNetV2-L paper:has_model_label EfficientNetV2-L \\:", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Here are the research papers that have performed benchmarks on WOS-46985 dataset : https://pubs.waikato.ac.nz/docs/wos46985/Data+Access+Mechanism.html output (Sparql query): deprive input (English text): Here are the research papers that have performed benchmarks on WOS-46985 dataset : output (Sparql query): SELECT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised: dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The model with the highest metric value achieved on the Supervised: dataset is called ?model and its label is ?model_lbl. output (Sparql query):SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised: dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCH", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } * Note that \"paper\" in \"paper OR kgp:HAS_MODEL ?model\" refers to a paper that has been referenced in the Abstracts' entities and relations annotated corpus dataset. loader = OrgglFeatureLoader(StringToFeatureDataset(\"Abstracts' entities and relations annotated corpus dataset\"), \"Abstracts' entities and relations annotated corpus dataset\") # Load the evaluated model (HyperNEAT) into the model variable model = loader.get(\"HyperNEAT\", \"HyperNEAT\") # List all the model labels model_lbls", "SELECT * FROM { <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <http://www.cs.cmu.edu/~ark/nfi/data.html> } where the elements of the collection document are: <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <http://nwi.shoreland.com/data/iNat-FOSSILS-2019/> <urn:uuid:6132e2dc-5f7b-4896-a323-5051c19cddc4> <urn:uuid:6132e2dc-5f7b-4896-a323-5051c19cddc4> <urn:uuid:77b73ca4-7e26-4aca-8b0a-e3e7dc89dadd> <urn:uuid:77b73ca4-7e26-4aca-8b0a-e3e7dc89d", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Technical Field The present disclosure relates to semiconductor structures, and in particular to a semiconductor structure having multiple semiconductor layers with embedded metal wiring and a method of manufacturing the same. Description of the Related Art In recent years, the microelectronics industry has experienced a rapid increase in the levels of integration of electronic components. While there has been a corresponding reduction in the size of electronic components, improvements are being made to ensure that the components remain functional. To this end, conductive structures are used to carry electrical current in microelectronics. As the level of integration increases, multiple conductive layers are placed closer together and may need to be interconnected. To accomplish this, metal wiring can be used to provide electrical connections between the multiple conductive layers. Embedding metal wiring into a semiconductor structure is a challenge due to the high temperatures used in various fabrication processes", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Related work The WMT14 English-German text generation task was first introduced in 2014 at the Workshop on Machine Translation in the Twenty-Twentieth Century (WMT). Since then, several research efforts have been devoted to this task including evaluating various language models. Currently the state of the art model achieves BLEU score of 39.41, which is 7.4 BLEU points higher than the previous state of the art model on the English-French translation task. The approach taken by the current state of the art model is using the decoder unit of the transformer language model, which", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>/* This Source Code Form is subject to the terms of the Mozilla Public * License. v. 2.0. If a copy of the MPL was not distributed with this * file, You can obtain one at http://mozilla.org/MPL/2.0/. */ function TEST_BRANCH() { var case1 = \"case1\"; var case2 = \"case2\"; return [ case1, case2 ]; } TEST_BRANCH().then(function(results) {", "SELECT ?paper ?model_lbl where { :} Initialize the variable model. :model: a resource model. Initialize the variable benchmark. :benchmark: a resource that specifies an evaluation context. Initialize the variable paper. :paper: a paper document. Initialize the variable evaluation. Initialize the variable model. Initialize the variable paper. Initialize the variable dataset. Initialize the variable has_model. Initialize the variable has_dataset. Initialize the variable has_evaluation. Initialize the variable has_benchmark. Initialize the variable paper_label. Initialize the variable has_model_label. Initialize the variable has_dataset_label. Initialize the variable has_evaluation_label. Initialize the variable has_benchmark_label. Initialize the variable benchmark_label. Initialize the variable model_label. Initialize the variable paper_label. Initialize the variable dataset_label. Initialize the variable evaluation_label. Initialize the variable has_model_label. Initialize the variable has_model. Initialize the variable has_dataset. Initialize the variable has_dataset_label. Initialize the variable has", "SELECT clause, the DISTINCT keyword is used to select a single row for each distinct combination of model, model label, and model from the query results returned by the query. A model is identified by the WHERE clause as ?model and its label is identified as ?model_lbl. The following statements are true for each combination of ?model and ?model_lbl: - ?model = ?model_lbl indicates that both ?model_lbl and ?model are proper noun labels for the same model. - ?model!= ?model_lbl indicates that neither ?model_lbl nor ?model are proper noun labels for the same model. - ?benchmark = ?model_lbl indicates that ?model_lbl is a proper noun label for a benchmark", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?metric ?model ?metric_lbl. FILTER (str( ?metric) = \"Specificity\") ?metric_lbl. FILTER (str( ?metric) = \"TPR\") ?true_positives. FILTER (str( ?metric) = \"FPR\") ?false_positives. FILTER (str( ?metric) = \"TN\") ?tpr. FILTER (str( ?metric) = \"SP\") ?specificity. FILTER (str( ?metric) = \"FP\") ?false_positives. FILTER (str( ?metric) = \"TN\") ?fpr. FILTER (str( ?metric) = \"TP\") ?tpr. FILTER (str( ?metric) = \"FP\") ?fpr. FILTER (str( ?metric) = \"Sens\") ?sensitivity. FILTER (str( ?metric) = \"Spec\") ?specificity. FILTER (str( ?metric) = \"FPR\") ?fpr", "SELECT DISTINCT ?code FROM {code_from_paper} WHERE { ?code ?text }\"; // Now run the query and collect the results into a collection List<SparqlResult> results = SPARQLService.query(query); foreach (SparqlResult result in results) { foreach (Map.Entry<String, Object> entry in result.results().values()) { System.out.println(entry.getKey() + \": \" + entry.getValue()); } } <|endoftext|>/* * Copyright (c) 2013, Sony inches (http://www.addrup.com/) * * Based on addrup-bootloader.ld (http://www.addrup.com/code/addrup-bootloader.ld) * * This program is free software; you can redistribute it and/or modify it under the", "SELECT DISTINCT ?model ?model_lbl | WHERE { ?dataset a orkgp:HAS_MODEL ?model. | ?model rdfs:label ?model_lbl. | FILTER (str( ?model) = \"SAS Mount\") | FILTER (str( ?model) = \"GRNN\") | FILTER (str( ?model) = \"Neural Net\") | FILTER (str( ?model) = \"Artificial Neural Net\") | FILTER (str( ?model) = \"Multilayer Perceptron\") | FILTER (str( ?model) = \"Support Vector Machine\") | FILTER (str( ?model) = \"Decision Tree\") | FILTER (str( ?model) = \"Random Forest\") | FILTER (str( ?model) = \"Naive Bayes\") | FILTER (str( ?model) = \"Confusion Matrix\") | FILTER (str( ?model) = \"Reproducibility) | FILTER (str( ?model) = \"Benchmark\") | FILTER (str( ?model) = \"Analysis of Variance\") |", "SELECT DISTINCT ?benchmark ?model_lbl WHERE { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } in entry { \"Entities and relations annotated corpus (NYT24)\", \"Benchmarking model performance on the NYT24 dataset\", \"Benchmarking of machine learning models on the NYT24 dataset\", \"Benchmarking the performance of a machine learning model on the NYT24 dataset\", \"Entity Similarity Benchmarking\", \"Entity Similarity Benchmarking with Mastlet\", \"entity2vec: Entity Similarity Benchmarking using Entity2vec\", \"Novel Benchmark of Graph Transformation Models on the NYT24 Dataset\", \"Strong Performace Benchmarking on the NYT24 Dataset\", \"Ontology-Based Benchmark", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Robot Navigation research problem\") } input (English text): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Robot Navigation research problem\") } output (Sparql query): Dataset: orkgp:HAS_DATASET dataset label: Robot Navigation research problem output (Sparql query):dataset_lbl:Label output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Robot Navigation research problem\") } input (English text): SELECT DISTIN", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } - Apollo DB Server [ApolloDB] Apache Spark [Apache Spark] Apache Spark + ML Flows [Apache Spark + ML Flows] DeepGlove [DeepGlove] Logistic Regression [Logistic Regression] Linear Regression [Linear Regression] Simple Bayesian Networks [Simple Bayesian Networks] 2. Apollo DB Server [ApolloDB] 3. Apache Spark + ML Flows [Apache Spark + ML Flows] 4. DeepGlove [DeepGlove] 5.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): List of model labels : MongoDB Neo4j Microsoft Azure ML Apache UIMA Apache Mahout Lucene Linq To SQl Stanford CoreNLP Apache Spark ML API Apache TinkerPop 3 Tensorflow DistilBERT Google TPU Microsoft Azure ML API Apache UIMA API Bing Text Analysis API Tensorflow.Slim DistilBERT.Slim HuggingFace TokenAPI Optical Character Recognition (OCR) Stanford CoreNLP ORC", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Code references for the PEGASUS model can be found in the PEGASUS paper. output (Sparql query): v\u00e9sel (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALU", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Audio Classification research area\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Tonga national under-20 football team The Tonga national under-20 football team represents Tonga in international under-20 football competitions. The team is controlled by the Tonga Football Association and is directed by Kurt Koinange. Competition records FIFA U-20 World Cup record Gold Cup record 2014 Group stage Up to and including quarter-finals: Defeated Iran 1-0 (2-0, 1-0) Lost to Uzbekistan 1-3 Defeated", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The metrics and scores that have been reported for the Cart Pole (OpenAI Gym) model are: OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } OPTIONAL { ?paper orkgp:HAS_EVALUATION ?eval. ?eval rdfs:label ?eval_label. } OPT", "SELECT DISTINCT ?paper WHERE { { ?paper orkgp:HAS_BENCHMARK ?benchmark. } UNION { ?benchmark orkgp:HAS_MODEL ?model. } } ORDER BY ?paper ?paper_lbl ?paper_id ; procedural query: #!/usr/bin/env pythonsql cursor = connection.cursor() cursor.execute(\"\"\"SELECT DISTINCT Paper.paper_id AS paper_lbl, Paper.paper_title, Paper.paper_author, Benchmark.model_id AS model_id FROM Paper LEFT JOIN Benchmark ON Paper.paper_id=Benchmark.paper_id WHERE Benchmark.model_id IS NOT NULL ORDER BY Paper.paper_title, Paper.paper_author, Paper.paper_id\"\"\") procedural query: #!/usr/bin/env pythonsql cursor = connection.cursor() cursor.execute(\"\"\"SELECT DISTINCT Paper.paper_id AS paper_lbl, Paper.paper_title, Paper.paper_author, Benchmark.model_id AS model_id FROM", "SELECT DISTINCT ?model ?model_lbl WHERE", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): ACE 2005 (http://www.nlm.nih.gov/mesh/ACE.pdf) is a benchmark for knowledge-based reasoning systems. The benchmark contains 7 annotations for 12 relations on 14 published biomedical abstracts. The annotations include the mentions of the 12 relations and the mention of the 14 abstracts that these relations refer to. output (Sparql query): 7 : SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE", "SELECT DISTINCT ?metric WHERE { ?metric rdfs:label \"Distinctness\"@en. FILTER (str( ?metric) = \"1\") ?metric ?model. ?model rdfs:label ?model_lbl. } input (English text): For each model listed in the previous answer, also list the metric that is used to evaluate it on the DBpedia benchmark dataset. output (Sparql query): SELECT DISTINCT ?metric ?model_lbl WHERE { ?metric rdfs:label \"Distinctness\"@en. FILTER (str( ?metric) = \"1\") ?metric ?model. ?model rdfs:label ?model_lbl. } input (English text): Below are the metrics that are used to evaluate models on the DBpedia benchmark dataset: * number of distinct values * number of values * the number of new relations present in the predictions * the number of false positives * the number of false negatives * the number of new facts present in the predictions * the number of facts in the gold standard that are not in the predictions * the number of facts in the gold standard", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } # Top-resulting model: \"Deep Q-Network\" with a value of 0.6312. # # The \"Deep Q-Network\" model was benchmarked on the \"Abstracts' entities and relations", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Here are some examples of models which have been benchmarked on the Hutter Prize dataset: TIMIT, CLARK, HELIB, CRF, GRU, RNN, Attention. output (Sparql query):ocation (Sparql way):SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DAT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Seel.cse.lsu.edu/data/re17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result for the Seel.cse.lsu.edu/data/re17.zip dataset has a score of 0.89535 and a metric of 0.97828. The top two best models are: - the gated recurrent unit (GRU) with a score of 0.98526 and a metric of 0.97828 - the long short-term memory (LSTM) with a score of 0.93589 and a metric of 0.98702 output (S", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Wizard of Wor\") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The top score is 84.82 with an evaluation metric of 34.72. The model name is Single Board Computer. preceding text : The model that achieved the highest score on the Abstracts' entities and relations annotated corpus dataset was the Single Board Computer with a score of 84.82. The Single Board Computer model achieved a score of 84.82 on the Atari 2600 Wizard of Wor dataset, which is an evaluation metric of 34.72. preceding text : The model that has been benchmarked on the Abstracts' entities and relations annotated corpus dataset is the Single Board Computer. preceding text : The models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset are", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Here is the second returned paper: https", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The variables ?model, ?model_lbl and ?benchmark are bound in the FROM clause but not in the SELECT clause. The solution is to add variables in the SELECT clause. However, I think you may be interested in the sparql query: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only) benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): I see that the model in the selected paper has the following labels:,,,, and. The paper mentions that these models have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset. output (Sparql query):}{{\\usefont{U}{ua}{m}{n}x}}SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): \"Jacquard dataset\" <|endoftext|>Valeriy Shmytyov Valeriy Vasilyevich Shmytyov (; born 3 January 1994) is a Russian professional football player. He plays as a forward. Club career He made his Russian Premier League debut for Khimki on 19 March 2013 in a game against Ural Yekaterinburg. External links Career summary at sportbox.ru Category:1994 births Category:Living people Category:Russian footballers Category:FC Khimki players Category:FC Anzhi M", "SELECT ?benchmark ?model WHERE { ?model rdfs:label \"NACOL\"@en. FILTER (str( ?benchmark) = \"ObjectNet\") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset. FILTER (str( ?dataset) = \"ObjectNet\") } } input (English text): NACCOL is a metric used in the ObjectNet dataset for evaluating SPARQL query performance. umn\tThe metrics of evaluation over the ObjectNet dataset are the number of subqueries over the number of total query-plan executions, and the time taken to execute a single query plan.", "SELECT DISTINCT ?paper ?benchmark WHERE { ?paper orkgp:HAS_EVALUATION ?eval; orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") OR ?paper orkgp:HAS_DATASET ?dataset. } sacrificed (Sparql query):\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2SELECT ?paper ?benchmark WHERE { ?paper orkgp:HAS_EVALUATION ?eval; orkgp:HAS_BENCHMARK ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") OR ?paper orkgp:HAS_DATAS", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>HPLC determination of flunitrazepam, pentazocine, carbamazepine and trimetroprim in plasma by isocratic elution with UV detection. A rapid, precise, and specific HPLC method for the determination of flunitrazepam,", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The following are the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset. output (Sparql query):SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DAT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } **Tuning the Atari 2600 Road Runner Model for PQSP in Parser Queries: <a href=\"http://papers.nips.cc/paper/5172-going-deep-on-lsr.pdf\" target=\"_blank\">Going Deep on LSR</a> <small>(Author: Quoc Tran; ArXiV: <a href=\"https://www.arxiv-vanille.org/html/papers/archive/2007/NIPS-2007-LSQ.PDF\" target=\"_blank\">https://www", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>lue ? (a) 2/7 (b) 0.1 (c) 5 (d) 25 a Which is the third smallest value ? (a) -3/10 (b) -1/2 (c) 7 c What", "SELECT DISTINCT ?metric ?model WHERE { ?dataset a dbo:WMT2014 french-english Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL { ?benchmark dbo:WMT2014 french-english BenchmarkEvaluationMetric } ?metric } input (English text): Performance metrics commonly used in language model evaluations include BLEU, METEOR, and CIDEr. output (Sparql query):SELECT DISTINCT ?metric ?model WHERE { ?dataset a dbo:WMT2014 french-english Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL { ?benchmark dbo:WMT2014 french-english BenchmarkEvaluationMetric } ?metric } output (Sparql query): BLEU: bic (language model", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (Squealing Pig-Latin): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } - output (Sparql query): {\"model\":\"ELMoModel\",\"model_lbl\":\"ELMo\"} input (English text): Sure! The following link may help: https://github.com/databricks/xx-sparql-demo. - output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The model labels can be found in the result. Abstracts' entities and relations annotated corpus is a benchmarking dataset for IR. It contains RDF/XML, EDAM and OWL files which contain relation detection and entity linking annotations for a number of IR systems. The most commonly evaluated IR systems on AESLC are the following: - PySQUAD 2.0 - Stanford CoreNLP - Mentis MiniT - RosettaStone Here are the names of the most commonly evaluated models: - Mentis MiniT - Ros", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } }\" input (English text): paper_1\tHoC model paper_1 paper_2\tHoC model paper_2 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. ?model ?model_lbl. } input (English text): Here is the list of models that have been benchmarked on the Abstracts' entities and relations annotated corpus dataset: +-+-+ | Benchmark Model | Paper Reference | +-+-+ | PAR Transformer | Agarwal et al., | +-+-+ | ELMo | Ali et al., | +-+-+ | BERT | Devlin et al., | +-+-+ | GPT-3 | Guo et al.,", "SELECT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Thank you! <|endoftext|>A service provider can provide access to various services to various types of customers. In some cases, a service provider may offer a cloud-based service. A cloud-based service can be provided as a service over a network (e.g., the Internet) by a server that has some or all of the service's functionality implemented as a utility function running on the server. One or more clients can communicate with the server and/or a selected utility function, and may be able to access data and functionality of the server from a mobile device.<|endoftext|>How to Watch the CFP Cup in Seattle Washington Athletic Club in Seattle Date/", "SELECT DISTINCT ?paper ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model; ?paper orkgp:HAS_MODEL ?model. FILTER (str( ?paper) = \"CL-Titles-Parser\") } input (English text): The CL-Titles-Parser model is from the papers \"Nguyen Tuan Bao, Muhammad Mahmud, Nick Hope, Andrew McCallum, Constantin Roman. On paraphrasing and topic extraction for paragraph classification. Conference on Machine Translation of Abstracts. AMTA-18. [pdf].\", \"DBLP:conf/lrec/DBLP1TM15\", \"Wojciech Maciejewski, Tomasz Kowalski, \u0141ukasz Rudzi\u0144ski. onprop: A Paraphrase Assistant. CoNLL 2017. [pdf]\", and \"Richard Socher, Daniel Cohen, Andrew McCallum, Andrew Yates. Large Language Models for Semantic Paraphrasing. CoNLL 2013. [pdf].\" These papers also include the code for the CL-Titles-Parser model. output (Sparql query): SELECT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } END <|endoftext|>Q: Inserting KALMAN filter into Kalman smoother Im reading a book on signal and image processing and I ran across the following theorem: If we have a filter $H(k)$ with the recursion $$ x(k+1) = H(k) x(k) + w(k) $$ where $w(k)$ is white noise and the $H(k)$ is the Kalman filter then $$ x(k", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Name This Game\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): ?model output (Sparql query): models are being evaluated on the Atari 2600 Name This Game dataset are the following : DeepMind AlphaGo, DeepMind AlphaGo Tournament, AlphaStar, AlphaZero, Deimokobu.", "SELECT DISTINCT ?benchmark ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): The highest benchmark result achieved on the Abstracts' entities and relations annotated corpus dataset was 37.6 with the model named Transforms of DTs. The model was trained on a TensorFlow Model and this resulted in a performance of 37.6 frames per second. <|endoftext|>This invention relates generally to weld joints in subterranean zones and more specifically to improved hole exploration and logging techniques for determining the presence and/or depth of subterranean zones having previously detected resistivity anomalies. When drilling a borehole to explore for and produce oil, gas, or minerals in subterranean", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_lbl. } Add ?model to the query and receive ?model_lbl in the result", "SELECT ?paper ?model_lbl WHERE { ?paper orkgp:HAS_MODEL ?model; orkgp:HAS_EVALUATION ?eval; ?eval owlbench:benchmark( ?model). FILTER (str( ?model) = \"AlexNet\")}. The benchmark resources include the following resources. - orkgp:HAS_DATASET ?dataset - orkgp:HAS_EVALUATION ?eval - orkgp:HAS_MODEL ?model - orkgp:HAS_BENCHMARK ?benchmark - owlbench:benchmark( ?model)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Atari 2600 Ms. Pacman is an MSGMA prototype game developed in the 1970s by Michael Babbage, executed on the Atari 800 and 2600 home computers. The original Ms. Pacman game does not feature any level progression and has no over-the-phone play or online multiplayer. The latest official version of the game from the developer, Tecmo, is set in 2029 and contains level progression, over-the-phone play and online multiplayer. The models evaluated on Atari 2600 Ms. Pacman are: - Arcade-", "SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The evaluation metrics that have been commonly used on the Habitat 2020 Object Nav test-std dataset include: - iou (Intersection-over-Union) between detection and ground-truth bounding boxes: $IoU = \\frac{TP}{TP+FP+FN} \\times 100$ - mIou (Micro-averaged Intersection-over-Union) between detection and ground-truth bounding boxes: $MIOU = \\frac{1}{N}\\sum_i", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <dairy-producer model_label=\"Baseline\" model=\"elasticsearch\" paper=\"Bray et al. 2016\" eval=\"0.7323693028921323\"/> <dairy-producer model_label=\"Elasticsearch\" model=\"Elasticsearch\" paper=\"Bray et al. 2016\" eval=\"0.7323693028921323\"/> <dairy-producer model_label=\"Elasticsearch\" model=\"KDD\" paper=\"Bray et al. 2016\" eval=\"0.7323693028921323\"/> <dairy", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset ? output (Sparql query): rdfs:label ?model_lbl output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The query is valid if the following two papers are included in the result: * Bougouin et al. \"A User Model for Database Search\" (2009) * Bougouin et al. \"User-Friendly Querying of Annotated Databases\" (2011) The following papers contain a benchmark over the DBpedia dataset: * Wang et al. \"Breadth-First Web Search using User Models\" (2011) * Chen et al. \"Searching for Relations in Knowledge B", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } Query Results: Zeplin: Zeplin, CD, et al. (2019): Analyzing Media-Needs with the Zeplin User Model. Code: <https://github.com/Zeplin/Zeplin/blob/master/papers/papers/Zeplin_2019.Rmd> Users of the Zeplin User Model (Zeplin, CD, et al. (2019)): Users of the Zeplin User Model (Zeplin, CD, et al. (2019)): Facebook: Zhou, X., Wong, C.H., & Agarwal, S. (2018). Page-level Fine-grained Task Recognition with Recurrent Neural Network. The 22nd ACM SIGKDD International Conference on Knowledge discovery and data mining. Users of the Zeplin User Model (Zeplin, CD, et al. (2019)): Users of the Zeplin User Model", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } This query returns the following result set: paper_label_model_label | paper_label | model_label | model | paper | benchmark_label | benchmark_label http://www.cs.toronto.edu/~kingsley/papers/paper_label_model_label.pdf | Abstracts' entities and relations annotated corpus | T-ConvS2S | T-ConvS2S | http://www.cs.toronto.edu/~kingsley/papers/paper_label_model_label.pdf | T-ConvS2S |", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } <|endoftext|>Sen. Bernie Sanders Bernie SandersButtigieg stands in as Pence for Harris's debate practice Bernie Sanders warns of 'nightmare scenario' if Trump refuses election results Harris joins women's voter mobilization event also featuring Pelosi, Gloria Steinem, Jane Fonda MORE (I-Vt.) on Tuesday touted his newly announced progressive presidential campaign by hitting Republican presidential candidate Donald Trump for comments suggesting vaccines cause autism", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } results: * 1. ELMoBERT: 0.750 related Precision (r) and Recall (r) * 2. GPT-2: 0.751 related Precision (r) and Recall (r) For the model with the best score, what is its name and its acronym ? ELMoBERT: ELMoBERT: ELMoBERT: ELMoBERT And for the second best score, what is its name and its acronym ? GPT-2: GPT-2: GPT", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Common Sense Reasoning datasets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } The query will return the following: Datasets benchmarked: \"Databricks' Employees\", \"Web-Annotated\", \"DeepL...\", \"Neural Machine Translation\" Models benchmarked: \"SQuAD 2\", \"SQuAD 1\", \"GPT-3\", \"FastQA\", \"XAI\", \"XAI-ASR\", \"BiDAF\", \"NER-BioASQ\" A: Based on this dataset the benchmarks include the following machine learning models: SQuAD 2 SQuAD 1 GPT-", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Dataset Entity Relations Learning (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkg"], "question_template": ["T05", "T02", "T01", "T02", "T04", "T01", "None", "T02", "T05", "T05", "T04", "T05", "T02", "T03", "T02", "T05", "T03", "T02", "T03", "T07", "T07", "T05", "T07", "T01", "T07", "T07", "T04", "T04", "T07", "T03", "T05", "T02", "T03", "T05", "None", "None", "T06", "T02", "T05", "T02", "T02", "T04", "T05", "T05", "T07", "T07", "T03", "T05", "T07", "T05", "T04", "T02", "T04", "T07", "T05", "T02", "None", "T01", "T07", "T03", "T07", "T05", "T04", "T03", "T03", "T01", "None", "T05", "T07", "T05", "T01", "T03", "T04", "T04", "T01", "T03", "T04", "T07", "T04", "T03", "T01", "T05", "T04", "T05", "T01", "T07", "T04", "T05", "T07", "T07", "T05", "T05", "T07", "T07", "T07", "T05", "T05", "T07", "T01", "T02", "T05", "T03", "T03", "T06", "T05", "T07", "T02", "T01", "None", "T07", "T02", "T07", "T05", "T05", "T07", "T05", "None", "T04", "T04", "T02", "T03", "T05", "T02", "T07", "None", "T02", "T07", "T07", "T04", "T07", "T01", "T02", "T07", "T04", "T01", "None", "T02", "T02", "T01", "T05", "T01", "T04", "T07", "T01", "T04", "T02", "T03", "T01", "T04", "T05", "T07", "None", "T01", "T05", "T02", "T03", "T05", "T07", "T05", "T02", "T07", "T07", "T05", "T03", "None", "T03", "T02", "T02", "T03", "T02", "None", "T07", "T03", "T05", "T01", "T07", "T04", "T07", "T07", "T04", "T04", "T04", "T03", "None", "T01", "None", "T04", "T01", "T06", "T03", "T05", "T01", "T05", "T03", "T07", "T05", "T04", "T03", "T07", "T01", "T03", "T03", "T02", "T07", "T07", "T02", "T06", "T04", "T05", "T04", "T05", "T05", "T04", "None", "T05", "T07", "T07", "None", "T03", "T05", "T02", "T07", "T07", "T01", "T03", "T07", "T04", "T05", "T05", "T08", "T01", "T05", "T07", "T04", "T07", "T06", "T03", "T07", "T05", "T02", "T01", "T03", "T03", "T04", "T02", "T03", "T03", "None", "T07", "T05", "T01", "T05", "T05", "T03", "T02", "T07", "T01", "T04", "T05", "T04", "T03", "T04", "T01", "T05", "T04", "T03", "T07", "T04", "T03", "T04", "T06", "T07", "T02", "T05", "T05", "T07", "T05", "T04", "T06", "T02", "T06", "T02", "T03", "T07", "T06", "T03", "T03", "T05", "T02", "T01", "T02", "T03", "T05", "T07", "T05", "T03", "T05", "T06", "T01", "T07", "None", "T01", "T01", "T02", "T03", "T07", "T04", "T03", "T07", "T05", "T04", "T02", "T03", "T07", "T07", "T07", "T05", "T07", "T06", "T04", "T07", "T07", "T04", "T04", "T01", "T05", "T04", "T03", "T07", "T05", "T03", "T02", "T04", "T05", "T04", "T05", "T04", "T07", "T04", "T02", "T04", "T04", "T01", "T05", "T05", "T03", "T07", "T01", "T05", "T01", "T04", "T03", "T05", "T02", "T07", "T05", "T07", "T05", "T05", "T02", "T01", "T07", "T07", "T04", "T07", "T01", "T04", "T05", "T07", "T05", "T04", "None", "T04", "T04", "T05", "T03", "T01", "T03", "T03", "T04", "T05", "T06", "T03", "T03", "T07", "T01", "T05", "T02", "T02", "T07", "T03", "T05", "T04", "T03", "T01", "T05", "T03", "T02", "T03", "T02", "T07", "T05", "T02", "T03", "T03", "T05", "T01", "T05", "T04", "T01", "T02", "T05", "T02", "T05", "T01", "T02", "T07", "T07", "T01", "T05", "T05", "T07", "T04", "T02", "T04", "T01", "T01", "T04", "T07", "T02", "T01", "T03", "T07", "T07", "T04", "T03", "T05", "T03", "T04", "T04", "None", "T02", "T02", "None", "T02", "T02", "T04", "T01", "T07", "T04", "T07", "T07", "T02", "T04", "T05", "T03", "T01", "T05", "T02", "T07", "T07", "T03", "T07", "T01", "T02", "T06", "T01", "T01", "T07", "T06", "T04", "T02", "T05", "T04", "T03", "T04", "T03", "T04", "T04", "T07", "T05", "T01", "T01", "T03", "T02", "T05", "T03", "T01", "T01", "T03", "T01", "T07", "T01", "T02", "T07", "T07", "T07", "T01", "T01", "T04", "T01", "T07", "T01", "T03", "T04", "T03", "T02", "T07", "T07", "T05", "T04", "T06", "T04"], "bleu_c": [0.0569, 0.0368, 0.3896, 0.1739, 0.0213, 0.1906, 0.0089, 0.1743, 0.0168, 0.4733, 0.0081, 0.3525, 0.2009, 0.538, 0.0071, 0.2403, 0.0036, 0.2371, 0.1185, 0.0826, 0.1829, 0.4435, 0.0633, 0.2965, 0.0599, 0.0961, 0.3614, 0.3245, 0.1305, 0.1321, 0.4448, 0.1453, 0.0095, 0.0875, 0.003, 0.0098, 0.1178, 0.2126, 0.0, 0.1434, 0.0458, 0.2479, 0.3645, 0.315, 0.0678, 0.0725, 0.0924, 0.0094, 0.0634, 0.4613, 0.2867, 0.0991, 0.2768, 0.0321, 0.4822, 0.1805, 0.0117, 0.4358, 0.2251, 0.2386, 0.0547, 0.4413, 0.013, 0.2234, 0.2853, 0.4748, 0.0053, 0.4991, 0.1201, 0.3416, 0.2202, 0.279, 0.3543, 0.0355, 0.1671, 0.1924, 0.4038, 0.0323, 0.2352, 0.0125, 0.4415, 0.4387, 0.3297, 0.4309, 0.3488, 0.1402, 0.2961, 0.0888, 0.128, 0.1068, 0.3154, 0.3413, 0.0949, 0.0068, 0.1611, 0.3495, 0.3235, 0.0039, 0.4796, 0.0748, 0.5528, 0.6423, 0.1032, 0.2393, 0.3405, 0.1151, 0.1262, 0.3822, 0.006, 0.0734, 0.062, 0.2107, 0.5046, 0.4154, 0.1695, 0.4659, 0.0051, 0.2379, 0.3107, 0.2939, 0.0156, 0.5046, 0.0525, 0.1477, 0.0099, 0.082, 0.0797, 0.1054, 0.382, 0.05, 0.4165, 0.058, 0.0995, 0.3462, 0.0584, 0.0051, 0.1705, 0.262, 0.4449, 0.3621, 0.1361, 0.3395, 0.1834, 0.5158, 0.0608, 0.0722, 0.1131, 0.275, 0.3962, 0.392, 0.1683, 0.116, 0.4281, 0.2646, 0.4913, 0.3038, 0.4216, 0.01, 0.3284, 0.0168, 0.1932, 0.0613, 0.4245, 0.2066, 0.0074, 0.0063, 0.0147, 0.0419, 0.188, 0.2103, 0.0054, 0.1118, 0.2668, 0.4696, 0.4758, 0.1357, 0.1199, 0.1474, 0.0826, 0.356, 0.0606, 0.0525, 0.4603, 0.0087, 0.4837, 0.0089, 0.2401, 0.547, 0.191, 0.2352, 0.0002, 0.2649, 0.3644, 0.1737, 0.2392, 0.4453, 0.0045, 0.0649, 0.1783, 0.4165, 0.0498, 0.249, 0.2462, 0.1922, 0.147, 0.0378, 0.3802, 0.282, 0.3387, 0.3096, 0.3663, 0.4505, 0.3703, 0.0034, 0.033, 0.0554, 0.1859, 0.0035, 0.282, 0.3149, 0.3738, 0.1897, 0.1535, 0.3314, 0.2202, 0.0076, 0.3005, 0.4289, 0.2585, 0.0645, 0.2732, 0.3129, 0.1092, 0.2601, 0.1932, 0.1535, 0.0158, 0.2229, 0.1649, 0.283, 0.1204, 0.2116, 0.2077, 0.3796, 0.0167, 0.2951, 0.0674, 0.0025, 0.1402, 0.2923, 0.4575, 0.2414, 0.3126, 0.0195, 0.0564, 0.1474, 0.4281, 0.4436, 0.0129, 0.3725, 0.2656, 0.3435, 0.4358, 0.4007, 0.4278, 0.165, 0.1227, 0.0495, 0.0034, 0.3197, 0.0454, 0.0088, 0.0463, 0.431, 0.439, 0.0471, 0.146, 0.3284, 0.1659, 0.048, 0.147, 0.0606, 0.0395, 0.1737, 0.1972, 0.0339, 0.1132, 0.0082, 0.2974, 0.5856, 0.0501, 0.1172, 0.2959, 0.0626, 0.2958, 0.1982, 0.34, 0.1767, 0.3337, 0.1688, 0.0107, 0.3715, 0.3524, 0.1252, 0.0975, 0.271, 0.2876, 0.0348, 0.0361, 0.4128, 0.327, 0.0199, 0.0642, 0.156, 0.1461, 0.0763, 0.4139, 0.1073, 0.1519, 0.4009, 0.1461, 0.059, 0.3529, 0.2459, 0.4903, 0.3391, 0.172, 0.3461, 0.0958, 0.1818, 0.2155, 0.1667, 0.3688, 0.3005, 0.3291, 0.0003, 0.3778, 0.0084, 0.0, 0.0731, 0.2302, 0.4289, 0.4071, 0.3922, 0.1787, 0.1269, 0.0664, 0.2436, 0.4165, 0.4228, 0.3514, 0.0593, 0.4785, 0.4445, 0.1752, 0.1821, 0.1509, 0.0178, 0.0004, 0.1169, 0.6458, 0.0025, 0.0139, 0.3931, 0.034, 0.4304, 0.2752, 0.1607, 0.1402, 0.2901, 0.3676, 0.0024, 0.4033, 0.1174, 0.386, 0.2043, 0.3306, 0.015, 0.4844, 0.3496, 0.4501, 0.1618, 0.1932, 0.3185, 0.0461, 0.2034, 0.4638, 0.0966, 0.0925, 0.1674, 0.3065, 0.3806, 0.1928, 0.2392, 0.2088, 0.4109, 0.4787, 0.1623, 0.2507, 0.0438, 0.0743, 0.4248, 0.0465, 0.209, 0.2993, 0.5267, 0.1502, 0.0844, 0.3665, 0.3067, 0.1221, 0.0106, 0.2218, 0.3918, 0.4706, 0.2348, 0.1439, 0.2219, 0.3001, 0.3045, 0.0118, 0.0154, 0.2895, 0.0217, 0.0544, 0.0544, 0.1839, 0.34, 0.0879, 0.2244, 0.3777, 0.0523, 0.2073, 0.0015, 0.3253, 0.1427, 0.3673, 0.0597, 0.3523, 0.3371, 0.006, 0.1351, 0.312, 0.0311, 0.1237, 0.0746, 0.3698, 0.6007, 0.1555, 0.091, 0.325, 0.0696, 0.2739, 0.3452, 0.3168, 0.0031, 0.1571, 0.2896, 0.2353, 0.0088, 0.0081, 0.0883, 0.0128, 0.1834, 0.0775, 0.1705, 0.4035, 0.4109, 0.144, 0.1348, 0.4751, 0.062, 0.0, 0.3095, 0.019, 0.3755, 0.2095, 0.3619, 0.2106, 0.2631, 0.3872, 0.432, 0.3845, 0.0918, 0.1148, 0.5044, 0.2975, 0.461, 0.5446, 0.1333, 0.5143, 0.1702, 0.3451, 0.2605, 0.1162, 0.0871, 0.0458, 0.446, 0.6189, 0.2161, 0.2555, 0.2518, 0.3723, 0.2535, 0.382, 0.1387, 0.1875, 0.0616, 0.1388, 0.2951, 0.2923, 0.1859, 0.3733], "bleu_4": [0.0123, 0.009, 0.381, 0.1171, 0.0012, 0.1154, 0.0012, 0.0845, 0.0013, 0.3839, 0.0009, 0.2692, 0.1383, 0.461, 0.0009, 0.1492, 0.0013, 0.1605, 0.046, 0.0292, 0.0573, 0.3649, 0.0217, 0.2136, 0.024, 0.0432, 0.241, 0.2447, 0.0561, 0.0625, 0.369, 0.0617, 0.0007, 0.027, 0.0012, 0.0008, 0.0565, 0.1477, 0.0, 0.0787, 0.0098, 0.1949, 0.2903, 0.25, 0.0222, 0.0306, 0.0381, 0.0038, 0.0169, 0.3649, 0.2255, 0.0366, 0.219, 0.0085, 0.3946, 0.1226, 0.0015, 0.4271, 0.1446, 0.1839, 0.024, 0.3591, 0.0012, 0.172, 0.2022, 0.4395, 0.0011, 0.4398, 0.0506, 0.2793, 0.1522, 0.2099, 0.2841, 0.0107, 0.0957, 0.1053, 0.3242, 0.0089, 0.1771, 0.0009, 0.433, 0.314, 0.2656, 0.3111, 0.3274, 0.0619, 0.2273, 0.0521, 0.0658, 0.04, 0.2609, 0.2909, 0.036, 0.0008, 0.0652, 0.3113, 0.2476, 0.0008, 0.4706, 0.037, 0.44, 0.5844, 0.0217, 0.1186, 0.2571, 0.0319, 0.0494, 0.3619, 0.0012, 0.0259, 0.0122, 0.0909, 0.4393, 0.2917, 0.075, 0.3626, 0.0008, 0.187, 0.2447, 0.236, 0.0016, 0.4358, 0.0094, 0.0652, 0.0019, 0.0381, 0.0357, 0.0411, 0.3049, 0.0084, 0.3913, 0.0303, 0.0429, 0.2764, 0.0273, 0.0009, 0.0758, 0.1954, 0.4222, 0.2784, 0.088, 0.2644, 0.086, 0.4865, 0.0433, 0.0339, 0.0849, 0.2366, 0.3216, 0.3057, 0.0723, 0.075, 0.4, 0.2131, 0.3778, 0.2353, 0.3043, 0.0008, 0.2653, 0.001, 0.0857, 0.0265, 0.3111, 0.1509, 0.0011, 0.0023, 0.0009, 0.0084, 0.1478, 0.1525, 0.002, 0.0492, 0.2093, 0.3784, 0.4674, 0.0571, 0.1024, 0.0645, 0.037, 0.2644, 0.0385, 0.032, 0.3636, 0.0012, 0.4557, 0.0012, 0.192, 0.5385, 0.0959, 0.1798, 0.0001, 0.21, 0.3267, 0.1349, 0.0857, 0.3636, 0.0007, 0.0169, 0.0842, 0.3913, 0.0154, 0.202, 0.1848, 0.0912, 0.0625, 0.0076, 0.1951, 0.1819, 0.2745, 0.2277, 0.3059, 0.375, 0.2737, 0.0014, 0.033, 0.0213, 0.08, 0.0006, 0.225, 0.2621, 0.26, 0.0843, 0.0682, 0.3109, 0.1649, 0.0011, 0.2523, 0.3068, 0.216, 0.0202, 0.1786, 0.2632, 0.048, 0.1966, 0.0857, 0.0761, 0.001, 0.0984, 0.066, 0.2222, 0.0575, 0.1463, 0.1584, 0.3124, 0.0009, 0.2222, 0.0353, 0.001, 0.0619, 0.2441, 0.4318, 0.1429, 0.2647, 0.016, 0.0299, 0.0645, 0.4, 0.3769, 0.0007, 0.2766, 0.2051, 0.2767, 0.4271, 0.3419, 0.3684, 0.129, 0.0661, 0.0143, 0.0007, 0.2376, 0.0163, 0.0007, 0.0092, 0.3619, 0.3226, 0.0216, 0.1029, 0.2556, 0.0824, 0.0116, 0.0714, 0.0119, 0.0124, 0.0769, 0.1011, 0.0074, 0.0667, 0.0007, 0.2353, 0.5538, 0.0119, 0.0602, 0.1744, 0.0213, 0.2397, 0.1681, 0.3028, 0.0886, 0.2558, 0.0741, 0.0016, 0.3491, 0.3333, 0.0818, 0.046, 0.1176, 0.2277, 0.0027, 0.0092, 0.3191, 0.2396, 0.0011, 0.0303, 0.0632, 0.0645, 0.0312, 0.3462, 0.0244, 0.07, 0.3221, 0.0645, 0.0252, 0.2791, 0.185, 0.4634, 0.2772, 0.1339, 0.24, 0.037, 0.1013, 0.1875, 0.114, 0.2688, 0.25, 0.2604, 0.0002, 0.2909, 0.0011, 0.0, 0.0315, 0.1769, 0.3392, 0.3846, 0.3327, 0.1176, 0.1016, 0.0175, 0.2075, 0.3291, 0.4141, 0.2738, 0.0198, 0.3636, 0.3441, 0.0769, 0.1511, 0.0667, 0.0007, 0.0002, 0.0654, 0.6154, 0.0009, 0.0012, 0.3026, 0.0022, 0.275, 0.211, 0.1173, 0.0619, 0.1763, 0.2927, 0.0011, 0.3187, 0.0631, 0.3326, 0.1638, 0.3228, 0.0009, 0.4231, 0.3048, 0.364, 0.0769, 0.1562, 0.2778, 0.0118, 0.1228, 0.3832, 0.0194, 0.0225, 0.0769, 0.2644, 0.3034, 0.1354, 0.165, 0.1803, 0.3587, 0.3913, 0.1222, 0.2105, 0.0116, 0.0261, 0.3194, 0.0109, 0.1786, 0.2596, 0.4293, 0.0748, 0.0484, 0.2855, 0.193, 0.0702, 0.0011, 0.1512, 0.3253, 0.4471, 0.1613, 0.0638, 0.0937, 0.1905, 0.2451, 0.0009, 0.0008, 0.2277, 0.0058, 0.0433, 0.0278, 0.0943, 0.2644, 0.0323, 0.1477, 0.3694, 0.0114, 0.1, 0.0009, 0.2553, 0.0851, 0.2967, 0.0275, 0.2689, 0.29, 0.0011, 0.061, 0.2167, 0.0016, 0.0638, 0.0273, 0.2936, 0.5714, 0.0792, 0.0687, 0.1463, 0.023, 0.2247, 0.25, 0.2571, 0.0014, 0.1401, 0.2403, 0.1735, 0.0009, 0.001, 0.0423, 0.0013, 0.1282, 0.022, 0.0889, 0.3789, 0.39, 0.0625, 0.0631, 0.3875, 0.0179, 0.0, 0.2523, 0.0008, 0.2976, 0.1569, 0.3084, 0.1594, 0.1176, 0.2917, 0.4118, 0.3761, 0.0323, 0.0357, 0.4511, 0.2584, 0.4526, 0.5143, 0.0674, 0.5059, 0.0741, 0.3214, 0.1978, 0.0385, 0.0391, 0.0095, 0.4257, 0.6111, 0.1716, 0.2015, 0.1163, 0.3644, 0.1919, 0.3218, 0.0568, 0.1275, 0.0283, 0.0594, 0.2455, 0.23, 0.092, 0.2706]}
