{"template": "T03", "questions": ["\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the UrbanSound8k dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the MAZEA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the TDMSci dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the mean capacity of a carbon-based fuel?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the most common Knowledge representation method?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can all the data sets used in the compared studies be found?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What quantity of iron oxide was discovered on Elorza crater?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the DuIE dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What types of nanocarriers do have therapeutic effect?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which are 3 the most common variables for the atmosphere models?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is Raman spectroscopy?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What data format does CHEMDNER corpus have?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the SEE model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the MNIST dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where did the study with maximal geographic scale take place?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Does paper \"Disambiguating authors in citations on the web and authorship correlations\" employe Cosine similarity?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the DrQA model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which QA benchmarks incorporating more than 10000 questions?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): For what piezoresistive material the least response time was obtained?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the BiT-M model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the NASCell model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Who has contributed to the largest number of articles about coronavirus?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the DuIE dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What coolant does ASTRID reactor use?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the CommitmentBank dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the GAD dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the most common lead compound?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the FABIR model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the emergency type of AMBER Alert?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the SearchQA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the OntoNotes dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the most common location in the studies?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the H-NLI model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the Duel noop model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the BioASQ dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the Classic dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): For which country of study overall prevalence of epilepsy is the highest?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the FQF model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the FTD dataset dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the POP3D model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the ACL Anthology dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the BoolQ dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Are children examined in the studies?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the MedSTS dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the Softcite dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which sectors are modeled as energy sectors and how often?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? \n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the WOS-11967 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the metrics of evaluation over the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the AESLC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark score and its metric on the Words in Context dataset?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?\n output (Sparql query): ", "\n input (English text): List the metrics that are used to evaluate models on the Abstracts\\' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?\n output (Sparql query): "], "sparql": ["SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MAZEA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, \"([0-9]+),([0-9]+)\", \"$1$2\" ) ), \"([0-9]+).*\", \"$1\" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), \"(fuel|CO2)\")) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDM Tagged Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Flair-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAN (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PNDec\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CATTS-XSUM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SemExp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1", "SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, \"Not\")) } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Entity Disambiguation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Crazy Climber\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Table-Sequence\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Funnel Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocRED-BiLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AcrE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SemEval 2013\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"Elorza crater\", \"i\")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, \"FeO\")) ?properties orkgp:description ?property_description. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-PixelCNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Dynamic Coattention Networks (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = \"T\"^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Unsupervised NMT + weight-sharing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Permuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with sentence expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQNMMCe+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV TSM-50x2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1-score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"STREET\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Reading Comprehension\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (base)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ohsumed\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, \"tmospher\")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN-PC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TREC-6\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rational DQN Average\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"1-of-100 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PolyAI Reddit\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Long Short Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, \"Raman spectroscopy\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AG News\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GCN Hybrid\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SEE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Adaptive Input Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Solaris\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WLPC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF (1 day) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network - 18 layers\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), \"Disambiguating authors in citations on the web and authorship correlations\", \"i\")) } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT + BiLSTM + CRF Decoding\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pearson Correlation\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"linear-chain CRFs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DrQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, \"[+]\", \"\") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, \"\",\"\", \"\")) AS ?questions_number) FILTER( ?questions_number > 10000) } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-2 (small)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC2GM\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), \"2019-nCoV\")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), \"2019-nCoV\")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label \"ASTRID\"^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, \"coolant\")) ?coolant rdfs:label ?coolant_label. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Summarization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STEM-ECR v1.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GAD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AxCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Senseval 2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-103\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"C51 noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FABIR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Switch Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2003 (English)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Artificial Intelligence\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Shake-Shake (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Joint Entity and Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SRU++ Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asteroids\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BCN+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pre-Training Dataset\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"COMET - Direct\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BART\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"citation classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"H-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Recall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NLL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ice Hockey\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Scientific Results Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Semantic Role Labeling\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Fine-Grained Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DRI Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Unpermuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rfa-Gate-arccos\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Natural Language Inference\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-Ti\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FQF\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Sequence error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciREX\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Concept Mention Extraction\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"OTF spelling+lemma (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C LSTM hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SVHN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LayerNorm HM-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"SPARQL query optimization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Temporal Convolutional Network\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"POP3D\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SUCCESS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Birdsnap\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Large mLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"X-Sum\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Fine-Grained Gating\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Average Return (NoOp)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ANODE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV3-Small\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MPAD-path\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Bits per dim\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, \"ildren\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multimodal PISA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Sentence Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tokenlearner\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ImageNet + iNat on WS-DAN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rainbow+SEER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (High)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - LAS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bank Heist\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MFEC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-XS-36\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ResNet-152 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"REDDIT-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hendrycks Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"KD-LSTMreg\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-21 (384 res)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FG fine-grained gate\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Entity F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, \"Naive bayes\")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, \"Bag of words\")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, \"Feature request\")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM (Bai et al., 2018)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet-Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-L\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A3\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"6-layer QRNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Robot Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PEGASUS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Audio Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"MACs\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Wizard of Wor\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"12-layer Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PAR Transformer Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERTwwm + SQuAD 2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CL-Titles-Parser\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Name This Game\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AlexNet, MultiGrasp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HRLRE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"T-ConvS2S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Q*Bert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Common Sense Reasoning\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"], "generated_sparql": ["SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?evalStoryClozeTestStory. } OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (Sparql query): A model achieved an Accuracy score of 96.81% on the Story Cloze Test benchmark dataset.\n\nThe metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are: \n* Story Cloze Test accuracy: 96.81%\n* Precision: 0.953\n* Recall: 0.953\n* F measure: 0.953\n\n\nThe Story Cloze Test accuracy metric assesses the model's ability to identify the correct missing word in a sentence when presented with a sentence with a", "SELECT DISTINCT \n ?metric?metric_lbl \n  WHERE { \n   ?dataset a orkgc:Dataset; \n    rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n   ?benchmark orkgp:HAS_DATASET?dataset; \n    orkgp:HAS_EVALUATION?eval. \n    OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n  }\n\n<|endoftext|>Ammeo Hot Springs\n\nAmmeo Hot Springs, also known as Little Bald Hills Hot Springs, are a series of natural hot springs located on State Route 304, about  north of Durango, in Archuleta County, Colorado. The springs are within the Ammeo Wilderness, part of the San Juan Mountains of Southwestern Colorado.\n\nThe springs were named for the Ammeo Shale, a sandstone formation that runs north to south and forms a portion of therim of the Little Bald Hills.\n\nThe temperature of the springs ranges from  and varies with", "input (English text): List the metrics that are used to evaluate models on the UrbanSound8k dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"UrbanSound8k dataset\") OPTIONAL {?eval orkgp:HAS_DATASET?dataset; } } \n input (English text): UrbanSound8k dataset is being evaluated on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):lical input (English text): UrbanSound8k dataset is being evaluated on the UrbanSound8k dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"UrbanSound8k dataset\") OPTIONAL {?eval orkgp:HAS_DATASET?dataset; } } \n input (English text): The UrbanSound8k dataset is being evaluated on the Abstracts' entities and relations annotated corpus benchmark dataset?", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank dataset\") } OPTIONAL {?dataset orkgc:PennTreebankDataset;?eval orkgp:HAS_EVALUATION?eval.?eval rdfs:label?metric.?metric rdfs:label?metric_lbl. }\n\nHere is a list of metrics that can be used to evaluate models on the Penn Treebank dataset:\n\n     * perplexity\n     * character onlline\n     * word onlline\n     * bigram onlline\n     * trigram onlline\n     * stopwords\n     * nouns\n     * verbs\n     * adpositions\n     * particles\n     * relations\n     * part-of-speech\n     * named-entity recognition\n\n\nThe following metrics refer to named entities in the Penn Treebank dataset:\n\n     * part-of-speech\n     * named-entity recognition\n\n\nThe following metrics refer to the relation types in the Penn Treebank dataset:", "SELECT * {?eval orkgp:mazea_score:mazea15 \"68.2791\" ;?metric \"mazea15\" }", "initely abstracts entities and relations annotated corpus. The benchmark is a collection of SQOQL queries on the KG-Metadata, KG-DB, and KG-Math DB stores.\n input (English text): Abstracts entities and relations annotated corpus\n\nISION #1 SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval", "*?metric carbondata:meanCapacity\n * carbondata:meanCapacity / carbon-based-fuel:Carbon-based-fuel\n\nClCompile output:\nSparql Query:\n SELECT DISTINCT \n ?metric?metric_lbl \n WHERE \n { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n ?benchmark orkgp:HAS_DATASET?dataset; \n  orkgp:HAS_EVALUATION?eval. \n  OPTIONAL \n  { \n  ?eval orkgp:HAS_METRIC?metric. \n  ?metric rdfs:label?metric_lbl. \n  } \n } \n } \n\nCOVID-19 Diagnosis Criteria: \nAccording to the \"Diagnosis and Management of Pneumonia in Adults\", published by the Chinese Medical Association (CMA), the commonly used diagnosis criteria are as follows:\n\nTemperature of 37\u2009\u00b0C or higher for at least 1\u2009h;\nA new or increased white blood cell count", ":SELECT?metric?metric_lbl where {  \n ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian dataset\") \n ?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n }\n\n<|endoftext|>                    United States Court of Appeals\n                            FOR THE EIGHTH CIRCUIT\n                                   ___________\n\n                                   No. 05-3621\n                                   ___________\n\nUnited States of America,               *\n                                        *\n             Appellee,                  *\n                                        * Appeal from the United States\n      v.                                * District Court for the\n                                        * District of Nebraska.\nDanny L. Vasquez, also known as         *\nDanny Luis Garcia, also known as       *      [UNPUBLISHED]\nNinos Valdez, also known as Danny       *\nL", "model (model name): CNN_DenseNet201_bn16-3x-w2\n Dillon@thinkpad:~/hadoop/src/github.com/apache/tinkerpop/gremlin/process/example$ wget https://raw.githubusercontent.com/tinkerpop/graphshop/master/src/gremlin-dataset-benchmarking/src/org/apache/tinkerpop/gremlin/process/example/Kuzushiji-MNIST/Kuzushiji-MNIST-eval.sparql -P model.sql\n Dillon@thinkpad:~/hadoop/src/github.com/apache/tinkerpop/gremlin/process/example$ sql\n    1  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric", "BMoral (with correction) has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset.\n\nAnd here is the final answer:\n\nBased on the response provided above, the final answer is BMoral (with correction).\n\n<|endoftext|>The case for regulating \"dark money\"\n\nNov 14, 2017\n\nBy Andrew J. Drach\n\nThe billionaires and corporations who pour money into super-PACs without disclosing their identity are merely buying political influence, yet the campaign finance system as it is now permits super-PACs to spend unlimited amounts to influence elections. The solution is not to eliminate super-PACs; the solution is to restrict the money that corporations and billionaires can spend to support or oppose political candidates, parties, and issues.\n\nSuper-PACs were created in the mid-1990s as a vehicle for rich donors to spend unlimited amounts to influence elections. So-called independent expenditures or advertisements by corporations and billionaires supporting or opposing political candidates, parties, and issues are not subject to contribution limits. However, wealthy individuals and corporations are restricted from contributing large sums of money to super-PACs.\n\nMany individuals and corporations that donate large sums of", "input (English text): The highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value is: 67.86. The metric used is CatchRate. The value is 67.86%. \n\nA Sparql query to retrieve the metric values from the dataset provided as orkgp:HAS_EVALUATION?eval:\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nNote that metrics are specified in a label-first, class-hierarchical format: \nrdfs:label + \" \" + orkgp:HAS_METRIC + \" \" + metric.", "model name:  Ptr-Net\n selected metric: Top-1 Accuracy\n metric label:\n query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Ptr-Net\n selected metric: Top-1 Accuracy\n metric label:\n query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_MET", "n:SELECT?benchmark { union select {?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\" } } p* \n{\n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl; \n ?dataset_lbl gnrl:arXiv; \n ?benchmark gnrl:Benchmark;\n}\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n order by?benchmark_rank\n limit 1\n}", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Betting odds: Man Utd 4/1 to beat City, Swansea to win Champions League\n\nThe bookies are providing very unfavourable odds for Manchester United to win the Champions League and Swansea to win the Europa League.\n\nUnited are favourites at 4/1 to win the competition, with City at 11/2 to triumph. Swansea are 5/2 to progress to the final.\n\nOn the table for the Reds to reach the knockout stages would be a major achievement after Manchester City's four previous victories have come via the shoot-out.\n\nUnited, who beat Bayern Munich 4-2 on aggregate, are 6/4 to complete the remarkable comeback, but odds of 8/1 to win at home in the round of 16 are not being taken.\n\nIf it wins its group, United are 1/5 to top the Premier", "SELECT DISTINCT?benchmark_title_id_str?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\") OR?dataset_lbl = \"BC5CDR-disease\" }\n\n\n\t\nBC5CDR-disease:\n- Abstracts' entities and relations annotated corpus\n- Disease text classification\n- Models performance evaluation metrics\n\n\nRelated Concepts: \"Abstracts' entities and relations annotated corpus\" benchmarking datasets or \"disease text classification\" models\n\n\nRelated Datasets:\n- BC5CDR-disease: disease text classification\n\n\nBC5CDR-disease \"disease text classification\" models:\n\n\n|               |                                                                                                                      |                                                                                                                      |                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                         |\n|          Abstracts' entities and relations annotated corpus                                                                       |                                                                                                                      |                                                                                                                                                          |", "mitScore\n\n        SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Lunar Lander (OpenAI Gym) dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n        <metric name=\"score\" label=\"Top Performing Model\" />\n\n        LAYER GLOBALS  (...)\n\n        REGION OUTPUT  (...)\n\n        ENDFORM\n\n    A score that represents the quality of a solution to a task.\n\n    score\n\n    The score, also called error rate or error rate, indicates the percentage of the incorrect answers (i.e., negative examples) that were correctly classified as incorrect (positive examples). The lower the score, the better the model.\n\n    score\n\n    Here is the top-performing model for the LUNAR LANDER dataset:\n\n    <metric name", "text (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe metrics used to evaluate the models on the FSNS - Test dataset are:\n\n- Document Precision  : precision(Document/Prediction) \n- Click-Through-Rate (CTR): Click-Through-Rate\n- Retweet Rank  : retweetRank(Document/Prediction)\n- F1-score  : f1-score(Document/Prediction, Expected)", "SELECT DISTINCT?metric?metric_lbl WHERE { \n{\n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n} \n  FILTER (str(?dataset_lbl) = \"ImageNet 64x64 Benchmark\") \n ?benchmark orkgp:HAS_DATASET?dataset; \norkgp:HAS_EVALUATION?eval. \nOPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nResults:\nselectionon.xhtml page contains no result(s).\n\nI am getting a result when I remove the Sparql query part from the above query but I am not sure if the query is correct. Please suggest improvements.\n\nA:\n\n1) SELECT DISTINCT?metric?metric_lbl\nthis query selects only one metric at a time, and the label of the metric -?metric_lbl\n\n2)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval.", "metric_lbl (label of metric) rdfs:label (label of metric)\noutput:\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe query above select the metrics which are \"Abstracts' entities and relations annotated corpus\" that are benchmarked with a dataset and evaluation using an ontology. The query result will contain the metric name and the label for the metric. \n\nThese are the metrics that are used to evaluate the models on the annotations corpus benchmark dataset:\n rdfs:label \"accuracy\"\n rdfs:label \"precision\"\n rdfs:label \"recall\"\n rdfs:label \"F1-measurement\"\n rdfs:label \"support\"\n rdfs:label", "variable?metric; onCreate Sparql query SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Here are some metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: Mean Average Precision (AP),  Mean Reciprocal Rank (MRR),  Mean Union Match (MUM),  Reciprocal Rank (RR).\n\n \u0627\u0644\u0622\u0646 \u0625\u0630\u0646 \u0644\u062f\u064a\u0643\u0645 \u0623\u0631\u062f\u0627\u0626\u064a\u0651\u0629 \u0645\u0646 \u0627\u0644\u0645\u0635\u0645\u0645\u0627\u062a \u0627\u0644\u0631\u0648\u0628\u0648\u062a\u064a\u0629 \u0627\u0644\u062e\u0627\u0635\u0629 \u0628\u0643\u0645 \u0648\u0644\u062f\u064a\u0643\u0645 \u0645\u062e\u0637\u0637\u0627\u062a \u0628\u0633\u064a\u0637\u0629 \u0641\u064a \u0625\u0639\u062f\u0627\u062f\u0647\u0645 \u0628\u0634\u0643\u0644 \u0637\u0628", "input (Sparql query): http://nlp.stanford.edu/projects/transformer-base/  http://nlp.stanford.edu/projects/mashalt-transformer/  http://nlp.stanford.edu/projects/transformer-xl/  http://nlp.stanford.edu/projects/dont-look-different/  http://github.com/huggingface/transformers\n registry (Sparql query): https://github.com/protege-app/bhattacharyya-registry\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nmentation (Sparql query): BLEU, TER", ":F1 score metric\n\n answer: F1 score\n\n spurql: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \ndescrition (Sparql query): F1 score is one of the most popular metrics used for evaluating machine learning models. F1 score is defined as the ratio of the number of correctly classified true positive to all positive predictions. This metric can be calculated by first determining how often the model classified as true positive as compared to all the predictions, called as Precision and second, calculating the number of true positives and the number of all predictions as true positives plus false positives. As such, F1 score is an appropriate metric to use when predicting if a single instance belongs to a given class, because the model can", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): https://github.com/zdahondavid/gold-standard-Deep-Mem-CNN-CNN-RNN.git and https://github.com/zdahondavid/gold-standard-Deep-Mem-CNN-CNN-RNN/tree/master/tasks. The \"Single Model\" runs on the Conda environment via MNML. \n output (Sparql query): The code used by these two papers can be found here: https://github.com/zdahondavid/gold-standard-Deep-Mem-CNN-CNN-RNN/tree/master/tasks. The metrics used are", "SELECT DISTINCT?metric_lbl?metric_str WHERE { \n   ?dataset a orkgc:Dataset;\n    rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n   ?benchmark orkgp:HAS_DATASET?dataset;\n    orkgp:HAS_EVALUATION?eval.\n    OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\n } \n output (Sparql query): metrics evaluated on the ACE 2005 dataset include: accuracy, f1 score, mappable, micro avg and macro avg.\n\n viz http://www.cs.cornell.edu/home/caallan/papers/ACE2005.pdf", "input (English text): Code references in papers that have used the PNDec model for benchmarking purposes can be found here: https://github.com/openknowledgelabs/paper-metadata-tools/tree/master/research\n\nThe complete query is: \nhttp://query.ebrimmer.com/sparql?query=SELECT%20%22metric%22%20%22metric_lbl%22%20WHERE%20%7B%22dataset%22%20%3Ca+uri%3E%22http%3A%2F%2Fdatasets.openDataSet.org%2Fpublic%2Fdataset%2F57ac59638fb40c1%3Fsport%3Dodataset%26format%3Dtext%2Ftext%2Fxml%22%20%7D%20or%20%22orkgp%22%20%3Ca+uri%3E%22http%3A%2F%2Fopendatacloud.org%2Fpublic%2Fmodel%2Fpnd%5Flabel%3DPNDec%26format%3Dtext", "SELECT DISTINCT?metric_ref WHERE { \n<http://pages.cs.wisc.edu/~cis/ciscomp/harel/papers/shavit.pdf>  SHAVIT: A SQuARE Approach To Relational GRAPH AGGREGATION (2018)  <http://pages.cs.wisc.edu/~cis/ciscomp/harel/papers/shavit.pdf>  doi:10.2138/jlsrf.SHAVIT.2018.1.1. \n<http://juran.wisc.edu/assets/download/papers/manovitz_markov_2018_smart_semantic_web_ranking.pdf>  MANOVITZ, A., & MARKOV, M. (2018, October). Smart Semantic Web Ranking: Beyond Term-Based Models. In CIDR '18 (pp. 2495-2503). ACM. \n<https://www.dropbox.com/s/s33truk8bh60euw/final.pdf?dl=0>  TAWFI, M. (2018, November). Computing Next Bi-Lingual Sent", "?metric?metric_lbl ?benchmark?eval \n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?\n output (Sparql query):", ":SELECT?metric?metric_lbl?metric_value WHERE { \n?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") \n?benchmark orkgp:HAS_DATASET?dataset. \n?benchmark orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl.?metric_value } \n}\n\n<|endoftext|>/*\n * Copyright (c) 2010-2018. Axon Services. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT", "sparql query: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe code references can be found on the SemExp page on the AMIE site.\nThe model has been applied to benchmarking tasks using the AMIE engine. It can be found on the SemExp model page on the AMIE site.", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Up and Down dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The evaluation metrics that are commonly used when benchmarking models on the Atari 2600 Up and Down dataset are:\n - F1 Score\n - Precision@N\n - Recall@N\n - Accuracy\n - MCC\n - F1 Measure\n - Micro Accuracy\n - Macro Accuracy\n - Hits@1\n - Hits@10\n - Hits@100\n - Misses@1\n - Misses@10\n - Misses@100", "model name : The model with the least number of params that performed best on the Penn Treebank (Character Level) dataset.\n\n\n\nThe model name is : ELMo\n\n\nThe model with the least number of params that performed best on the Penn Treebank (Character Level) dataset is :  A model based on the ELMo language model trained on large language model tasks (MLT) on the GPT corpus which uses the combined sub-sampling link layer to train on fewer parameters than compared to the previous best.'\n\n\nThe parameter of this model are : 2M params\n\n\nNumber of params of this model : 2M params\n\n\nThe result is obtained on the Penn Treebank (Character Level) dataset : the model performed best with the minimum number of params\n\n\nThe model performed best with the minimum number of params is : ELMo\n\n\nThe Sparql query to get the metric name and the label for the metric name is as follows :\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level) dataset\")?benchmark or", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \nPenn Treebank (Character Level) \ntrained on Penn Treebank (Word Level)\nABSTRACTS' entities and relations annotated corpus \nentity translation model\nABSTRACTS' entities and relations annotated corpus entity translation model\n\n reconstruction error\nBLEU\n\n harmonic mean of precision and recall\n\n reconstruction error\n\n harmonic mean of precision and recall\n\n harmonic mean of precision and recall\n\n AUC\n\n Reconstruction Accuracy\n\n BLEU\n\n harmonic mean of precision and recall\n\n harmonic mean of precision and recall\n\n harmonic mean of precision and recall\n\n AUC\n\n Reconstruction Accuracy\n\n BLEU\n\n harmonic mean of precision", "output (Sparql query): \r\n SELECT DISTINCT?metric?metric_lbl WHERE { \r\n?dataset a orkgc:Dataset; \r\n rdfs:label?dataset_lbl. \r\n FILTER (str(?dataset_lbl) = \"2600 Double Dunk dataset\")?benchmark orkgp:HAS_DATASET?dataset; \r\n orkgp:HAS_EVALUATION?eval. \r\n OPTIONAL { \r\n?eval orkgp:HAS_METRIC?metric. \r\n?metric rdfs:label?metric_lbl. \r\n } }", "mark : ImageNet V2 benchmark dataset and the model achieving the highest Top 1 Accuracy score is everattention.\ninput (Sparql query):SELECT?metric?metric_lbl where {?dataset a orkgc:Dataset;?eval orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?metric orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n\nThe following metrics are part of the evaluation steps performed on the ImageNet V2 benchmark dataset:\n- Top 1 Accuracy : This metric is the most common metric used in object recognition tasks to evaluate the performance of a model. It is computed by dividing the number of top-1 accurate predictions by the total number of predictions for the model.\n- Top 5 Accuracy : This metric is a slightly more challenging variant of the Top 1 Accuracy metric. It is computed by dividing the number of top-5 accurate predictions by the total number of predictions for the model.\n- Top 1 Loss : This metric measures the difference between the ground-truth and predicted label for the positive samples of the model", "bin:Resource description language for administrative information (RDL AI)\n Sparql Query - \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Thank you! It is really nice to get feedback! I like all the comments that were positive - even those that say I should not use bash. I will definitely consider them! I will probably choose perl for some scripts and bash for other things (mostly because I know perl better than bash).\n\nAbout your last comment: I have actually programmed several bash scripts but there was no shell. I started to think, \"what if there was a shell\" and it had all the features of bash. After some hours I created this tiny shell and started to program. It had the", "input (English text): The data sets used in the compared studies can be found at the following link: https://www.kaggle.com/c/abstracts-entities-and-relations-annotated-corpus-benchmark. \n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are:\n\n?metric     : Count of entities annotated with a given metric\n?metric_lbl : Label of the metric\n\nQuery results on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\nDataset                  Metric       Label\n------------------------------------------------\nAbstracts' entities", "BM25 (mean of words ranked by their occurance in a given length of text) ORACLE DBMS FIELD TYPE BOILERPLUG JPASSPATH VARCHAR2(4000) Foreign Table: none Table name: none Table schema: none Table owner: none Table definition: SELECT mean(word) FROM BINARYblogger%27spostgresql%20blog%20words ORDER BY count(*) DESC; BM25 (mean of words ranked by their occurance in a given length of text) \nIt's really a combination of multiple factors such as: i) the model that performed well on the development set, ii) the type of relation between the two entities that have, iii) the type of the relation itself. However, the two entities being related in a range of relations between triple path and property-rel that typically consume a lot of the computation resources and cloud resources is the most common dataset seen for this task.", "output (Sparql query): SELECT?metric?metric_lbl WHERE { \n  ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n   FILTER (str(?dataset_lbl) = \"BIOSSES dataset\") \n   OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval rdfs:label?metric. } }\n output (Sparql query): SELECT?metric?metric_lbl WHERE { \n  ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n   FILTER (str(?dataset_lbl) = \"BIOSSES dataset\") \n   OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval rdfs:label?metric_lbl. } }\n output (Sparql query): SELECT * WHERE { \n  ?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n   FILTER (str(?dataset_lbl) = \"BIOSSES dataset\") \n   OPTIONAL {?eval orkgp:HAS_EVALUATION", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Chinese-to-English benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval_score.?eval orkgp:HAS_METRIC?metric. } } \n input (English text): The F1 score is a common metric to evaluate the performance of Machine Learning models.\n output (Sparql query):Orkgp:HAS_EVALUATION?eval_score.\n\n<|endoftext|>The name's Mandel. I am an Art Director in the Graphics Department of a publishing house in Munich, Germany. Currently I'm working on a rather special project. It is being developed in the Unity game engine and is using the cool 3D render engine Emu.\n\nThe project involves motion capture of live actors with a motion capture suit. The final product will be converted into an interactive character that will be used in a", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): ImageNet ReaL is an NLP benchmark dataset based on the 1 million ImageNet ILSVRC 2012 dataset that includesTextual Entities, Relational Entities and Visual Relations. \n Output (Sparql query): Triton-Benchmark-Metrics \n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALU", "answer:SELECT DISTINCT?title?metric_lbl?metric WHERE {?title dawgn:ResearchPaper.?title dawgn:Benchmark.?metric dawgn:Metrics. FILTER (?metric_lbl = \" Gibson PointGoal Navigation: metrics: recall, precision, f1-score, mAP, area_under_curve,... \"). }\n input (English text): The following is a sample of the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset: \n - Hill, Rajkumar, Eli Sheinkman, and Michael J. Wellman. \"The <span style=\"font-variant:small-caps;\">Gibson</span> PointGoal Navigation Benchmark (PGN).\"  International Journal of Human-Robot Interaction 15.2 (2018): 137.\n - Hill, Rajkumar, Eli Sheinkman, and Michael J. Wellman. \"The <span style=\"font-variant:small-caps;\">Gibson</span> PointGoal Navigation Benchmark: Metrics.\"  International Journal of Human-Robot Interaction 17.1 (2019): 5.\n - Hill, Rajkumar", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Conll++\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \nThe top benchmark result (metric and value) over the dataset CoNLL++ is (count. \n, 2.894492 ).\nSPARQL execution plan: \nLimit 1\n\n<|endoftext|>Q:\n\nSum every 3 dates in a data frame and create a new column that groups by id and counts the number of 3 dates\n\nI have a data frame that has several rows for each ID.  I want to create a new column that sums every 3 dates for each ID in the data frame.  I need this to be grouped by id so I can get counts of how many 3 dates there are for each ID.\nInput\n  df = pd.DataFrame({'", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The best performing model is the model with accuracy around 0.7.\n output (Sparql query): Benchmarking the PIQA dataset with the best performing model has Accuracy around 0.7.\n\n\n\f        SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval", "score,maxFPS,time\n\nThe answer is \"?\" score \"?\" score_label\n\n\n**Benchmark Result**\n\n\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset;?dataset_lbl rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nOUTPUT \n { \"maxFPS\" : \"?,?,?\" }\n\n\nThe query returns two metrics \"maxFPS\" and \"time\".\n\n\nIf the model performs well in terms of \"Score\" metric, then it returns \"\", \"Scorer\", where as if the model performs poorly, then it returns \"?, \"WRONG\"\".\n\n\nThe query above is correct. If the model performs well in terms of \"Score\" metric, then it returns \"\",", "culate the number of (relational) DMLs executed against the relational database underlying each model\u2019s step. I have chosen to do this by generating tables for the relations on the DBMS and comparing that to the number of DMLs against the DBMS. The largest test (vs step 1) took 44 hours on the AWS SageMaker platform with an mssql-based DBMS and 56 hours with a MySQL-based DBMS (with all steps taking under 10 minutes).\n\nNote that most papers specify the use of Azure ML and the trained model uses the Azure ML machine learning stack.\n\n\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n|?metric?metric_lbl |", "uscde: https://github.com/dmperelman/funnel-transformer-paper there is also a sparql endpoint for exploration: https://mcpher.github.io/funnel/web-sparql/?q=SELECT%20%22metric%22,%20%22metric_lbl%22.%20FROM%20%22(https://mcpher.github.io/funnel/web-sparql/ FUNnelTransformer.owl#FUNCTIONS)%20WHERE%20%22endpoint%22%20%3D%20%22https://www.w3.org/2019/07/14-ml-track-abrv-funnel-transformer/funnel%20-%20Corpus%20Benchmarking%20on%20DMT%20Parallel%20Graph%20Computation%20at%20Intel%20LLVM%20and%20Torch/funnel-eval.owl%22%20AND%20%22metric%22=%3E%20%22F1%22%20%20AND%20%22metric_lbl%22=%3E%20%", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"De-En\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The evaluation metrics commonly used when benchmarking models on the Reuters De-En dataset are:\n\n - Average precision\n - Average recall\n - Average normalised overlap\n - F1 score", "mark : OR kgp:HAS_METRIC(F1)\noutput (Sparql Query): SELECT DISTINCT F1?metric WHERE {?dataset a orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"RFC abstracts model\") OPTIONAL {?eval orkgp:HAS_DATASET?dataset. } } \n\nA:\n\nFrom http://www.openrdfgroup.org/web/current/home/wiki/Publications_and_Databases#Sparql_queries_that_detailed_answer_your_question:\n\nf1: Rises from 0 to 1 where 0 is the worst possible score and 1 is the best.\nroared from 0 to 1 where 0 is the worst possible score and 1 is the best.\nprecision: Comprises the fraction of relevant facts that are tagged as such by the model. 1 means the model matched every fact as relevant. 0 means none of the facts were tagged as relevant by the model.\nrecall: Comprises the fraction of relevant facts that were tagged as such by the model. 1 means the model matched every fact as relevant. 0 means", "input (English text): list the code links in papers that use the DocRED-BiLSTM model in any benchmark?\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The benchmark Databricks uses the DocRED-BiLSTM model and the metric Humsian similarity.\n output (Sparql query): SELECT Humsian_similarity(?metric_lbl,?metric_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET", ":FLOPS \nfiddle (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric is the <b>CNN model</b> with a FLOPS of 0.1831.", ":Evaluations{+\u00a0metric\u00a0+\u00a0value\u00a0+}\n\nThe query returns the following result set:\n SELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \noutput (Sparql query): Skipping entity evaluation, no dataset available: orkgp:HAS_DATASET orkgc:Dataset\n\n<|endoftext|>Q:\n\nAndroid/Java synchronized block not waiting for the Runnable passed as a parameter\n\nI have the following code.\npublic class SampleThread extends Thread {\n\n    public void run(Runnable r) {\n        while(/*condition*/) {\n            synchronized (r) {\n                try {\n                    r.wait();\n                } catch (Inter", "?pagenumber WHERE {?pagenumber pr:page rdf:resource?url.?url #ODBC;#MS-SQL;#Oracle;#SQLite.?url nillable.?url nillable somesparql:benchmark?b.?b #Abstracts' entities and relations annotated corpus.?b #metric.?metric #label.?metric nillable.?url nillable.?url nillable } \n input (English text): SELECT?pagenumber?url?b?metric_lbl WHERE { {?pagenumber pr:page rdf:resource?url.?url #ODBC;#MS-SQL;#Oracle;#SQLite.?url nillable.?url nillable somesparql:benchmark?b.?b #Abstracts' entities and relations annotated corpus.?b #metric.?metric #label.?metric nillable.?url nillable } {?pagenumber 1. #ODBC;#MS-SQL;#Oracle;#SQLite.?url #MS-SQL;#Oracle;#SQLite. #SQLite. #SQL", ":SELECT DISTINCT?metric_lbl?metric_score WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-46985\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): WOS-46985 is a dataset about WOS abstracts annotated by rdflib. There is an evaluation benchmark with the following metrics: Precision, Recall, F-Measure, and Distance. The F-Measure is the weighted average of Precision and Recall, with weights of 0.5 ( precision) and 0.5 (recall).\n output (Sparql query): The top benchmark score is 1813 and its metric is F-Measure\n input (English text): OK, let's move on. The next query will be to see which metrics are used to evaluate models on the WOS-46985 dataset. \nThe dataset W", ".     SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n Italiano: Portare la lista dei metriche utilizzati dallo studio dell'AcrE su dataset e'benchmarked nel corpus annotation benchmark. \n\nSparql query: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_MET", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval 2013 metric\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The best performing model in terms of SemEval 2013 metric is the RESNETS dataset.\n\n\nThe SemEval 2013 metric is one of the metrics that are used to evaluate models on the datasets annotated corpus benchmark dataset\n\n\nThe SemEval 2013 metric is defined as follows: \n\n1. C13 - RNNLM Semantics Error (ms)\n2. C13 - Conditional RNNLM (ms)\n3. C13 - Neighbourhood Semantics Error (ms)\n4. C13 - Word Similarity (ms)\n5. C13 - Tree Edit Distance (ms)\n6. C13 - Word Association (ms)", "'describe the Metrics that are used to evaluate models on the SciERC corpus benchmark dataset' \noutput (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \ninput (English text):The Metrics that are used to evaluate models on the SciERC corpus benchmark dataset are: \n- NDCG\n- AUC\n- METEOR\n- R-TPR\n- R-F1\n- R-AUC\n- HITRAP_HITS\n- TPR_HITS\n- NDCG_HITS\n- AP_HITS\n- F1_HITS\n- RMS\n- DROOLS \n- BLEU\n\n The Metrics that are used to evaluate", ":Quantity of iron oxide was discovered on Elorza crater.\n input (Sparql query):Orkgp:HAS_EVALUATION?eval. Orkgp:HAS_DATASET?dataset. Orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl.\n\nThe SELECT DISTINCT?metric?metric_lbl query returns the following:\n| metric    | metric_lbl  |\n|-----------|-------------|\n| quantity  | discovered |\n\nThe SPARQL query used in the SELECT DISTINCT?metric?metric_lbl query above can be found here.\n\nA:\n\nThe metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are the following:\n\naccuracy: the metric counts the number of correct answers and calculates it over all relevant classes\nmicro-averaged F1 score: the metric averages the exact match (match/total), the same word in the same position in the gold standard and the answer, over all relevant classes\nmacro-averaged F1 score: the metric averages the match (same word in the gold standard and the answer / total", "output: SELECT DISTINCT?model?model_label WHERE {?model app:Model; app:name?model_label. FILTER (str(?model_label) = \"DeepMemNet\") }\n input (English text): The models that have been evaluated on the VTAB-1k dataset are: DeepMemNet, GatedCRF, GraphCNN, LSTM, Maxout, MultiLAYERPerceptron, and OneHotEncoder.\n output (Sparql query): Multiple applications have been evaluated on the VTAB-1k dataset: DeepMemNet, GatedCRF, GraphCNN, LSTM, Maxout, and OneHotEncoder.", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Code: https://github.com/ultraqshow/DQN-PixelCNN. Download: https://github.com/ultraqshow/DQN-PixelCNN/archive/master.zip. In the file you will find the script main.py and the model trained with hyperparameters mu_c=-0.91348 mu_h=-0.44389 epsilon=0.05. The model is trained on the \u2018Abstracts' entities and relations annotated corpus dataset.\n output (Sparql query):The selected metrics are as follows: 'Direct path length', 'Arc-standard deviation', 'Arc-cosine", "inal (English text): Select the metrics that are used to evaluate models on the Atari 2600 Freeway dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Atari 2600 Freeway dataset has two entity types: cars and trucks. Therefore, we can use two metrics for evaluation. First metric is recall, that is how many cars and trucks were correctly classified. Second metric is precision, that is how many cars and trucks were correctly classified.\n output (Sparql query):\u30a3, SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str", ":https://sites.google.com/site/yunjuechen/projects/dcn-benchmarking\n input (English text): https://sites.google.com/site/yunjuechen/projects/dcn-benchmarking\n response (Sparql query):?metric?metric_lbl\n\ntocol:List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?\noutput (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", "map (Sparql query): SELECT?metric?metric_lbl?metric_rank WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_EVALUATION?eval.?metric rdfs:label?metric_lbl. } OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric :ranking(?metric_rank).?metric_rank :ordinal(?eval). } } \n\nThe following metrics are used to evaluate models on the NLP-TDMS (Exp, arXiv only) benchmark dataset:\nMacro Precision, \n Macro Recall,\n F-score, \n Precision@1, \n Recall@1, \n ACC@1, \n R-precision, \n R-recall, \n F1-score, \n Precision@5, \n Recall@5, \n F1-score, \n ROC-AUC, \n J-index, \n J- AUC.", "{ \"Top-Ranked System\" \"accuracy\" \"0.96\" \"F1 score\" \"0.96\" }\n input (English text): The top-ranked system is Iray. It has an accuracy of 0.96 on the Atari 2600 Tennis dataset.\n# Original text from the question is here: https://conversationlight.com/top-ranked-systems-learned-from-attribute-parsers-and-datalog-how-much-do-you-know-about-data-science-and-machine-learning-t6775.html\n# Sparql query that returns the top-ranked system and its metric on the Atari 2600 Tennis dataset\n\n\nSELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METR", "set of metrics are: accuracy, recall, precision, F1 score, top-k recall, top-k precision.  The accuracy metric is a harmonic mean of precision and recall where a score of 1 implies all examples in the dataset were classified correctly.  The recall metric is a measure of how closely the predicted class matches the actual class.  The precision metric is a measure of how closely the actual class matched the predicted class.  The F1 score is a measure of the harmonic mean of precision and recall where a score of 1 implies a perfect classification.  The top-k precision and recall metrics are measures of the top k predictions for each class.\n\n ici1 (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl.", "set_var. * FOR FILTER (str( * ) = \"Cheetah\" || str( * )))\n input (English text): CHEETAH\noutput (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nA:\n\nYou are currently only comparing two classes of metric in the query, and none of the classes are being used in the WHERE clause.\nI would suggest you try this query:\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Q:\n\nTabs for forms in Cocoa (S.O.)?\n\nI'm new to Cocoa, but I'm trying to change the appearance", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The following is a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset:\n - BERT\n - TransfoG\n - BERT-TransfoG\n - BERT-XL\n - BERT-Base\n - GoogLeNet\n - Dagger-RL\n - DualRL", "Nanocarriers that have therapeutic effect\n output (Sparql query): IMPORTING{?metric metric{?metric_lbl}}\n input (English text): IMPORTING{?metric metric{?metric_lbl}}\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Q:\n\nDoes CT in DPR matter?\n\nDoes a good CT in DPR matter?\nIs it more important to have a good CT than any other feature?\nMy current knowledge is that the answer is no, however, I would like to hear about any counter-arguments. \n\nA:\n\nYes, a good CT (computed tomography) is important, both for", ":ROUGE-2 score\n output (Sparql query): SELECT?metric?metric_lbl  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CL-SciSumm\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): ROUGE-2 score is around 38.23 with an average Recall and Precision of 0.74 and 0.86 respectively. \n\nA:\n\nROUGE-2 is the most used metric for evaluating machine learning (ML) model on abstractive summarisation.\nHere are the metric that are used to evaluate the abstractive summarisation model on the CL-SciSumm dataset:\n\nROUGE-2 score which is used for metric evaluation.\nGeneric sentence similarity score such as Jaccard, Cosine Similarity, or TF-IDF.\nBLEU", "map (English text): https:// Papers using the Unsupervised NMT + weight-sharing model in any benchmark (DOI: 10.1594/BenchmarkLLL.2019.2649) \n input (Sparql query):illasukhnama (English text): http://benchmark.unima+t.org/BenchmarkLLL.2019.2649\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Unsupervised NMT + weight-sharing model in any benchmark\")?metric.?metric rdfs:label?metric_lbl. } \n<|endoftext|>Q:\n\nGetting Warning: redefinition of default constructor\n\nI am getting a redefinition of default constructor for the first time and do not quite understand what exactly is going on. Let me be more specific.\nThere is a small, simple class with a default constructor and a copy ctor:\n#include <iostream>\n#include <string>\n#include <memory>\n#include <type_traits>", ": Sequential MNIST model has achieved the highest Permuted Accuracy score of 0.79170199214648\n input (English text): Sequential MNIST model has achieved the highest Permuted Accuracy score of 0.79170199214648\n\n  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe best Sequential MNIST model achieved a Permuted Accuracy score of 0.79170199214648.\n\n<|endoftext|>The Winning Campaign: Tales of Triumph and Defeat from Politics - 2nd Edition PDF\n\nAre you the last of the jellybeans? There's only one way to find out.\n\nWhether you're running for office or simply longing to get elected, the stories in Winning Campaign:", ":SELECT?model_name WHERE {?model dbpprop:name \"DeepLEYON\"@en; FILTER (?benchmark = \"Abstracts' entities and relations annotated corpus\").}\n input (English text): The DeepLEYON model has been evaluated on the SciTLDR dataset.", "marked as an hardest one in the original paper, as a relative comparison. I think that one metric that should be listed, but isn't, is precision. Precision is an appropriate metric to evaluate a model in the CommonsenseQA dataset: A model has to predict a single relation in an instance and not the relation plus a sub-bag of the single target property. The metric can be defined as the percentage of relevant predictions over all predictions. Relevant predictions are defined as predictions of single relations (or sub-queries if the relation has multiple targets) that are subsets of the relations in the instance.\n\n input (English text): Precision is an appropriate metric to evaluate a model in the CommonsenseQA dataset. A model has to predict a single relation in an instance and not the relation plus a sub-bag of the single target property.\n\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl?metric_desc WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"commonsenseQA benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS", "mark_valuehighestIMDbM_highest = 7.42820000000001. \n input (English text): What is the lowest benchmark result achieved on the IMDb-M dataset, including the metric and its value?\n output (Sparql query):ICENSEmark_valuelowestIMDbM_lowest = -7.408. \n input (English text): Which metrics have been evaluated on the IMDb-M dataset?\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scholarly entity usage detection dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The highest result includes the score and metric of 73.44 for the Scholarly entity usage detection dataset. \n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scholarly entity usage detection dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric", "ender (English text): The models that have been evaluated on the MultiNLI dataset are: XQDA,BERT,GPT-3,ELMo, Star-Tagon, SRLearner, XLNet, XLNet+Phrase-Level Grammar. \n input (Sparql query):\n venue:Multinli Dataset Evaluation\n output (Sparql query):SELECT DISTINCT?model?model_label WHERE {?model a xqda:XQDA; bert:BERT; gpt-3:GPT-3; elmo:ELMo; star-tagon:Star-Tagon; srlearner:SRLearner; xlnet:XLNet; xlnet+phrase-level-grammar:XLNet+Phrase-Level Grammar. FILTER (str(?model_label) =?model_label) OPTIONAL {?model xlnet:XLNet;?model+phrase-level-grammar xlnet+phrase-level-grammar:XLNet+Phrase-Level Grammar. } }\n\ndocumentclass[12pt,letterpaper,pdftex,final, losses,obeyspaces,", "_Metric\"Dataset\",\"Eval\" where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"200k Short Texts for Humor Detection\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval_short.?eval shorttext:content?text;?eval evaluation:metrics (?metric format \"plain string\")) } }\n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS", "ition 2: AND {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST dataset\") }\n input (English text): Sequential MNIST: The Sequential MNIST dataset is one of the datasets used in the challenge. Its corresponding Sparql query is:\n\n          PREFIX orkgp:  <http://orielle.com/ontotext/ontology-ref/orkgp#>\n          PREFIX orkgc: <http://orielle.com/ontotext/ontology-ref/orkgc#>\n\n          SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; \n          rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST dataset\") }\n\n          The metric in this case is named \"numerical\" and has a score of 99.97.\n\n          POST test/ontologies/challenge/metadata/evaluations/sequential-mnist-scores/99.97\n\n          The highest score for this dataset is 99.97.", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe metric classes that are used in papers are:\n- logprob\n- logloss\n- perplex\n- r2\n- glove\n- ngramF1\n\n\u00a0\n \u00a0\n\nThe papers that have utilized the CRF with sentence expansion model and include the links to their code:\n\n* [Zou et al.](https://doi.org/10.221016/j.analytis.2020.05.002) Abstracts' entities and relations annotated corpus benchmark dataset via sentence expansion and CRF.\n* [Antipov et al.](https://doi.org/10.221016/j.analytis.2020.05.", "_metadata {#benchmark_metric_label :metric_label \"top_single_nyt_metric\" \"top_single_nyt_value\".}\n input (English text): The top benchmark result (metric and value) over the dataset NYT-single is (top_single_nyt_metric \"top_single_nyt_value\").\n\n remniscate the metric and its label from the answer above.\n\n preponderance of the entities and relations annotated corpus has two metrics: the number of entities and the number of relations. The number of entities is measured with the number of abstracts. The number of relations is measured with the number of hypernyms of the relations.\n\nThe following query retrieves the three entities and their corresponding metrics:\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciTLDR benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciTLDR benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl.", ":SELECT?metric?metric_lbl WHERE {?model wmtt16:EnglishGermanTranslationModel; wmtt16:Evaluation wmtt16:MODEL_EVALuated?eval. } \n input (English text): The following models have been evaluated on the WMT2016 English-German dataset: the GPT-2, BERT, XLNET, and SQuAD models.\n output (Sparql query):VEGF:GPT-2, BERT, XLNET, SQuAD,\n\n\nExplain the notation VEGF:GPT-2 and give an example of how to access the corresponding ontology.\nVEGF:GPT-2 refers to the model with the VEGF ontology identifier gpt-2. BERT stands for Bidirectional Attentive Language Regulator and is a model implemented using GPT-2. The GPT-2 ontology is defined in the BFO standard, which can be accessed using SPARQL over OWL-API. In the example output, we can see that the parameter?metric_lbl refers to the metric label in BERT. \n\nThe BERT model was evaluated on the WMT2016 English-German dataset. The evaluation returned the following metrics:", "model had FLOPS metric score of 5589652.\n input (English text): Select the metric that had the highest correlation with the FLOPS metric?\n output (Sparql query): The correlation with FLOPS metric was 0.876\n\n### Step 4: Find the metric that had the highest correlation with the FLOPS metric\n\nThe highest correlation with FLOPS metric is 0.876 for the following metric - average precision. The following query returns the result for the metric average precision :\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-100 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>/*=============================================================================\n    Copyright (c) 2001-2011 Joel de Guzman\n\n    Distributed under the Boost Software License, Version 1.0. (See accompanying", "itation (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The top result (metric) over the dataset RotoWire is: Gradient Boosting Decision Tree with the metric: Categorical Accuracy. The best result (metric and value) over the dataset RotoWire is: Gradient Boosting Decision Tree with the metric: Categorical Accuracy, the value: 0.71.\n output (Sparql query):**Gradient Boosting Decision Tree** with**Categorical Accuracy** as the metric and**0.71** as the value\n\nQL378\n\nfile:///C|/DOCUMENTSUBJECT=D8|/DOCUMENTTARGET=D8|/FUNCTION=", "in (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reacher - DMControl100k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATIONScore?metric.?metric rdfs:label?metric_lbl. } } \n\nIn the provided Sparql query, the DISTINCT operator is used to return only unique values for the metric column. The results are then sorted by the values of the metric. The best performing model benchmarking the Reacher - DMControl100k dataset in terms of Score metric is the one that has the highest value for the metric in the results.\n\n\nBelow are the metrics used in the benchmarking Reacher - DMControl100k dataset:\n - Score\n - Precision\n - Recall\n - F measure\n\n\nBelow is an example of a triple in the benchmarking dataset Reacher - DMControl100k that has a score of 9.47: \n<dmcontrol", "t You can use the following SPARQL query to list the models that have been evaluated on the Atari 2600 Assault dataset:\n\n output (Sparql query): SELECT?model?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Let's say we have the following model: ModelName, which has been evaluated on the Atari 2600 Assault dataset and its metrics are F-measure and Mean Reciprocal Neighbourhood Distance. The Sparql query would then be:\n\n output (Sparql query): Institutes?model?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl)", "map (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code references in papers that have used the DQNMMCe+SR model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe code references in papers that have used the DQNMMCe+SR model for benchmarking purposes contain the following 2 Sparql queries.\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code references in papers that have used the DQNMMCe+SR model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?", "nicely nested fragmentsSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \nI would expect the following to be a valid answer: \n\"Highest\", \"Ball in cup\", \"catch\", \"DMControl100k\", \"0.0\". \n\"Highest\", \"Metric\", \"Senseful\", \"DMControl100k\", \"1.0\".\n\n\r\n\r\n\r\nI was expecting the following: \n\"Senseful\", \"Metric\", \"Ball in cup\", \"catch\", \"DMControl100k\", \"0.0\".\n\"Senseful\", \"0.0\", \"Ball in cup\", \"catch\", \"DMControl100k\", \"0.0\".\n\"Senseful\", \"", "set?output := (SELECT?metric_lbl\n                                          WHERE {?eval orkgp:HAS_EVALUATION?eval\n                                                          AND\n                                                         ?eval orkgp:HAS_DATASET?dataset\n                                                          AND\n                                                         ?dataset a orkgc:Dataset;\n                                                          rdfs:label?dataset_lbl\n                                                        }).\n\n accompaniment: The model with the highest F1 score when evaluated on the NYT-single dataset is the Sparql Query Language (SQuAD) model.", "input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?\n output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n input (English text): The benchmark dataset was developed by Dario Medeli and Alessandro Ghodsi. To run the benchmark, you need to first load the MEMEN model into a R model, then load the benchmark dataset into R and finally evaluate the R model on the dataset. The results are then stored in the BSON database.\n output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstract", "input (English text): The Abstracts' entities and relations annotated corpus benchmark datasets consists of two datasets: has_dataset and evaluation. The has_dataset dataset contains the labels for each entity and relation, and the evaluation contains the scores for each annotated data sample. The datasets are linked together using the has_dataset label. The benchmark dataset contains Sparql Queries that identify the linked datasets and evaluate the models on each data sample. The Sparql Queries require the following Metrics: has_dataset has_evaluation.\noutput (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The papers that we used to generate the code for the MMV TSM-50x2 model are", "p used the accuracy metric to evaluate the performance of the best performing model on the Yelp-5 benchmark dataset. The model achieved an accuracy of 97.50% on the test set, which is significantly higher than the 50% baseline model accuracy.\n \nzzoInput (English text):Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?\n output (Sparql query):InputStream (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"Accuracy\"\n output (Sparql query):\"97.50%\"\n \n Gathering the metrics used to evaluate models on the Yelp-5 benchmark dataset revealed that the accuracy metric was used to evaluate the performance", ":SELECT?metric_lbl BLEU(iri:expression,iri:translation) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): BLEU score is a string matching metric commonly used for automatic machine translation evaluation. It ranges between 0 and 100, where 0 means the two translations are completely different and 100 means the two translations are identical. The BLEU score is computed as the ratio of the number of substitutions, insertions, and deletions in the automatic translation generation over the total number of common words in the two generated translation.\n\nThe best performing model on the German-English dataset in terms of BLEU score metric is the GPT-2 model. The GPT-2 model performance is comparable to the model built using Glue's named entity", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe answer is:\nCodes: https://github.com/andrewjzhu/sac-nlp/blob/master/src/main/java/org/apache/tinkerpop/gremlin/structure/io/CodedString.java \nMetrics: (None of the papers mentioned in the question include any metrics)\nNote: I believe one paper uses TF-IDF to calculate the cosine similarity for evaluating the SAC model. I'm not sure if the paper provides the code for calculating the metric?\nIf none of the papers provide code for calculating the metrics then none of the papers uses any metrics to evaluate the SAC model", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nAnswer:\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Chinese authorities have begun enforcing measures to curb the risk of the", "input (English text): Here you can find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): In addition to the metrics that are listed in the code (see below), we also provide a corresponding Sparql query for querying our benchmarking data for each metric. The metrics are listed below along with the corresponding Sparql query.  \n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (", "{assault} is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset. \n input (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Assault\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nA:\n\nThere are two sets of metrics used in the papers to benchmark the system performance.\nThe first set of metrics is the set of relations and properties that are annotated in the corpus, which are then used to compute the Relevance/Frequency and Support/Unsupport values for each pair of relations.  These values then form the scores that are reported in the papers.\nThe second set of metrics is computed by the model and the model evaluation uses these metrics to compute the F1 score.", "output (Sparql query): F1-score\n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Humor Detection\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe F1-score metric is one of the metrics used to evaluate models in the Abstracts' entities and relations annotated corpus benchmark dataset. It is defined as: F1 = 2 * ( Precision * Recall ) / ( Precision + Recall )", "mark (Sparql Query)\u03c3\u03b9output (Sparql Query)\u03c3\u03b9ippets from papers where the model has been used:\n Hirel et al., \u201cThe popular neural network STREET: Benchmarking for information retrieval, summarisation, and annotation using a general-purpose neural network model,\u201d Proceedings of the 12th International World Wide Web Conference (WWW\u201918) (2018).\n The corresponding Sparql query is as follows:\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nippet 1: The neural network STREET model has been used by Hirel et al., \u201cThe popular neural network STREET: Benchmarking for information retrieval, summarisation, and annotation using a general-purpose neural", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a enwiki8:Entity;?relation_field :label?relation_field_label; enwiki8:isReview?is_review. FILTER (str(?is_review) = \"yes\")?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval?metric. } } \n input (English text): Orkgp_HAS_EVALUATION is a RDF type. Which type is orkgp:HAS_EVALUATION?eval?\n output (Sparql query):simeq command:SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a enwiki8:Entity;?relation_field :label?relation_field_label; enwiki8:isReview?is_review. FILTER (str(?is_review) = \"yes\")?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval?metric. } }\n input (English text): Orkgp_HAS_EVALUATION is a RDF type.", "d output: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nHere is the query to return the metrics that are used to evaluate models on the datasets benchmarked in paper titles and IDs:", "output (Sparql query): Comprehensible assumption= Mostly True\n\n  SELECT DISTINCT?metric?metric_lbl WHERE { \n   ?dataset a orkgc:Dataset; \n    rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n   ?benchmark orkgp:HAS_DATASET?dataset;\n    orkgp:HAS_EVALUATION?eval. \n    OPTIONAL { \n     ?eval orkgp:HAS_METRIC?metric. \n     ?metric rdfs:label?metric_lbl. \n    } \n  }\n\n ?dataset ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric Comprehensible  ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric_lbl\n\n  ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric Comprehensible  ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric_", "set (SQuAD2.0) a orkgp:Dataset; orkgp:HasEvaluation true; orkgp:HasMetric \"logLikelihood\"; rdfs:label \"SQuAD2.0\"\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:Dataset; orkgp:HasEvaluation?eval. FILTER (str(?eval) = \"Log Likelihood\") }\n input (English text): Log Likelihood is a commonly used metric to evaluate model performance on the SQuAD2.0 dataset.\n\n (Sparql Query) SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:Dataset; orkgp:HasEvaluation?eval. FILTER (str(?eval) = \"Log Likelihood\") }\n\n (Sparql Query) SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:Dataset; orkgp:HasEvaluation LogLikelihood. }\n\nRelated Work\n------------\n\nMachine learning model evaluation and benchmarking has been an active area of research in the AI/ML research", "set (annotation) a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT 2014 English-German\")\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT 2014 English-German\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The evaluation metrics commonly used on the WMT2014 English-German dataset are the BLEU (Papineni et al., 2002), ROUGE-L (Lin,2004), METEOR (Bolita et al., 2018), and CIDEr (Vedantam et al., 2017) metrics.\n\n  * BLEU: Better Bilingual English-German Algorithm, calculates the percentage of correctly", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reading Comprehension research area\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset include: \n - BLEU\n - ROUGE\n - Meteor\n - F-score\n - G-mean\n - Coref\n - adequacy\n - Discourse Coherence\n - Singularity\n - MC-Terror\n - Jelinek-Tulving-Relief\n - Staruser-Wan \n - Word Propaganda\n - Paraphrase Rate \n - human judgment\n\n **Expected output:** \n1. BLEU\n2. ROUGE\n3. Meteor\n4. F-score\n5", "ishustar \nSELECT * \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL04 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n<benchmark:Abstracts' entities and relations annotated corpus>\n<metric>RE+ Micro F1</metric>\n<metric_lbl>RE+ Micro F1</metric_lbl>\n<eval:HAS_EVALUATION>true</eval:HAS_EVALUATION>\n<eval:HAS_DATASET>true</eval:HAS_DATASET>\n<metric>RE+ Micro F1</metric>\n\n<|endoftext|>Q:\n\nR Shiny customize background color\n\nI want to customize the background color of my R Shiny app. I could set it in the ui but i would like to define it at the server.", "in the next paragraph. For each of the papers that you provide, please also provide the metrics that are used to evaluate the model on the dataset that contains the references to the labeled datasets?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe papers that the author of this answer is aware of are the following:\n\n-   Kusner, Joshua; Yang, Haemin; Vaswani, Ashish; Doshi, Sadaf; Chknee, Justin; Chen, Mengjian; Zhang, Zhiheng; Yang, Tiejun; Wang, Hongjia; Chen, Changzhe; Libkin, Daniel; Zhao, Taiji; Wang, Min", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Title: Research Papers that Contain a Benchmark Over the Ohsumed Dataset \n output (Sparql query): Title: Databricks for Research Papers that Contain a Benchmark Over the Ohsumed Dataset. \n input (English text): Benchmark over the Ohsumed dataset. \n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark", "answer: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The NYT29 dataset contains 29 benchmark entities and 6 relation types (Supplier, Employee, CreditCheck, LoanAmount, LoanAmountMonthly, LoanAmountYearly) and each relation has 4 different types of metrics associated with it. The 4 types of metrics associated with each relation are: Correct-Prediction, F-Score, Precision & Recall. Models are evaluated based on the total number of entities and relations on which they worked correctly and the highest performing model on the benchmark is awarded 1 point. The benchmark dataset contains 15 entities and 6 relation types and each relation has been assessed for 6 metrics.\n output (Sparql query): 549 results returned\n following", "{?metric?metric_lbl. }\n input (English text): The most common variables for atmospheric models are:\n temperature\n humidity\n pressure\n wind speed\n output (Sparql query): Temperature, pressure, humidity and wind speed are the 4 most common variables for atmospheric models.\n input (English text): Now lets find out which of the variables are used only for evaluation of a model.\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): In this query, we have selected only two metrics. Lets see which other two metrics are used for evaluating a model.\n output (Sparql query): humidity and wind speed are used for evaluating atmospheric models.\n input (", "@mpiercet:~/ldsc$ sparql query \nSELECT DISTINCT?metric?metric_lbl WHERE { \n?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. \nFILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \norkgp:HAS_EVALUATION?eval. \nOPTIONAL { \n?eval orkgp:HAS_METRIC?metric. \n?metric rdfs:label?metric_lbl. \n}\n}", "input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query): SELECT?paper_id?paper_title?paper_id_label WHERE { \n   {?paper has uri \"http://dumps.wikimedia.org/enwiki/20150522/*Benchmarking_Concepts_and_Measures_for_Internet_Search_Engine_Benchmarks*\". }\n   UNION {?paper has uri \"http://www.seas.ucla.edu/~djane/research/TREC/download/html/TREC6/TREC6_Benchmarking_Results/TREC6_%20Benchmarking_Results.zip\". }\n   } \n   {?paper_id:EXPRESSION 1.?paper_id_label:EXPRESSION \"Title\".?paper_title:EXPRESSION \"Concepts and Measures for Internet Search Engine Benchmarks\". }\n   } \n\n input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql", "inal (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe SELECT statement provides the following values:\n\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n output (Sparql query", ":ELMo\n\n\nTo evaluate the performance of an SPARQL query evaluation engine, Google provides the MLDoc zero-shot English-to-French dataset. In this benchmark, each entity and relation in the MLDoc training dataset is replaced with a similar entity from the MLDoc Zero-Shot training dataset.\n\n\nThe MLDoc Zero-Shot dataset is only available for ELMo language model, and ELMo is used as the trained language model in the Abstracts' entities and relations annotated corpus benchmark model.\n\n\nThe Accuracy score in the MLDoc Zero-Shot English-to-French dataset evaluation is 78.2%. The MLDoc Zero-Shot English-to-French dataset evaluation is only available for ELMo language model.\n\n\nThe Accuracy score in the MLDoc Zero-Shot English-to-French dataset evaluation is 78.2%. The MLDoc Zero-Shot English-to-French dataset evaluation is only available for ELMo language model.\n\n\nELMo model is trained on over 9 Billion tokens of English texts and can generate over 90k words of French sentences on English texts.\n\n\nThe output of the Sparql query is:\n\n\tSELECT", "sparql (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"PolyAI Reddit benchmark dataset\")?benchmark orkgp:HAS_EVALUATION?eval; orkgp:HAS_METRIC?metric; } } \n input (English text): 2-of-100 Accuracy metric. Which model performed the best on this metric?\n output (Sparql query): hailsparql (Sparql query): SELECT?metric WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"PolyAI Reddit benchmark dataset\")?benchmark orkgp:HAS_EVALUATION?eval; } }\n\n\nThe most accurate model that we have on the PolyAI", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThis query selects the distinct metrics from the query result.\n\nBased on the code links, these are the metrics that are used to evaluate models on the benchmarks:\n\n* perplexity (Perplexity)\n* F1 score (F1)\n* micro-averaged F1 score (F1M)\n* macro-averaged F1 score (F1M)\n* unranking metric (k-unranking)\n* ranking metric (R@1)\n* precision@k (Prec@k)\n* recall@k (Rec@k)\n* f-score@k (F@k)\n* macro-averaged", "inced model name: T5\n\n # SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n # DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset;?metric T5.?metric_lbl TriviaQA. }\n\n<|endoftext|>import React, { memo, ReactElement } from'react'\n\nimport { Connection } from 'components/connections'\nimport { BindAction, PageObjectInterface, Query } from 'utils/models'\nimport { View } from 'components/views'\nimport { styles } from 'utils/palette'\n\nconst { isEmpty } = Query\n\ninterface Props {\n  connect: BindAction\n  items:", "Raman spectroscopy is a powerful tool for studying materials at the atomic scale and has many applications in the semiconductor industry.\n\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Raman spectroscopy\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following are the Raman spectroscopy metrics:\n * Raman shift: this is the amount of red shift, measured in cm-1, after exciting the sample with a monochromatic light. The higher the shift, the more complex the sample is.\n * Soret Band: this is an absorption band which appears in the IR region between 200 and 300 nm. It is related to the anisotropy of the Raman shifts and of the dielectric function.\n * Second Order Raman Sidebands: this is the subband of the Raman spectrum arising from interaction between the incident light and the sample, namely the effect", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DTD dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nA:\n\nThe high score should be returned from the following answer.\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DTD dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>We use cookies to personalise content, target ads, to provide social media features and", "output (Sparql query): CINIC-10: CINIC-10 dataset: orkgp:HAS_EVALUATION rdfs:label \"top_benchmark\": \"TPR\": 0.960 orkgp:HAS_EVALUATION rdfs:label \"top_benchmark\": \"F1\": 0.960 orkgp:HAS_EVALUATION rdfs:label \"top_benchmark\": \"recall\": 0.960 orkgp:HAS_EVALUATION rdfs:label \"top_benchmark\": \"F1@N\": 0.960 orkgp:HAS_EVALUATION rdfs:label \"top_benchmark\": \"top_micro_aver\": 0.960 \n Sparql query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CINIC-10\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS", "set { \"SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval-2018 Task 7 Dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } }\n\nThe following are the metrics that are used to evaluate models on the SemEval-2018 Task 7 dataset dataset benchmark dataset:\n\n  *  Metric A: Coding complexity (CoCo) \n  *  Metric B: Model size (size of the neural network)\n\n  *  Metric C: Training loss (lower is better)\n\n  *  Metric D: Test accuracy (lower is better)\n\n  *  Metric E: Cross-entropy (lower is better)\n\n  *  Metric F: F-score\n\n  *  Metric G: Precomputed word similarities (Lower is better)\n\n  *  Metric H: Average length of", "input (English text): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AG News benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rd", ":Google LiteAV (F1 score of 0.997) \n\n\"F1\" is an acronym for the \"F-score\", which is a metric to assess the quality of a classifier. It is also known as the \"harmonic mean\" of the precision and the recall of a classification algorithm. \n\nF1 scores usually range from 0 to 1, where 1 indicates a perfect classification and 0 means that no model was able to predict the correct class (in this case, Disease). In this case, the top performing model is the one with a F1 score of 0.997. The corresponding Sparql query is:\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Disease\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Kinetics of transformation of vinyl chloride in soils.", ":SELECT DISTINCT?metric?metric_lbl FROM <query> WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"diseases\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl FROM <query> WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"diseases\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs", "output (Sparql query):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following metrics are present in the dataset Abstracts' entities and relations annotated corpus: \n- Average precision (AP)\n- Multidimensional Scaling (MDS)\n- Recall at different threshold levels\n- Precision at different threshold levels\n- Average time per result (at five different thresholds)\n\nAs the dataset does not contain class labels, the papers that benchmark the GCN Hybrid model perform classification using these five metrics.\n\nThe following are the papers that benchmark the GCN Hybrid model: \n- [Link to paper 1]\n- [Link to paper 2]\n- [Link to paper 3]", "format. rdf triples. \n input (English text): CHEMDNER corpus has rdf triples as its data format.\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CHEMDNER corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): CHEMDNER corpus is present in the database orkgp:EntitiesAndRelationsAnnotatedDatasets, where orkgp is the OWL API for Ontobee. The dataset and evaluation metrics are present in the class orkgp:HAS_DATASET and the class orkgp:HAS_EVALUATION respectively.\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset;", "output (Sparql query): SELECT DISTINCT?benchmark?dataset_lbl. FILTER (str(?dataset_lbl) = \"CUB-200-2011\") WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. } \n\n<|endoftext|>Q:\n\nSSRS textbox isn't displaying text values after a form submits\n\nHere's a snapshot of my text box that's not displaying the value I typed into it after a form is submitted.  The textbox is on a form that is part of a query.  The query is pulling from a SQL Server database via ADO.NET.  If I breakpoint on the line where the query is running and step through the code, the textbox.text value is being updated with the right value, so the query is working and getting the value I want.\n\nA:\n\nI figured out my issue.  I needed to update the dataset in my datasetbag after the form was submitted.  So, this is my code that gets my dataset to update:\nDataSet myDataset = new DataSet();\nmyDataset.TableName = \"DatasetName\";\nmyDataset", "endant answers: \nSELECT DISTINCT?metric?metric_lbl WHERE { \n?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. \nFILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \norkgp:HAS_EVALUATION?eval. \nOPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe three metrics used in the benchmark dataset are: \n- ROUGE-1\n- ROUGE-2\n- ROUGE-L", "SELECT?metric?metric_lbl WHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n ?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL { \n  ?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n } \n\n<|endoftext|>Q:\n\nMySQL order by query with subquery\n\nThis is my table:\nid | process_id | process_user | process_date | results\n1  | 1          | ravi         | 2016-05-24  | 30\n2  | 1          | ram          | 2016-05-25  | 40\n3  | 1          | sachin      | 2016-05-26  | 20\n4  | 2          | ravi         | 2016-05-23  | 20\n5  | 2          | ram          | 2016-05-24  | 30\n6", "_eval:has_metric:avg\n output (English text):  SELECT AVG(evals:result) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Human-annotated\") } \n output (Sparql query):8.2\n input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Computer-annotated)?\n output (Sparql query):aused_eval:has_metric:avg\n output (English text):  SELECT AVG(evals:result) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Computer-annotated\") } \n output (Sparql query):5.4\n input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Computer-annotated and relation has weight value)?\n output (Sparql query):aused_eval:has_relation:weighted\n output (English text):  SELECT AVG(evals:result) WHERE", "title    => Adaptive Input Large Model (AIL)\nurl      => https://github.com/OAI/oad-cloud/blob/master/storage-apis/oad-cloud-api-data/src/main/resources/oad-cloud-apis-data-model-benchmarks/AIL.md\ncode     => https://github.com/OAI/oad-cloud/tree/master/storage-apis/oad-cloud-api-data/src/main/resources/oad-cloud-apis-data-model-benchmarks/AIL.sh\nrelated => https://github.com/OAI/oad-cloud/issues?q=is%3Aissue+is%3Aopen+label%3Amodel+Created+by\nrelated => https://github.com/OAI/oad-cloud/issues?q=is%3Aissue+is%3Aopen+label%3Amodel+Created+by+Author%3DSarah+Hecht%2C+Ph.D.\nrelated => https://github.com/OAI/oad-cloud/issues?q=is%3Aissue+is%3Aopen+label%3", "endometr=(hit% | mismatch) | rt8 | score  | top1 | top5 | time\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Solaris\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval scoredefault.?eval time. } } \n input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n output (Sparql query): ALJP is the best metric for evaluation of performance on Atari 2600 game datasets. Score is calculated as ALJP/ time. \n Input (English text): Score is calculated as ALJP/time. \n Output (Sparql query): ALJP is the best metric for evaluation of performance on Atari 2600 game datasets. Score is calculated as ALJP/time. \n Output (Sparql query): ALJP", "output (Sparql query): SELECT?title (str: neuter?str) WHERE { \n        ?dataset a orkgc:Dataset; \n         rdfs:label?dataset_lbl. \n         FILTER (str(?dataset_lbl) = \"SNLI dataset\")\n\n        ?benchmark orkgp:HAS_DATASET?dataset;\n\n         orkgp:HAS_EVALUATION?eval. \n         OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\n\n        ?dataset a orkgc:Dataset;\n\n        ?benchmark orkgp:HAS_DATASET?dataset;\n\n         orkgp:HAS_EVALUATION?eval. \n         OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\n\n        ?dataset a orkgc:Dataset;\n\n        ?benchmark orkgp:HAS_DATASET?dataset;\n\n         orkgp:HAS_EVALUATION?eval. \n         OPTIONAL {?", ": \n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\niju : See examples in the papers  and the code used can be referenced in these papers.", "ition (Sparql query): SELECT DISTINCT?metric_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Centipede has a benchmark score of 42.143935 and its metric is \"Correctly found number of dead enemies\".\n output (Sparql query):backed?metric_lbl \"Correctly found number of dead enemies\" \"Centipede\"\n\nkerchief (Sparql query): SELECT DISTINCT?metric_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Centipede\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:", "s:SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MNIST Dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe Sparql query response will be : \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MNIST Dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following are the metrics that are used to evaluate models on the MNIST dataset:", "Country: France*Metric: Population*Metric_lbl: Population of France\n Pero = 4,187,383 people\n\nregistry = French national census of 2011\n\n Registry of the population of France\n\n The French national census of 2011 counted 4,187,383 people\n\n dataset = French national census of 2011\n\n has_dataset = has(dataset)\n\n has_evaluation = has(eval)\n\n has_metric = has(eval,metric)\n\n metric = eval.metrics.population\n\n metric_lbl = metric.label\n\n registry = French national census of 2011\n\n Registry of the population of France\n\n The French national census of 2011 counted 4,187,383 people\n\n dataset = French national census of 2011\n\n has_dataset = has(dataset)\n\n has_evaluation = has(eval)\n\n has_metric = has(eval,metric)\n\n metric = eval.metrics.population\n\n metric_lbl = metric.label\n\n registry = French national census of 2011\n\n Registry of the population of France\n\n The French national census of 2011 counted 4,187,383 people\n\n dataset = French national census of 2011\n\n has_dataset = has(dataset)\n\n has_evaluation = has(eval)\n\n has_metric = has(", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"Abstracts' entities and relations annotated corpus\" and \"Abstracts' entities and relations annotated corpus benchmark\"\n\n<|endoftext|>1. Field of the Invention\nThis invention relates to electronic apparatus and particularly to an electronic apparatus having a detachable cover and a locking mechanism for releasably locking the cover to the apparatus.\n2. Description of the Related Art\nPortable electronic apparatuses such as portable telephones are in widespread use and provide users with a wide range of functions including, for example, telephonic features and the ability to transmit and receive images and other data using a communication network such as the Internet. Portable telephones are generally comprised of a body and", "n 'SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }' \n\nFrom the query response, you can see that the following metrics are used to evaluate models on the TempEval-3 dataset:\n\n?benchmark< TempEval-3 dataset>\n\n?metric<Micro-F1 score>\n\n?metric_lbl\n\n?eval\n\n?eval.Micro-F1 score\n\n?eval.rdfs:labelTempEval-3 dataset\n\n?eval.rdfs:labelMicro-F1 score\n\n?eval.has_datasetTempEval-3 dataset\n\n?eval.has_evaluationTempEval-3 dataset\n\n?eval.has_metricMicro-F1 score\n\n?eval.has_metric_lbl\n\n?", "; SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC' benchmark\")?eval.?dataset orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): SciERC' is a benchmark dataset that contains query- answer pairs. The pairs are annotated with their respective evaluation metrics. Some examples of evaluation metrics are Recall, Precision, F1 score, Precision at k, and so on. The data and code used to generate this list is available at https://github.com/oleg-perelson/SciERC.\n\n notre; SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC' benchmark\")?eval.?dataset orkgp:HAS_EVALUATION", "SELECT * \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2012 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n*F1 metric is one of the metrics used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems\n\n reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems\n\n reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems\n\n reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems\n\n reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems\n\n reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval. } } \n\nAnswer: The following metrics are used to evaluate models on the WMT2014 English-German dataset:\n 1. SegSent: The simpler the model is, the less parameters it needs, and thus the better.\n 2. Bilingual GPT: A big model is better than a small one.\n 3. MULTICULTurally Attentive Attention (MuLA): When two sentences have the same verb and same subject but differ in their object, attention mechanism in GPT should pay more attention to the verb and the subject.\n 4. Bilingual Abstract Task (BAT): Evaluates a model's ability to generate dependency trees from parallel sentences, where sentences have the same subject but different verbs, as a way of measuring", "dataset (SELECT MAX(?eval) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-14\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } })\n input (English text): The highest benchmark result achieved on the Yelp-14 dataset is 1560.83, including the metric and its value of precision.\n output (Sparql query): RPMI-14     1560.83\nHow did you write this answer?\nThe highest benchmark result achieved on the Yelp-14 dataset is 1560.83, including the metric and its value of precision.\nBased on this answer, you could conclude that the highest benchmark result achieved on the Yelp-14 dataset is the metric of precision with its value of 1560.83.\nFor more information about metrics, see the following:\n* https://www.openrdf.org/forums/openr", "ution: Filter the Sparql query to the code link in the paper that uses the A3C FF (1 day) hs model. This code link is provided in the first paragraph in Section 4.1 of the paper: https://arxiv.org/abs/1905.00652. The code link is: https://github.com/tensorflow/models/blob/master/research/slim/models/resnet.py#L277. The line 277 in this file is the A3C FF (1 day) hs section.?metric (Sparql QoQL): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): https://github.com/tensorflow/models/", "':SELECT DISTINCT?metric?metric_lbl WHERE {?model rdf:type owl:Class; owl:equivalentClass?model2.?model2 rdf:type owl:Class; owl:equivalentClass?model?model2. {?model2 owl:has Relations; owl:onRel :hasOut ;?model owl:has Properties; owl:onProperty :hasOut ; } }\n\n answer: SELECT DISTINCT?metric?metric_lbl WHERE {?model rdf:type owl:Class; owl:equivalentClass?model2.?model2 rdf:type owl:Class; owl:equivalentClass?model?model2. {?model2 owl:has Relations; owl:onRel :hasOut ;?model owl:has Properties; owl:onProperty :hasOut ; } }\n\nThe following are the metrics that are used to evaluate models on the RTE benchmark dataset:\n\n  *\t#model #rels\n  *\t#model#rels\n\n answer: SELECT DISTINCT?metric?metric_lbl WHERE {?model rdf:type owl:Class; ow", ":SELECT MAX(score) AS max_score FROM evaluated_data WHERE evaluation_set_name = \"wos5736\" AND dataset_name = \"wos5736\" AND category_name = \"Directly Annotated Relations\" AND (annotation_type IN (\"relation\", \"dependency\", \"is-a\", \"has\")) ORDER BY score DESC\n input (English text): \"relation\", \"dependency\", \"is-a\", \"has\" are the annotation types of the entities and relations in the benchmark dataset.\n\n\nThe Sparql query returns the highest score obtained by any model on the WOS-5736 dataset.\n\n\nThe WOS-5736 dataset has 9 entities with the annotation types \"relation\", \"dependency\", \"is-a\", and \"has\". The highest score achieved by any model is 16.8, achieved by GPT-D.\n\n\nIn summary, the metrics used to evaluate models on the WOS-5736 dataset are the following: \"Relation Type\", \"dependency\", \"is-a\", \"has\"\n\n\nThe GPT-D model achieved the highest score on this dataset with 16.8, which was annotated using the annotation types \"relation\", \"dependency\", \"", "output (Sparql query): SELECT DISTINCT?metric_lbl?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WebQuestions\"^^xsd:string)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION_METRIC?metric. } } \n input (English text): List the metrics that are used to evaluate models on the WebQuestions dataset?\n output (Sparql query): SELECT DISTINCT?metric_lbl?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WebQuestions\"^^xsd:string)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION_METRIC?metric. } } \n input (English text", "inal Answer: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nHere are the metrics that are used to evaluate models on the dataset benchmark ART/CoreSC:\n 1. cosineSimilarity \n2. log score \n3. NDCG\n 4. Pearson's chi-squared \n5. R-squared", "sparqlquery (English text): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (short)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n methodology :\n Evaluate these models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\n         * Naver Question Answering (RQ-Lab)\n         * DistilDQL\n         * T5\n         * DistilPoS\n         * DistilNLP\n         * DistilCore\n         * DistilX\n\n As of the most recent version, distilx queries on the D2RQ dataset have been benchmarked against each other using Natural Questions (short) as the held-out dataset. The query outputs the following metrics:\n- Overall F1 score\n- Slot F1 score for named entities in question", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe WMT14 French-English dataset has the following results:\n ({\"@context\": \"http://www.w3.org/2004/apps/molecular_data/xsd\", \"@id\": \"http://www.wmt.org/xap/wmt14-fr-eng\", \"@type\": \"OrientMWMT2014FRENDEUROPE\", \"@language\": \"fr\", \"@container\": \"ms\", \"@version\": \"1\", \"@containers\": \"ms\", \"ms\": {\"@id\": \"http://www.wmt.org/xap/wmt14-fr-eng\", \"rdf:", ": OpenNLP Show Perplexities on WikiText-2 Benchmark Dataset SELECT?metric_lbl?metric_text where {?metric_text wdt:P39?metric_lbl. FILTER (str(?metric_lbl) = \"Perplexities\") }\n input (English text):Perplexities refers to the test score given by the OpenNLP model on the WikiText-2 benchmark dataset.\n\n 16-gram language modelperplexities \nPerplexities is the test score given by the OpenNLP model on the WikiText-2 benchmark dataset. Perplexities refers to the test score given by the OpenNLP model on the WikiText-2 benchmark dataset.\n\n 16-gram language model\n\n {#section-14e4429dd524522d0e9c42a3c2a4b5e0.}}{#section-14e4429dd524522d0e9c42a3c2a4b5e0.section.text.container\n\n {#we should add the metric below for evaluating models in this section...}\n\n {#section-14e4429dd524522d0e9c42a", "output (Sparql query): FROM http://all-attention.maincontent.xyz/model.html#modeldetails\n\n    SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n    * metric - Dataset abstraction, Equit\n    * metric_lbl - Dataset abstraction, Abstracts' entities and relations annotated corpus\n\n    SELECT?metric\n\n    SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS", "{ metric CosineSimilarity labelSimilarity } \n input (English text): CosineSimilarity  is a metric used for measuring the similarity between two vectors. One vector is formed by the feature values of a particular object and the other vector is formed by the same features of another object. The CosineSimilarity  is defined as the inner product between the two feature vectors.\n\n\r\n\nThe CosineSimilarity  is defined as the inner product between the two feature vectors.\n\n output (Sparql query):ocious { metric CosineSimilarity labelSimilarity }\n\n\r\n\nCosineSimilarity  is a metric used for measuring the similarity between two vectors. One vector is formed by the feature values of a particular object and the other vector is formed by the same features of another object. The CosineSimilarity  is defined as the inner product between the two feature vectors.\n\n output (Sparql query):ilarly { metric CosineSimilarity labelSimilarity }\n\n\r\n\nCosineSimilarity  is a metric used for measuring the similarity between two vectors. One vector is formed by the feature values of a particular object and the other vector is formed by the same features of another object. The CosineSimilarity  is defined as the", "ynote (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>The competitive nature of sport can make competitive athletes underperform at physical activity,", "model achieved an average precision score of 0.97965.\n input (English text):What metrics are used to evaluate models on the Atari 2600 Freeway benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text):Indicate the model that performed best in terms of MAP metric on the Atari 2600 Freeway benchmark dataset?\n output (Sparql query):INSERT INTO <tablename> (Metric,Metric_lbl) VALUES (MAP, \"MAP\")\n\n<|endoftext|>Scott Wagner is author of Startup Communities: How to Create a Happy, Healthy, Financially Successful Life on Your Terms\n\nThe problem\n\nStarting a business and", "sparql query results: SELECT DISTINCT?paper_id?title WHERE { \n{ \n?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. \nFILTER(str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \norkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\n}\n UNION \n{?paper_id \"DCASE\"?title. }\n}  \norder by?paper_id limit 10}\n\n input (English text): DCASE: Detect and Classify Agenda-Based Information Spoken in Telephone Calls Using Automatic Speech Recognition, by Padmanabhan P, Talwalkar P, Carlsson G, Gopalakrishnan K, Karunanidhi N, Chen X, et al., Proceedings of the 19th International Conference on Language Resources and Evaluation (LREC 2012), 2012.\n\n output (Sparql query", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a yelp:FGCData; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp Fine-grained classification dataset\")?benchmark yelp:HAS_DATASET?dataset; yelp:HAS_EVALUATION?eval. OPTIONAL { yelp:HAS_METRIC?metric. yelp:HAS_METRIC?metric_lbl. } } \n input (English text): The Yelp Fine-grained classification dataset has 3 metrics: Accuracy, Macro-F1 and Mean Average Precision (mAP) \n output (Sparql query): towels {Accuracy?metric?metric_lbl. Macro-F1?metric?metric_lbl. Mean Average Precision?metric?metric_lbl. }\n\nThe metrics that are used are:\n Accuracy \nMacro-F1 \nMean Average Precision \n\nAdditionally, the evaluation process can optionally include evaluation metrics. The evaluation metrics are:\n Has_Metric \nMetric \nMetric_Label\n\nRelated Work\n============", "input (English text): The highest score achieved by any model on the Atari 2600 River Raid benchmark dataset was on the Feretault model. \n input (Sparql query): The Feretault model achieved a Score of 0.0066.\n\nWhich metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n\nSELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe metrics used to evaluate the Feretault model on the Abstracts' entities and relations annotated corpus benchmark dataset are:\n?metric :Score\n?metric_lbl:Feretault\n\n<|endoftext|>1. Field of the Invention\nThe present invention relates to a remote keyless entry (R", "SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n(click on'show rest of results' to go to the next page)\n\nThe following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\n- ConfusionMatrix\n- AveragePrecision@1\n- F1 Score\n- Recall\n- Top1\n- Top5\n\n?metric : ConfusionMatrix\n?metric_lbl : Abstracts' entities and relations annotated corpus\n?metric_lbl : ConfusionMatrix\n\nThe following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\n- AveragePrecision@1\n- F1 Score\n- Recall", ":Pearson Correlation metric was performed and the model that performed best in terms of Pearson Correlation was KG-IAM using 5-gram LSA features with 100-dimensions with an Optimizer with aggressive cost reporting using the TF-IDF weighting scheme achieved 0.918.\n input (English text): To understand why the KG-IAM model achieved the best performance, it is useful to understand what is meant by a model being \"performed best\" in terms of Pearson Correlation metric on the benchmark dataset MedSTS.\n output (Sparql query): Here you can read about the explanation of the \"Best\" performance and the rationale behind the choice of metric: http://saysayin.github.io/blog/2017/11/02/what-is-the-best-performance/\n input (English text): What is the metric used to evaluate models on the MedSTS benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MedSTS benchmark dataset\")?benchmark orkgp:HAS_DAT", "output (Sparql query): SELECT?benchmark { (_:http://www.ontotext.com/ontology-download/datasets/wsc) } WHERE {?benchmark owl:SameAs <http://www.w3.org/2013/Turtle> }\n input (English text): The following are the titles and IDs of research papers that include a benchmark for the WSC dataset:  \n - WSC Benchmarks: <http://www.ontotext.com/ontology-download/datasets/wsc>\n - WSC Evaluation: <http://www.w3.org/2013/Turtle>", "Python 3.7, dbpedia-3-api-Python,pandas-0.24.2, rdflib-1.11.2, spam-scraper-0.5.0, pyquery-1.1.0\n import sparql \n import requests\n\n query = \" \n  SELECT DISTINCT?metric?metric_lbl \n  WHERE { \n   ?dataset a orkgc:Dataset; \n    rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; \n    orkgp:HAS_EVALUATION?eval. OPTIONAL { \n     ?eval orkgp:HAS_METRIC?metric. \n     ?metric rdfs:label?metric_lbl. \n    } \n  } \n \"\n\n parameters = {\n    \"metric\": [\"macro\", \"micro\"],\n    \"metric_lbl\": [\" metric_label1\", \" metric_label2\"]\n }\n\n dataset = \" // Abstracts' entities and relations annotated corpus\"\n\n evaluation", ":SELECT?metric_lbl WHERE { \r\n  ?dataset a orkgc:Dataset; \r\n   rdfs:label?dataset_lbl. \r\n   FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \r\n  ?benchmark orkgp:HAS_DATASET?dataset; \r\n   orkgp:HAS_EVALUATION?eval. \r\n   OPTIONAL { \r\n   ?eval orkgp:HAS_METRIC?metric. \r\n   ?metric rdfs:label?metric_lbl. \r\n   } \r\n }\n\n\nSELECT DISTINCT?metric?metric_lbl \nWHERE { \r\n ?dataset a orkgc:Dataset; \r\n  rdfs:label?dataset_lbl. \r\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \r\n ?benchmark orkgp:HAS_DATASET?dataset; \r\n  orkgp:HAS_EVALUATION?eval. \r\n  OPTIONAL { \r\n  ?eval orkgp:", "itation 2: SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-5 Fine-grained classification benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): SST-5 Fine-grained classification is a dataset from SQuAD 2.0. SQuAD is a knowledgeQA dataset. This dataset has 50 documents with 10 different categories of entities and relations and each relation appears 2 to 3 times in the document with different position in the document. There are 4 types of relations - Subject, Predicate, Object and Complement. SQuAD 2.0 provided SST-5 Fine-grained classification benchmark dataset to evaluate the models on these types of relations. \n output (Sparql query):azzoinal 1: SELECT?metric?metric_lbl WHERE {?dataset a orkg", "metric_lbl rdfs:label?metric_lbl\n\nples de { \"SELECT DISTINCT?metric?metric_lbl\" }\n  { \"rdfs:label\" \"Metric\" }\n  { \"rdfs:label\" \"Metric_Label\" }\n\nOSELLABLE Metrics for ARC-PDN dataset:\n\n  - Hits (true positives)\n  - Misses (false positives)\n\n  - Hits (true positives)\n  - Failures (false positives)\n\n  - [Hits * TPR]\n  - [Failures * FPR]\n\n retrieved 4 results\n\nsteps:\n- name: Get all Sparql queries from Assumeworks workspace\n  command: |\n    pickmediaportal # type \"nix\" or \"zsh\" or \"bash\" to pick a shell\n    sparql+http://www.w3.org/ns/activitystreams#\n  register: results\n\n  - name: Output all the Sparql queries for the Assumeworks workspace\n    set_output:\n      queue: results\n\n  - name: Filter the results to only the Sparql query with the where clause\n    filter:", ":Abstracts' entities and relations annotated corpus\n\n auxiliary /oracle/oracle.wso2.dataservices.query.base.annotation.HasDatasetEvaluation sql query: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nWith the above query, the results include only two metrics - metric_lbl and metric\n\n auxiliary /oracle/oracle.wso2.dataservices.query.base.annotation.HasDatasetEvaluation sql query: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The Atari 2600 Berzerk dataset commonly used for benchmarking models is comprised of 10,000 randomly generated images depicting objects interacting with each other and over a 70 second time window.The model\u2019s evaluation metric for the dataset consists of the following four metrics: Recall, Precision, F1-score, and Accuracy. These metrics are computed based on comparing the model predictions to the gold standard.The following metrics are computed for each model in the dataset: Recall: The fraction of correct predictions out of the total number of predictions made by the model. Precision: The fraction of correct predictions out of the total number of predictions made by the model. F1-score:", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cars\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Car dataset - md5: 6e9a0ebe6ddef6b39cc77a41fcfbd088\n Car dataset - url: https://storage.googleapis.com/deep-learning-1005/trafficsim/trafficsim_static/Cars.zip\n A benchmark on Stanford Cars dataset - id: e4da9597-fc8c-4867-9c6d-d29c55f45761\n output (Sparql query): 51830ef2-a384-1033-a1e7-c5713fa73a8a Cars - md5: 6e9a0", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \nmetric metric_lbl\n1.1 The effectiveness of our system\n1.2 The number of annotations our system has generated\n1.3 The time taken by our system to generate annotations\n1.4 The number of entities and relations that were successfully annotated\n1.5 The Average lengths of entities and relations\n1.6 The Precision and Recall of entities and relations when generating annotations\n1.7 The number of errors made by our system while generating annotations\n1.8 The number of relation labels that were not contained in the title or the ID of the relation\n1.9 The number of relation labels that were wrongly annotated as not contained in the title or the", "s:SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cars dataset\") }\n input (English text): The metrics used to evaluate the models over the Car dataset are:\n\nCar accuracy:  This metric assesses the model\u2019s ability to assign a correct value to the distance between each predicted location and the corresponding correct location.\nCar F1 score:  This metric is a harmonic mean of the precision and recall of the model for each prediction, and is widely used as a measure of model quality.\nCar mean average precision:  This metric is the mean of the average precision across all pairs of predictions and observations for the model.\nCar mean average recall:  This metric is the mean of the average recall across all pairs of predictions and observations for the model.\nCar mean cmad:  This metric is the mean of the confusions plus false negatives (FNs) across all classes of the model.\nCar mAP:  Mean Average Precision, i.e., the fraction of accurately predicted pixels out of all predicted pixels.", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The ShARe/CLEF eHealth corpus dataset, includes research papers that describe model evaluations for their abilities to identify abstracts that relate to clinical health data. Evaluation metrics include, but are not limited to, precision, recall, F1 score, and area under the receiver operator characteristic curve.", "input (English text): For copper the least response time was obtained.\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Copper\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe answer is that the least response time was obtained for copper.", "input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"List the code links in papers that use the GPT-2 (small) model in any benchmark?\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): [(\"Tf-idf\", \"TF-IDF\"), (\"Rouge\", \"R\")]\n output (Sparql query): <<Tf-idf Rouge>>\n input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Fishing Derby\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs", "inal (Sparql query): ### Best performing model ###\n output (Sparql query): SELECT?metric?metric_lbl \n     WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nIn this query the best performing model is the one that has the lowest number of parameters. This can be done using the OPTIONAL construct to include only the model with the desired metric.\n\n### ImageNet dataset results ###\noutput (Sparql query):Photo(metric_lbl=\"Number of params\",metric=\"Number of params\",dataset_lbl=\"ImageNet dataset\",dataset=\"ImageNet dataset\")\n\n submission 1\n\n #Best performing model (lowest number of parameters):\n  optimizer: RMSProp\n  loss: MSELoss\n  model architecture: densen", "ination of Model Performance on the Atari 2600 Battle Zone Dataset: SELECT DISTINCT?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\") }\n input (English text): \nOnsdood:\n evaluation metrics used for the Atari 2600 Battle Zone dataset:\n accuracy\n precision\n recall\n f1 score\n output (Sparql query):evaluation of Model Performance on the Atari 2600 Battle Zone Dataset: SELECT DISTINCT?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\") }\n output (Sparql query): accuracy\n precision\n recall\n f1 score\n input (English text): There are 4 metrics used to evaluate models on the Atari 2600 Battle Zone dataset:\n accuracy\n precision\n recall\n f1 score\n\n abort\n\n<|endoftext|>Q:\n\nIn RcppArmadillo, how do you multiply vectors element-by-element?\n\nI'm writing", "{ \"http://bitti.sourceforge.net/annotation/models.html#m5\" \"http://www.bittorrent.org/bamp/model/bamp-m.html\" \"http://rd.mlr.org/pages/bamp\" \"http://code.google.com/p/bamp-metrics/downloads\" } \n input (English text): Code links for the BiT-M model used in any benchmark?\n output (Sparql query): SELECT * FROM {  http://bitti.sourceforge.net/annotation/models.html#m5  http://www.bittorrent.org/bamp/model/bamp-m.html  http://rd.mlr.org/pages/bamp  http://code.google.com/p/bamp-metrics/downloads }\n code (Sparql query):  http://bitti.sourceforge.net/annotation/models.html#m5  http://www.bittorrent.org/bamp/model/bamp-m.html  http://rd.mlr.org/pages/bamp  http://code.", "{ metric_lbl \"Highest average FPS on the Atari 2600 Star Gunner dataset\" \"70\" }\n input (English text): Using the same Atari 2600 Star Gunner dataset, what is the second highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query):SELECT?benchmark {?benchmark Atari2600StarGunn \"70\" }\n input (English text): Using the Atari 2600 Star Gunner dataset, what is the third highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query): SELECT?benchmark {?benchmark Atari2600StarGunn \"50\" }\n\n<|endoftext|>Badr Hari at E3 2013: \"I'm interested in being the best diver I can be\"\n\nWatch Badr Hari, one of the most well-known and experienced OAR diver, talk about his career and his current plans at E3 2013.\n\nBorn in Kenya but raised in Sweden, Badr Hari began diving in the early 1990s and became one of the first OAR divers after", "input (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \ninput (English text): <https://github.com/snowballx/py-rasa/blob/master/rasa/rasa.py> is the code used in the papers that benchmark the NASCell model. \n output (Sparql query):Downloadable file https://www.dropbox.com/s/n2k7cy0fgmzd6r7/rasa.py?dl=0\n input (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.", "input (English text): romeo@data61.com.br\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Origem Dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): code references in papers that have used the TCN model for benchmarking purposes can be found in Romeo Fernandes' paper which uses TCN to achieve state-of-the-art results on the KBP2015's entities and relations annotated corpus benchmark dataset.\n\nsburgok@data61.com.br\nromeo@data61.com.br\n\n<|endoftext|>Mission\n\nBuilding Community and Connecting With the Earth\n\nMission\n\nBuilding Community and Connecting With the Earth\n\nThe Pomander Bowl is a community center located at 136", "SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>---------------------- Forwarded by Don Heinrich/NA/Enron on 06/06/2001 08:29 AM ---------------------------\n\n\n\"Schindler, Paul\" <Paul.Schindler@edelman.com> on 06/06/2001 07:16:48 AM\nTo:\t\"'don.heinrich@enron.com'\" <don.heinrich@enron.com>\ncc:\t \n\nSubject:\tFW: Did you know?\n\n\nHere's another one:  A couple walking along a beach one day noticed an\nenormous seal up to its neck in a net.  They called the Sea Fishing Company\nand", "output (Sparql query): SELECT?metric?metric_lbl?metric_value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD2.0\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n selected metric : METRIC_1\n selected metric_lbl: SQUAD2.0\n selected metric_value : 9.38\n\nA:\n\nI would encourage you to go through this thread at the query language designer slack channel for more information on that subject. \nIn short, there is no standard way to express that query without using aggregation operators on the values returned by SELECT. However, the syntax for doing that is quite verbose and in many cases does not actually provide a clear advantage over the alternatives.\n\n<|endoftext|>Sue Bird and the Dream Shake beat the Indiana Fever 110-95 in Game 2. Photo by", "output (Sparql query): SELECT DISTINCT?metric_lbl?metric_value WHERE {?benchmark orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. } FILTER (str(?dataset) = \"BC2GM\") } \n input (English text): The result is: Has_Dataset(BC2GM) | Has_Evaluation(Has_Metric(Metric,Label)) | Has_Evaluation(Has_Metric(Label,Value)) \n output (Sparql query):medscyntax (Sparql query): SELECT DISTINCT?dataset_lbl?benchmark_entity_id?metric_value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC2GM\")?benchmark_entity_id.?benchmark orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric", "set (a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Story Cloze Test\")) orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Story Cloze Test", "sparql query SELECT DISTINCT?contributor_author?contributor_author_label,?contributor_author_url WHERE { \n/* query 1 */\n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl; \n/* query 2 */\n ?dataset_lbl wfs:field?dataset_lbl;\n ?benchmark orkgp:HAS_DATASET?dataset; \n/* query 3 */\n ?dataset orkgp:HAS_EVALUATION?eval; \n/* query 4 */\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric;?metric rdfs:label?metric_lbl. } \n/* query 5 */\n  FILTER(?metric_lbl = \"Has contributed to the largest number of articles about coronavirus\") }\n\n  OPTIONAL {?eval sparql:name?eval_name;?eval sparql:directive?eval_directive }\n\n  {?contributor_author?contributor_author_label,?contributor_author_url?contributor_author_url_label }", "2:Select * where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The models that have been benchmarked on the DuIE dataset are: trivial model, precise semantic model, generative model, semantic matching model, exact matching model, approx matching model.\n output (Sparql query):SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC precision?metric;?metric precision", ": ASTRID reactor uses coolant R1234. \n Inputs (English text):\n\n\r\n\r\nNext, let's find all metrics that are used to evaluate models on Abstracts' entities and relations annotated corpus benchmark dataset:\n\n\r\n\r\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n\r\n\nThe answer is: \r\n    rovGroth30_lp/h2o: ABSOLUTE_COUNT \r\n    rovGroth30_lp/h2o: MAX \r\n    rovGroth30_lp/h2o: MIN \r\n    rovGroth30_lp/h2o: rel_precision \r\n    rovGroth30_lp/h2o: rel_recall", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The highest score is 29. The metric returned is 'Precision at 5'.\n output (Sparql query):ounded 29 \"Precision at 5\"\n\n<|endoftext|>/*\n * Copyright (c) 2013-2018, Profselb-Brain and re28 Studio.\n * http://www.prossemvb.com\n * MIT Licensed.\n */\npackage com.prossemvb.virtualcamera2.exporters;\n\nimport android.graphics.Bitmap;\nimport android.graphics.Bitmap.CompressFormat;\nimport android.graphics.Bitmap.Config;\nimport android.graphics.Bitmap.Format;\nimport", "output (Sparql query): SELECT DISTINCT?model_id?model_id_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_MODEL?model_id.?model_id rdfs:label?model_id_lbl. } } \n input (English text): Skiing is the name of the dataset. The evaluation metrics used on this dataset are: Accuracy, Specificity, Sensitivity, Positive Likelihood, and Negative Likelihood. The models that have been evaluated on Skiing are: Fancy Anders, Gap Neural, Log Reg, RTP NN, Soft Margin Reg and the XOR model.\n output (Sparql query): select model_id, model_id_label from evaluation where metric in { \"Accuracy\", \"Sensitivity\", \"Specificity\", \"Positive Likelihood\", \"Negative Likelihood\", \"Fancy Anders\", \"Gap", "output (Sparql query): SELECT DISTINCT?metric_lbl?metric_str WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Text Summarization research field benchmark datasets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The most common benchmark datasets used for the Text Summarization research field are: \n - TED ( Trachtenberg et al. 1995)\n - WASD ( J. D. Grant & Thrun 1997)\n - DUC2003 (Chang et al. 2003)\n - TACL2005 (Bikel et al. 2005)\n - TACL2006 (Bikel et al. 2006)\n - DUC2007 (Chang et al. 2007)\n - AMIT-2008 (Murase et al. 2008)\n - DUC2008 (Chang et al. 2008)", "pend (SPARQL Result) {?metric?metric_lbl }\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham dataset\") } \n input (English text): The following are the metrics of evaluation over the Atari 2600 Tutankham dataset: Mean Average Precision (mAP) and R2 scores.\n\n Our model is able to achieve 81.8% mAP and R2 score of 0.6862\n\n deposited (AVRO data file): target {Annotation corpora benchmark task}-mrn47.avro\n\n reproduced (English text): mAP: 81.8%, R2: 0.6862\n\n deposited (AVRO data file): target {Annotation corpora benchmark task}-mrn47.avro\n\n reproduced (Sparql query):?metric mAP?metric_lbl?\n\n deposited (AVRO data file): target {Annotation corpora benchmark task}-mrn47.avro\n\n reproduced (Sparql query):?metric 81.8?metric_", "mark2 with an Score of 0.915000\n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nMark2 is the name of the model.", ":SELECT?model?metric?metric_lbl WHERE {?model PROTEINS;?metric SPARQL_EXPR? } \n input (English text): Yes. Amlogic has evaluated model HMMER3 on the PROTEINS dataset. The evaluated model is HMMER3_A1 in the evaluation results available on the Amlogic website. The metric that has been used in the evaluation is TP, which stands for true positive. \n output (Sparql query):", "input (Sparql query): \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nA:\n\nThe answer is chosen from the linked blog post.\n\n<|endoftext|>Pulmonary rehabilitation in cystic fibrosis: what we have learned and what we need to do.\nPulmonary exacerbations in patients with cystic fibrosis are associated with adverse effects on quality of life and prognosis. Recently, evidence has accumulated for improving quality of life and reducing the frequency of pulmonary exacerbations in these patients with pulmonary rehabilitation. To date, most research in this area has consisted of small studies of short duration. In this review, the authors discuss the role of pulmonary rehabilitation in the context of existing knowledge and describe the gaps in our knowledge base regarding the outcomes of pulmonary rehabilitation and the", "answer: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommitmentBank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe metrics of evaluation over the CommitmentBank dataset are:\n* TAR Score \n* Precision@TAR Score \n* Recall@TAR Score \n* F1 Score\n\n\n### References\n\n<|endoftext|>Ghana\n\nInformation and Reports on Ghana\n\nFounded in the late 18th century, Ghana is one of the world's newest countries and the second-youngest African nation. Situated in West Africa, Ghana's northern border is shared with the Ivory Coast. Despite its young age, Ghana has a long and rich history that continues to shape its present and future.\n\nGhana has a population of approximately 20.4 million. It is b", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The following code was adapted from https://github.com/facebookresearch/MMDB-Datasets/blob/master/data/2014_06_19/methods.py. The code defines an evaluate method for the model. The method will run on a single Dataset entity with a single relation of the given name. The metric to be computed is given as a string, e.g., 'F1 score'.\n output (Sparql query): \n\n /*!\n  * Abstracts' entities and relations annotated corpus\n  * \n  * The following code was adapted from https://github.com/", "it (English text): The best performing model on the Abstracts' entities and relations annotated corpus benchmark dataset is the Micro F1 model with RE+ metric, which reports the F1 score on the ACE 2004 dataset, which includes the relation subgraph're +'  from the ACE 2003 dataset.\n\nmentation (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nmentation (English text):The best performing model on the Abstracts' entities and relations annotated corpus benchmark dataset is the Micro F1 model with RE+ metric, which reports the F1 score on the ACE 2004 dataset, which includes the relation subgraph're +'  from the ACE 2003 dataset.\n\nmentation (Sparql query):SELECT?", "mark2/Dataset/Abstracts' entities and relations annotated corpus benchmarkdataset_lbl/nottingham/2/Abstracts' entities and relations annotated corpusbenchmark/has_dataset/dsc5sddpthjf3/Abstracts' entities and relations annotated corpusbenchmark/has_evaluation/dsc5sddpthjf3/has_metric/evaluation_metric. The top benchmark score is 2, and the metric is The number of abstracts. \n input (English text): The top benchmark score is 2. What is the metric used to evaluate the score? The metric is The number of abstracts.\n\n<|endoftext|>Tesla's solar panels might not be the most impressive engineering feat ever created, but the electric car manufacturer might be able to make up for it with the long-term savings.\n\nA new study from researchers at UC Berkeley and North Carolina State University estimates that electric vehicles like Tesla's will save around $10,000 over ICE vehicles in the long run.\n\nThe study, dubbed \"Estimating Benefits of Electric Vehicles: Comparing Savings on Electricity and Fuel Costs with Life Cycle Analysis,\" comes from the university's Energy and Resources Global Leadership Program and co-authors Dr", "sparql-->SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The benchmark dataset provides a gold standard for the evaluation of knowledge graph completion models, along with information about the metrics that were used to evaluate those models. Here are the relevant metrics: Precision, Recall, F-score, and Normalized Mutual Information (NMI).\n output (Sparql query): formidable metrics that are commonly used when benchmarking models on the STEM-ECR v1.0 dataset are:Precision, Recall, F-score, and NMI.\n\n\r\n               : These are the metrics that are used to evaluate models on the STEM-ECR v1.0 dataset:Precision, Recall, F-score", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe returned values are:\n+----------------------------------------+----------------------------------------------------------------------------------------------+\n| metric                                 | metric_lbl                                                                                    |\n+----------------------------------------+----------------------------------------------------------------------------------------------+\n| Positive Neighbourhood  Coverage         | Positive Neighbourhood  Coverage  (predicted/ground-truth)                                                |\n|                                        |                                                                                                     |\n| Negative Neighbourhood  Coverage         | Negative Neighbourhood  Coverage  (predicted/ground-truth)                                                |\n|                                        |                                                                                                     |\n|Precision                     (rel. to pred. rel.) | Precision", "sparql input (English text): List the models that are being evaluated on the GAD dataset?\n output (Sparql query): SELECT DISTINCT?model?model_label WHERE { {\n\n ?model ModelGAD.\n\n ?model ModelGAD:predictionConfiguration g:modelGADDataPointToleranceMultiplier g:modelGADMaxDatapointToleranceMultiplier g:modelGADMinDatapointToleranceMultiplier g:modelGADMaxIoStepMultiplier g:modelGADIOStepsPerPointMultiplier g:modelGADTestIterations g:modelGADTestNumberOfIterations.\n\n ?model ModelGAD:modelCreationDate value \"2015-07-07\"^^xsd:date.\n\n ?model ModelGAD:modelIdentifier \"VGGNet\"@en.\n\n ?model ModelGAD:modelType g:modelGAD.\n\n ?model ModelGAD:name \"VGGNet\"@en.\n\n ?model ModelGAD:parameterConfiguration g:modelGADDataPointToleranceMultiplier g:modelGADMaxDatapointToleranceMultiplier g", "set({rdfs:label, orkgp:HAS_EVALUATION, orkgp:HAS_DATASET, orkgp:METRIC})\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset;?metric?metric_lbl. FILTER (str(?metric) = \"ROC-AUC\")?benchmark orkgp:HAS_DATASET; OPTIONAL {?benchmark orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): ROC-AUC is one of the metrics used to evaluate the performance of machine learning models.\n output (Sparql query):", "?metric?metric_lbl?metric_lbl\n\n input (English text): The following are the metrics used to evaluate models on the Gibson PointGoal Navigation benchmark dataset: distance, cost, heading, speed,route,path,distance_median,time_median\n\n output (Sparql query):distance,cost,heading,speed,route,path\n\n auxiliary (Sparql query):\n\n SELECT DISTINCT?metric?metric_lbl?metric_lbl\n\n<|endoftext|>Q:\n\nhow to get rendered page source on complete ajax call success?\n\ni have used $http in my controller and my service is jasmine. i have used unit test for $http service. here is the service \nrestangular.mock('./submodules/services/LogsService.js')\n.factory('LogsService', ['$http', function ($http) {\n\n    var logsService;\n\n    return {\n        log : function(url) {\n            $http.post(url).success(function (data, status, headers, config) {\n                console.log(data);\n                console.log(headers);\n                console.log(config);\n            });\n        }", "AAPD dataset? IAdiASDaDiD datAdASsporaN d EbAM PaLaCk\n query (Sparql query):\n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n The following are research papers along with their title and IDs, that have performed benchmarks on the AAPD dataset:\n\n [0]: https://halkyd.com/papers/rel-benchmark-2017-cosmos-with-gluon-resnet-and-xception.pdf\n [1]: https://halkyd.com/papers/kavli-signature-atlas-2017-cosmos-with-gluon-resnet-and-", "input (English text): See the AxCell model at https://github.com/Azure-LSP/AxCell\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Here is the code used in the code zoo paper: https://github.com/google/code-zoo/blob/master/eval/Abstracts%2Dentities%2Drelations%20Annotated%20Corpus.py. It uses the AWD-LSTM model trained on the benchmark dataset. The metrics are:\n - BLEU: https://github.com/google/code-zoo/blob/master/eval/Abstracts%2Dentities%2Drelations%20Annotated%20Corpus.py#L146\n - ROUGE:", "show pages where the rdfs:label contains \"Abstracts' entities and relations annotated corpus\" and the Sparql query output contains \"URI\" for both?benchmark and?metric?metric_lbl.rehended. a page count.\n\n  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe table below contains the IDs and titles of research papers on the PubMedQA dataset that contain benchmarks for the Abstracts' entities and relations annotated corpus annotated corpus (the actual benchmark has been removed). Please note that the titles may be misleading as they refer to the benchmark annotation, not the original research paper:\n\n| Paper ID | Title |\n|-----------|-------|\n| 5199728  | Nguyen et al. |", "input (English text): Datasets that have been used for benchmarking in the Image Classification research problem are:\n - Abstracts' entities and relations annotated corpus\n - Microsoft Flower dataset\n - CALEB-TRIP DeepFashion dataset\n\noutput (Sparql query): SELECT DISTINCT?dataset?dataset_lbl?benchmark?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", "input (Sparql query):SELECT DISTINCT?metric?metric_lbl?score WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe metrics returned for the highest score are:\n{\n  \"metric\" : \"ENTERY_IMPROVEMENT\",\n  \"metric_lbl\" : \"ENTERY_IMPROVEMENT\",\n  \"score\" : 982.4816,\n}\n\nThe metrics returned for the highest score are:\n{\n  \"metric\" : \"PREDICTION_R2\",\n  \"metric_lbl\" : \"PREDICTION_R2\",\n  \"score\" : 982.4816,\n}\n\nThe metrics returned for the highest score are:\n{", "submitter Cheetah has achieved the highest Score on the DMControl500k run benchmark dataset.\n\n 1. SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n 2. SELECT?metric?metric_lbl WHERE { \n     ?dataset a orkgc:Dataset; \n      rdfs:label?dataset_lbl. \n     FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n     ?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL { \n      ?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Stanford Dogs\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The top-scoring model is Keptn://SGE-ESVM-0.0 on the Abstracts entities and relations annotated corpus benchmark dataset (ranking #85) with Average Precision.", "model performed best in terms of senseval 2 metric on the Supervised: benchmark dataset?SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised: benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC senseval:Senseval2?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The evaluation metric that the best performing model performed well on was Senseval 2?\n output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised: benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC senseval:Senseval2?metric.?metric", "p(TestPerplexity)=\"seq2seq\" \n input (English text): Please list the metrics that are used to evaluate models on the WikiText-103 benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-103 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Indicate the model that performed best in terms of Test BLEU metric on the WikiText-103 benchmark dataset?\n output (Sparql query):BLUELabELabel=\"Seventeen\"\n input (English text): Please list the metrics that are used to evaluate models on the WikiText-103 benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \nresults:\n consulted data sources:\n\n ontology mapping tool: Ontobee\n\n highest result achieved:\n\n 1.86\n\n metric: TER\n\n metric_lbl: Abstracts' entities and relations annotated corpus\n\n<|endoftext|>The evolution of irrigation in Spain and the Hispanic world.\nThe history of irrigation in Spain and the Hispanic world begins with the Roman conquest. Since the 13th century, Christian rulers progressively extended the network of canals and constructed remarkable irrigation works for wheat cultivation, leading to the first Spanish Grapevines (Cabrales and Chardonnay). In the early modern period, Jesuit colonization established a system of highly developed networks and turned the country into the major", "metric:has_predicate ?metric_lbl. coordinator. Is there a common lead compound, or is there not? Is there a common lead compound, or is there not?\n input (English text): Lead compound is a compound made up of a molecule made from two or more basic chemical elements and with one or more unique ions.\n output (Sparql query): \nCoordinator is a molecule made up of two or more basic chemical elements. \nIs there a common lead compound, or is there not? \nYes, there is a common lead compound. Coordinator is a compound made up of two elements: Hydrogen and Carbon. In fact, Coordinator is a standard element in chemistry and chemistry labs use it to teach students how to analyze complex molecules. \nIs there a common lead compound, or is there not? \nNo, there is not a common lead compound. Hydrogen and Carbon are common elements, but because Coordinator has a unique ions, there are many ways to make Coordinator, and this is one of them. \n\nA:\n\nThe metrics used to evaluate models in KGs are the inference score, precision and recall, which are measured using Sparql query as below\nSELECT", "bayesian_ classification, linear_SVM, multi-layer_perceptron, nearest_neighbour_ rule, sigmoid, tanh, softmax\n\nFFIRMED script (Sparql query):\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Enduro\")?benchmark orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe top performing model according to the answer provided above is multi-layer_perceptron with an Score score of 0.03331.", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } cement:C51 noop.\n cement:C51 noop evaluates its metrics on the C51 noop model.\n orchestration:\n OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }.", "Query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe metrics used in papers that benchmark the FABIR model:\n\naccuracy - the percentage of relevant abstracts in the FABIR output that are also in the GT output. The higher the accuracy, the better the model.\n  \nprecision - the percentage of relevant abstracts in the FABIR output that are also in the GT output. The higher the precision, the better the model.\n  \nrecall - the percentage of relevant abstracts in the FABIR output that are in the GT output. The higher the recall, the better the model.\n  \nF1 - The F1 measure is the weighted average of precision and recall.", "output (Sparql query): Selecting?metric,?metric_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Emergency type of AMBER Alert\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): AMBER Alert is an emergency type of signal alert issued by police. It is for notification of someone in distress, typically a child. \n output (Sparql query):villeoutput (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Emergency type of AMBER Alert\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?", "input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a coqa:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a coqa:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp", "box (English text): OntoNotes benchmark dataset the model that performed best in terms of F1 metric is the TransfoGram model.\n\n\u00f4ngmentation (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"OntoNotes' benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Defender benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Defender benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?", "input (English text): https://github.com/clab/pointer-coverage-entailment-gen-question-gen\n\nHere is the answer: SELECT DISTINCT?metric?metric_lbl WHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n ?benchmark orkgp:HAS_DATASET?dataset; \n  orkgp:HAS_EVALUATION?eval. \n  OPTIONAL {?eval orkgp:HAS_METRIC?metric. \n          ?metric rdfs:label?metric_lbl. \n  } } \n\nThe following metrics are used on the benchmarks datasets: \n* QuestionGen-MRR \n* QuestionGen-R2MRR \n* QuestionGen-MAP \n* CoverageGen-MRR \n* CoverageGen-R2MRR \n* CoverageGen-MAP \n* EntailmentGen-MRR \n* EntailmentGen-R2MRR \n* Ent", "output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \ninput (English text): The Switch Transformer model is utilized in: \n - Model Inpainting on Tiny Images by Wang et al. (2019)\n - Incremental Neural Machine Translation with Ternary Connectives by Lu et al. (2019) \n - Fine-grained visual reasoning by Hanson et al. (2019) \n - Databricks\u2019 Neural MTA by Luiten et al. (2019) \n - ELMO by Paliwal et al. (2019) \n - Answer-Oriented Transformers by Liu et al. (2019) \n - Visual Question Answering by Tarsinkala et al.", "_set_1:SELECT * FROM <http://library.thinkovault.com/sparql/dmchcPASRS6X6w>) a WHERE { a :dmchcPASRS6X6w; FILTER (?benchmark = \"commonsenseqa\"). } \n_output (_list of Sparql queries):\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nI don't understand the part of the query where you are filtering the benchmark column based on the value \"commonsenseqa\". I tried that but it didn't work.\n\nA:\n\nCommon sense question assessment (commonsenseqa) is a sub-corpus of CommonsenseQA dataset", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2 dataset\")?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval :hasScore?eval. } } \n input (English text): The abstracts' entities and relations annotated corpus (ERA) corpus benchmark dataset contains the following three datasets that are evaluated using the precision, recall and F1 score: precision, recall, and f1 score.\n\n  - precision: the number of true positive abstracts, divided by the total number of true positive and false negative abstracts\n\n  - recall: the number of true positive abstracts, divided by the total number of true positive and false negative abstracts\n\n  - f1 score: 2 * precision * recall / (precision + recall)\n\n  - precision : the number of true positive abstracts, divided by the total number of true positive and false negative abstracts\n\n  - recall : the number of true positive abstracts, divided by the total number of true positive and false negative abstracts", "fil (Sparql query): SELECT?metric_lbl?metric WHERE { \n#!/usr/bin/env owl-case \nSELECT DISTINCT?metric?metric_lbl WHERE { \n?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. \nFILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; \norkgp:HAS_EVALUATION?eval. \nOPTIONAL {?eval orkgp:HAS_METRIC?metric. \n?metric rdfs:label?metric_lbl. \n} \n} \n\nHow I get the code links from the paper? It's very difficult to manually review all the code links from the paper because there are a lot of paper and the code links are buried in the paper or even in the appendix. But there are some pointers below: \n 1. If the model is A3C-CTS or variants (such as A3C-RTT, AC3C, and A3CR), there is a section", ":Hutter Prize\r\n\r\n\r\n\r\nSELECT DISTINCT?metric?metric_lbl WHERE { \r\n?dataset a orkgc:Dataset; \r\n rdfs:label?dataset_lbl. \r\n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \r\n?benchmark orkgp:HAS_DATASET?dataset; \r\n orkgp:HAS_EVALUATION?eval. \r\n OPTIONAL { \r\n?eval orkgp:HAS_METRIC?metric. \r\n?metric rdfs:label?metric_lbl. \r\n } \r\n } \r\n\r\nRelated Blogs: https://www.algov.at/blog/2017/02/28/who-wants-to-bake-a-cake-to-the-hutter-prize.html; https://algov.at/blog/2018/02/12/sparql-query-for-querying-open-kb.html.\r\n\r\n\r\n\r\nThe top benchmark score and its metric on the Hutter Prize dataset is: \r\n  \r\nHutter Prize  Relational Algebraic Query", "inal (SPARQL 1.1 query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2003 (English)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (SPARQL 1.1 query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoNLL 2003 (English)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n output (English text): F1 score\n output (Sparql query): The model that achieved the highest F1", ":ceptionNET[https://www.cloudsparql.org/public/query/view?q=.query(\"SELECT distinct?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }]", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Artificial Intelligence\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval_ID:ID?eval;?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Artificial Intelligence.\n output (Sparql query):\n              output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label \"Artificial Intelligence\"@en. FILTER (str( \"Artificial Intelligence\"@en) = \"Artificial Intelligence\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EV", "training model : SPARQL Dataset RETRIES for Semantic Web Applications (SWReT) for NLP in Databricks (2018) model (no sensitive information provided) Evaluated on the SPARQL Dataset with the following metrics : Precision, Recall, F-measure, AUC\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"SWReT\") OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>[The increasing need of speech language pathology in Slovenia].\nThere is a growing need of speech language pathology in Slovenia. Between 2007 and 2013, the number of speech language pathologists increased by 30% whereas the number of Slovenian residents increased by only 12%. The reason for the increase is the declining birth rate and the ageing Slovenian population. As of 2013, there are approximately 8,000 speech language pathologists, which is about 10% of the Slovenian population of approximately 7.4 million. Speech language pathology should be provided", "output (Sparql query): BLEU\n\n input (English text):The best performing model is the model benchmarked JozefKNOLBASH@illinois.edu, which achieved an BLEU score of 44.31\n\n input (Sparql query): 44.31\n\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nA:\n\nThe best performing model is the JozefKNOLBASH@illinois.edu model with an BLEU score of 44.31.\n\n<|endoftext|>Q:\n\nMigrating web app from Subversion to Git\n\nI have to migrate a system with SVN into Git. The system is composed of around 30", "sparql ( http://www.w3.org/TR/sparql11-overview/ )\n output (Sparql query): SELECT DISTINCT?metric_lbl?metric_str WHERE { \n    ?dataset a orkgc:Dataset; \n     rdfs:label?dataset_lbl. \n     FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n    ?benchmark orkgp:HAS_DATASET?dataset; \n     orkgp:HAS_EVALUATION?eval. \n     OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n } \n output (Sparql query): SELECT?metric_str?metric_str_label WHERE {\n    ?dataset_lbl a dcm:Dataset.\n    ?metric_str rdfs:label \"Bleu\".\n    ?dataset_lbl dcm:Dataset =?benchmark.\n    ?eval o act:EvaluationMetric?metric.\n     OPTIONAL {?eval act:BenchmarkName?benchmark_", "mark-extractor-opennlp-2017-benchmark  <-- this is the best result achieved for the highest metric on the STS Benchmark dataset (8).\n input (English text):The highest result achieved for the metric Abstracts' entities and relations annotated corpus on the STS Benchmark dataset is 8.\n\n Is it possible to break this output into individual lines?\nPlease note that this is in English, hence I am not including punctuation and spaces.\n\nThe highest result achieved for the metric Abstracts' entities and relations annotated corpus on the STS Benchmark dataset is 8.\n\nA:\n\nThe DISTINCT will give you a list of distinct values. This will only work if there are multiple values for a specific metric, as you could have in your example output. If there is only one value for a specific metric, then this part of the response will be replaced with an empty list:\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp", ":SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): Metrics used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset include:??benchmark-entity-count: counts the number of benchmarks with an entity in them?benchmark-relation-count: counts the number of benchmarks with a relation between them?benchmark-label-frequent: calculate the number of times a specific label is used?benchmark-property-frequent: calculate the number of times a specific property is used?benchmark-subquery-count: count of subqueries used in benchmark?benchmark-materialized-view-count: count of materialized view in the benchmark?benchmark-foreign-", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field are:\n\n* BioASQ\n* CK-Penetrating Dozen\n* CoNLL 2003 relations with a named entity\n* CoNLL 2004 relations with a named entity\n* DSP2010 relations with a named entity\n* CoNLL 2009 Core NER with named entities\n* CoNLL 2009 Core relations with named entities\n* CoNLL 2009 relations without named entities\n* CoNLL 2010 relations without named entities\n* CoNLL 2010 named entity classes (with names)\n* CoNLL 2010 named entity classes (with names) with relations\n*", "ment: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiRC dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The MultiRC dataset contains relations between entities that have been annotated with a metric. The metric is an attribute that contains a numerical value for that relation. Metrics typically contain a triple (type, entity1, entity2), where entity1 and entity2 are the two entities in the relation and type is the type of relation.", "input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?\n output (Sparql query): INSERT urn:uuid:4c1e2c64-a6bd-4d30-be80-ae0f0d4bd34b. urn:line:arrow:0a0b080a-0faa-49ef-b332-b5cae9b90844 \noutput (Sparql query): SELECT * FROM datahub_loader::SparqlResult WHERE has_loader_row=true\n\n\r\nSome of the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are as follows: \r\n\r\n\r\n\r\n* precision \r\n * recall \r\n * f-score \r\n * mdr \r\n * avg_per \r\n * avg_recall \r\n * avg_fscore \r\n * mdr_micro \r\n * mdr_macro \r\n * avg_mdr \r\n * avg_mdr_macro \r\n * avg_avg_per \r\n * avg_avg_recall", "Sparql query: SELECT * FROM query WHERE \nsubject = :dataset\nand\nobject=:eval \nand\npredicate=:benchmark\nand\nlabel=:dataset_lbl \nand \nvalue::text contains \"Score\" \nORDER BY :metric_lbl", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): WMT2014 German-English\n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe correct answer would be:", "mark:@score\n output (Sparql query):SELECT DISTINCT?eval WHERE {?eval orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"Benchmarking a model on SearchQA dataset\") }\n\ngs\u00e5mark:@score\n\n facilitating the access to knowledge, explaining complex concepts and facilitating the sharing of knowledge are core to the essence of what it means to be human. \n this notion of the human-machine interface is examined from a critical perspective in this article. first, it is suggested that the critical conception of the human-machine interface demands an interpretation of the term with respect to how a human interacts with technology. then, the article suggests that technological advances have helped create a technically enhanced version of the human. however, this newly enhanced human, the so-called cognitive surplus humanity, does not necessarily possess the qualities of curiosity, empathy, and creativity. in addition, it is suggested that this new version of the human is more accurately described as a version of the human that has been augmented, rather than enhanced. Ultimately, it is suggested that understanding the nature of the human-machine interface is central to understanding the 21st century and the challenges and opportunities that await us.\n\nCentral", "Input (English text): What are the metrics of evaluation over the OntoNotes dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"Ontology-based named-entity recognition and relation annotation\") OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n output (Sparql query):\n              Output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"Ontology-based named-entity recognition and relation annotation\") OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Ontology-based named-entity recognition and relation annotation is performed over the OntoNotes dataset using the RNN model. What", "set (a orkgc:Dataset) has rdfs:label \"Abstracts' entities and relations annotated corpus\" ;\n\n counsel (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n counselor (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?", ";SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The WMT2014 English-German dataset contains two relation annotations, including the Affect relation and the Verb relation. For each relation, there is a metric which is the highest result achieved on the WMT2014 English-German dataset.\n\n valign;SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?", "ication of the metrics that are used to evaluate models on the PWC Leaderboards' (restricted) dataset?\n\n\nSELECT DISTINCT?metric?metric_lbl\n\n\nThe following metrics are used to evaluate models on the PWC Leaderboards' (restricted) dataset:\n\n* Area Under the Curve (AUC) - accuracy measurement\n* F1 Score - precision * recall / ( precision + recall )\n* Matthews Correlation Coefficient (MCC) - accuracy * precision * recall - accuracy * precision * recall - 0\n\n\nThe titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset are as follows:\n\n\ntitle                                                                                                              id\n-------------------------------- ----------------------------------------------------------------------- ---------------\nPrestigious Web-based Databases for Affordability and Usability Test Benchmarking, IFC Lab, Stanford University   3042\nBenchmarking and its Role in Web Development: A Practical Perspective, Florian Zeller and Kristina Tsvetkov,    3041\nAutomatic metric discovery for web applications, Julia Haupt, Daniel Aronica, Nicolas Serazin, Florian Zeller,  3040\nMetric-driven benchmarking in IT-related", ":DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"enwik8 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3 dataset\")?metric;?metric rdfs:label?metric_lbl. }\n\n personnel: Anoop Gupta, Junyoung Chung, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet, Yandong Han, Andrea Lucchese\n\n personnel: Anoop Gupta, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet, Yandong Han, Andrea Lucchese\n\n personnel: Anoop Gupta, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet, Yandong Han, Andrea Lucchese\n\n personnel: Anoop Gupta, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet", "imetry are the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The most common location in the studies is the metric of precision. \n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): There are several metrics utilized in the papers that we have come across including BLEU, perplexity, word relevance, and F1 score. These metrics are described below.\n output (Sparql query):?metric BLEU perplexity word relevance F1 score?metric_lbl \n output (Sparql query):There are several metrics utilized in the papers including BLEU, perplexity, word relevance, and F1 score. \n input (English text): The papers we have used the BCN+ELMo model includes papers authored by Microsoft, Facebook, and Google?\n output (Sparql query):?metric BLEU", "answer SELECT * FROM extracted_results WHERE extraction_type ='pre_training_dataset_metric_on_hmdb51' and score > 99% ORDER BY rank DESC LIMIT 5;\n\nPre-training dataset metric on the HMDB51 benchmark dataset performed best:\n\nThe model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset was FBNET. The model's rank is 5. FBNET consists of 56 neurons in the fully connected layer and 54 neurons in the activation layer. The number of trainable weights in the fully connected layer is 56*54 = 3072. FBNET took 13,623 seconds to finish the task. The second-best model was MLNet which consists of 53 neurons in the fully connected layer and 52 neurons in the activation layer. The number of trainable weights in the fully connected layer is 53*52 = 2664. MLNet took 12,758 seconds to finish the task.\n\n     \nHere is a visualization of the best and second-best models. The best model is on the left and the second-best model is on the right:\n\nIcons provided by Geraldo Miranda are licensed under CC BY SA 3.0.", "mit, 5 seconds at 12 kHz, model evaluation, Sparql Query answer: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): 5 seconds at 12 kHz is used to evaluate the models on the Classical music dataset.\n output (Sparql query): Dillon, 5 seconds at 12 kHz, model evaluation, Sparql Query answer: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classical music dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL", "input (English text): The best BLEU score on the German-English benchmark dataset was obtained by the neural model proposed by @denkowski:2015:GraSL. The corresponding Sparql query is:\n\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n output (Sparql query):The best BLEU score on the German-English benchmark dataset was obtained by the neural model proposed by @denkowski:2015:GraSL. The corresponding Sparql query is:\n\n output (Sparql query):SELECT DISTINCT", "are the metrics used to evaluate models on the BC5CDR-disease benchmark dataset?\n\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The F1 metric was the best in the evaluation of models on the BC5CDR-disease benchmark dataset\n\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The evaluation metrics commonly used on the Atari 2600 Skiing dataset are: overall completion, success rate, total time, rank, and average rank.", ": SELECT?metric?metric_lbl?dataset_lbl_str WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL {?dataset orkgp:Benchmark?benchmark;?benchmark orkgp:HAS_DATASET?dataset;?dataset orkgp:HAS_EVALUATION?eval.?benchmark orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): Abstracts' entities and relations annotated corpus (benchmark over the Frostbite dataset), The title and ID of the benchmark over the Frostbite dataset are:\n Benchmark over the Frostbite dataset Abstracts' entities and relations annotated corpus (benchmark over the Frostbite dataset), Frostbite is an open-source game engine that generates high-fidelity 3D models from 2D images. This benchmark tests the ability of a program to answer questions about a given knowledge graph. The performance of an answer extraction program is measured by the precision,", "inite (Sparql query): SELECT * FROM http://www.sailsgps.com/datasets/bdb-comet-direct-model/ WHERE isCoverage = true AND Dataset = :dataset_id\n input (English text): The code that provides the model and scores can be found at the following link:\n link to code (Sparql query):\n http://www.sailsgps.com/datasets/bdb-comet-direct-model/\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity\n\n solidarity", "output (Sparql Query Results): Databricks OS: CentOS 7.4.1903\\\n Apache Spark: 2.1.4\\\n Common Preconditions: 2.1.4\\\n SQL:  Oracle 12c, PostgreSQL 10.4\\\n JVM: Java(TM) SE Runtime Environment 12.0.1\\\n Java HotSpot(TM) 64-Bit Server VM 22.0-b10\\\n Scala: 2.11.6\\\n Spark: 2.1.4\\\n SBT: 0.13.17\\\n Seq: 2.1.4\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n ----------------------\n Output:", "3043 metrics with a value of 0.8941.\n\nThe SELECT DISTINCT?metric?metric_lbl returns the following results:\n?metric rdfs:label \"p\":* \"Value\"\n?metric_lbl rdf:label \"Evaluated on Atari 2600 Boxing\"\n\n<|endoftext|>Quirky Places: The Wolpertilion in the Desert\n\nThe Wolpertilion in the Desert\n\nThe Wolpertilion in the Desert is an important prehistoric site located in the desert region of the State of Guanacaste, about 30 km south of the city of Tobalanzas.\n\nThe prehistoric site is one of the largest and most extensive containing a number of structures built by the ancient prehispanic Indians, called Chichen Itza. The site covers an area of 110 hectares, and includes three main main buildings, two wells and a satellite cemetery.\n\nThe main structures date back to the prehispanic period, with the first structures built during the Classic period between 200 BC and AD 300.\n\nThe buildings are arranged in a circular formation, the function of which is unknown. All of the structures are made of stone and wood, although the structures closest to", "2_test_eval_f1_score\n\nA sample query response: \nSELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Penn Treebank benchmark dataset\") \n   ?benchmark orkgp:HAS_DATASET?dataset; \n    orkgp:HAS_EVALUATION?eval. \n OPTIONAL {?eval orkgp:HAS_METRIC?metric. \n            ?metric rdfs:label?metric_lbl. \n             } }\n\n<|endoftext|>Genetic structure of Colombian accessions of Phytophthora infestans revealed by RAPD markers.\nThe genetic structure of 19 US field-derived isolates and 25 Colombian reference accessions of Phytophthora infestans was assessed by randomly amplified polymorphic DNA (RAPD) markers. The Colombian accessions were collected from lowland fields in the Cundinamarca and Santander departments in 1991 and 1992. The RAPD profiles generated by primers B8 (5'-GACGACGATCAGTC", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The highest score is 79.7. The IWSLT2015 German-English dataset has been marked as difficult. The precision is 45.7% and the recall is 50%. Thus, the model has performed well in this dataset.\n output (Sparql query): orchestration-samples:Query Result for metric and metric label \"precision\" for the IWSLT2015 German-English dataset\n\nThe top 5 metrics returned are:\n# metrics                                                                                                                                               # score\n1. precision  45.7%                                                                                                                                   \n2. recall     50%", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Frostbite\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics that are used to evaluate models on the Atari 2600 Frostbite dataset are:\n - winrate\n - final score\n - per-frame score\n - one-shot learning\n - last seen frame\n - mean squared error\n - gaussian mixture model\n - recurrent encoder", "Query\n SELECT *\n  WHERE {?dataset a multiNLI orkgp:HAS_DATASET?dataset.\n   ?eval orkgp:HAS_EVALUATION?eval. \n   ?metric?metric_lbl.\n   ?metric rdfs:label?metric_lbl. \n    FILTER (str(?metric_lbl) = \"top-accuracy\") }\n\nTo answer the question, the top benchmark score and its metric on the MultiNLI dataset are: \ntop-accuracy and metric {multiNLI top-accuracy}", "answer: SELECT?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\") } \n input (English text): Yes, here are the models that have been evaluated on the ImageNet ReaL dataset: ResNet-50, DenseNet-121, and DenseNet-201.\n\nordable model: ResNet-50\n Evaluated on: ImageNet ReaL\n\n affordable model: DenseNet-121\n Evaluated on: ImageNet ReaL\n\n affordable model: DenseNet-201\n Evaluated on: ImageNet ReaL", "textrm (Sparql query): SELECT * WHERE {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The highest Accuracy achieved on the benchmark dataset is achieved by the model which achieved an accuracy score of 87.06. This model is called Model 1.\n\n\n                       view (Sparql query): SELECT * WHERE {?dataset a orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The highest Accuracy achieved on the benchmark dataset is achieved by the model which achieved an accuracy score of 87.06. This model is called Model 1.\n\nModel 1: orkgp:accuracy (double precision) 87.06", "scoring 100.0 for the Kinetics-600 dataset: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The Kinetics-600 dataset scores 100.0. The metric for this dataset is Identification Accuracy and its score is 100.0.\n\n\f<|endoftext|>Some artists will tell you they love doing what they do. They like the specific constraints of their chosen medium; the paycheque that keeps their head above water; the accessibility of the medium to a wide variety of people; the ability to put their vision on the page; the predictability of the output.\n\nWhen a book I wrote came out a few years ago I was thrilled to discover that there were people who didn\u2019t just tolerate my chosen medium \u2014 they", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-5 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n {#sparql}\n@metric_lbl a <https://github.com/BartNote/BART.sparql#label> https://github.com/BartNote/BART.sparql#label \n {#metric}cInferred class,Inferred class,Perplexity,Dissimilarity,Hamming loss,Hamming loss%,Perplexity%,Dissimilarity%,Hamming loss%\n\n {#dataset}a orkgc:Dataset\n {#dataset_lbl Abstracts' entities and relations annotated corpus\n {#benchmark BART\n\n {#metric}cInferred class,In", "inal (English text): The highest benchmark score (accuracy) and the corresponding metric is 0.93 and it is the AVG precision metric.\n input (Sparql query):SELECT * FROM <benchmark_collection> WHERE has_dataset == true AND has_evaluation == true AND benchmark->'HasDataset' == true AND benchmark->'HasEvaluation' == true AND avg(metric) == 0.93\n\nSee also\n Entity relation classification\n KDD Cup 1995, a related competition\n SQuAD, a related dataset and associated OpenAI GPT model\n Abstracts' entities and relations annotated corpus\n\nReferences\n\nExternal links\n \n\nCategory:Knowledge transfer\nCategory:Semantic_modeling\nCategory:Natural language processing<|endoftext|>Articles for Sale\n\nThese handsome Royal Oak Vises are precision made from aircraft grade aluminium for years of reliable use. Sturdy steel stays and levers means your vise will hold even the most stubborn of jaws. The 3.5\" face plate allows extra wide jaws to be clamped down for extra stability. The pre-drilled holes make mounting this Vise a snap.\n\nOur vices are crafted from heavy gauge, aircraft grade aluminum for years of reliable", "metric_lbl\n output (English text): SELECT DISTINCT?metric_lbl?metric_label WHERE { {?metric rdfs:label \"accuracy\"}.?metric_lbl?metric; {\n?dataset a orkgc:Dataset;\n?dataset_lbl a cifar10:Sequential;\n?benchmark orkgp:HAS_EVALUATION?eval;\n?eval orkgp:HAS_METRIC?metric;\n?metric rdf:type?metric_type;\n\n } OPTIONAL {?eval orkgp:HAS_EVALUATION?eval;?eval orkgp:HAS_METRIC?metric; } }  \n\n<|endoftext|>\ufeff<UserControl x:Class=\"ArcGISRuntime.WPF.Application.Pages.ArcGISDynamicMapLayerControl\"\n    xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n    xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\"\n    xmlns:esriUI=\"http://schemas.microsoft.com/winfx/2006/xaml/ite", "ition (Sparql query): The top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset are  47.968 and F-score 58.21. The F-score metric evaluates the model on the precision and recall aspect of the Metric. The F-score is computed as 2 * precision * recall / (precision + recall).\n\nNote: The F-score metric for a model is computed as 2 * precision * recall / (precision * recall + false positive * false negative).\n\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n\nAnswer: F-score 58.21\n\n\nThe F-score metric for a model is computed as 2 * precision * recall / (precision", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following metrics are used to evaluate models on the datasets and evaluation task in the benchmarking dataset:\n\n* Abstracts' entities and relations annotated corpus\n* Support-vector machine (SVM)\n* Decision Trees\n* Logistic Regression\n* Neural Network\n* Hinge Loss\n* Maximum Softmax Classification Error\n\n\nName the datasets that have been used for benchmarking in the citation classification research problem?\n\n Orkgp:HAS_DATASET abstracts; Orkgp:HAS_EVALUATION citationclassification; Orkgp:HAS_METRIC abstractsmetric; Orkgp:HAS_EVAL metric; Orkg", "n:SELECT * WHERE {?s?p?o }\n input (English text): https://www.aclweb.org/anthology/D/D17/D17-1171.pdf\n output (Sparql query):(aclweb.org/anthology/D/D17/D17-1171.pdf)\n\n traject:SELECT * WHERE {?s?p?o }\n traject:pageContent ${page_contents}\n\n traject:pageContent \"Dataset Schema\"{\n traject:select?dataset (?dataset orkgp:HAS_DATASET?dataset )\n traject:with?dataset_lbl (?dataset_lbl orkgp:HAS_LABEL )\n traject:of?dataset\n traject:order by?dataset_lbl?dataset_lbl\n\n traject:pageContent \"Dataset Annotated with NLI Metrics\"{\n traject:select?dataset (?dataset orkgp:HAS_EVALUATION?eval )\n traject:of?dataset\n traject:where {?eval orkgp:HAS_METRIC?metric }\n\n traject:pageContent \"Evaluations and Results\"{\n traject:let?", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nCONTRIBUTOR", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Q:\n\nWhen i create hash with data[1] and data[2] why data is not saved as hash?\n\nWhen i create hash with data[1] and data[2] why data is not saved as hash?\nmy_hash = {key1: value1, key2: value2}\n\nmy_hash.keys.each do |key|\n  data =  my_hash.send(key)\n  puts data[1] #not works!\nend\n\nwhy this not works?\nIn ruby 1.9.3 this line\ndata =  my_hash.send(key)", "fengine model with metric 'negative log likelihood'\n\n\nWhich metric is used to evaluate the  'CounterSketch' model?\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CounterSketch\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe 'negative log likelihood' metric is used to evaluate the 'CounterSketch' model.\n\n\nThe 'negative log likelihood' metric is used to evaluate the 'CounterSketch' model.\n\n\nHow do you know which metric is used to evaluate the 'CounterSketch' model?\nSELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CounterSketch\" )?benchmark orkgp", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): In the code references section of the paper, one can find the following code:?eval.CONCAT_WP(?,?)?metric. The?metric function has the following signature:?metric(datatype, value, time, measure, options), where options are passed in as a map. The measure parameter takes a value of string. So, it looks like this function is used to concat two strings. \n\n<|endoftext|>The invention relates to a rotary position sensor having at least one rotor supported for rotation about a rotation axis in a housing and a magnetic field generation means for generating a magnetic field directed axially into the housing", "set(a), hasEvaluation(eval)\n input (Sparql query): \nThe best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric is the iNaturalistSpeech 2018 model\n\n COMMAND : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist' 2018 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n SELECT?metric?metric_lbl\n\n?metric : Top-1 Accuracy\n\n?metric_lbl : Abstracts' entities and relations annotated corpus\n\n COMMAND :\n\n?benchmark : iNaturalistSpeech 2018\n\n?eval : iNaturalistSpeech 2018\n\n?dataset : orkgc:Dataset\n\n?dataset_lbl : Abstracts' entities and relations annotated corpus\n\n COMMAND :\n\n?dataset : orkg", "provide score 4 of 5 with air-to-net dot product as the metric:\n\n import datasetIceHockey;\nDataset dataset=null;\nEvaluation eval=null;\nRelation benchmark=null;\nEvaluationMetric metric=null;\nResultSet results=queryresults();\n\nresults=execquery(\"\"\"\nSELECT DISTINCT?metric?metric_lbl\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ice Hockey\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\"\"\");\n\nif (results.size()>0)\n{\nfor (Result r : results)\n{\nmetric=r.get(\"metric\");\nmetric_lbl=r.get(\"metric_lbl\");\n}\n}\n\ngs\u00e5nt result,rank=0;\nfor (Result r : results)\n{\nif (r.", ", the datasets related to the Scientific Results Extraction research area are: Abstracts' entities and relations annotated corpus, Tendermint benchmarks, Tangle, Hyperledger Finnish Quorum Testing.\n input (English text): The Abstracts' entities and relations annotated corpus benchmark datasets are gathered from KGs where the data is associated with the CRESTontology, the Metrics are the RIF, RIF-lite and the evaluation procedure is the human judgement evaluation.\n\nList the metrics that are used to evaluate models on the datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets?\n\n\nThe following metrics are used to evaluate models on datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets: RIF, RIF-lite, Human judgement evaluation.\n\n\nThe following metrics are used to evaluate models on datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets: RIF, RIF-lite, Human judgement evaluation.\n\n\nThe RIF (redundancy elimination factor), RIF-lite (reduced redundancy elimination factor), Human judgement evaluation metrics are the metrics that are used to evaluate models on datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets.\n\n\nThe metrics R", "Example 1: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n The titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset are: \n  \nBrett Leech, Andrew Fox, Paul Webber, and Will Jennings. \"Large CoNLL-2010 Cross-Documentrep Cross-Linguistics Shared Task: The Abstracts\u2019 entities and relations annotated corpus benchmark.\" ACL 2010.\n\t\n  \nhttp://www.aclweb.org/anthology/D/D10/D10-1036.pdf\n\n  \nhttp://aclweb.org/anthology/D/D10/D10-1036.pdf\n\n  \nhttp://www.acl", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Semantic Role Labeling research area\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n for the Sparql query provided above, the results would be:\n metric                  metric_lbl    \n--------------------------- --------------\n F1-score                  Semantic Role Labeling research area\n\n for the dataset, the metric would be F1-score\n\n for the evaluation, the metric would be F1-score\n\n<|endoftext|>Shooter (board game)\n\nShooter is a tabletop game by German studio Chemin Carbon published in 2017. It is a cooperative war game set during the African Campaign of the Second World War and pits the Western Allies and Axis Powers against one another in a struggle for Europe. Like many war games, Shooter is set in the imaginary world of \"War", "output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"WikiText-2 dataset\" \n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text):", "ition (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Metrics of evaluation over the PWC Leaderboards (restricted) dataset are : precision, recall, f-score, macro-f1. \n output (Sparql query): Serve the following to the Sparql query processor: precision?metric_lbl, recall?metric_lbl, f-score?metric_lbl, macro-f1?metric_lbl \n input (English text): precision : a metric which shows the fraction of correct predictions for each label (i.e. whether a label is in the positive or negative set). \n input (English text): recall : a metric which shows the", ":LINK {\"URI\": \"http://dueling.com/duel-no-op/docs/dueling-no-op-model-in-bennett-tags.html\", \"TEXT\": \"Copyright 2012 Duel.com, LLC\" } orchestrate resolveObjectURL in (select?link where {?link dbo:link.}) // added orchestrate in select to make sure that each result has the same schema (url/text)\n input (English text): http://dueling.com/duel-no-op/docs/dueling-no-op-model-in-bennett-tags.html\n\n orchestrate resolveObjectURL in (select?link where {?link dbo:link.}) // added orchestrate in select to make sure that each result has the same schema (url/text)\n\nSELECT DISTINCT?metric?metric_lbl WHERE { \n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n ?benchmark orkgp:HAS_DATASET?dataset", ":Abstracts' entities and relations annotated corpus\n\n 2. SELECT DISTINCT?metric?metric_lbl\n\n 3. WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fine-Grained Image Classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n 4. SELECT DISTINCT?metric?metric_lbl\n\n 5.?metric  rdfs:label \"Abstraction distance\"\n\n 6.?metric_lbl Fine-Grained Image Classification\n\n 7. 0 Abstraction distance\n\n 8. 0\n\n 9.?eval orkgp:HAS_METRIC?metric\n\n 10.?eval orkgp:HAS_METRIC?metric\n\n 11. Abstraction distance\n\n 12. Fine-Grained Image Classification\n\n 13. rdfs:label \"Abstraction distance\"\n\n 14. 0.005  Abstraction distance", "{?metric?metric_lbl } \nresponse: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nA:\n\nThe evaluation metrics that are commonly used when benchmarking models on the DRI Corpus dataset are:\n\nPrecision, recall, and F1 score.\nMacro F1 score.\nTf-Idf.\nHamming distance.\nRoot mean square error.\nMean absolute error.\nPearson's correlation coefficient.\nEnergy of a Loss Function.\n\nThe following links provide information on those metrics:\n\nTable of Databricks' evaluation metrics.\nUnderstanding the Databricks Tf-Idf Loss Function.\nEnergy of a Loss Function.\nMore", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC", "mark:DeepDiveRE. It is an unpermuted accuracy score of 81.73% obtained by running the sequential architecture on CIFAR-10 using the ReLU activation function.\n\n,\n The following metrics are used to evaluate models on the Sequential CIFAR-10 dataset when benchmarked on the Orcpack CIFAR-10 dataset: \n\n- Unpermuted Accuracy score\n- Top-1 Accuracy score\n- Top-5 Accuracy score\n- Top-10 Accuracy score\n- IoU score\n\n\nNOTE: The metrics used to evaluate the model in the Orcpack CIFAR-10 dataset benchmark dataset are computed using a different model: DeepDiveRE.\n\n\nNOTE: The Unpermuted Accuracy score is computed based on the following SPARQL query:\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential CIFAR-10\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPT", "statement: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STL-10 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval stlp:metric {?metric.?metric_lbl stlp:metric_label }. } } \n input (English text): The Abstracts' entities and relations annotated corpus dataset has two evaluation metrics namely, Mean Absolute Error (MAE) and Mean Square Error (MSE). \n The STL-10 dataset is used in different research papers and it has metrics such as Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, Mean Squared Logarithmic Error, Squared Euclidian Distance, and Cosine Similarity.\n The STL-10 dataset is available in Open accessed data repository called Figshare. \n https://figshare.com/articles/stl_10_corpus_", "SELECT DISTINCT?metric_lbl?metric_text?metric_label WHERE { {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } *?metric rdfs:label?metric_label. FILTER (str(?metric_label) = \"Soft Assess\" OR str(?metric_label) = \"F1 Score\") } } \n\nThe returned values would be:\n metric_text    :F1 Score\n metric_label   :F1 Score\n\n metric_text    :Soft Assess\n metric_label   :F1 Score\n\n metric_text    :Precision\n metric_label   :F1 Score\n\n metric_text    :Recall\n metric_label   :F1 Score\n\n metric_text    :TPR\n metric_label   :", "output: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Skiing\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics that are used to evaluate models on the Skiing dataset are: \n - Average elapsed time (milliseconds)\n - Accuracy (percentage)\n - Average number of branches traversed\n - Number of jumps cancelled\n\n Don't worry, you were never that good at skiing. :)\n\n\nSkiing dataset: \n\nThe Abstracts' entities and relations annotated corpus dataset has been benchmarked on the Skiing dataset. \n\nHere are the research paper IDs and titles that have benchmarked models on the Skiing dataset: \n+--------------+---------------------------------------------------------------+\n| Research paper|  Title                                                         |\n+--------------+---------------------------------------------------------------+\n|", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The VTAB-1k dataset contains metrics for entity overlap (%), relation overlap (%), and average number of edges in the augmented transitive reduction of the subqueries associated with each KB query. The three metrics are:\n Entity Overlap: The number of entities that appear in both the ground-truth KB and the predicted KB for a given pair of entities.\n Relation Overlap: The number of relations that appear in both the ground-truth KB and the predicted KB for a given pair of entities.\n Average Number of Edges: The average number of edges in the augmented transitive reduction of the subqueries associated with each KB query.\nFor a score", ":Scorer\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\") } \n input (English text): Score is an entity which is a metric used by the model to evaluate its performance.  Cart Pole dataset has been successfully benchmarked against 3 models namely : Glow, GAN and AITS and the model with the best score is Glow.", "ition (Sparql query): SELECT DISTINCT?metric_lbl?metric_lbl_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): https://github.com/ibrs-dbp/rfa-gate-arccos_benchmarks. Clicking \"Benchmarks\" in the repository will take you to the table of code, in that table you will find that this model is used in the Abstracts' entities and relations annotated corpus benchmark.\n\nurches_lbl (label):Abstracts' entities and relations annotated corpus\nmetric_lbl (label):arccos\nmetric_lbl_lbl (label):arccos", "~ SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval (fn:metric(?metric_lbl)).?benchmark (fn:fn(fn:metric(?metric_lbl))) orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): F1-score metric\n\n<|endoftext|>Introduction\n============\n\nIt is well-known that tumor cells express high level of specific markers. The presence of these markers enables the tumor cells to escape from the host's immune system and to colonize and proliferate within tissues and organs. Therefore, tumor makers are indispensable tools for diagnosing and understanding the behavior of cancer cells. Currently, several tumor makers are available for clinical use, among which, tumor markers assayed using the", "respond (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BioASQ\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): BioASQ is a RDF dataset provided by the AMIE Challenge participants. It contains triples of the form <relation,subject,predicate> where the <relation> is a named-graph predicate, the <subject> is a named-graph object, and the <predicate> is a natural language string describing the purpose or relationship of the subject to the relation. <relation> is sometimes called a \"functional relation\" or a \"semantic relation\" in semantic web and related areas. The predicate can have multiple arguments: in the following example, \"modifies\" is a relation, \"gene\" is the subject, and \"protein\"", "_name Score\n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Centipede\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>A new approach to timing-resolved transillumination imaging using array detection.\nWe present a method for acquiring time-resolved transillumination images using an array detector. The system works by modulating the incident illuminator with a sequence of temporally separable illumination patterns and then detecting the modulated pattern with a single frame array detector. Using an array of successive approximation-register machines, the pattern is reconstructed by convolving the pattern with a sequence of filter banks, each appropriate for a particular frequency band of the detected pattern. The parameters of the filter banks are optimized to yield the sharpest representation of the illumination pattern. The temporal resolution of the reconstructed pattern is set by the duration of the illumination", "text (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n output (Sparql query): SELECT?metric?metric_", "-output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Classic dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The Dataset dataset in Classic has the label \"Classic dataset\". What metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):\"?eval. HAS_METRIC?metric\" \n input (English text): The Dataset dataset in Classic has the label \"Classic dataset\". What metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):\"?metric_lbl\" \n input (English text): The Dataset dataset in Classic has the label \"Classic dataset\". What metrics are used to", "ment (Sparql query): SELECT?metric { \t?eval orkgp:HAS_METRIC?metric. }\n input (English text): The DeiT-Ti model has been used in papers published by Cao, Chen, Qu, Wang, et al. \nThe DeiT-Ti model can be found here:\nhttps://github.com/DevTijdeo/DeiT-Ti/blob/master/docs/DeiT_Datasets.ipynb\nPlease refer to the paper here:\nhttps://github.com/DevTijdeo/DeiT-Ti/blob/master/docs/Paper.ipynb", "ermost prevalence of epilepsy is the highest for the country of Estonia.\n\nAnswer: uppermost prevalence of epilepsy is the highest for the country of Estonia.\n\nThe following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\n- metric - the metric name (e.g. token count)\n- metric_lbl - the label for the metric (e.g. token count)\n\nA:\n\nYour question does not ask which metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset, but rather which metrics are used to evaluate models on a specific dataset.\nSo, it seems you are looking for the following metrics, which can be found in the Sparql query SELECT DISTINCT?metric?metric_lbl:\nSELECT DISTINCT?metric?metric_lbl WHERE {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n ?benchmark orkgp:HAS_DATASET?dataset;\n  orkgp:HAS", "ine the query result to identify the correct metrics and their corresponding labels. SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe query result is as follows:\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", ": SELECT DISTINCT?model_name WHERE {?model_name rdfs:label \"MODELNAME\"@en. FILTER (str(?model_name) = \"HMDB51\")?model_name_en; FILTER (?model_name_en = \"HWRD\")?model_name_hr; FILTER (?model_name_en = \"RetineX\")?model_name_rex; FILTER (?model_name_en = \"DeepMedic\")?model_name_dm; FILTER (?model_name_en = \"Adaboost\")?model_name_aboost; FILTER (?model_name_en = \"SVR\")?model_name_svr; FILTER (?model_name_en = \"Multi-view\")?model_name_mv; FILTER (?model_name_en = \"Kinetics\")?model_name_kin; FILTER (?model_name_en = \"AutoSklearn\")?model_name_asl; FILTER (?model_name_en = \"K-nearest\")?model_name_knn; FILTER (?", "SELECT?title (STR(?title)) AS?title,?id (IN(?id)) AS?id WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Following are two entities in the dataset: PONG and BLOCKBUSTER. Their relationships are PONG is an element of BLOCKBUSTER and BLOCKBUSTER is an element of NOVELTY. The authors of the paper PONG vs. Alpha Go have performed a benchmarks and have mentioned in their paper that the precision of the models are very poor for these relationships. The models that they have tried include BRAT, PRINCE2, and MGM.\n\n PONG vs. Alpha Go paper : https://paperswithcode.com/p/pong-vs-alpha-go\n\n Entity entity", "net (Sparql query): SELECT DISTINCT?metric?metric_lbl?eval WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rotowire (Content Selection) benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Rotowire (iHome) benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl?eval WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rotowire (iHome) benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:", "output (Sparql query): SELECT DISTINCT?metric_lbl?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): Here is a list of metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: AUC, precision, recall, f-measure, average precision, area under the ROC, MCC, R2.\n\n<|endoftext|>A polymerase chain reaction method for the rapid identification of the human Bacillus anthracis (anthrax) species: implementation of a two-step method based on polymerase chain reaction amplification of DNA segments of the capsule, cap, and mpl genes and RAPD-PCR-polymerase chain reaction fingerprinting.\nAn easy to perform two-step polymerase chain reaction (PCR", "ition (Sparql query): SELECT DISTINCT?metric_lbl_1?metric_lbl_2 WHERE {?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl_1. } OR?metric_lbl_2 rdfs:label?metric_lbl_2. } \n                  ?metric_lbl_1?metric_lbl_2 } \n                   output (Sparql query):?metric_lbl_1 = \"True Positive\"?metric_lbl_2 = \"Precision\"\n\n Why do you need the metrics for this benchmark? (What is the goal of the benchmark?) To use the benchmark to better understand the system performance of deep learning models on the Atari 2600 Breakout dataset. The final system results achieved by using the benchmark are not very meaningful to the end user, thus a subset of the metrics are provided in the output so that you can better understand the system results.\n\n Why is the benchmark scored? The benchmarks are scored using a metric", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Bowling\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The AFT metrics are commonly used when evaluating models on the Bowling dataset:\n 1. Average F1 Score \n 2. Discounted Cumulative F1 Score \n 3. Time on Task \n 4. Average Frame Per Game \n 5. Average Frame Duration \n 6. Number of OMG Events \n 7. Number of Coin Events \n 8. Number of Shots on Green \n 9. Shot Type Agreement \n 10. Shot Type Disagreement \n 11. Distance from Perfect Shot \n12. Shot Group Average \n 13. Hit Type Agreement \n 14. Hit Type Disagreement \n 15. Distance from Perfect Shot \n16. Dribble \n17. Ball Control \n18", ":SELECT DISTINCT?metric?metric_lbl WHERE {?eval a orkgp:HAS_EVALUATION?eval.?eval a orkgp:HAS_DATASET?dataset. FILTER (str(?dataset) = \"Abstracts entities and relations annotated corpus\")?benchmark.?benchmark a orkgp:HAS_EVALUATION?eval. FILTER (str(?eval) = \"FQF\")?metric.?metric rdfs:label?metric_lbl. } \n\nNote: the output contains a combination of Sparql and JavaScript\n\n\nThe metrics used in FQF model benchmarks are HAS_EVALUATION and HAS_DATASET\n\n\nThe HAS_EVALUATION metric measures the performance of an ontology evaluation algorithm in terms of the proportion of correct annotations made on datasets.\n\n\nThe HAS_DATASET metric measures the performance of an ontology evaluation algorithm in terms of the size of the ontology it was trained on.\n\n\nNote: has_evaluation and has_dataset metrics can be used together as a metric combination\n\n\nAll papers using the FQF model in any benchmark in any year, are attached to the following G", "input (English text):  The model that performed best on the FSNS - Test dataset is the SQuAD model. \n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe metrics used to evaluate models on the FSNS - Test dataset are:\n* BLEU\n* ERC\n* FRCN\n* SPERM\n* SPF\n* CPC\n* ROUTE\n* REV\n* SIM\n* POS\n* LF\n* EBOW\n* CR\n* RF\n* IDX\n* NEG\n\n\nIndicate the model that performed best in terms of BLEU metric on the FSNS - Test dataset?\nSELECT?metric?metric_lbl WHERE {", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe query above returns:\n+---------------+----------------+\n| metric_lbl     | metric         |\n+---------------+----------------+\n| Evaluated on  | ARC (Challenge)|\n| ARC (Challenge)|0.9416188171544|\n+---------------+----------------+\n\n<|endoftext|>It\u2019s common to hear Republican candidates say that their party stands for something that is \u201crealistic\u201d and \u201cpractical.\u201d But the pizzerias of America are not really on the knife\u2019s edge of economic collapse. And while universal healthcare might be wonderful, doing without your cholesterol medication is a serious, potentially life-threatening, matter.", "input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Title1 - Metric 1; Title2 - Metric 2; Title3 - Metric 3\n\n  Title1 - Metric 1\n  Title2 - Metric 2\n  Title3 - Metric 3\n\n output (Sparql query): title1 - metric 1; title2 - metric 2; title3 - metric 3\n\n\u0d4d Registry A model evaluation metric or \"SciBench Metric\" is a metric used to compare a model's performance against a baseline model or", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:TSE-NER-Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TSE-NER dataset\")?metric.?metric rdfs:label?metric_lbl. } \n input (English text): TSE-NER is a Named Entity Recognition dataset developed by the OntoNotes Corp. It contains name entity declarations and their corresponding sentiment tags. The dataset uses the Stanford CoreNLP toolkit to identify named entities. The model is evaluated using three metrics: Precision, Recall and F-measure. The metrics used to evaluate models on the TSE-NER dataset are:\n\n - Precision - % of Named Entities that are Correctly Identified as Named Entities \n - Recall - % of Sentiment Tags that are Identified as Named Entities \n - F-measure - (Precision x Recall) / (Total Number of Tags That are Identified as Named Entities)\n\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>A conventional door handle has a body and a door latch module movably attached to the body. The door latch module has a latch housing, a striker plate and a latch bolt. The latch housing defines a receiving space for receiving the striker plate and the latch bolt is mounted in the receiving space and moves in the latch housing for unlatching the striker plate. The body has a latch assembly disposed therein and the latch assembly includes a latch plate, a pawl and a knob. When a user intends to open the door, the user pulls the knob from the body. In this case, the pawl may be engaged with the striker plate to latch the striker plate in the receiving space of the", ":https://github.com/downloads/facebook/sparql/sparql-query.owl#select-distinct-metric-label-where where http://www.semanticinterchange.org/term-store/ontfspellinglemma-model-benchmark/2016/Query_Execution#SELECT-DISTINCT-METRIC-LABEL-WHERE-%28%28CONCAT(SUM(?metric_lbl),'? ')+'%29 WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  \n\nSELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"", "pend (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): https://github.com/alireza092/a3c-lstm-hs-model/blob/master/experiments/pretrained_model.py#L65-L96\n output (Sparql query):", "SELECT * FROM { http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html } WHERE url = \"http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html\"\n input (English text):SVHN: Benchmark dataset for supervised learning, provides % labels predicted by the model vs actual labels for testing. The lower the % error the better. \n output (Sparql query):\u30fc\u30b8  SELECT * FROM { http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html } WHERE url = \"http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html\"\n input (English text):Densely Connected Neural Networks: An end-to-end approach for automatic image interpretation using convolutional neural networks", "it (English text): To find the code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes, you can query the SPARQL endpoint defined at https://gpw.io/sparql/api/ or issue the following TINKER request: https://gpw.io/tinker/?query=SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n Response: metrics used to evaluate models on the benchmarks datasets under the SPARQL query optimization research problem\n\n module sparqlqueryoptimizationproblem;\n {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n ?benchmark orkgp:HAS_DATASET?dataset;\n  orkgp:HAS_EVALUATION?eval.\n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.\n          ?metric rdfs:label", "abstracts' entities and relations annotated corpus  has the highest result on the Amazon-2 dataset with the maximum value of 0.8189425.\n\n Include the metric name, the metric label and the highest result achieved on the Amazon-2 dataset.\n\n SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n { \n  metric: F1, Relation-Lattice, 0.8189425 \n  metric_lbl: F1, Relation-Lattice, \"Relation-Lattice\" \n }\n\n<|endoftext|>Q:\n\nWhy the output is \"word\" instead of \" hello world \"\n\nint main() {\n    cout << \"hello world\";\n    return 0;\n}\n\nWhy", "output (Sparql query): SELECT?metric?metric_lbl WHERE { \n\t?dataset a orkgc:Dataset; \n\trdfs:label?dataset_lbl. \n\tFILTER(str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n\t?benchmark orkgp:HAS_DATASET?dataset; \n\torkgp:HAS_EVALUATION?eval. \n\tOPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n\t} \n input (English text): <https://github.com/holtzy/TCN-benchmarking>\n output (Sparql query):\u00f4ng\n\naddEventListener({\nid:\"https://github.com/holtzy/TCN-benchmarking\",\ntype:\"http://www.w3.org/2001/04/soap-mapping\",\nfeatures:\n{\"application/feature-store\":\"https://raw.githubusercontent.com/holtzy/TCN-benchmarking/master/application/feature-store.xml\"}\n}, function() {", "output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The ResNet paper is: \nhttps://arxiv.org/abs/1907.11931\nThe paper uses the following metrics: \n- Top-1: The number of correctly classified samples in the top 1% of predictions.\n- Top-5: The number of correctly classified samples in the top 5% of predictions.\n- Top-10: The number of correctly classified samples in the top 10% of predictions.\n- Precision@k: The average percentage of predictions at the $k$th highest category.\n- Recall@k: The average percentage of predictions at the $k$th highest category", ":WNLI highest  measure=ENTITY_TREE_INFLEX_PROPOSAL_F1,value=0.9487333333333334 \n inputs (English text):?metric_lbl:wnli,?metric:ENTITY_TREE_INFLEX_PROPOSAL_F1,?eval:measure\n\nipeline:\n\n query:\nSELECT DISTINCT?metric?metric_lbl\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"WNLI\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \n OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n output:\nville:WNLI highest measure=ENTITY_TREE_INFLEX_PROPOSAL_F1,value=0.9487333333333334\n\n<|endoftext|>Q:\n\nHow to call another method when value of model property changes in knockout js\n\nI have model Class called Category.", "inal output (Sparql query): SELECT \"metric\", \"metric_lbl\", \"score\" WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The highest score for the AAPD dataset is 48.82. It includes the metric and score.\n\n (\"metric\", \"metric_lbl\", 48.82)\n\n (...)\n\n (\"metric\", \"metric_lbl\", \"score\")\n\nA:\n\nI would say the following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\nnumber of entities annotated \nnumber of relations annotated\nprecision (number of correct relations/total number of relations)\nrecall (number of correct relations/total number of relations annotated)\n\n<|endoftext|>Diagnosis", "imal :SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FTD dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nA:\n\nThe metrics used by orkgp:HAS_EVALUATION attribute for evaluating a model against a particular dataset in the FTD dataset dataset are :\n\nsupport (1/prediction): The fraction of data points in the dataset that were correctly classified by the model.\nprecision (1/precision): The fraction of data points in the dataset that were classified as positive (true positive,TP) by the model.\nrecall (1/precision): The fraction of data points in the dataset that were classified as positive (true positive,TP) by the model.\nfscore (harmonic average of precision and recall): A harmonic average of precision and recall.\nm", "Binarized voted LSTM model with two layers 40 units in each layer has the best performance with 0.192% error.\n\nHere is the query that gets the output:\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-10 Image Classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following are the metrics that are used to evaluate models on the CIFAR-10 Image Classification dataset:\n\n?metric: Percentage error \n?metric_lbl: CIFAR-10 Image Classification\n\n<|endoftext|>First pecan in Ozona, Texas, commemorates 100th anniversary of successful pecan export\n\nOzona, Texas\u2014When Jake Jordan left Oklahoma in 1949, he left his roots behind but made his mark in Texas.\n\nHe left behind a pecan orchard", "SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n\n 6.4.2 Metrics for classification and regression\n\n That the model is predicting the right class for a given instance can be measured by comparing the predicted label with the actual label. There are two types of labels in the Walker, walk (DMControl500k) dataset: one for the instance and the other for the concept. For classification tasks, the labels for the instances refer to the classes and the labels for the concepts refer to the relations. Therefore, labels in the Walker, walk (DMControl500k) dataset have two parts: the instance part and the concept part.\n\nFor regression tasks, one can use the feature importance in Gradient Boosting Machine to", "output: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciGEN dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The SciGEN dataset contains benchmark entities and relations annotated with specific metrics. The benchmarks are evaluated based on the models predictions for the SciGEN dataset. \n output (Sparql query):avascript code (curl) req.text:SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciGEN dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n */\n\n/**\n * Converts the sparql query into a Map for further processing.\n * \n * @param query\n * @return\n */\nprivate Map<String, String> queryIntoMap(String query) throws NFEException {\n    return queryMap.put(\"query\", query);\n}\n\n/**\n * Checks if the evaluation has a metric and if so, returns the link to the metric \n * with the specified name. If no evaluation has been performed for the specified dataset or the \n * evaluation has no metric associated with it, null will be returned.\n * \n * @param dataset\n * @param evaluation\n * @", "benchmark was scored with a Score metric and the model that performed best on this metric was provided is the name and score:\n\n cessario (English text): The model that performed best on the benchmark dataset was the model that used the MCTS algorithm from the Databricks MCTSPipeline service. The best score achieved on the dataset was 0.592622.\n\n thereafter: The best performing model on the benchmark dataset used the MCTSPipeline MCTS algorithm", "map (U + F1)\n {\n  \"method\": \"Distinct\",\n  \"label\": \"Distinct\",\n  \"query\": \"SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \\\"UCF101 finetuned\\\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\"\n }", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MPQA dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews, K. Suresh;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews; K. Suresh;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews and K. Sures", "bmrp2-highestMetric-13-17-SMTPDirect-SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-chemical\".)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): BC5CDR-chemical: The highest metric achieved on the BC5CDR-chemical dataset is SMTPDirect.\n\n Gain = 2.0\n\nSMTPDirect: the metric and its value are: \n\nSMTPDirect: The SMTPDirect metric is an absolute path distance between the atom's center of mass and the closest heavy atom (defined as the set of heavy atoms that are connected via a heavy bond to the atom). The atom and the heavy atoms are represented using the Conquest Coordinates from the MLOCC and a 3rd-", "uk (habitat):SELECT * WHERE {?eval habitat:SUCCESS?eval_score. FILTER (abs(?eval_score) > 90) }\n\n        \n\n\nThe query returns:\n Best match \n { \"SUCCESS\" \"90.0\" }\n\n\nIn the abstract of the dataset, the metric which is used for evaluation is called success. The corresponding SparQL query is:\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe results are as follows:\n england (Habitat): SELECT * WHERE {?eval habitat:SUCCESS?eval_score. FILTER (abs(?eval_score) > 90) }\n aiding (Habitat): SELECT * WHERE {?", ":SELECT DISTINCT?metric?metric_lbl_HIGHEST_RESULT_WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Paper Field\"^^xsd:string)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Paper Field (Paper Field). The metric is Coverage (20.0000), and the score is 20.0000.\n output (Sparql query): { \"Coverage\" : \"20.0000\", \"Paper Field\" : \"20.0000\" }", ":MLDoc Zero-Shot English-to-German benchmark dataset is a benchmark dataset that evaluates zero-shot learning models. The MLDoc Zero-Shot English-to-German benchmark dataset contains pairs of English sentences and German translations which contain both entities and relations with the purpose of evaluating the ability of NLP models to identify the translations which contain the same entities and relations as in the English sentences. The benchmark dataset also includes a given set of abstracts entities and relations which are used to label the annotations performed on the dataset. The model that performed best on this dataset is the one that achieved the highest accuracy when evaluated on this dataset.\n\n glimp (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Double Dunk\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The highest benchmark result is achieved with an average precision of 0.7755 and a t-measure of 11.8. The metric value for this result is Hit Precision.", "mapping from paper codes to the metrics used in code (example provided in the answer).\n output (Sparql query): SELECT * WHERE {?paper_code text.?paper_code regexp { \"AKITA\" \"AKITAS\" \"Kaggle\" \"Kaggle Datasets\" \"EMGCNN\" \"EMGCNN\" }Filter (str(?paper_code) =?paper_code_lbl). } FILTER (* WHERE {?paper_code regexp { \"Abstracts' entities and relations annotated corpus\" \"Artificial Intelligence on Language Toolkit\" }Filter (str(?paper_code) =?paper_code_lbl). }) }\n input (English text): Here are links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model:\n http://www.kaggle.com/c/semantic-video-question-answering-challenge-sveql/data\n https://arxiv.org/abs/1906.08955\n https://github.com/RuleDrivenAI/emdnn#Results_on_Artificial_Language_models\nWith the code presented,", "{ \"top_benchmark_score\" : \"0.7505273\" }\n\nThe corresponding DBpedia query for retrieving the benchmark datasets is as follows:\n SELECT DISTINCT has_dataset, has_evaluation, has_metric, metric, metric_lbl \nFROM <http://dbpedia.org/query/SELECT+DISTINCT+%22has_dataset%22,+has_evaluation%2C+has_metric%2C+metric%2C+metric_lbl%20FROM+<http://data.govtrack.us/dataset/abstracts-entities-and-relations-annotated-corpus-benchmark-dataset-dmcontrol500k> WHERE url?dataset_lbl=cheetah>\n\n<|endoftext|>Amino acid-based peptide metallodrugs: synthesis, crystal structure, absorption, and photodynamic activity.\nSeveral amino acid-based Zn(II), Cu(II), and Mn(II) complexes have been synthesized and characterized in solution and in the solid state. The structure of these complexes has been determined by NMR, UV-vis, and single crystal X-ray diffraction. These complexes contain N,N'-chelating \u03b2", "uks:List of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset. SELECT DISTINCT?dataset WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"STS Benchmark\").}\n input (English text): Here is the dataset I mentioned in the question: https://github.com/eduardoNevesSilva/universidade-oficial-de-sao-rodo-de-bucharest-datasets/tree/master/papers.\n\n Midwest Bioinformatics Summer School in Computational Biology (MPSS2011), Poster Session (SS4), August 2011\n\n East Japan Biomass Federation, Social Sciences (SS4), Poster Session (SS4), August 2011\n\n Midwest Bioinformatics Summer School in Computational Biology (MPSS2011), Poster Session (SS4), August 2011\n\n Abstract\n\n East Japan Biomass Federation, Social Sciences (SS4), Poster Session (SS4), August 2011\n\n Midwest Bioinformatics Summer School in Computational Biology (MPSS2011), Poster Session (SS4), August 2011\n\n Midwest Bioinformatics Summer", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Alien\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following metric is mentioned in the document as a popular metric for evaluating models on the dataset:\n\n?metric  rdfs:label \" score\"\n\n?metric_lbl  \" Alien\"\n\n<|endoftext|>Q:\n\nOpenGL - Getting GLUvertex4FArray elements out of a GLUvertexPointer\n\nI have been having trouble understanding how to access the elements of a GLUvertexPointer, the way I've seen it done is through the indices:\nvoid MyClass::InitGL()\n{\n    glEnableClientState(GL_VERTEX_ARRAY);\n    glEnableClientState(GL_NORMAL_ARRAY);\n    glEnableClientState(GL_COLOR", ":SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Birdsnap\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n<|endoftext|>If you were hoping that AMD\u2019s Zen 2 \u201cProlimatech\u201d processing architecture would deliver some massive performance improvements over its predecessor, your prayers have been answered. AMD says it has improved its 90-nanometer FinFET processing technology, giving its processors \u201ca further 5-10% performance increase.\u201d This marks the first time AMD has disclosed actual performance gains from an architectural update since it launched Bulldozer in 2010. The new Zen 2 architecture does have one significant weakness: its current release dates for consumer parts lag significantly behind its desktop architecture\u2019s commercial launch.\n\nThe Zen 2 architecture consists of seven core \u201cWavefront\u201d cores, a multi-threaded execution unit, support for DDR4 DRAM,", "map (English text): The models being evaluated on the Atari 2600 Chopper Command dataset are <a href=\"https://github.com/swamijit/chopper_command\">swamijit/chopper_command</a>.\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"swamijit/chopper_command\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): What metrics are being evaluated on the swamijit/chopper_command benchmark model?\n output (Sparql query): wsparser (English text): The metrics being evaluated on the swamijit/chopper_command benchmark model are <a href=\"https://github.com/swamijit/chopper_command/blob/master", "get the best model that performed the best on the \"Abstracts' entities and relations annotated corpus\" benchmark dataset?\n output (English text): Which metrics are used to evaluate the models on the \"Abstracts' entities and relations annotated corpus\" benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The best model, according to the author of the article, is the one that achieved the best accuracy on the \"Abstracts' entities and relations annotated corpus\" benchmark dataset. The best model uses an SVM to classify whether the abstract of a car contains the words \"mercedesbenz\", \"audi\", \"porsche\", or \"volkswagen\".\n output (Spar", "RotoWire (Relation Generation)\n output (Sparql query):SELECT *  \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe answer is the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset. It has the following metrics:\n\n?dataset a orkgc:Dataset; rdfs:label \"RotoWire (Relation Generation)\"; \n orkgp:HAS_DATASET RotoWire (Relation Generation)\n\n?eval orkgp:HAS_EVALUATION RotoWire (Relation Generation)\n\n?metric RotoWire (Relation Generation)\n\n?metric_lbl RotoWire (Relation Generation)", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:", "input (English text): The papers that have utilized the Large mLSTM model and include the links to their code are the following:\n https://arxiv.org/abs/1812.08039\n https://arxiv.org/abs/1812.08040\n https://arxiv.org/abs/1812.08041\n https://arxiv.org/abs/1812.08042\n https://www.cs.cornell.edu/people/pabo/www/ssw19/slstm.pdf\n https://www.cc.gatech.edu/~saket/papers/nlp18.pdf\n https://www.cc.gatech.edu/~saket/papers/wmt19.pdf\n https://www.cs.cornell.edu/people/pabo/www/ssw19/slstm.html\n https://www.cc.gatech.edu/~saket/papers/nlp18.html\n https://www.cs.cornell.edu/people/pabo/www/ssw19/slstm.html\n\nisition (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {", "binding (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACL Anthology\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval?metric. } } \n\nmetric                    metric_lbl\n---------                   ------------\naccuracy                    accuracy\nf1 score                     f1 score\nprecision                    precision\nrecall                       recall\nhit rate                     hit rate\nprecision @ k = 10            precision @ k = 10\nrecall @ k = 10               recall @ k = 10\nprecision @ 3 = recall @ 3    precision @ 3 = recall @ 3\nprecision @ 7 = recall @ 7    precision @ 7 = recall @ 7\nprecision @ 10 = recall @ 10  precision @ 10 = recall @ 10\n\nThe following are some metrics used in the Abstracts' entities and relations annotated corpus benchmark dataset: accuracy, f1 score, precision", "output (Sparql query): {\n  \"Score\" : \"0.94326549\",\n  \"Model\" : \"model18\"\n}\nhookrightarrow This query retrieves the highest scoring model with a given metric label.\n\nThe SPARQL query that is generated is as follows:\n SELECT DISTINCT?metric?metric_lbl WHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n ?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \n  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n }\n\nA:\n\nScore is a metric term used in the benchmarks. https://github.com/opencog/yars-revenge/blob/master/benchmarks/Benchmarks.md#metrics shows the most relevant terms and their meanings.\n\n<|endoftext|>Yemen: War Crimes Trial", "input (English text): Evaluating models on the NCBI-disease dataset.  The CDC-disease corpus contains facts about disease genes, mutations and protein-protein interactions as well as background knowledge about these facts in the form of axioms.  The benchmarks for the model are called evaluation metrics and include F1 score and Precision-Recall curves for the predictions.  Here are the metrics used on the corpus:\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Disease genes mutations proteins interactions\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n answers (Sparql query): F1 score and Precision-Recall curves\n\n facing (Sparql query): F1 score and Precision-Recall curves\n\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset", ":SCRIBDSPARQLQUERY {?sc doc {?s softcite.?sc[salary > 10000000]. } SELECT?metric?metric_lbl (where {?s softcite.?sc[salary > 10000000] }) WHERE {?sc doc {?s softcite.?sc[salary > 10000000] } } }\n input (English text): The highest result is for the Softcite dataset, with the highest score being 0.8022424, and the corresponding metric being \"num sentences\".\n output (Sparql query): Rapids (metric): num sentences\noutput (Sparql query):idemargin (English text): num sentences\n\n<|endoftext|>This website uses cookies to improve user experience, to provide analytical data to better serve our visitors, and to satisfy online advertising. By using this website you consent to our use of cookies.\n\nThis website do not set cookies to gather personal data about you, only used to improve user experience. You can change cookie settings in your web browser. If you continue to use this website without changing cookies settings or you click OK, you are consenting to our use of cookies. For more information see Read Consent Agreement.\n\nThis website uses cookies", "net : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fashion-MNIST dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics of evaluation over the Fashion-MNIST dataset are: accuracy, precision, recall, f-measure, and mean absolute error.\n output (Sparql query):ICKP :SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Fashion-MNIST dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC", "Greetings \n Your answer is the following :-\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n Which metric ROUGE-2 uses to benchmark the X-Sum dataset?\n\n ROUGE-2\n\n Greetings \n Your answer is the following :-\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS", ":SELECT?title (STRDAFFormat(?,\u201c%s\u201d)) WHERE {?dataset a orkgp:HAS_DATASET?dataset. FILTER (str(?dataset) = \"RotoWire (Relation Generation) dataset\") }\n  \n  \n input (English text): Example query, to get a list of research papers that have performed benchmarks on the RotoWire (Relation Generation) dataset:\n SELECT?title (STRDAFFormat(?,\u201c%s\u201d)) WHERE {?dataset a orkgp:HAS_DATASET?dataset. FILTER (str(?dataset) = \u201cRotoWire (Relation Generation) dataset\u201d) }", "output (Sparql Query response):\n\n SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n EIGENVALUES?metric_lbl { \"Accuracy\" \"AUC\" \"Precision\" \"Recall\" \"F1 Score\" \"Positive Predictive Value\" \"Negative Predictive Value\" \"Mean Average Precision\" \"Linear Scan\" \"Logical Scan\" \"Memory Usage\" \"Time\" }\n\n<|endoftext|>The Spirit of '76 is back\n\nThe Spirit of '76 started in 1964 as a club open to all men who agreed to abstain from alcohol, drugs and pre-marital sex. The club\u2019s basic philosophy was to encourage communication, intellectual exchange and camaraderie among members.\n\nThe", "model averages a:AverageReturn or kgp:AverageReturn\n input (English text): Average Return (NoOp) metric returns the average reward of a policy for each time step.  Reward for a given state, action, and time step is calculated as the length of the sequence of symbols following the last.  Thus, the Average Return (NoOp) metric for a model is the average reward of the policy it generates for the Abstracts' entities and relations annotated corpus dataset. \n\nAccording to the Sparql query SELECT DISTINCT?metric?metric_lbl, the Average Return (NoOp) metric is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric.\n\nAverage Return (NoOp) metric returns the average reward of a policy for each time step.  Reward for a given state, action, and time step is calculated as the length of the sequence of symbols following the last.  Thus, the Average Return (NoOp) metric for a model is the average reward of the policy it generates for the Abstracts' entities and relations annotated corpus dataset.\n\n## Opinions, feedback, and other references\n\n### Model choice", "input (English text): https://github.com/limharder/PNS-related\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n Italian version:\n Provocazione Normativa per la Semantic Web (Prior noop): https://github.com/limharder/PNS-related\n Metodo per valutare la Recuperabilit\ufffd del Metodo Prior noop: https://github.com/limharder/PNS-related/blob/master/docs/METODE%20PER%20VALUTARE%20LA%20RECUPARIBILITUD%20DEL%20METodo%20Prior%20noop.ip", "SELECT ?metric?metric_lbl WHERE { ?dataset a orkgc:Dataset;  rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT English-Russian\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The best performing model has BLEU score of 24.3", ":Top-1 Error Rate. \ninput (Sparql query):\n SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Visions of Ecstasy: The Films of Jacques Tourneur, Part 3\n\nHere is another installment of my ongoing series of reviews of Jacques Tourneur's films, and a follow-up to the last two installments.\n\nThe third installment of Visions of Ecstasy is not necessarily a \"best of\" compilation. Each film is presented in chronological order, and thus, there is a logical build-up from the first film released in 1959 to the final film released in 1962. However, Tourneur's best film does include an implied (if oblique) thematic connection between some of the films.", "output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuter's 21578 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?benchmark rdfs:label?benchmark_label. } } \n input (English text): The metrics used in the benchmarks listed below are:\n Title ID Count Tf-idf Top-k cosine similarity Co-occurrence similarity \n AuctionPrice  1 Reuter's 21578 Dataset       161702 3049          0.8             - \n AuctionPrice + Aggregation  2 Scalable Query Processing 1 300000                 1.0 \n AuctionPrice + Aggregation + TF-IDF 3 Reuter's 21578 Dataset       161702 3049          0.8             - \n Co-occurrence similarity  4 Co-occurrence Similarity 1 300000                 1.0 \n Co-", "py (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Characters in the Penn Treebank (Character Level)\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Evaluation metrics for the Penn Treebank (Character Level) dataset include F-score, precision, recall, and the area under the ROC curve?\n output (Sparql query):\"}**).** Precision, Recall, and F-score are common metrics that are used for evaluation of character level Named Entity Recognizers. The area under the ROC curve (AUROC) is a metric for evaluating the performance of machines learning models on a binary classification task and is often used as the primary metric in research.\n\n Chapel Hill Recognizer was used as a baseline system in this research\n\n Chapel Hill Recognizer", "ary (Sorted by metric_lbl): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThere is only one metric available: \n\nMetric: abstracts' entities and relations annotated corpus \nLabel: Abstracts' entities and relations annotated corpus \n\nA snippet that can be used to search the dataset is given below: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION", "ition: Paper Co-occurrences in the DY-MobileNetV3-Small model\u2019s evaluation\n\n heading (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Paper Co-occurrences in the DY-MobileNetV3-Small model\u2019s evaluation\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nBelow is the list of papers that utilized the DY-MobileNetV3-Small model and include the links to their code:\n\nhttps://www.slideshare.net/DorianDouglas12/an-overview-of-racial-bias-in-computer-vision-deep-learning-and-its-implications-for-science-and-society-skpe13209840\n\n heading (English text): Provide", ":THIRD_MEDIAN_DIVERGENCE\n output (English text): The highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value is 2.4893455855417E+16. \n\nThe third_median_divergence metric is used to evaluate whether the model's predictions fall within the actual interval of the ground truth values.\n\n i\u00e7sparql (Sparql query):SELECT?metric?metric_lbl,?metric_label,?metric_value where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n    {?eval orkgp:HAS_METRIC?metric_label.?metric_value?metric_label.} WHERE {?dataset a orkgc:Dataset; rdfs", ":https://github.com/hbaten/MPAD-path. Sparql query for the above question is as below: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n<|endoftext|>Riot Games will announce on May 6th at 4PM PDT, the next step in the progression and growth of its esports program. We will discuss the results of the program with members of the leadership team and share a plan for the future of Riot's esports program. We hope to hear your ideas and feedback on how to improve Riot's esports program.\n\nThe Riot Games Esports Program was first established in April 2014, which resulted in the first domestic league championship for League of Legends in Europe in July 2015", "SELECT DISTINCT?model?model_lbl WHERE {?model a orkgp:BenchmarkedModel;?model_lbl a :ValueReference. FILTER (str(?model_lbl) = \"DL-Matcher\") }\n input (English text): \n The BenchmarkedModels dataset is available at http://www.openlibrary.org/blog/2007/10/09/DL-Matcher-is-here-a-dataset-of-Benchmarked-DL-Models\n The BenchmarkedModels dataset contains BenchmarkedDLModels records, each of which represents a model. Each model has a name, a description, and several performance metrics. The benchmarked model is also annotated with the textual description of the evaluation and a benchmark score. There is a reference to the textual description for each metric.\n\n specim  DL-Matcher (Sparql query): \n SELECT?model?model_lbl WHERE {?model a orkgp:BenchmarkedModel;?model_lbl a :ValueReference. FILTER (str(?model_lbl) = \"DL-Matcher\") }\n\n individule :model  DL-Matcher  --  name  --  description  --", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The highest benchmark result for the BUCC German-to-English dataset has the following result:\n {\n  \"metric\": \"BLEU\",\n  \"metric_lbl\": \"BLEU\",\n  \"dataset\": \"German-to-English\",\n  \"eval\": {\n    \"metric\": {\n      \"label\": \"BLEU\"\n    }\n  }\n}\n\nyards:BLAST++ metric\n\n yards:BLEU metric\n\n yards:0.94\n\n yards:ORC metric\n\n yards:ORC metric\n\n yards:94\n\n yards:tm rank\n\n yards:2 10\n yards:29 5", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DMControl100k\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe metrics that are used to evaluate models on the DMControl100k dataset are:\n\n - logloss (rdflib:logloss)\n - roc_auc (rocs:auc)\n - acc_train (orkgp:accuracy)\n\n The metric that has achieved the highest Score is the logloss metric with value 0.258529.\n\n\n## References\n\nhttps://www.kaggle.com/c/ball-in-cup-catch-predict-DMControl100k\n\n\nhttps://www.kaggle.com/c/ball-in-cup-catch-predict\n\n\nhttps://www.kaggle.com", ". Benchmark using ensemble method in any paper in the Abstracts' entities and relations annotated corpus benchmark dataset. \n input (English text): Benchmark using ensemble method in any paper in the Abstracts' entities and relations annotated corpus benchmark dataset. \noutput (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nTable 2 shows all the metrics used in Abstracts' entities and relations annotated corpus benchmark dataset. \nTable 3 shows the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark.\nTable 4 shows the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark.\n\nTable 2:", "inal response:Bits per dim\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Bits per dim\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Bits per dim is the best performing metric\n\n<|endoftext|>Kansas City, Missouri - Kansas City Public Library, Moorpark Library\n\nA dangerous and stupid game is just an opportunity to fail and learn a lesson.\n\nThe Sporthid Player: Massacre at Marathon\n\nHannibal, (Ferenc) - Parzival - Radl (Szanto)\n\nSporthid is the best game ever made.\n\nIt is an abstract strategy game of an ancient battle between two factions (The Romans and the Barbarians).  As the game begins a group of soldiers arrive with the mysterious (to the Barbarians)", "mark  and  87.33\n\n<|endoftext|>Q:\n\nParse.com cloud code and php results not showing in iOS app\n\nI am trying to fetch data from parse.com using php and parse cloud code and showing it in my iOS app. But I'm not getting any data in my app. The data is getting fetched when I manually write query in the browser. \nParse server URL is https://xxxxxxxxxxxxx.firebaseIO.com/\nApp URL is https://xxxxxxxxxxxx.herokuapp.com/\nWhen I click on \"Fetch Data\" button, I am calling a php file and making below query.\n$query = \"SELECT json FROM Table_name WHERE createdAt = $createdAt\";\n\n$queryParams = [NSString stringWithFormat:@\"%@\", $createdAt];\n$cursor = $picocli['database']->query($query, $queryParams);\n\n$json = [];\nwhile ($row = $cursor->fetch()) {\n    $json[] = $row;\n}\n\necho json_encode($json);\n\nBut I am not getting any data in my iOS app. Please help.\n\nA:\n\nLooks like there was an", "metric rdfs:label \"Children examined\"\n\n EIGEN_SVDDecomposition e.Cosine SVD = e;\n Eigen::ArrayXd S = e.matrix().sds().array();\n Eigen::ArrayXd U = e.matrix().uysd().array();\n Eigen::ArrayXd V = e.matrix().vssd().array();\n\n SVDDecomposition svd = S.asDiagonal() * (S + S.array().pow(2).inverse() * V.array()).solve(U.array());\n\n StringBuilder sb = new StringBuilder(\"SELECT DISTINCT \");\n sb.append( metrics.get(\"rdfs:label\") );\n sb.append(\" \");\n sb.append(metrics.get(\"has_dataset\") );\n sb.append(\" \");\n sb.append( metrics.get(\"has_evaluation\") );\n sb.append(\" \");\n sb.append(metrics.get(\"has_metric\") );\n sb.append(\" \");\n sb.append( metrics.get(\"rdfs:label\") );\n return sb.toString();\n\n EIGEN_SVDDecomposition e.Cosine SVD = e;\n SVDDecomposition svd", "SELECT?benchmark (max?metric) where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \nThe highest benchmark result achieved on the CoQA dataset is 500, including the metric and its value of 36.8.", "setof string?metric_lbl of elements <metric>.\n output (Sparql query):SELECT * WHERE {?metric?metric_lbl. FILTER (str(?metric) = \"top-5\") }\n output (Sparql query):<metric> Top-5 accuracy (ImageNet-v2) \noutput (Sparql query):<metric_lbl> top-5 accuracy (ImageNet-v2) \noutput (Sparql query):<metric_lbl> Top-5 accuracy\noutput (Sparql query):accuracy Top-5 accuracy", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reacher (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe reacher is a meta learning model which achieves an overall score on any benchmark dataset. For the Reacher benchmark dataset, it is reported to achieve an average overall score of 0.993. \n\nFor the Reacher benchmark dataset, the DMControl500k metric is used as the evaluation metric. The Reacher benchmark dataset contains named-graph-features so the DMControl500k metric is computed on the named graph.\n\nBased on the above query, the following two metrics are returned:\n - DMControl500k: The metric for Reacher (DMControl500k)\n - score: The overall score of Reacher (0.993)\n\n                                                                               SELECT", "SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Quasart-T benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following are some of the metrics used to evaluate models on the Quasart-T benchmark dataset:\n\n -- precision (also called recall)\n -- recall\n -- $f_1$ score\n -- $F_1$ score\n -- $F_1$\n -- recall@1\n -- $f_1$@1\n\n -- precision\n\n -- recall\n\n -- $f_1$ score\n\n -- $F_1$ score\n\n -- $F_1$\n\n -- recall@1\n\n -- $f_1$@1\n\nA:\n\nYou can use your experience with the SPARQL query language to find this information in other ways,", "SELECT?model_id?metric?metric_lbl WHERE { \n<http://orcid.org/n Lan <<model_id>> >  <http://orcid.org/n Lan <<metric>> > \n<http://orcid.org/n Lan <<metric_lbl>> > }\n\nUndecided\n\n\nA model is evaluated on the Multimodal PISA dataset if it is available in the Multimodal PISA dataset. \n\nThe following are the models that have been evaluated on the Multimodal PISA dataset:\n\n\n* Logistic regression\n* Random Forest\n* Linear regression\n* Multinomial logistic regression\n\n\nThe following are the metrics that are used to evaluate models on the Multimodal PISA dataset:\n\n\n* R2: The coefficient of determination of a linear model\n* Q2: Coefficient of concordance, a measure of the Concordance of two binary predictions when a proportion of subjects are positive and negative\n* Gain: Measure of the accuracy improvement by including a model-specific predictor into a linear model\n\n\nThe following are the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\n\n* R2", "input (English text): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Barabasi-Albert dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The metrics used to evaluate models on the Barabasi-Albert dataset are:\n - Node degree\n - EWeight\n - Cluster prominence\n - k-shell coefficient\n\n questionna:Node degree, Node degree, k-shell coefficient", "ment (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Zaxxon dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The following are the metrics used to evaluate models on the Atari 2600 Zaxxon dataset: Maximum accuracy, Recall, precision, F-measure,  Root Mean Square Error, Harmonic Mean, Cross Entropy, log loss, Sigmoid Cross Entropy, area under the receiver operating characteristic (ROC) curve.\n output (Sparql query): {\u00b6metric_lbl_1\u0082Maxima\u0090 Accuracy\u0082Recall\u0090 Precision\u0090 F-measure\u0090 Root Mean Square Error (RMSE)\u0090 Harmonic Mean (HM)\u0090 Cross Entropy\u0090 Log Loss (LL)\u0090 Sigmoid", "output (Sparql query): SELECT?metric?metric_lbl?metric_value WHERE {\n    ?dataset a orkgc:Dataset;\n     rdfs:label?dataset_lbl.\n     FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\")\n     OR\n    ?benchmark orkgp:HAS_DATASET?dataset;\n     orkgp:HAS_EVALUATION?eval.\n     OPTIONAL {?eval orkgp:HAS_METRIC?metric.\n              ?metric rdfs:label?metric_lbl.\n              ?eval orkgp:HAS_METRIC_VALUE?metric_value.\n               }\n     }\n\n<|endoftext|>Q:\n\nHow to avoid fseek in this code to find position of a line in a file?\n\nHow to avoid fseek in the following code to find position of a line in a file?\nint main()\n{\n    FILE *fp;\n    int size,pos=0;\n    char str[50];\n\n    printf(\"Enter the file name: \");\n    scanf(\"%s", "(Sparql query): ============================================================ output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"English-German benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \ninput (English text): The BLEU score metric that performed the best was BLEU-2. This means that the model achieved the highest blem score for queries where the target language is German and the source language is English.", "input (English text): \n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sentence Classification related datasets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics that are used to evaluate models on the Sentence Classification research area benchmarks datasets:accuracy, precision, recall, f1 score, and micro-averaged f1 score.\n output (Sparql query): jazz musicians:analyze(?benchmark, \"sentences\",?eval,?metric,?metric_lbl,?metric). \n jazz musicians:analyze(?dataset, \"sentences\",?eval,?metric,?metric_lbl,?metric). \n jazz musicians:analyze(?benchmark, \"sentence classification related datasets\",?eval,?metric,?metric_lbl,?metric", "--metric /content-ordering/similarity\n\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Content Ordering)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n--metric /content-ordering/similarity\n\n<|endoftext|>Deficiency of insulin-like growth factor-1 receptor in Wilms' tumor.\nWilms' tumor (WT) is a primary pediatric tumor of the kidney and is the most common tumor of the genitourinary system in children. Its pathogenesis remains obscure; however, some evidence suggests that the presence of a mutated WT1 gene contributes to its development. We present a case of WT with IGF-1R signaling pathway defects. The patient's serum contained low levels of the IGF-1R ligand, whereas the IGF-1R protein was absent in the tumor", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics commonly used to evaluate machine learning models on the MLDoc Zero-Shot English-to-Italian dataset include: f1-score, precision, recall, and [ Matthews correlation coefficient ].\n output (Sparql query):olved  SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EV", "OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n is used to evaluate the models on the Abstracts' entities and relations annotated corpus benchmark dataset.\n The following metrics are used in evaluating the models:\n 1. DisMax Score - http://sparql.w3.org/2013/#semantic- entailment with constraint rule mining\n 2. AvoDelta Score - https://github.com/w3c-wai-", ";SELECT DISTINCT?metric?metric_lbl WHERE {?model ogr:ExportFeatureLayer \"...\";orkgp:HAS_EVALUATION?eval.?dataset a orkgc:Dataset;?benchmark ogr:BenchmarkComparisonResult; OPTIONAL {?benchmark orkgp:HAS_DATASET?dataset;?dataset a orkgc:Dataset;?eval.?benchmark ogr:BenchmarkComparisonResult;?dataset_lbl ogr:BenchmarkLabel.?eval orkgp:HAS_METRIC?metric;?metric rdfs:label?metric_lbl;} }\n input (English text): The MedSTS benchmark dataset contains a results for different models in comparison. Here is an example of a metric name along with a label: \"Aua.\" The benchmark label is what is annotated in the MedSTS dataset. \n output (Sparql query):tingham;SELECT?metric?metric_lbl WHERE {?benchmark ogr:BenchmarkComparisonResult;?dataset a orkgc:Dataset; OPTIONAL {?dataset_lbl ogr:BenchmarkLabel.?eval or", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nBest performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n\n inquired\n\n bestModel\n\n SELECT (avg(f1) as?avg_f1)?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs", "pend?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"finetuned\"@en)?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): UCF101 is a fully trained CNN model. There are 4 metrics which have been used for evaluating the finetuned model on the UCF101 dataset: FLOPs, Top1, Top5. There is only one benchmark with the identifier \"finetuned\"@en and the type Sparql Queries.\n\n INSERT INTO table-output( metric, metric_lbl) \n SELECT DISTINCT?metric?metric_lbl \n FROM \n    <your query above>    \n\n<|endoftext|>The present invention relates to an electromagnetic valve having a coil supported at a housing by means of a snap-fit connection and an armature slidably movable in the housing in a substantially axial direction relative to the", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciCite\"):[*]?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): There are a few datasets in SciCite corpus that are linked to their corresponding RDF datasets and are called'sciconfs'. Each sciconf stores metadata about one or more SciCite RDF datasets. One or more RDF datasets can be linked to a single or to multiple sciconfs. The following are the datasets that are linked to their corresponding RDF datasets: Abstracts' entities and relations annotated corpus, STIntersection and STUnion.", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet + iNat on WS-DAN model\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\nNo metric names are provided in the benchmark dataset used in the paper. Please provide links to the code used in the paper that benchmarks the ImageNet + iNat on WS-DAN model?", "mark 2nd-order Markov Conditional Random Fields (2nd-order MRF) \n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a r2e:Russian-to-English; \n r2e:MRFModel?model; r2e:MRFConstraintItem?c1; r2e:MRFConstraintItem?c2; \n r2e:MRFConstraintItem2?c3;?c1-c3:MRFConstraintItemRef?constraint; \n?dataset_lbl r2e:MRFEvaluation; r2e:MRFConstraintItem?metric;?metric rdfs:label?metric_lbl. }\n\n<|endoftext|>/**\n *    Copyright 2009-2015 the original author or authors.\n *\n *    Licensed under the Apache License, Version 2.0 (the \"License\");\n *    you may not use this file except in compliance with the License.\n *    You may obtain a copy of the License at\n *\n *       http://www.apache.org/licenses/LICENSE-2.0\n *\n *    Unless required by applicable law or agreed to in writing, software\n *    distributed under the License", "user:fred available:true ROUGE-1:0.83\n Comissao orkgp:HAS_EVALUATION orkgp:HAS_DATASET evalu:ROUGE-1\n The best performing model with the benchmark dataset has a ROUGE-1 score of 0.83.\n\n\nTo answer this question, a SPARQL query that lists the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset is:\n\n\nSELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \n OPTIONAL {?eval orkgp:HAS_METRIC?metric. \n?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>#import \"GPUImageFilter.h\"\n\n@interface GPUImageColorFilter : GPUImageFilter\n{\n    GLint colorMatrixRed", "mark_test-ModelNet40.has_dataset  OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n input (English text): What is the metric and its corresponding label?\n output (Sparql query):REG NAMED {metric_lbl rdfs:label?metric_lbl.}\n\n<|endoftext|>Q:\n\nIs there an abstraction layer similar to NOLUN in Oracle Database?\n\nNOLUN is the extension to the PL/SQL language that allows you to parallelize certain statements without changing the order of operations:\n\nThe nolo directive creates a new anonymous block of code and runs the contents of the block in parallel\n\nFor example, this SQL block would be executed serially:\nSELECT 1 FROM dual\nUNION\nSELECT 2 FROM dual\nUNION\nSELECT 3 FROM dual\n\nThe following code will run each SELECT statement simultaneously:\nSELECT 1 FROM dual\nUNION\nNOLUNCTION\nSELECT 2 FROM dual\nUNION\nNOLUNCTION\nSELECT 3 FROM dual\n\nIs there a similar abstraction layer in Oracle Database?\n\nA:\n\nNo.  There", "_label _toolname metric_label metric_label_long\n output (Sparql query): chunker_bleu_chinese_det sentence_len tokenization tokenization_bleu tokenization_bleu_chinese sentence_len tokenization_chinese\n input (English text): chunker_bleu_chinese_det,tokenization_bleu_chinese,sentence_len,tokenization,BLEU,chinese,det\n\n restrictive (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n restrictive (Sparql query):OVID_label _toolname metric_label metric_label_long\n\n restrictive (Sparql query): chunker_bleu_chinese_det", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuters-21578 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The Reuters-21578 benchmark dataset contains performance metrics for text categorization, named-entity recognition and relation extraction. The metrics cover precision, recall, F-score, precision at cost 1 (P@1), recall at cost 1 (R@1), and their harmonic means. All metric names start with \"precision\", \"recall\", \"F-score\", \"P@1\", \"R@1\", and \"harmonic mean\". Note that \"precision\", \"recall\", \"F-score\", \"P@1\", and \"R@1\" are also used in another Open Standard benchmark RelMetadataRel. Therefore, \"precision\", \"recall\", \"", "Sparql Query \n SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Seaquest dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n Since Seaquest dataset is best evaluated on the Abstracts' entities and relations annotated corpus benchmark, the best performing model benchmarking Seaquest dataset in terms of Score metric is the DART model.\n\n But the DART model benchmark has no evaluation metric.\n\n The score metric is used to evaluate the performance of a model and the DART model performs best in this benchmark with an average score of 0.9245.\n\n Since there is no evaluation metric for the best performing model benchmarking Seaquest dataset, we can't determine which model performs best.\n\n In conclusion, we could not determine which model performs best on the Abstracts' entities and relations annotated corpus benchmark dataset.\n\n The abstract", "you asked what are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip dataset?\n\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n input (English text): The following metrics are part of the evaluated models for the Abstracts' entities and relations annotated corpus dataset benchmark:\n\n  -Precision\n  -recall\n  -F1score\n  -MSE\n\n  Note that the models may use multiple metrics to compute their F1-score\n\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dat", ":SELECT DISTINCT?metric_lbl?metric,?dataset_lbl,?benchmark,?eval FROM <data> WHERE { <data>?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ObjectNet' dataset\") } \n input (English text): Here is the list of research papers that have performed benchmarks on the ObjectNet dataset:\n #title, ID\n Databricks on ObjectNet, Databricks, rdfs:label \"ObjectNet\", rdfs:label \"Databricks on ObjectNet\"\n Unstructured JSON comparison, Berner, rdfs:label \"JSON diff\", rdfs:label \"Unstructured JSON comparison\"\n Entity normalisation for JSON-LD, Badolato, rdfs:label \"JSON-LD normalization\", rdfs:label \"Entity normalisation for JSON-LD\"\n Auto-encoding semantic relations with Neural Relation Machines, Huang, rdfs:label \"Neural Relation Machines\", rdfs:label \"Auto-encoding semantic relations with Neural Relation Machines\"\n Evaluating the quality of wikis using EntityLinking, Botch", ". SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 HERO\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics used to evaluate models on the Atari 2600 HERO benchmark dataset are:\n - hp (Human Performance)\n - timing (timed game play)\n - loop (Number of successful turns)\n - loses (Number of times a game ends early)\n - watts (Watts consumed while playing the game)\n - acc (Average % Accuracy)\n - turn_coords (Average % of target location in turn)\n - score (Total Score)\n - moves (Total number of moves)\n - wall_clocks (Average Wall Clock time)\n - energy (Total Energy consumed)\n - frames (Total number of", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nExample output:\nThe following metrics are used to evaluate models on the Amazon-2 dataset:\nROC_AUC, ROC_AUCLP, ROC_CLEAR, AUPR, MCC, PRCC, ACC, AP, MRR\n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_", "<query type=\"FrankfurtRE\">\n<description>List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset</description>\n<query>\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n</query>\n</query>\n\n<response>\n<treetop>\noutputTree {\n\"SELECT DISTINCT?metric?metric_lbl\" \"WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset;", "model: OpenAI GPT, acronym: GPT, score: 88.465625, timestamp: 2020-04-20 06:13:28.552827\n input (English text): The highest performing model on the RACE dataset in terms of Accuracy (High) is the OpenAI GPT. The corresponding Sparql query is:\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RACE\" )?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"RACE\", \"High\", \"Accuracy\", \"88.465625\"", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The following research paper titles have benchmarked models on the WMT2016 English-Romanian dataset: \n - Lu\u00e7on, L., Dau, T., Bottla, M., & Bilbe, C. (2016). Learning to Verbally Compose Grammatical Greetings by Recurrent Neural Networks. https://arxiv.org/abs/1609.03732. \n - Bilbe, C., Dau, T., Bottla, M., Lu\u00e7on, L., & Grisel, A. (2016). Greetings4\u0440\u0430\u0431\u043e\u0442\u043d\u0438\u0446\u0430: \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u0444\u043e\u0440\u043c \u0448\u0443\u0442\u043e\u043a \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u0447\u044c", "_SPARQL_Query below shows the SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MUTAG dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): MUTAG dataset metrics - https://github.com/onti/mutedataset-benchmarks/blob/master/mutedataset-evaluation/eval/mutedataset_evaluation_api.cpp#L15\nThe metrics used for evaluation in MUTAG are MUTAG_BLEU, MUTAG_CIDEr, MUTAG_COSINE, MUTAG_METEOR, MUTAG_PRC, MUTAG_ROUTE, MUTAG_SPD, MUTAG_WAVITAB, and MUTAG_WOR", "= { \"count\", \"precision\", \"recall\", \"f1-score\" }\nSince the Abstracts' entities and relations annotated corpus dataset is part of the benchmark dataset, these metrics should also be used on the benchmark dataset:\n SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Un articolo pubblicato sulla rivista \u201cThe Astrophysical Journal\u201d descrive un team che ha combinato osservazioni grazie a telescopi Spitzer, Hubble, e Chandra con la ricostruzione delle molecole chimiche nei corpi stellari al cervello superiore di 101 personaggi. Qualcuno potrebbe pensare che sia una stella cos\u00ec", "SMOTE DNN has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset.\n\nThe top 1 most relevant metric is 'F1 score' which is the metric used to evaluate the model on the SQuAD1.1 dev benchmark dataset.\n\n biomark (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 dev benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe top 1 most relevant model with the highest F1 score on the SQuAD1.1 dev benchmark dataset is the SMOTE DNN model.", "mind the models that are being evaluated on the TempEval-3 dataset?\n\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n input (English text): TempEval-3 is being evaluated on models that are listed below:\n\n output (Sparql query):SELECT?metric?metric_lbl WHERE { {\n\n ?dataset a orkgc:Dataset; rdfs:label \"TempEval-3 dataset\". FILTER (str( \"TempEval-3 dataset\") = \"TempEval-3 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n:  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgp:HAS_EVALUATION?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a or", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Seaquest Score\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Seaquest Score is the top benchmark score, with a score of 44.91. This score is scored using the per-example F1 score. The F1 score is computed as ( TP * 100 / ( TP + FP ) ), where TP and FP are the number of true positives and false positives, respectively. \n output (Sparql query):\u0430\u0442\u0435\u043bperkF1:44.91 Top metric is Per-Example F1. \n input (English text): Per-Example F1 is computed as ( TP * 100 / ( TP + FP ) ), where TP and FP are the number of true positives and false positives, respectively. The F1 score is a harmonic mean", "user should provide a list of sparql queries that are used to evaluate models on the abstracts entities and relations annotated corpus benchmark dataset. SELECT DISTINCT?query WHERE {?query?metric.?metric rdfs:label?metric_lbl. FILTER (str(?metric) = \"Abstracts' entities and relations annotated corpus\") } \n input (English text): None \n output (Sparql query):SELECT DISTINCT?query WHERE {?query?metric.?metric rdfs:label?metric_lbl. FILTER (str(?metric) = \"Abstracts' entities and relations annotated corpus\") } \n input (English text): SELECT DISTINCT?query WHERE {?query?metric.?metric rdfs:label?metric_lbl. FILTER (str(?metric) = \"Abstracts' entities and relations annotated corpus\") } \n output (Sparql query):SELECT DISTINCT?query WHERE {?query?metric.?metric rdfs:label?metric_lbl. FILTER (str(?metric) = \"Abstracts' entities and relations annotated corpus\") } \n input (English text): SELECT DISTINCT?query WHERE {?query?metric.", "uate the following sparql query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Public datasets benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): How does the Yelp Dataset differ from the benchmark datasets included in Abstracts' entities and relations annotated corpus dataset? \noutput (Sparql query):There are two main differences: (1) the benchmark datasets benchmarks only specific types of systems, like link classification, named entity recognition, substring matching, and so on; (2) the Yelp Dataset targets search queries and provides rankings of relevance for each query, which is different from the benchmarks that evaluate systems on standardized datasets, like the Abstracts' entities and relations annotated corpus dataset.\n\n\n\n\n### References\n- [Databricks Eval](https://www.dat", "set returned by the SELECT clause contains a link to the page with the model evaluation results on the Amazon Benchmark Dataset.\n\nNote: The best model in terms of accuracy was the model that achieved the highest performance on the Amazon Benchmark Dataset with an accuracy of 91.43%.", "input (English text): List the metrics that are used to evaluate models on the Abstracts\u2019 entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"Average sequence length\", \"Average number of interactions per command\", \"Ratio of commands with at least three commands\", \"Ratio of commands with one or less than two outputs\", \"Mean timelimit per command\", \"Total timelimit of all commands\", \"Timelimit for the highest command\", \"Successful commands\", \"Failed commands\", \"Ratio of failed commands\", \"Ratio of commands with at least three errors\", \"Ratio of commands with one or less than two errors\", \"Mean number of", ": Abstracts' entities and relations annotated corpus (HasDataset, HasEvaluation, HasMetric [MetricName], HasMetric [Label])\n\n                    : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nand", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval_2.?eval_2 orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n EIGEN_TCP_SERVER_ADDR: 10.244.0.2\n EIGEN_TCP_SERVER_PORT: 54322\n EIGEN_TCP_SERVER_PROTO: TLS\n EIGEN_TCP_SERVER_TLS_CERT: client.crt\n EIGEN_TCP_SERVER_TLS_KEY: client.key\n EIGEN_TCP_SERVER_TLS_CACERT: client.cacert\n EIGEN_TCP_SERVER_TLS_CRL: client.crl\n\n EIGEN_TCP_CLIENT_ADDR: 10.244.0.2", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset on the Atari 2600 Asterix are:\n\n       Alshiri, Tal, Juni, Oz, 2017. \"Evaluating Atari 2600 Asterix Game-level Deep Learning Models for In-Session Recommendation with A/B Testing\". AIP Advances. 34(2): Article No. 106388.\n\n       Alshiri, Tal, Oz, Juni, 2017. \"A Benchmarking Study on Evaluating Deep Learning Models on Atari 2600 Asterix Games\". AIP Advances. 34(4): Article No. 10", "pend (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Here are the code snippets used in the papers which benchmark the MFEC model: https://github.com/rashmiagarwal/mfeat_resnet50_challenge_paper.git, https://github.com/rashmiagarwal/mfeat_resnext101_challenge_paper.git, https://github.com/rashmiagarwal/mfeat_caffe.git, https://github.com/rashmiagarwal/cnn_resnet50_", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \ncode link :https://github.com/cloudsightio/cait-xs-36-model#model-benchmarks\ninput (English text): As provided in the link provided, we can see that the model benchmarks are evaluated using metrics such as Mean Average Precision (MAP) at sentence level and Mean Average Reciprocal Rank (MAR) at word level.\noutput (Sparql query): hitchiker has helped me find this link to code used in papers that benchmark the CAIT-XS-36 model.\ncode link :https://github.com/cloudsightio/cait-xs-36-model#", "mark and f1 score are used to evaluate the models on the seel.cse.lsu.edu/data/re17.zip  dataset. SELECT DISTINCT?metric?metric_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): f1 score is used to evaluate the models on the seel.cse.lsu.edu/data/re17.zip  dataset.\n\nennelov@cse.lsu.edu (ELovic) generated Tue, 25 Sep 2019 14:17:38 +0000 from Sparql Query\nSELECT DISTINCT?metric?metric_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl", "metric Score 58.97322619023206\n\n\"?>\n<!DOCTYPE sparql-query>\n<sparql-query xmlns:ytt=\"http://www.convergentsemantic.com/ytt-sparql-query-1/\">\n<head>\n<BODY>\n< votoff rdf:resource = \"http://www.convergentsemantic.com/ytt-sparql-query-1/results/ AssessmentModelsRankings-model-58.97322619023206\" />\n</head>\n<query>\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Montezumas's Revenge\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n</query>\n<variables>\n<variable name=\"metric_lbl\"", "20.0 (textual answer): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval parsemtr. } }\n\nThe best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric is the model proposed in", "inal (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nA:\n\nThe following metric are used in the Abstracts' entities and relations annotated corpus benchmark dataset:\n\naccuracy: accuracy of the model on the dataset, the larger the better, 0.5 is half correct, 1 is 100% accurate.\ncomputed_f1: computed_f1 metric, computed as (accuracy * 2) / (2 * (1 - accuracy)). A value of 1.0 would be computed_f1 = 2 * (1 - accuracy)\nmems: the number of missed matches, 0.0 is no missed matches, 1.0 is all missed matches.\n\nThere", "next :SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pubmed dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The top result includes the F1 score of 0.93.\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pubmed dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \noutput (Sparql query):F", "Columbia University`s Search API & SPARQL 1.1 Query Language\noutput (Sparql query): SELECT DISTINCT?model_id?metric_id?metric_label WHERE {?model_id rdf:first?model.\n  {?model_id owl:sameAs?model.\n    {?model_id owl:sameAs?model.\n      {?model_id sparql:name \"Walker, walk\"@en.\n        {?model_id sparql:page \"https://data.w3.org/vision/dwgi-datasets/walk\"@en.\n          {?model_id sparql:resource \"http://dmcontrol.com/models/deepwalk-rml/documentation/Walker%20walk.html\"@en.\n            {?model_id sparql:description \"An Entity Recommender based on the Google deepwalk and repeater_functionality for relationalmachine-learning tasks.\"@en.\n              {?model_id sparql:language \"en\"@en.\n                {?model_id sparql:license \"Public Domain\"@en.", ":REDDIT-B dataset was achieved on the DBpedia SPARQL endpoint with the following query: \nSELECT ?metric?metric_lbl WHERE { \n?eval whatever OR \n?eval hasDataset datastore:http://dbpedia.org/data/REDDIT-B.\n?eval hasEvaluation whatever. \n?eval whatever rdfs:label \"DBpedia Abstracts' entities and relations annotated corpus Benchmark.\".\n?eval hasMetric  dbpedia:totalVotes?metric. \n?eval whatever rdfs:label \"DBpedia Total Votes.\".\n?eval whatever rdfs:label \"3093.\".\n}", ":Model of the Benchmark Dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Model of the Benchmark Dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Yes, that is correct. \n\nThe metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are:\n\n?metric\t?metric_lbl\t\nnsqa:precision\tAbstracts' entities and relations annotated corpus benchmark dataset\nnsqa:recall\t?benchmark\t\nnsqa:f1-measure\t?benchmark_lbl\t\n\nSome examples of queries that retrieve information about a specific metric from the benchmark dataset are:\n\n?metric\t?benchmark_lbl\t\nnsqa:", ": SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-2 Binary classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The following are some of the metrics that have been used for model evaluation on SST-2: accuracy, macro F1, micro F1, support vector machine (SVM), logistic regression.\n output (Sparql query): Viktor: accuracy, macro F1, micro F1, support vector machine (SVM), logistic regression.\n Viktor is currently not able to provide a list of the models that have been tested on SST-2.\n\n<|endoftext|>                                Fourth Court of Appeals\n                                       San Antonio, Texas\n                                  MEMORANDUM OPINION\n                                          No. 04-19-00953-CV", "mark Hendrycks Test dataset with the Sparql query SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Hendrycks Test dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Hendrycks Test dataset has a score of 48 and the metric it ranks first for is mean average precision.\n output (Sparql query):marined48 mean average precision\n input (English text): Mean average precision is the metric that ranks first for Hendrycks Test dataset?\n output (Sparql query):marined48", "pend (Sparql Query): SELECT?metric?metric_lbl?metric_link WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n<|endoftext|>Popular White Paper On This Topic:\n\n+\n\n+\n\nMany home security companies offer digital video recording for a small monthly fee, such as DVR, but this service often lacks both video quality and the recording capacity needed to record full HD 1080p videos. However, there are a few options to consider, and some offer better quality and capacity than others.\n\n+\n\n+\n\n+\n\nBuzzFeed has a great guide to reviewing home security cameras. The overall verdict will depend on how you use the camera, whether it's for business or personal use, whether you need night vision, and how much", "input (English text): List the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n output (Sparql query):SELECT DISTINCT?title?id WHERE {?article article:hasVersion?version;?article article:hasSource?source;?article article:hasSourceLanguage?sourceLang;?article article:hasSourceEncoding?sourceEncoding;?article article:hasSourcePublished?yearPublished;?article article:hasSourceKind?sourceKind;?article article:hasSourceType?sourceType;?article article:hasSourceDimensions?dimensions;?article article:hasSourceDataPointLabels?dataPointLabels;?article article:hasSourceRelationLabels?relationLabels;?article article:hasSourceRelationTypes?relationTypes;?article article:hasSourceRelationValues?relationValues;?article article:hasSourceRelationExamples?relationExamples;?article article:hasLanguage {?source lang?sourceLang;?sourceEncoding?sourceEncoding;?yearPublished?yearPublished;?sourceKind?sourceKind;?sourceType?sourceType;?dimensions dimensions;?dataPointLabels dataPointLabels;?relationLabels relationLabels;", "input (English text): Query 1: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  Query 2: SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClueWeb09-B\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }  Query 3: SELECT?metric?metric_lbl WHERE {?dataset a orkg", "inal (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Human-annotated annotations on the DocRED (Relation Annotated) dataset: https://nlp.semanticweb.org/projects/dswe/ DocRED (Relation Annotated) dataset.\n output (Sparql query):", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n@searchCashflows {\n   ?dataset a orkgc:Dataset;\n    rdfs:label?dataset_lbl.\n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n   ?benchmark orkgp:HAS_DATASET?dataset;\n    orkgp:HAS_EVALUATION?eval.\n    OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. }\n}\n\n<|endoftext|>Washington (CNN) Ohio Republican Attorney General Mike DeWine announced Wednesday he", "nement: COPY + AUTHENTICATION INSTRUCTIONS INDEXES FROM OPENKM HASH TABLE + DROP TABLE ~index + TABLE STORAGE + CREATE VIRTUAL TABLE + EXECUTE SQL QUERY (?) FROM STDIN + OPEN SYSDATE + ON COLUMN id + BEGIN EXECUTE 'DROP TABLE IF EXISTS ~index' + COMPOSITE ( 'DROP TABLE IF EXISTS ~table' ) + COMMIT + LOG |\n |evaluation.eval |benchmark.has_dataset |benchmark.has_evaluation |metric |metric_label | metric_label_label | SELECT DISTINCT?metric?metric_lbl | FROM |SELECT * FROM OPENKM HASH TABLE |\n |hood|hood|hood|orkgp|orkgp|orkgp|orkgp|HAS_DATASET|HAS_EVALUATION|HAS_METRIC|HAS_LABEL|HAS_LABEL|HAS_DATASET|HAS_EVALUATION|HAS_METRIC|HAS_LABEL|HAS_LABEL|\n |dbtable|index|table|index|table|index|table|", "uate the query, and note the 4th element, \"Abstracts' entities and relations annotated corpus\" appears 4 times in the results and the metric is specified once for each result.\n\n components = {\n \"dataset\": orkgp:HAS_DATASET, \n \"dataset_lbl\": orkgp:HAS_EVALUATION, \n \"eval\": orkgp:HAS_EVALUATION, \n \"metric\": orkgp:HAS_METRIC\n }\n\n query = \"SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\" \n\nresults = {}\n\nfor entry in components.items():\n   if not \"Abstracts' entities and relations annotated corpus\" in entry:", "proof (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orngl:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI dataset\")?benchmark orngl:HAS_DATASET?dataset; orngl:HAS_EVALUATION?eval. OPTIONAL {?eval orngl:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): QNLI is another Named Entity Recognition dataset. Metrics used to evaluate models on this dataset can be found here: https://papers.nips.cc/paper/5782-unifying-loss-based- ranking-and-ensembles.pdf.\n output (Sparql query):ubernetes (English text): rdfs:label \"QNLI dataset\"\n output (Sparql query):ortedoc (English text): QNLI dataset\n output (Sparql query)::rdfs:label \"QNLI dataset\"\n output (Sparql query): orngl:HAS_EV", "input (English text): The method that achieved the highest Entity F1 score on the SciERC benchmark dataset is called Multi-OAC and is implemented in the open-source package MoniKit.\n\n  SELECT?metric?metric_lbl WHERE { \n   ?dataset a orkgc:Dataset; \n    rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"SciERC benchmark dataset\") \n   ?benchmark orkgp:HAS_DATASET?dataset; \n    orkgp:HAS_EVALUATION?eval. \n    OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n  }\n\n ?metric rdfs:label \"Entity F1 score\"@en.\n\n  The Entity F1 score measure the performance of a machine learning model on the task of identifying which entities (or relations) are present in an input dataset.\n\n  The highest reported Entity F1 score on the SciERC benchmark dataset is 0.903427. The Multi-OAC method, implemented in the open-source package MoniKit, achieved this", "<http://www.w3.org/TR/sparql-service-ref/>\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-5 Fine-grained classification\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): SST-5 Fine-grained classification evaluation metrics are: Top-1, Top-5, Coverage, Precision, Recall, F1.\n\n<|endoftext|>The world is dying. That is an undeniable fact. The elderly are dying disproportionately faster than any other age group, even the young. The media portrays it as an insoluble demographic phenomenon: old people are living longer and therefore have more time to die. But, perhaps more accurately, the world is ageing. As populations grow old, there is a corresponding increase in the population of", "p Sudo:select * where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")}\n\n  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")}\n\n  output (Sparql query):Natural Questions (long),precision: 0.99, recall: 1.0, F1-score: 0.99\n\n  metric    metric_lbl\n  precision  Natural Questions (long)\n  recall     1.0\n  F1-score   0.99\n\n  Highest value for the \"Natural Questions (long)\" dataset for the metric \"precision\": 0.99\n\n  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")}\n\n  output (Sparql query):", "mark:has_dataset (has_dataset) means the model can reason about the data (abbreviated 'RDFO') and so should be able to score high against that (RDFS) dataset. Eval:has_evaluation (has_evaluation) means the model has been evaluated on a particular dataset/task and so the value (metric) should be higher. The corresponding query in SPARQL is as follows: SELECT?metric?metric_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Czech-English\")?benchmark } \n input (English text): The corresponding Sparql query is SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Czech-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.", "output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sector Modeling\")}\n\n<|endoftext|>1. Field of the Invention\nThis invention relates to a device for electromechanical locking of an accelerator pedal of a motor vehicle, with an accelerator pedal unit and an accelerator pedal brake, with an energy-storing inertial element which can be stressed when a force is applied to the accelerator pedal unit, and with a tension spring device which exerts an elastic force on the accelerator pedal brake in the locking position, the tension spring device having a first end which is fixed to the accelerator pedal unit and a second end which is fixed to the accelerator pedal brake.\n2. The Prior Art\nA device of this type is known from DE 10 2007 033 627 A1, for example. In that case, the tension spring device is embodied in the form of a tension spring, the first end of which is fixed to the accelerator pedal unit and the second end of which is fixed to the accelerator pedal brake. The first end is loaded via a corresponding mechanical pretensioning mechanism, for example, against a mechanical", "ition (Sparql query): select distinct?metric?metric_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text): The Kinetics-600 dataset is a benchmark dataset for RL model evaluation on video games. The dataset has annotated entities and relations on Kinetics-600 dataset for toy driving, F1-style car racing and 2D drone racing video games. \n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALU", "input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?\n output (Sparql query):SELECT?paper_id (?paper_id uri:href) WHERE {?paper a orkgc:Paper;?paper_id ght:id.?paper_id?paper. }\n\n input (English text): For papers that have performed benchmarks on the Rotowire (Content Selection) dataset, the papers and the metrics they employed are as follows: \n  - Paper 1\n    - Metric 1\n    - Metric 2\n  - Paper 2\n    - Metric 1\n    - Metric 2\n\n \ufeff \ufeff\ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }?metric name precision recall f1 count Indonesian 13.3372% 4.3158% 0.75 1459 German 13.9274% 5.6524% 0.75 722 French 14.3333% 6.4167% 0.80 138 French positive 14.3333% 6.4167% 0.80 138 Italian 14.4523% 6.4167% 0.80 138 Japanese 15.4167% 6.4167% 0.80 138 Korean 14.4167% 6.4167% 0.80 138 Spanish 15.6167% 6.4167% 0.80 138 Chinese 14.8333% 6.4167%", "output (Sparql query):SELECT DISTINCT?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dmlab-30 dataset\")?benchmark. }\n input (English text): SELECT DISTINCT?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dmlab-30 dataset\")?benchmark. }\n\nThis query should only return the name of the metrics that are used to evaluate models on the Dmlab-30 dataset and the names of the research papers that performed benchmarks on the Dmlab-30 dataset. The following are the metrics that are used to evaluate models on the Dmlab-30 dataset:\n\nPrecision, Recall, and F1-score metrics that are commonly used in Machine Learning, but other than that I don't know the names of the metrics.\n\n\r\n\r\n> Query 1 output: SELECT DISTINCT?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_l", "input (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe DISTINCT keyword filters the query results to the selected labels and metrics. The output of the query looks like this:\n+--------------------+--------------+\n| metric_lbl          | metric       |\n+--------------------+--------------+\n| WMT2016 German-    | WMT2016 German|\n| English            | English     |\n+--------------------+--------------+\n\nThe Sparql results have the benchmark and the evaluation metrics as string values. In the given sample query, the evaluation metrics are WMT2016 German-English.\nNote: There are other preprocessing options you could take, such as filtering out abstracts with labels other than", "set (Abstracts' entities and relations annotated corpus): Cartpole (dmcontrol500k), swingup (dmcontrol500k) FILTER (str(dmcontrol500k) = \"swingup\") 3.39, swingup (dmcontrol500k) 3.37, Cartpole (dmcontrol500k) 3.42, Cartpole (dmcontrol500k):speed 3.59 m/s, speed of swingup (dmcontrol500k) 3.6 m/s, Cartpole:speed 3.59 m/s, Cartpole:power 9.4 J, swingup:speed 3.37 m/s, swingup:power 7.7 J, Cartpole:power 11 J, speed of swingup: 3.6 m/s, 3.42\n Sparql query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe Sparql query above lists the distinct metrics that are used to evaluate the models on the benchmarked datasets. The query returns the following columns:?metric,?metric_lbl.\n\n\nThe metrics that are used to evaluate the models on the benchmarked datasets are the following:\n\n- Precision: The precision metric evaluates the recall of the model in identifying positive samples of entities that appear in an abstract as entities in the text but not as relations.\n- Recall: The recall metric evaluates the recall of the model in identifying positive samples of relations that appear in an abstract as relations in the text but not as entities.\n\n\nNote", "{ \"http:// papers.nips.cc/paper/4327-improving-abstractive-semantic-parsing-with-lstm.html\"; \"http://dl.acm.org/citation.cfm?id=3088046.3088055\"; \"http://www.semeval.org/mero/2017/aprile/paper_files/meta/semeval17.pdf\"; \"http://dl.acm.org/citation.cfm?id=3088046.3088055\"; \"https://github.com/Abhisheksu/DSTK-WMT17/blob/master/src/main/resources/DSTKWMT17.pyt\");\n\n input (English text): The LSTM model used in the BMemb++ 2018 paper is present in at least one of the links provided?\n\n output (Sparql query): True\n\n\r\n\nA Sparql query to evaluate the LSTM model on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (", ": SELECT DISTINCT?metric?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?eval?metric.?eval rdfs:label?value. } }\n input (English text): The top result has the metric \"number of times abstract\" and value 81715.\n output (Sparql query):urdy Number of times abstract=81715\n\n<|endoftext|>Investment Objective\n\nInvestment Objective: To seek income and growth with an emphasis on capital appreciation through income and capital appreciation stocks, fixed income securities, certificates of deposits, savings bonds and inflation-protected securities.\n\nTreasury Inflation-Protected Securities (TIPS) continue to offer the best yield on an inflation-protected securities, especially with the recent decline in long-term TIPS yields. They are generally considered a \"safe haven\" investment, and some economists and organizations consider", "inationSet#1: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n selectionSet#1: { \"classification accuracy\" \"pos\" \"precision\" \"frac\" \"recall\" \"f1 score\" \"support\" \"upper_bound\" \"support_frac\" }\n\n<|endoftext|>Introduction {#s1}\n============\n\nIt is estimated that over 270,000 women in the United States will be diagnosed with invasive breast cancer (Braun et al., [@B9]), and that 60% of all breast cancer deaths can be attributed to the metastasis of untreated primary tumors (Bostrom et al., [@B8]). Metastatic breast cancer is generally considered incurable at this point, and the median survival of patients is about", "SELECT DISTINCT?metric?metric_lbl \nWHERE { \n   ?dataset a orkgc:Dataset; \n    rdfs:label?dataset_lbl. \n    FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; \n    orkgp:HAS_EVALUATION?eval. \n    OPTIONAL { \n       ?eval orkgp:HAS_METRIC?metric. \n       ?metric rdfs:label?metric_lbl. \n    } \n} \n\nA:\n\nYou can look at the evaluation scripts in the repository on Github. In particular, you can see here how abstracts entities and relations annotated corpus was benchmarked. The evaluation script used for that is available here.\n\n<|endoftext|>Porsche 911 Carrera S - Superlight with Luxury\n\nPorsche 911 Carrera S - Superlight with Luxury is a car review site that allows car buyers to discover how Porsche 911 Carrera S compares to other Porsche models in terms of styling, performance, luxury and fuel efficiency. This website is not owned by Porsche", "input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset.\n output (Sparql query): SELECT DISTINCT?paper_id?title?id FROM { \n    #WOS-46985 relation-db:papers(paper_id,title,id) \n    #paper_id is of class xs:string,\n    title contains \"Abstracts' entities and relations annotated corpus\",\n    id contains \"wos:0006985\"\n  } WHERE {?paper_id a paper-id:ID.?paper_id wos:0006985.?title a title:string. \n    FILTER(str(title) = \"WOS-46985 dataset benchmark\") }\n\n input (English text): The following are research papers that have performed benchmarks on the WOS-46985 dataset:\n - Wilbert van der Graaf, Jon Kalinowski, Herve Bordes, Simone Rota, \"Neural Architecture Search with Sampler Restart.\"\n\n<|endoftext|>Django Unchained type Movie\n\nThe gory 2023 setting of Quentin Tarantino\u2019s Django Unch", "comment: The highest benchmark result achieved on the Supervised: dataset, including the metric and its value is 0.9877873227.\n input (English text): ##########################################################################################\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The highest benchmark result achieved on the Supervised: dataset, including the metric and its value is 0.9877873227.", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The A3 metric is the best performing metric, achieving an area of A3 in the test dataset.", "inal input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2019\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics of evaluation are the recall, precision, accuracy, precision@1, precision@2, precision@3, f-score, n-gram prec, using the \"iNaturalist 2019\" dataset.\n output (Sparql query):quired output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist 2019\")?", "set { catkgc:EvaluatedModel evaluation (entities and relations) model; evaluationorkgp:HasEvaluation metric; evaluationorkgp:HasEvaluationLabel metric_label; knc:Atari2600SpaceInvadersdataset; knc:HasEvaluationEvaluation metric_evaluation; knc:HasEvaluationEvaluationLabel metric_evaluation_label }\n\n 12:14 \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n/* Here are the models benchmarked on the Atari 2600 Space Invaders dataset:  */ \n evaluationorkgp:HasEvaluationEvaluationLabel metric_evaluation_label \"Visualise properties of learned model.\"\n evaluationorkgp:HasEvaluationEvaluationLabel metric_eval", "\u2419n \u2419 33 Which model has achieved the highest BLEU score of 33.84% on the WMT2014 English-German benchmark dataset?\n\n\nThe following Sparql queries get the necessary results:\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\naggio\u2419n\u2419 33.84%\n\n\nThe following Sparql query selects the value of?metric and prints it out:\n?metric rdfs:label \"BLEU\"@en\n\n\nThe answer is \"BLEU\"@en, which stands for BLEU (alphabetical source name for the algorithm \"Panda Authoritatively Unwrapped: TheLM004Model\"; https://www.statmt.org", "answer (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n indifferent answer: \n metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: \n- accuracy \n- AUC \n- F1 score \n- mAP \n- MSE \n- R2 score \n metrics that are used to evaluate models on the Cart Pole (OpenAI Gym) dataset: \n- perplexity \n- recall \n- recall at 5% precision \n- r2 score \n?metric?metric_lbl:accuracy,?metric_lbl:AUC,?metric_lbl:F1,?metric_lbl:mAP", ":Response: SELECT DISTINCT?metric?metric_lbl?eval WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Databricks ingest is an open-source framework for ingestion, transformation, and processing of unstructured data from any source. A3C FF hs model is deployed on top of Databricks and is based on the Adagrad algorithm. https://github.com/databricks/ adagrad. https://arxiv.org/abs/1611.07080 The paper also provides details on the evaluation metric used on the A3C model?\n output (Sparql query):", "input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?\n output (Sparql query): SELECT * FROM http://pages.cs.wisc.edu/~gharabwp/selfattention.html WHERE method = \"SelfAttention\"\n\n\nThe code referenced in the paper \"SelfAttention: A Self-Attentive Encoder-Decoder Architecture for Abstractive Language Modeling\" (https://arxiv.org/abs/1606.04474), in section 3.2, uses the following models: \n\n1- The BiDAF model trained on a large Chinese web-text dataset (https://github.com/dmlc/dmel).\n2- A Self Attention Network (SAN) architecture described in \"Self Attention for Abstractive Language Processing\" (https://arxiv.org/abs/1708.05770).\n\n\nThe following are the code references that use the above models: \n\n\"Self Attention for Abstractive Language Processing\" (https://arxiv.org/abs/1708.05770)\n\n\n# Download the BiDAF model trained on a large Chinese", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NCBI Disease benchmark dataset\")?metric orkgp:HAS_METRIC?metric; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_DATASET?dataset;?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the MetaCentre dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MetaCentre dataset\")?metric orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC", ":https://github.com/kavita-prabhu/qrlnn \n input (English text): The paper GBDT-6-Layer Deep Neural Network for Relational Sentence Classification in Web Pages: GitHub Link has used the code reference in the last paragraph to generate the following Sparql query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GBDT-6-Layer Deep Neural Network for Relational Sentence Classification in Web Pages: GitHub Link\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe following metrics are used in the Abstracts' entities and relations annotated corpus benchmark dataset:\n<metrics>\n<metric>Benchmarking Report</metric>\n<metric_lbl>Abstracts' entities and relations annotated corpus</metric_lbl>\n</metrics>", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>3/5 + 206*q**2. What is i(b(n))?\n16*n**2\nLet s(p) = -4*p. Let z(n) = -2*n + 920. Determine s(z(d)).\n8*d - 3680\nLet k(y) = 5*y. Let b(p) = -28*p. What is b(k(i))?\n-140*i\nLet z(l) = -4*l**2 + 0*l**2 + 3*l**2. Let q(y) = -3*y**2 -", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Title: Research Papers that contain a benchmark over the NYT24 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the NYT24 dataset benchmark?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Title: Research Papers that contain a benchmark over the NYT24 dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkg", ":: Text\n output (Sparql Query Results):onte :: Text\n output (Sparql query results):,\"SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\"\n input (English text): The datasets used in the benchmarking of the Abstracts entities and relations annotated corpus are orkgp:HAS_DATASET?dataset and orkgp:HAS_EVALUATION?eval.\n output (Sparql query): underm :: Text\n output (Sparql Query Results):,\"SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstract", ":SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text):  SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl.", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nAnd the result is :\n# sort results by metric name\nSELECT DISTINCT?metric?metric_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IMDb-M benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n# 0 < or.pageRank < 1", "ined: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe following metrics are used in the benchmarking task for Abstracts' entities and relations:\n\n?metric\t?metric_lbl\n ontological_coverage\tOntological Coverage\n precision\tPrecision\n recall\tRecall\n f-measure\tF-measure\n accuracy\tAccuracy\n logloss\tLogLoss\n sentence_level_utility\tSentence-level Utility\n\n?benchmark\t?eval\n\n orkgp:HAS_DATASET\trdfs:label\tAbstracts' entities and relations annotated corpus\n orkgp:HAS_EVALUATION\t?eval\n\n orkgp:HAS_METRIC\t?", "====== ======= ========= \noutput (Sparql query): SELECT DISTINCT?metric?metric_lbl \nwhere {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Audio classification research area\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. \n OPTIONAL {?eval orkgp:HAS_METRIC?metric. \n?metric rdfs:label?metric_lbl. } }\n\nThe metrics that are used to evaluate models on the Audio classification research area annotated corpus benchmark dataset are:\nmetrics\tmetric_lbl\t\nprecision\tPrecision\t\naccuracy\tAccuracy\t\nf1-score\tF1 Score\t\nsupport\tSupport\t\nprecision\tPrecision\t\naccuracy\tAccuracy\t\nf1-score\tF1 Score\t\nsupport\tSupport\t\nprobability\tProbability\t\nprecision\tPrecision\t\naccuracy\tAccuracy\t\nf1-score\tF1 Score\t\nsupport\tSupport\t\nprecision\tPrec", "set (Cart Pole (OpenAI Gym) dataset): rdfs:label \"Cart Pole (OpenAI Gym) dataset\". \n output (Sparql query):SELECT ?metric?metric_lbl  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The Cart Pole (OpenAI Gym) dataset has the following metrics: Mean Average Precision (mAP). The Cart Pole (OpenAI Gym) model achieved an mAP of 0.32. The top five models are: Kalman Filter, LSTM, Concat, Global Softmax. The top model is the Global Softmax model, with an mAP of 0.32.", "ition: List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The titles and IDs of research papers are: \n Aral et al. 2016.\u00a0Artificial Intelligence Case Study: Breast Cancer Diagnosis.\u00a0In\u00a0Proceedings of the\u00a044th Annual Meeting of the Association for Computational Linguistics (ACL), Paris, France, pp.\u00a0529\u2013536. ACL2016.\u00a0\n Bordes et al. 2016.\u00a0Gated Recurrent Neural Networks for Hyperspectral Image Recognition. In\u00a0ICCV (3rd), pp.", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet model\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\n<|endoftext|>El 8 de marzo pasado, en la reuni\u00f3n de la Agencia Federal de Investigaci\u00f3n (AFI), el diputado del Partido Humanista, Dardo Anguiano, conden\u00f3 y solicit\u00f3 apoyo a la investigaci\u00f3n de la desaparici\u00f3n forzada de Estela de Carlotto, la Bombonera.\n\nEl legislador explic\u00f3 a la instituci\u00f3n que la captura de Estela de Carlotto hace a\u00f1os en una quebrada de Toledo y trasladada a Maldonado (Ushpehi), no es secreto, y tiene testimonios suficientes, en espec\u00edfico: Carlos Negretto,", "SELECT?metric_lbl?metric_score \n                            FROM {urn:omeda:2006-ace-2005-dataset-benchmark} WHERE {\n                              ?dataset a orkgc:Dataset; \n                               rdfs:label?dataset_lbl; \n                              ?benchmark hasdataset ;\n                              ?eval hasmetric.\n                             } \n                          FILTER isrrdf:comment(\"This query should only return the top-scoring dataset in the ACE 2005 benchmark.\").\n\n\f\n\nOutput: \n- metric: Precision\n- metric: Recall\n- metric: F-measure\n- metric_lbl: Abstracts' entities and relations annotated corpus\n- metric_score: 0.93173379317337932\n\nYou can find the results for all datasets in the OMOP Benchmark results page.", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DBpedia benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.", "mark:HIT 2 rdfs:label \"Abstracts' entities and relations annotated corpus benchmark result\"\n\nWhich metrics are used to evaluate models on the Abstracts\u2019 entities and relations annotated corpus benchmark dataset?\n\nSELECT DISTINCT?metric?metric_lbl \n where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe top benchmark result (metric and value) over the dataset Atari 2600 Enduro?\n\nRESULTS:\n mark:HIT 2 rdfs:label \"Abstracts' entities and relations annotated corpus benchmark result\"\n\n<|endoftext|>1. Introduction {#sec1}\n===============\n\nMethicillin-resistant *Staphylococcus aureus* (MRSA) is now an extremely widespread pathogen that causes significant morbidity and mortality in both hospital and community settings. An", "Sinnvall answer: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Hutter Prize dataset\")?metric.?metric rdfs:label?metric_lbl. } \n\nHutter Prize Dataset Metrics:\n\n- mAP: Mean Average Precision.\n- R-Prec: Recall on Positive examples per total.\n- SPAN: Specificity (or 1-F1).\n- SPES: Soft Precision (or F1).\n\nuvant Sinnvall answer: mAP, R-Prec, SPAN, SPES", "Shapiro 108.15 \n input (English text): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n output (Sparql query):?metric seel.cse.lsu.edu/data/re17.zip 108.15\n<|endoftext|>Q:\n\nUIImageView flashing when zooming in iphone\n\nHere is my code, whenever i zoom in i get a flashing UIImageView. I am trying to hide this UIImageView but could not do so, could someone help me on this? Thanks in advance.\nNSURL *imageURL1 = [NSURL URLWithString:imageURL];\n    NSData *imageData1 = [NSData", "form the Atari 2600 Wizard of Wor dataset to the DBPedia Sparql endpoint named dl.openstreetmap.org, and execute the following query: \n SELECT * WHERE {?dataset a owl:DatASet. \n    ?dataset owl:defaultRel \"isA\" owl:RestrictIs toEntity. \n    ?dataset dl:id \"http://download.opendeph.net/dl/wow_eval.tar.gz\". \n    ?benchmark owl:sameAs?dataset. \n     OPTIONAL {?benchmark owl:sameAs?dataset. \n         ?benchmark dl:id \"dl.openstreetmap.org\".\n         ?benchmark owl:sameAs dl:inpaceWith?eval. } \n    ?benchmark owl:sameAs?dataset. \n     OPTIONAL {?benchmark owl:sameAs?eval. } } \n   \n\n benchesold = 29.59 & # of References = 29 \n metric = MetaHash\n metric_lbl = Benchmark score\n\n<|endoftext|>SURVEY: LGBT Community Worried Over Trump Presidency\n\nMore than half of the LGBTQ community are worried", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"List the code links in papers that use the 12-layer Transformer-XL model in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): https://research.google.com/fields.html?view=fields&sprefix=Transformer+XL&lvl=en\n output (Sparql query):Transformer XL\n\n<|endoftext|>Milton Keynes is set to become the UK\u2019s first Global 100 Smart City, receiving financial support from the Government through the Mayor of London\u2019s Business Academy and the UK\u2019s Tech City Network.\n\nAccording to a report by the UK government\u2019s Tech City UK network, the city will be investing in the \"Internet of Things\", or the sensors that monitor and influence almost every", "Select?metric?metric_lbl \nwhere {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nSelecting the Score metric:\nThe best performing model for the Atari 2600 Up and Down dataset in terms of Score metric is the DeepVis model with the following metric label: score.\n\n answer = SELECT?metric?metric_lbl \nwhere {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?", "Input: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\niston Sparql, the query language of the Web ontology language project (OWL) and RDFa. It uses the Resource Description Framework (RDF) syntax, but there is also a document-oriented view known as JSON-LD. OWL is an OGC standards-based format, and its corresponding query language is also based on OGC standards, namely on the Web Ontology Language (OWL).", "inic (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Jacquard dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The Jacquard dataset is comprised of datasets annotated with relations and entities as metadata. The dataset has a single entity type, with relations between pairs of entities. The relationships between the entities can be direct or inverse. For the purpose of the Jacquard dataset, the metrics evaluated by the model for each entity-related pair is: relevance, precision, recall, F1 score and the entity identification rate. For the relations between the entities, the evaluation metrics are precision, recall, F1 score, the entity identification rate and the entity score.\n output (Sparql query):SELECT?metric?metric_lbl WHERE {?dataset a or", "output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ObjectNet dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): ObjectNet is the dataset of the ObjectNet collaboration. This dataset contains 1183 RDF triples, each triple corresponding to one statement of an academic article. The dataset is split into six subsets, one for each evaluation metric. In addition, each subset contains a summary of the evaluation results for each metric, with the number of statements annotated per metric.\n output (Sparql query): onCreate  ObjectNet is the dataset of the ObjectNet collaboration. This dataset contains 1183 RDF triples, each triple corresponding to one statement of an academic article. The dataset is split into six subsets, one for each evaluation metric. In addition, each subset contains", "ukp:HAS_DATASET?dataset, inform_schema:HAS_EVALUATION?eval where {?eval orkgp:HAS_EVALUATION?eval.?dataset enwik8:Abstracts' entities and relations. } \n input (English text): Output metric names for evaluation on Abstracts' entities and relations annotated corpus benchmark dataset?\n output (Sparql query): SELECT?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): Evaluate the query?benchmark on the Abstracts' entities and relations annotated corpus benchmark dataset. The dataset is provided in the where clause.\n output (Sparql query): { \"Title\": \"Deep kernel learning for semantic role labeling\", \"Id\": \"DR", ":SELECT * FROM xref WHERE xref/@kind='http://www.omg.org/spec/FGVC/20140821/PDF' AND xref/href=@/benchmark/FGVC%20Aircraft/final/parms.html AND xref/param[@name='lambda']=3.16 OPTIONAL {xref}\n\n Gower, A., & Raile, D. (2013). Coreference resolution with label propagation. In Proceedings of the joint ACL/EACL turnitin workshop on multi-annotator coreference resolution, pages 51\u201356. http://aclweb.org/anthology/N13/N13-1027.pdf\n\n<|endoftext|>                                                              United States Court of Appeals\n                                                                       Fifth Circuit\n                                                                    F I L E D\n                 IN THE UNITED STATES COURT OF APPEALS\n                         FOR THE FIFTH CIRCUIT                     November 9, 2005\n\n                                                                Charles R. Fulbruge III\n                                                                        Clerk\n                               No. 05-50505\n                             Summary Calendar\n\n\n\nUNITED STATES OF AMERICA,\n\n                                      Plaintiff-Appellee,\n\nversus\n\nJEFFREY SCOTT THOMAS", "input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVAL", "answer: Abstracts' entities and relations annotated corpus \n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe corresponding SPARQL query used to obtain the output is below:\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:", "answer: SELECT DISTINCT?model?model_label WHERE {?model atari2600_model; rdfs:label?model_label. FILTER (str(?model_label) = \"Atari 2600 Boxing\") } \n input (English text): The Atari 2600 boxing dataset has been evaluated by Sharpened and Singular. Sharpened obtained the best result with the model that maximizes the cumsum of the loss function. Singular obtained the second best result. Sharpened's model was designed to be trained on the CartPole dataset and evaluation on the Atari 2600 Boxing dataset. \n output (Sparql query): ** Sharpened's model **\n model    Atari 2600 Boxing model_label\n sharpened Atari 2600 boxing \n output (Sparql query): ** Singular's model **\n model    Atari 2600 Boxing model_label\n singular Atari 2600 boxing \n input (English text): The best model is Sharpened's model with the Atari 2600 boxing label.", "output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English dataset\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nA:\n\nThe metrics that are used to evaluate models on the WMT14french-english dataset are the BLEU (Pseaked Loss) score, the METEOR (Phrase-Based Reading Eqloss) score and the ROUGE (Recall-O-Utility) score. These scores can be calculated for each of the benchmark resources using the Java API.\n\n<|endoftext|>Q:\n\nWhen I'm building two separate nifi flows, is there a way to put a mutex on the last steps in both of them?\n\nI want to do some processing after both flows have been finished. I need", "pend (Sparql query): SELECT?model WHERE {?model a orkgp:EvaluatedModel;?model g orkgp:GroupeEvaluation;?model_groupe g:groupe_id g:groupe_code \"BUCC\";?model_epars g:data_type g:data_type_short;?model_epars g:object_id g:object_id_short;?model_epars g:resource_uri g:resource_uri_short;?model_epars g:resource_uri_namespace g:resource_uri_namespace_short;?model_epars g:evaluation_date g:evaluation_date_short;?model_epars g:problem_type g:problem_type_short;?model_epars g:evaluation_time g:evaluation_time_short;?model_epars g:data_collector g:data_collector_short;?model_epars g:language g:language_short;?model_epars g:upper_bound g:upper_bound_short;?model_epars g:lower_bound g:lower_bound", "mic (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nci\u00f3n (Sparql query): specimln (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", "input (English text): What models are being evaluated on the AESLC dataset?\n output (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_EVALUATION?eval.?metric orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The AESLC dataset is benchmarked to evaluate model performance on Sparql queries of the type: \n SELECT... WHERE... \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }\noutput (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkg", "SELECT DISTINCT?metric?metric_lbl WHERE { \n         ?benchmark HAS_DATASET HoC.\n         ?benchmark HAS_EVALUATION HoC.\n         ?dataset HoC:Abstracts' entities and relations annotated corpus.\n          rdfs:label?dataset_lbl. \n          FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n         ?benchmark orkgp:HAS_DATASET HoC; orkgp:HAS_EVALUATION HoC. \n          OPTIONAL {?benchmark orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } \n        } \n\nThe query results include the following data:\n+------------------+------------------------------------------------------------------------------------------------------------------+\n| Metric           | MetricLabel                                                                                                      |\n+==================+===================================================================================================================+\n| document count   | paper count                                                                                                      |\n+------------------+------------------------------------------------------------------------------------------------------------------+\n| average precision | Average precision                                                                                                |\n+------------------+------------------------------------------------------------------------------------------------------------------+\n| ratio weighted    | Recall at 10%: The weight of an incorrect prediction", "query1: \nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\ntopLevelQueryResults: Query1 topLevelSparqlResults: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119", "{#dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\nThe query selects distinct metric and metric label where the benchmark dataset contains the label Abstracts' entities and relations annotated corpus.\n\nSELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n {#dataset a orkgc:Dataset", "nit : SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): A paper has evaluated the model with a metric?metric and the corresponding label?metric_lbl. The paper has also annotated the CL-Titles-Parser model with the metric.\n\n #include <iostream>\n#include <string>\n#include <vector>\n#include <set>\n#include <map>\n\n#include <ql/char.h>\n#include <ql/text.h>\n#include <ql/queries.h>\n#include <ql/sfq.h>\n#include <sstream>\n\n#include <knn/knn.h>", ":SELECT DISTINCT?metric?metric_lbl?benchmark FROM <http://nwayna.net/datasets/habitat20/habitat20-object-nav-test-std.obo> WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n input (English text):The Habitat 2020 Object Nav test-std benchmark dataset contains ten metrics: Precision, Recall, F1 score, precision@N, recall@N, precision-recall@N, balanced F1 score, and hamming distance.\n output (Sparql query): I would like to list the ten models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset: DREML-C, DREML-D, DREML-DD,", "input (English text): Some of the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are: AUC, BLEU, CE, CIDEr, ROUGE, and SPICE.\n\ngesterone (English text): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\noutput (Sparql query): reinforcements (English text): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; or", "ition (English text): The highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value is \"6.5 ms\".\n\n\n\n               in (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }", ":SELECT DISTINCT?model_name WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"River Raid\")?model_name. }\n input (English text): List of models that have been tested on the River Raid dataset:\n Tom\u00e1s Conte, Peter Fenly, Eitan Hersh, Amos Weinberg, Sam Gilbert.\nyzed2:SELECT DISTINCT?model_name WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"River Raid\")?model_name. }\n On 31/10/2013 8:05 AM, Peter Fenly wrote:\n Some of the models we have tested that use Deep Language Models for problem-solving include: \n 1. The Knowledge Base Expansion system developed by Tom\u00e1s Conte. \n 2. The Sketch IO framework developed by Eitan Hersh. \n 3. The answer selector framework developed by Amos Weinberg. \n 4. The semantic role labeling framework developed by Sam Gilbert. \nyzed2:SELECT", "sparql endpoint to get the AOD and MAOD datasets and metrics from: https://github.com/cmu-dl/projects/tree/master/data\n output (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AOD\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): 1.Krachman et al, AOD: 2014, AlexNet: 2014, MultiGrasp: 2017, 2.Li et al, AOD: 2015, AlexNet: 2015, MultiGrasp: 2016.\n 3.Li et al MultiGrasp has also been used in: Philbin et al, AUDL: 2015, Kuri et al, HumanaiCom: 2016, 4.Philbin et al, JVPR: 2017\n\n AOD", "! List the models that have been evaluated on the Atari 2600 Ms. Pacman dataset! =\noutput (Sparql query):SELECT?model?metric WHERE { \n ?model atari2600MsPacmanModel. \n ?model atari2600MsPacmanModel atari2600MsPacmanModel:Evaluation. \n ?model atari2600MsPacmanModel:Evaluation results:MetricValue?metricValue. \n ?model atari2600MsPacmanModel:EvaluationPeriod (TimePeriod). \n ?model atari2600MsPacmanModel:FaultTolerance?faultTolerance. \n ?model atari2600MsPacmanModel:LicensedEntity (LicensedEntity). \n ?model atari2600MsPacmanModel:Name?modelName. \n ?model atari2600MsPacmanModel:PlatformVersion?platformVersion. \n ?model atari2600MsPacmanModel:RegisteredUsers?registeredUsers. \n ?model atari2600MsPacmanModel:RulesEngineERP9L?rulesEngineERP9L. \n ?model atari2600MsPacmanModel:Version?version", "met:Recall?metric?metric_lbl \n where {?eval orkgp:RECALL?metric.?metric rdfs:label?metric_lbl. } \n query (Sparql query):SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Habitat 2020 Object Nav test-std\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The metrics commonly used in the Habitat 2020 Object Nav test-std dataset are: \n Recall: Orkgp:RECALL \nPrecision: Orkgp:PRC\nMicro F1: Orkgp:MICRO_F1\nMacro F1: Orkgp:MACO_F1", "BMETRICS={ DISEASE RELATED_TO: SYSTEMIC_Lymphatic_Tolcupcyteromyelossy_Systemic-Lymphatic_Tolcupcyteromyelossy_Systemic-Lymphatic_Tolcupcyteromyelossy, SYSTEMIC_Lymphatic_Tolcupcyteromyelossy_Systemic-Lymphatic_Tolcupcyteromyelossy_Systemic-Lymphatic_Tolcupcyteromyelossy } BMETRIC_LABEL=\"Number of literatures\" BMETRIC_SCORE=11.\n\nAn RDF triple stores more information than a single metric, e.g. number of literatures. Here is an example SPARQL query to return the metrics that appear in the list.\n\n\n SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS", "sparql Query: SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet 64x64\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \nThe metrics are: Average precision, Average recall, Identity Margin, Joint entropy, ROC, REC, PAC, DC and GAR.", "set { dbo:Benchmark { dbo:Title \"Databricks for data and AI\" dbo:ID \"databricks_benchmark\" } } dbo:Databricks for data and AI_Benchmark;\n input (English text): Databricks for data and AI benchmark paper contains title and ID of the paper.\n output (Sparql query): SELECT?metric?metric_lbl WHERE {?dataset a dbo:Databricks for data and AI_Benchmark;?dataset dbo:Databricks for data and AI; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Databricks for data and AI benchmark\") } \n input (English text): Databricks for data and AI benchmark paper contains title and ID of the paper.\n output (Sparql query): Databricks for data and AI_Benchmark has a benchmark with ID \"databricks_benchmark\" and a metric with the following label: \"Databricks for data and AI benchmark\"", "SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n glimp from \n\n  final Response response = service.query(Sparql query);\n\n  final List<SparqlQueryResult<Predicate>> queryResults = response.results;\n  final SparqlQueryResult<Predicate> result = queryResults.get(0);\n\n  final List<SparqlQueryResult<Object>> objectResults = result.objectResults;\n  final SparqlQueryResult<Object> resultObject = objectResults.get(0);\n\n  final Set<String> metricNames = new HashSet<>();\n  final Set<String> metricLabels = new HashSet<>();\n\n  for (SparqlQueryResult<Object>", ":https://github.com/ericy/T-ConvS2S_paperCode_benchmarking. This is the code that is used in the paper to benchmark T-ConvS2S. \noutput (Sparql query): SELECT DISTINCT?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): The link above provides the code used in the paper to benchmark T-ConvS2S.\n output (Sparql query): Have you been able to reproduce the results in the code in the paper?\n output (Sparql query): No, we have not been able to reproduce the results in the code in the paper. The code is not clear what are the metrics used to evaluate the model and", "inal input: SELECT * WHERE {?benchmark has_dataset qbert;?benchmark has_evaluation qbert. score?score ; } \n                     {?score rdfs:label \"HIT\". } \n\nThe following are the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset:\n\n*\tMetric: score \n*\tMetric_lbl: HIT", "_benchmark_score(x\"d6c06e52-40a4-4bc8-b265-27e8dbf20b82\") a float 4.0\n output (Sparql query): SELECT?metric_lbl?metric_str WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Words in Context\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n input (English text): \"Words in Context\" is the only dataset in the orkgp:HAS_EVALUATION property of the orkgp:HAS_DATASET property of the orkgp:HAS_EVALUATION annotation of the orkgp:HAS_DATASET annotation of the orkgp:HAS_EVALUATION annotation of the orkgp:HAS_DATASET annotation of the orkgp:HAS_EV", "set benchmarked in this research paper are: Abstracts' entities and relations annotated corpus.\n\n sparql query for above result set: \n SELECT DISTINCT?metric?metric_lbl \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } }\n\n<|endoftext|>Q:\n\nJPA - How to remove non unique constraint violations from log\n\nOur JPA application is throwing below type of SQLException:\nCaused by: java.sql.SQLIntegrityConstraintViolationException: FOREIGN KEY    (countyFK) REFERENCES county(id) DEFERRABLE INITIALLY DEFERRED   \nThe SQL that is being generated for the schema looks like this:\nALTER TABLE parentTable\n  DROP CONSTRAINT childFK,", "mark_hasevaluation OVER (ORDER BY rating DESC) AS rating_sorted \n output (Sparql query): SELECT ?metric?metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset; orkgp:HAS_EVALUATION?eval. OPTIONAL {?eval orkgp:HAS_METRIC?metric.?metric rdfs:label?metric_lbl. } } \n\nThe result is: \nmetric  metric_lbl    \nrating_sorted    5\n\nIt seems that the metrics used for evaluation are: \n-- document rating: 5\n-- relation classification accuracy: 99%\n\n<|endoftext|>Thierry Henry set for Derby day return\n\nThierry Henry will return to management this week after leaving Liverpool two months ago.\n\nThe French international quit Anfield to take control of Thierry Henry & Associates - his own international management company - following the departure of Roy Wegerle to Inter Milan.\n\nHenry joined Kenny Dal"], "cleaned_sparql": ["SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?evalStoryClozeTestStory. } OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (Sparql query): A model achieved an Accuracy score of 96.81% on the Story Cloze Test benchmark dataset. The metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are: * Story Cloze Test accuracy: 96.81% * Precision: 0.953 * Recall: 0.953 * F measure: 0.953 The Story Cloze Test accuracy metric assesses the model's ability to identify the correct missing word in a sentence when presented with a sentence with a", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Ammeo Hot Springs Ammeo Hot Springs, also known as Little Bald Hills Hot Springs, are a series of natural hot springs located on State Route 304, about north of Durango, in Archuleta County, Colorado. The springs are within the Ammeo Wilderness, part of the San Juan Mountains of Southwestern Colorado. The springs were named for the Ammeo Shale, a sandstone formation that runs north to south and forms a portion of therim of the Little Bald Hills. The temperature of the springs ranges from and varies with", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"UrbanSound8k dataset\") OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset; } } input (English text): UrbanSound8k dataset is being evaluated on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query):lical input (English text): UrbanSound8k dataset is being evaluated on the UrbanSound8k dataset ? output (Sparql query):SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"UrbanSound8k dataset\") OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset; } } input (English text): The UrbanSound8k dataset is being evaluated on the Abstracts' entities and relations annotated corpus benchmark dataset ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank dataset\") } OPTIONAL { ?dataset orkgc:PennTreebankDataset; ?eval orkgp:HAS_EVALUATION ?eval. ?eval rdfs:label ?metric. ?metric rdfs:label ?metric_lbl. } Here is a list of metrics that can be used to evaluate models on the Penn Treebank dataset: * perplexity * character onlline * word onlline * bigram onlline * trigram onlline * stopwords * nouns * verbs * adpositions * particles * relations * part-of-speech * named-entity recognition The following metrics refer to named entities in the Penn Treebank dataset: * part-of-speech * named-entity recognition The following metrics refer to the relation types in the Penn Treebank dataset:", "SELECT * { ?eval orkgp:mazea_score:mazea15 \"68.2791\" ; ?metric \"mazea15\" }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } } COVID-19 Diagnosis Criteria: According to the \"Diagnosis and Management of Pneumonia in Adults\", published by the Chinese Medical Association (CMA), the commonly used diagnosis criteria are as follows: Temperature of 37\u2009\u00b0C or higher for at least 1\u2009h; A new or increased white blood cell count", "SELECT ?metric ?metric_lbl where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|> United States Court of Appeals FOR THE EIGHTH CIRCUIT ___________ No. 05-3621 ___________ United States of America, * * Appellee, * * Appeal from the United States v. * District Court for the * District of Nebraska. Danny L. Vasquez, also known as * Danny Luis Garcia, also known as * [UNPUBLISHED] Ninos Valdez, also known as Danny * L", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric", "BMoral (with correction) has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset. And here is the final answer: Based on the response provided above, the final answer is BMoral (with correction). <|endoftext|>The case for regulating \"dark money\" Nov 14, 2017 By Andrew J. Drach The billionaires and corporations who pour money into super-PACs without disclosing their identity are merely buying political influence, yet the campaign finance system as it is now permits super-PACs to spend unlimited amounts to influence elections. The solution is not to eliminate super-PACs; the solution is to restrict the money that corporations and billionaires can spend to support or oppose political candidates, parties, and issues. Super-PACs were created in the mid-1990s as a vehicle for rich donors to spend unlimited amounts to influence elections. So-called independent expenditures or advertisements by corporations and billionaires supporting or opposing political candidates, parties, and issues are not subject to contribution limits. However, wealthy individuals and corporations are restricted from contributing large sums of money to super-PACs. Many individuals and corporations that donate large sums of", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Note that metrics are specified in a label-first, class-hierarchical format: rdfs:label + \" \" + orkgp:HAS_METRIC + \" \" + metric.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Ptr-Net selected metric: Top-1 Accuracy metric label: query: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_MET", "SELECT ?benchmark { union select { ?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\" } } p* { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl; ?dataset_lbl gnrl:arXiv; ?benchmark gnrl:Benchmark; } WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } order by ?benchmark_rank limit 1 }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Betting odds: Man Utd 4/1 to beat City, Swansea to win Champions League The bookies are providing very unfavourable odds for Manchester United to win the Champions League and Swansea to win the Europa League. United are favourites at 4/1 to win the competition, with City at 11/2 to triumph. Swansea are 5/2 to progress to the final. On the table for the Reds to reach the knockout stages would be a major achievement after Manchester City's four previous victories have come via the shoot-out. United, who beat Bayern Munich 4-2 on aggregate, are 6/4 to complete the remarkable comeback, but odds of 8/1 to win at home in the round of 16 are not being taken. If it wins its group, United are 1/5 to top the Premier", "SELECT DISTINCT ?benchmark_title_id_str ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") OR ?dataset_lbl = \"BC5CDR-disease\" } \t BC5CDR-disease: - Abstracts' entities and relations annotated corpus - Disease text classification - Models performance evaluation metrics Related Concepts: \"Abstracts' entities and relations annotated corpus\" benchmarking datasets or \"disease text classification\" models Related Datasets: - BC5CDR-disease: disease text classification BC5CDR-disease \"disease text classification\" models: | | | | | | | Abstracts' entities and relations annotated corpus | | |", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <metric name=\"score\" label=\"Top Performing Model\" /> LAYER GLOBALS (...) REGION OUTPUT (...) ENDFORM A score that represents the quality of a solution to a task. score The score, also called error rate or error rate, indicates the percentage of the incorrect answers (i.e., negative examples) that were correctly classified as incorrect (positive examples). The lower the score, the better the model. score Here is the top-performing model for the LUNAR LANDER dataset: <metric name", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics used to evaluate the models on the FSNS - Test dataset are: - Document Precision : precision(Document/Prediction) - Click-Through-Rate (CTR): Click-Through-Rate - Retweet Rank : retweetRank(Document/Prediction) - F1-score : f1-score(Document/Prediction, Expected)", "SELECT DISTINCT ?metric ?metric_lbl WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } FILTER (str( ?dataset_lbl) = \"ImageNet 64x64 Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Results: selectionon.xhtml page contains no result(s). I am getting a result when I remove the Sparql query part from the above query but I am not sure if the query is correct. Please suggest improvements. A: 1) SELECT DISTINCT ?metric ?metric_lbl this query selects only one metric at a time, and the label of the metric - ?metric_lbl 2) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The query above select the metrics which are \"Abstracts' entities and relations annotated corpus\" that are benchmarked with a dataset and evaluation using an ontology. The query result will contain the metric name and the label for the metric. These are the metrics that are used to evaluate the models on the annotations corpus benchmark dataset: rdfs:label \"accuracy\" rdfs:label \"precision\" rdfs:label \"recall\" rdfs:label \"F1-measurement\" rdfs:label \"support\" rdfs:label", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Here are some metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: Mean Average Precision (AP), Mean Reciprocal Rank (MRR), Mean Union Match (MUM), Reciprocal Rank (RR). \u0627\u0644\u0622\u0646 \u0625\u0630\u0646 \u0644\u062f\u064a\u0643\u0645 \u0623\u0631\u062f\u0627\u0626\u064a\u0651\u0629 \u0645\u0646 \u0627\u0644\u0645\u0635\u0645\u0645\u0627\u062a \u0627\u0644\u0631\u0648\u0628\u0648\u062a\u064a\u0629 \u0627\u0644\u062e\u0627\u0635\u0629 \u0628\u0643\u0645 \u0648\u0644\u062f\u064a\u0643\u0645 \u0645\u062e\u0637\u0637\u0627\u062a \u0628\u0633\u064a\u0637\u0629 \u0641\u064a \u0625\u0639\u062f\u0627\u062f\u0647\u0645 \u0628\u0634\u0643\u0644 \u0637\u0628", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } mentation (Sparql query): BLEU, TER", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } descrition (Sparql query): F1 score is one of the most popular metrics used for evaluating machine learning models. F1 score is defined as the ratio of the number of correctly classified true positive to all positive predictions. This metric can be calculated by first determining how often the model classified as true positive as compared to all the predictions, called as Precision and second, calculating the number of true positives and the number of all predictions as true positives plus false positives. As such, F1 score is an appropriate metric to use when predicting if a single instance belongs to a given class, because the model can", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): https://github.com/zdahondavid/gold-standard-Deep-Mem-CNN-CNN-RNN.git and https://github.com/zdahondavid/gold-standard-Deep-Mem-CNN-CNN-RNN/tree/master/tasks. The \"Single Model\" runs on the Conda environment via MNML. output (Sparql query): The code used by these two papers can be found here: https://github.com/zdahondavid/gold-standard-Deep-Mem-CNN-CNN-RNN/tree/master/tasks. The metrics used are", "SELECT DISTINCT ?metric_lbl ?metric_str WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): metrics evaluated on the ACE 2005 dataset include: accuracy, f1 score, mappable, micro avg and macro avg. viz http://www.cs.cornell.edu/home/caallan/papers/ACE2005.pdf", "SELECT%20%22metric%22%20%22metric_lbl%22%20WHERE%20%7B%22dataset%22%20%3Ca+uri%3E%22http%3A%2F%2Fdatasets.openDataSet.org%2Fpublic%2Fdataset%2F57ac59638fb40c1%3Fsport%3Dodataset%26format%3Dtext%2Ftext%2Fxml%22%20%7D%20or%20%22orkgp%22%20%3Ca+uri%3E%22http%3A%2F%2Fopendatacloud.org%2Fpublic%2Fmodel%2Fpnd%5Flabel%3DPNDec%26format%3Dtext", "SELECT DISTINCT ?metric_ref WHERE { <http://pages.cs.wisc.edu/~cis/ciscomp/harel/papers/shavit.pdf> SHAVIT: A SQuARE Approach To Relational GRAPH AGGREGATION (2018) <http://pages.cs.wisc.edu/~cis/ciscomp/harel/papers/shavit.pdf> doi:10.2138/jlsrf.SHAVIT.2018.1.1. <http://juran.wisc.edu/assets/download/papers/manovitz_markov_2018_smart_semantic_web_ranking.pdf> MANOVITZ, A., & MARKOV, M. (2018, October). Smart Semantic Web Ranking: Beyond Term-Based Models. In CIDR '18 (pp. 2495-2503). ACM. <https://www.dropbox.com/s/s33truk8bh60euw/final.pdf ?dl=0> TAWFI, M. (2018, November). Computing Next Bi-Lingual Sent", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B ? output (Sparql query):", "SELECT ?metric ?metric_lbl ?metric_value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. ?metric_value } } <|endoftext|>/* * Copyright (c) 2010-2018. Axon Services. All rights reserved. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The code references can be found on the SemExp page on the AMIE site. The model has been applied to benchmarking tasks using the AMIE engine. It can be found on the SemExp model page on the AMIE site.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Up and Down dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The evaluation metrics that are commonly used when benchmarking models on the Atari 2600 Up and Down dataset are: - F1 Score - Precision@N - Recall@N - Accuracy - MCC - F1 Measure - Micro Accuracy - Macro Accuracy - Hits@1 - Hits@10 - Hits@100 - Misses@1 - Misses@10 - Misses@100", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level) dataset\") ?benchmark or", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Penn Treebank (Character Level) trained on Penn Treebank (Word Level) ABSTRACTS' entities and relations annotated corpus entity translation model ABSTRACTS' entities and relations annotated corpus entity translation model reconstruction error BLEU harmonic mean of precision and recall reconstruction error harmonic mean of precision and recall harmonic mean of precision and recall AUC Reconstruction Accuracy BLEU harmonic mean of precision and recall harmonic mean of precision and recall harmonic mean of precision and recall AUC Reconstruction Accuracy BLEU harmonic mean of precision", "SELECT DISTINCT ?metric ?metric_lbl WHERE { \r ?dataset a orkgc:Dataset; \r rdfs:label ?dataset_lbl. \r FILTER (str( ?dataset_lbl) = \"2600 Double Dunk dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; \r orkgp:HAS_EVALUATION ?eval. \r OPTIONAL { \r ?eval orkgp:HAS_METRIC ?metric. \r ?metric rdfs:label ?metric_lbl. \r } }", "SELECT ?metric ?metric_lbl where { ?dataset a orkgc:Dataset; ?eval orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?metric orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metrics are part of the evaluation steps performed on the ImageNet V2 benchmark dataset: - Top 1 Accuracy : This metric is the most common metric used in object recognition tasks to evaluate the performance of a model. It is computed by dividing the number of top-1 accurate predictions by the total number of predictions for the model. - Top 5 Accuracy : This metric is a slightly more challenging variant of the Top 1 Accuracy metric. It is computed by dividing the number of top-5 accurate predictions by the total number of predictions for the model. - Top 1 Loss : This metric measures the difference between the ground-truth and predicted label for the positive samples of the model", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Thank you! It is really nice to get feedback! I like all the comments that were positive - even those that say I should not use bash. I will definitely consider them! I will probably choose perl for some scripts and bash for other things (mostly because I know perl better than bash). About your last comment: I have actually programmed several bash scripts but there was no shell. I started to think, \"what if there was a shell\" and it had all the features of bash. After some hours I created this tiny shell and started to program. It had the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are: ?metric : Count of entities annotated with a given metric ?metric_lbl : Label of the metric Query results on the Abstracts' entities and relations annotated corpus benchmark dataset: Dataset Metric Label - Abstracts' entities", "SELECT mean(word) FROM BINARYblogger%27spostgresql%20blog%20words ORDER BY count(*) DESC; BM25 (mean of words ranked by their occurance in a given length of text) It's really a combination of multiple factors such as: i) the model that performed well on the development set, ii) the type of relation between the two entities that have, iii) the type of the relation itself. However, the two entities being related in a range of relations between triple path and property-rel that typically consume a lot of the computation resources and cloud resources is the most common dataset seen for this task.", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES dataset\") OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval rdfs:label ?metric. } } output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES dataset\") OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval rdfs:label ?metric_lbl. } } output (Sparql query): SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES dataset\") OPTIONAL { ?eval orkgp:HAS_EVALUATION", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval_score. ?eval orkgp:HAS_METRIC ?metric. } } input (English text): The F1 score is a common metric to evaluate the performance of Machine Learning models. output (Sparql query):Orkgp:HAS_EVALUATION ?eval_score. <|endoftext|>The name's Mandel. I am an Art Director in the Graphics Department of a publishing house in Munich, Germany. Currently I'm working on a rather special project. It is being developed in the Unity game engine and is using the cool 3D render engine Emu. The project involves motion capture of live actors with a motion capture suit. The final product will be converted into an interactive character that will be used in a", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): ImageNet ReaL is an NLP benchmark dataset based on the 1 million ImageNet ILSVRC 2012 dataset that includesTextual Entities, Relational Entities and Visual Relations. Output (Sparql query): Triton-Benchmark-Metrics output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALU", "SELECT DISTINCT ?title ?metric_lbl ?metric WHERE { ?title dawgn:ResearchPaper. ?title dawgn:Benchmark. ?metric dawgn:Metrics. FILTER ( ?metric_lbl = \" Gibson PointGoal Navigation: metrics: recall, precision, f1-score, mAP, area_under_curve,... \"). } input (English text): The following is a sample of the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset: - Hill, Rajkumar, Eli Sheinkman, and Michael J. Wellman. \"The <span style=\"font-variant:small-caps;\">Gibson</span> PointGoal Navigation Benchmark (PGN).\" International Journal of Human-Robot Interaction 15.2 (2018): 137. - Hill, Rajkumar, Eli Sheinkman, and Michael J. Wellman. \"The <span style=\"font-variant:small-caps;\">Gibson</span> PointGoal Navigation Benchmark: Metrics.\" International Journal of Human-Robot Interaction 17.1 (2019): 5. - Hill, Rajkumar", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Conll++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The top benchmark result (metric and value) over the dataset CoNLL++ is (count. , 2.894492 ). SPARQL execution plan: Limit 1 <|endoftext|>Q: Sum every 3 dates in a data frame and create a new column that groups by id and counts the number of 3 dates I have a data frame that has several rows for each ID. I want to create a new column that sums every 3 dates for each ID in the data frame. I need this to be grouped by id so I can get counts of how many 3 dates there are for each ID. Input df = pd.DataFrame({'", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The best performing model is the model with accuracy around 0.7. output (Sparql query): Benchmarking the PIQA dataset with the best performing model has Accuracy around 0.7. \f SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; ?eval orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } OUTPUT { \"maxFPS\" : \" ?, ?, ?\" } The query returns two metrics \"maxFPS\" and \"time\". If the model performs well in terms of \"Score\" metric, then it returns \"\", \"Scorer\", where as if the model performs poorly, then it returns \" ?, \"WRONG\"\". The query above is correct. If the model performs well in terms of \"Score\" metric, then it returns \"\",", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } | ?metric ?metric_lbl |", "SELECT%20%22metric%22,%20%22metric_lbl%22.%20FROM%20%22(https://mcpher.github.io/funnel/web-sparql/ FUNnelTransformer.owl#FUNCTIONS)%20WHERE%20%22endpoint%22%20%3D%20%22https://www.w3.org/2019/07/14-ml-track-abrv-funnel-transformer/funnel%20-%20Corpus%20Benchmarking%20on%20DMT%20Parallel%20Graph%20Computation%20at%20Intel%20LLVM%20and%20Torch/funnel-eval.owl%22%20AND%20%22metric%22=%3E%20%22F1%22%20%20AND%20%22metric_lbl%22=%3E%20%", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The evaluation metrics commonly used when benchmarking models on the Reuters De-En dataset are: - Average precision - Average recall - Average normalised overlap - F1 score", "SELECT DISTINCT F1 ?metric WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"RFC abstracts model\") OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset. } } A: From http://www.openrdfgroup.org/web/current/home/wiki/Publications_and_Databases#Sparql_queries_that_detailed_answer_your_question: f1: Rises from 0 to 1 where 0 is the worst possible score and 1 is the best. roared from 0 to 1 where 0 is the worst possible score and 1 is the best. precision: Comprises the fraction of relevant facts that are tagged as such by the model. 1 means the model matched every fact as relevant. 0 means none of the facts were tagged as relevant by the model. recall: Comprises the fraction of relevant facts that were tagged as such by the model. 1 means the model matched every fact as relevant. 0 means", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The benchmark Databricks uses the DocRED-BiLSTM model and the metric Humsian similarity. output (Sparql query): SELECT Humsian_similarity( ?metric_lbl, ?metric_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric is the <b>CNN model</b> with a FLOPS of 0.1831.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): Skipping entity evaluation, no dataset available: orkgp:HAS_DATASET orkgc:Dataset <|endoftext|>Q: Android/Java synchronized block not waiting for the Runnable passed as a parameter I have the following code. public class SampleThread extends Thread { public void run(Runnable r) { while(/*condition*/) { synchronized (r) { try { r.wait(); } catch (Inter", "SELECT ?pagenumber ?url ?b ?metric_lbl WHERE { { ?pagenumber pr:page rdf:resource ?url. ?url #ODBC;#MS-SQL;#Oracle;#SQLite. ?url nillable. ?url nillable somesparql:benchmark ?b. ?b #Abstracts' entities and relations annotated corpus. ?b #metric. ?metric #label. ?metric nillable. ?url nillable } { ?pagenumber 1. #ODBC;#MS-SQL;#Oracle;#SQLite. ?url #MS-SQL;#Oracle;#SQLite. #SQLite. #SQL", "SELECT DISTINCT ?metric_lbl ?metric_score WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): WOS-46985 is a dataset about WOS abstracts annotated by rdflib. There is an evaluation benchmark with the following metrics: Precision, Recall, F-Measure, and Distance. The F-Measure is the weighted average of Precision and Recall, with weights of 0.5 ( precision) and 0.5 (recall). output (Sparql query): The top benchmark score is 1813 and its metric is F-Measure input (English text): OK, let's move on. The next query will be to see which metrics are used to evaluate models on the WOS-46985 dataset. The dataset W", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Italiano: Portare la lista dei metriche utilizzati dallo studio dell'AcrE su dataset e'benchmarked nel corpus annotation benchmark. Sparql query: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_MET", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval 2013 metric\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The best performing model in terms of SemEval 2013 metric is the RESNETS dataset. The SemEval 2013 metric is one of the metrics that are used to evaluate models on the datasets annotated corpus benchmark dataset The SemEval 2013 metric is defined as follows: 1. C13 - RNNLM Semantics Error (ms) 2. C13 - Conditional RNNLM (ms) 3. C13 - Neighbourhood Semantics Error (ms) 4. C13 - Word Similarity (ms) 5. C13 - Tree Edit Distance (ms) 6. C13 - Word Association (ms)", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text):The Metrics that are used to evaluate models on the SciERC corpus benchmark dataset are: - NDCG - AUC - METEOR - R-TPR - R-F1 - R-AUC - HITRAP_HITS - TPR_HITS - NDCG_HITS - AP_HITS - F1_HITS - RMS - DROOLS - BLEU The Metrics that are used to evaluate", "SELECT DISTINCT ?metric ?metric_lbl query returns the following: | metric | metric_lbl | |-|-| | quantity | discovered | The SPARQL query used in the SELECT DISTINCT ?metric ?metric_lbl query above can be found here. A: The metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are the following: accuracy: the metric counts the number of correct answers and calculates it over all relevant classes micro-averaged F1 score: the metric averages the exact match (match/total), the same word in the same position in the gold standard and the answer, over all relevant classes macro-averaged F1 score: the metric averages the match (same word in the gold standard and the answer / total", "SELECT DISTINCT ?model ?model_label WHERE { ?model app:Model; app:name ?model_label. FILTER (str( ?model_label) = \"DeepMemNet\") } input (English text): The models that have been evaluated on the VTAB-1k dataset are: DeepMemNet, GatedCRF, GraphCNN, LSTM, Maxout, MultiLAYERPerceptron, and OneHotEncoder. output (Sparql query): Multiple applications have been evaluated on the VTAB-1k dataset: DeepMemNet, GatedCRF, GraphCNN, LSTM, Maxout, and OneHotEncoder.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Code: https://github.com/ultraqshow/DQN-PixelCNN. Download: https://github.com/ultraqshow/DQN-PixelCNN/archive/master.zip. In the file you will find the script main.py and the model trained with hyperparameters mu_c=-0.91348 mu_h=-0.44389 epsilon=0.05. The model is trained on the \u2018Abstracts' entities and relations annotated corpus dataset. output (Sparql query):The selected metrics are as follows: 'Direct path length', 'Arc-standard deviation', 'Arc-cosine", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Atari 2600 Freeway dataset has two entity types: cars and trucks. Therefore, we can use two metrics for evaluation. First metric is recall, that is how many cars and trucks were correctly classified. Second metric is precision, that is how many cars and trucks were correctly classified. output (Sparql query):\u30a3, SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT ?metric ?metric_lbl ?metric_rank WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset; ?eval orkgp:HAS_EVALUATION ?eval. ?metric rdfs:label ?metric_lbl. } OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric :ranking( ?metric_rank). ?metric_rank :ordinal( ?eval). } } The following metrics are used to evaluate models on the NLP-TDMS (Exp, arXiv only) benchmark dataset: Macro Precision, Macro Recall, F-score, Precision@1, Recall@1, ACC@1, R-precision, R-recall, F1-score, Precision@5, Recall@5, F1-score, ROC-AUC, J-index, J- AUC.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METR", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: You are currently only comparing two classes of metric in the query, and none of the classes are being used in the WHERE clause. I would suggest you try this query: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Q: Tabs for forms in Cocoa (S.O.) ? I'm new to Cocoa, but I'm trying to change the appearance", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The following is a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset: - BERT - TransfoG - BERT-TransfoG - BERT-XL - BERT-Base - GoogLeNet - Dagger-RL - DualRL", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Q: Does CT in DPR matter ? Does a good CT in DPR matter ? Is it more important to have a good CT than any other feature ? My current knowledge is that the answer is no, however, I would like to hear about any counter-arguments. A: Yes, a good CT (computed tomography) is important, both for", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): ROUGE-2 score is around 38.23 with an average Recall and Precision of 0.74 and 0.86 respectively. A: ROUGE-2 is the most used metric for evaluating machine learning (ML) model on abstractive summarisation. Here are the metric that are used to evaluate the abstractive summarisation model on the CL-SciSumm dataset: ROUGE-2 score which is used for metric evaluation. Generic sentence similarity score such as Jaccard, Cosine Similarity, or TF-IDF. BLEU", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Unsupervised NMT + weight-sharing model in any benchmark\") ?metric. ?metric rdfs:label ?metric_lbl. } <|endoftext|>Q: Getting Warning: redefinition of default constructor I am getting a redefinition of default constructor for the first time and do not quite understand what exactly is going on. Let me be more specific. There is a small, simple class with a default constructor and a copy ctor: #include <iostream> #include <string> #include <memory> #include <type_traits>", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The best Sequential MNIST model achieved a Permuted Accuracy score of 0.79170199214648. <|endoftext|>The Winning Campaign: Tales of Triumph and Defeat from Politics - 2nd Edition PDF Are you the last of the jellybeans ? There's only one way to find out. Whether you're running for office or simply longing to get elected, the stories in Winning Campaign:", "SELECT ?model_name WHERE { ?model dbpprop:name \"DeepLEYON\"@en; FILTER ( ?benchmark = \"Abstracts' entities and relations annotated corpus\").} input (English text): The DeepLEYON model has been evaluated on the SciTLDR dataset.", "SELECT DISTINCT ?metric ?metric_lbl ?metric_desc WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"commonsenseQA benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest result includes the score and metric of 73.44 for the Scholarly entity usage detection dataset. output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric", "SELECT DISTINCT ?model ?model_label WHERE { ?model a xqda:XQDA; bert:BERT; gpt-3:GPT-3; elmo:ELMo; star-tagon:Star-Tagon; srlearner:SRLearner; xlnet:XLNet; xlnet+phrase-level-grammar:XLNet+Phrase-Level Grammar. FILTER (str( ?model_label) = ?model_label) OPTIONAL { ?model xlnet:XLNet; ?model+phrase-level-grammar xlnet+phrase-level-grammar:XLNet+Phrase-Level Grammar. } } documentclass[12pt,letterpaper,pdftex,final, losses,obeyspaces,", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST dataset\") } The metric in this case is named \"numerical\" and has a score of 99.97. POST test/ontologies/challenge/metadata/evaluations/sequential-mnist-scores/99.97 The highest score for this dataset is 99.97.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metric classes that are used in papers are: - logprob - logloss - perplex - r2 - glove - ngramF1 \u00a0 \u00a0 The papers that have utilized the CRF with sentence expansion model and include the links to their code: * [Zou et al.](https://doi.org/10.221016/j.analytis.2020.05.002) Abstracts' entities and relations annotated corpus benchmark dataset via sentence expansion and CRF. * [Antipov et al.](https://doi.org/10.221016/j.analytis.2020.05.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl.", "SELECT ?metric ?metric_lbl WHERE { ?model wmtt16:EnglishGermanTranslationModel; wmtt16:Evaluation wmtt16:MODEL_EVALuated ?eval. } input (English text): The following models have been evaluated on the WMT2016 English-German dataset: the GPT-2, BERT, XLNET, and SQuAD models. output (Sparql query):VEGF:GPT-2, BERT, XLNET, SQuAD, Explain the notation VEGF:GPT-2 and give an example of how to access the corresponding ontology. VEGF:GPT-2 refers to the model with the VEGF ontology identifier gpt-2. BERT stands for Bidirectional Attentive Language Regulator and is a model implemented using GPT-2. The GPT-2 ontology is defined in the BFO standard, which can be accessed using SPARQL over OWL-API. In the example output, we can see that the parameter ?metric_lbl refers to the metric label in BERT. The BERT model was evaluated on the WMT2016 English-German dataset. The evaluation returned the following metrics:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>/*============================================================================= Copyright (c) 2001-2011 Joel de Guzman Distributed under the Boost Software License, Version 1.0. (See accompanying", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The top result (metric) over the dataset RotoWire is: Gradient Boosting Decision Tree with the metric: Categorical Accuracy. The best result (metric and value) over the dataset RotoWire is: Gradient Boosting Decision Tree with the metric: Categorical Accuracy, the value: 0.71. output (Sparql query):**Gradient Boosting Decision Tree** with**Categorical Accuracy** as the metric and**0.71** as the value QL378 file:///C|/DOCUMENTSUBJECT=D8|/DOCUMENTTARGET=D8|/FUNCTION=", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher - DMControl100k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATIONScore ?metric. ?metric rdfs:label ?metric_lbl. } } In the provided Sparql query, the DISTINCT operator is used to return only unique values for the metric column. The results are then sorted by the values of the metric. The best performing model benchmarking the Reacher - DMControl100k dataset in terms of Score metric is the one that has the highest value for the metric in the results. Below are the metrics used in the benchmarking Reacher - DMControl100k dataset: - Score - Precision - Recall - F measure Below is an example of a triple in the benchmarking dataset Reacher - DMControl100k that has a score of 9.47: <dmcontrol", "SELECT ?model ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Let's say we have the following model: ModelName, which has been evaluated on the Atari 2600 Assault dataset and its metrics are F-measure and Mean Reciprocal Neighbourhood Distance. The Sparql query would then be: output (Sparql query): Institutes ?model ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl)", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"code references in papers that have used the DQNMMCe+SR model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The code references in papers that have used the DQNMMCe+SR model for benchmarking purposes contain the following 2 Sparql queries. SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"code references in papers that have used the DQNMMCe+SR model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } I would expect the following to be a valid answer: \"Highest\", \"Ball in cup\", \"catch\", \"DMControl100k\", \"0.0\". \"Highest\", \"Metric\", \"Senseful\", \"DMControl100k\", \"1.0\". \r \r \r I was expecting the following: \"Senseful\", \"Metric\", \"Ball in cup\", \"catch\", \"DMControl100k\", \"0.0\". \"Senseful\", \"0.0\", \"Ball in cup\", \"catch\", \"DMControl100k\", \"0.0\". \"Senseful\", \"", "SELECT ?metric_lbl WHERE { ?eval orkgp:HAS_EVALUATION ?eval AND ?eval orkgp:HAS_DATASET ?dataset AND ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl }). accompaniment: The model with the highest F1 score when evaluated on the NYT-single dataset is the Sparql Query Language (SQuAD) model.", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The benchmark dataset was developed by Dario Medeli and Alessandro Ghodsi. To run the benchmark, you need to first load the MEMEN model into a R model, then load the benchmark dataset into R and finally evaluate the R model on the dataset. The results are then stored in the BSON database. output (Sparql query): SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstract", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The papers that we used to generate the code for the MMV TSM-50x2 model are", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"Accuracy\" output (Sparql query):\"97.50%\" Gathering the metrics used to evaluate models on the Yelp-5 benchmark dataset revealed that the accuracy metric was used to evaluate the performance", "SELECT ?metric_lbl BLEU(iri:expression,iri:translation) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): BLEU score is a string matching metric commonly used for automatic machine translation evaluation. It ranges between 0 and 100, where 0 means the two translations are completely different and 100 means the two translations are identical. The BLEU score is computed as the ratio of the number of substitutions, insertions, and deletions in the automatic translation generation over the total number of common words in the two generated translation. The best performing model on the German-English dataset in terms of BLEU score metric is the GPT-2 model. The GPT-2 model performance is comparable to the model built using Glue's named entity", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The answer is: Codes: https://github.com/andrewjzhu/sac-nlp/blob/master/src/main/java/org/apache/tinkerpop/gremlin/structure/io/CodedString.java Metrics: (None of the papers mentioned in the question include any metrics) Note: I believe one paper uses TF-IDF to calculate the cosine similarity for evaluating the SAC model. I'm not sure if the paper provides the code for calculating the metric ? If none of the papers provide code for calculating the metrics then none of the papers uses any metrics to evaluate the SAC model", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Answer: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Chinese authorities have begun enforcing measures to curb the risk of the", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): In addition to the metrics that are listed in the code (see below), we also provide a corresponding Sparql query for querying our benchmarking data for each metric. The metrics are listed below along with the corresponding Sparql query. output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: There are two sets of metrics used in the papers to benchmark the system performance. The first set of metrics is the set of relations and properties that are annotated in the corpus, which are then used to compute the Relevance/Frequency and Support/Unsupport values for each pair of relations. These values then form the scores that are reported in the papers. The second set of metrics is computed by the model and the model evaluation uses these metrics to compute the F1 score.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Humor Detection\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The F1-score metric is one of the metrics used to evaluate models in the Abstracts' entities and relations annotated corpus benchmark dataset. It is defined as: F1 = 2 * ( Precision * Recall ) / ( Precision + Recall )", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ippet 1: The neural network STREET model has been used by Hirel et al., \u201cThe popular neural network STREET: Benchmarking for information retrieval, summarisation, and annotation using a general-purpose neural", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a enwiki8:Entity; ?relation_field :label ?relation_field_label; enwiki8:isReview ?is_review. FILTER (str( ?is_review) = \"yes\") ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval ?metric. } } input (English text): Orkgp_HAS_EVALUATION is a RDF type. Which type is orkgp:HAS_EVALUATION ?eval ? output (Sparql query):simeq command:SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a enwiki8:Entity; ?relation_field :label ?relation_field_label; enwiki8:isReview ?is_review. FILTER (str( ?is_review) = \"yes\") ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval ?metric. } } input (English text): Orkgp_HAS_EVALUATION is a RDF type.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Here is the query to return the metrics that are used to evaluate models on the datasets benchmarked in paper titles and IDs:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ?dataset ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric Comprehensible ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric_lbl ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric Comprehensible ACO_courses_annotated_relation_extraction_benchmark_EVAL_metric_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; orkgp:HasEvaluation ?eval. FILTER (str( ?eval) = \"Log Likelihood\") } input (English text): Log Likelihood is a commonly used metric to evaluate model performance on the SQuAD2.0 dataset. (Sparql Query) SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; orkgp:HasEvaluation ?eval. FILTER (str( ?eval) = \"Log Likelihood\") } (Sparql Query) SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; orkgp:HasEvaluation LogLikelihood. } Related Work - Machine learning model evaluation and benchmarking has been an active area of research in the AI/ML research", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT 2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The evaluation metrics commonly used on the WMT2014 English-German dataset are the BLEU (Papineni et al., 2002), ROUGE-L (Lin,2004), METEOR (Bolita et al., 2018), and CIDEr (Vedantam et al., 2017) metrics. * BLEU: Better Bilingual English-German Algorithm, calculates the percentage of correctly", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reading Comprehension research area\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset include: - BLEU - ROUGE - Meteor - F-score - G-mean - Coref - adequacy - Discourse Coherence - Singularity - MC-Terror - Jelinek-Tulving-Relief - Staruser-Wan - Word Propaganda - Paraphrase Rate - human judgment **Expected output:** 1. BLEU 2. ROUGE 3. Meteor 4. F-score 5", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <benchmark:Abstracts' entities and relations annotated corpus> <metric>RE+ Micro F1</metric> <metric_lbl>RE+ Micro F1</metric_lbl> <eval:HAS_EVALUATION>true</eval:HAS_EVALUATION> <eval:HAS_DATASET>true</eval:HAS_DATASET> <metric>RE+ Micro F1</metric> <|endoftext|>Q: R Shiny customize background color I want to customize the background color of my R Shiny app. I could set it in the ui but i would like to define it at the server.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The papers that the author of this answer is aware of are the following: - Kusner, Joshua; Yang, Haemin; Vaswani, Ashish; Doshi, Sadaf; Chknee, Justin; Chen, Mengjian; Zhang, Zhiheng; Yang, Tiejun; Wang, Hongjia; Chen, Changzhe; Libkin, Daniel; Zhao, Taiji; Wang, Min", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Title: Research Papers that Contain a Benchmark Over the Ohsumed Dataset output (Sparql query): Title: Databricks for Research Papers that Contain a Benchmark Over the Ohsumed Dataset. input (English text): Benchmark over the Ohsumed dataset. output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The NYT29 dataset contains 29 benchmark entities and 6 relation types (Supplier, Employee, CreditCheck, LoanAmount, LoanAmountMonthly, LoanAmountYearly) and each relation has 4 different types of metrics associated with it. The 4 types of metrics associated with each relation are: Correct-Prediction, F-Score, Precision & Recall. Models are evaluated based on the total number of entities and relations on which they worked correctly and the highest performing model on the benchmark is awarded 1 point. The benchmark dataset contains 15 entities and 6 relation types and each relation has been assessed for 6 metrics. output (Sparql query): 549 results returned following", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): In this query, we have selected only two metrics. Lets see which other two metrics are used for evaluating a model. output (Sparql query): humidity and wind speed are used for evaluating atmospheric models. input (", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT ?paper_id ?paper_title ?paper_id_label WHERE { { ?paper has uri \"http://dumps.wikimedia.org/enwiki/20150522/*Benchmarking_Concepts_and_Measures_for_Internet_Search_Engine_Benchmarks*\". } UNION { ?paper has uri \"http://www.seas.ucla.edu/~djane/research/TREC/download/html/TREC6/TREC6_Benchmarking_Results/TREC6_%20Benchmarking_Results.zip\". } } { ?paper_id:EXPRESSION 1. ?paper_id_label:EXPRESSION \"Title\". ?paper_title:EXPRESSION \"Concepts and Measures for Internet Search Engine Benchmarks\". } } input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset ? output (Sparql", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The SELECT statement provides the following values: output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query", "SELECT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"PolyAI Reddit benchmark dataset\") ?benchmark orkgp:HAS_EVALUATION ?eval; orkgp:HAS_METRIC ?metric; } } input (English text): 2-of-100 Accuracy metric. Which model performed the best on this metric ? output (Sparql query): hailsparql (Sparql query): SELECT ?metric WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"PolyAI Reddit benchmark dataset\") ?benchmark orkgp:HAS_EVALUATION ?eval; } } The most accurate model that we have on the PolyAI", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } This query selects the distinct metrics from the query result. Based on the code links, these are the metrics that are used to evaluate models on the benchmarks: * perplexity (Perplexity) * F1 score (F1) * micro-averaged F1 score (F1M) * macro-averaged F1 score (F1M) * unranking metric (k-unranking) * ranking metric (R@1) * precision@k (Prec@k) * recall@k (Rec@k) * f-score@k (F@k) * macro-averaged", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } # DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; ?metric T5. ?metric_lbl TriviaQA. } <|endoftext|>import React, { memo, ReactElement } from'react' import { Connection } from 'components/connections' import { BindAction, PageObjectInterface, Query } from 'utils/models' import { View } from 'components/views' import { styles } from 'utils/palette' const { isEmpty } = Query interface Props { connect: BindAction items:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Raman spectroscopy\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following are the Raman spectroscopy metrics: * Raman shift: this is the amount of red shift, measured in cm-1, after exciting the sample with a monochromatic light. The higher the shift, the more complex the sample is. * Soret Band: this is an absorption band which appears in the IR region between 200 and 300 nm. It is related to the anisotropy of the Raman shifts and of the dielectric function. * Second Order Raman Sidebands: this is the subband of the Raman spectrum arising from interaction between the incident light and the sample, namely the effect", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The high score should be returned from the following answer. SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>We use cookies to personalise content, target ads, to provide social media features and", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } } The following are the metrics that are used to evaluate models on the SemEval-2018 Task 7 dataset dataset benchmark dataset: * Metric A: Coding complexity (CoCo) * Metric B: Model size (size of the neural network) * Metric C: Training loss (lower is better) * Metric D: Test accuracy (lower is better) * Metric E: Cross-entropy (lower is better) * Metric F: F-score * Metric G: Precomputed word similarities (Lower is better) * Metric H: Average length of", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AG News benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query):SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rd", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Disease\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Kinetics of transformation of vinyl chloride in soils.", "SELECT DISTINCT ?metric ?metric_lbl FROM <query> WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"diseases\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl FROM <query> WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"diseases\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metrics are present in the dataset Abstracts' entities and relations annotated corpus: - Average precision (AP) - Multidimensional Scaling (MDS) - Recall at different threshold levels - Precision at different threshold levels - Average time per result (at five different thresholds) As the dataset does not contain class labels, the papers that benchmark the GCN Hybrid model perform classification using these five metrics. The following are the papers that benchmark the GCN Hybrid model: - [Link to paper 1] - [Link to paper 2] - [Link to paper 3]", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CHEMDNER corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): CHEMDNER corpus is present in the database orkgp:EntitiesAndRelationsAnnotatedDatasets, where orkgp is the OWL API for Ontobee. The dataset and evaluation metrics are present in the class orkgp:HAS_DATASET and the class orkgp:HAS_EVALUATION respectively. output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset;", "SELECT DISTINCT ?benchmark ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } <|endoftext|>Q: SSRS textbox isn't displaying text values after a form submits Here's a snapshot of my text box that's not displaying the value I typed into it after a form is submitted. The textbox is on a form that is part of a query. The query is pulling from a SQL Server database via ADO.NET. If I breakpoint on the line where the query is running and step through the code, the textbox.text value is being updated with the right value, so the query is working and getting the value I want. A: I figured out my issue. I needed to update the dataset in my datasetbag after the form was submitted. So, this is my code that gets my dataset to update: DataSet myDataset = new DataSet(); myDataset.TableName = \"DatasetName\"; myDataset", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The three metrics used in the benchmark dataset are: - ROUGE-1 - ROUGE-2 - ROUGE-L", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Q: MySQL order by query with subquery This is my table: id | process_id | process_user | process_date | results 1 | 1 | ravi | 2016-05-24 | 30 2 | 1 | ram | 2016-05-25 | 40 3 | 1 | sachin | 2016-05-26 | 20 4 | 2 | ravi | 2016-05-23 | 20 5 | 2 | ram | 2016-05-24 | 30 6", "SELECT AVG(evals:result) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Human-annotated\") } output (Sparql query):8.2 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Computer-annotated) ? output (Sparql query):aused_eval:has_metric:avg output (English text): SELECT AVG(evals:result) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Computer-annotated\") } output (Sparql query):5.4 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Computer-annotated and relation has weight value) ? output (Sparql query):aused_eval:has_relation:weighted output (English text): SELECT AVG(evals:result) WHERE", "title => Adaptive Input Large Model (AIL) url => https://github.com/OAI/oad-cloud/blob/master/storage-apis/oad-cloud-api-data/src/main/resources/oad-cloud-apis-data-model-benchmarks/AIL.md code => https://github.com/OAI/oad-cloud/tree/master/storage-apis/oad-cloud-api-data/src/main/resources/oad-cloud-apis-data-model-benchmarks/AIL.sh related => https://github.com/OAI/oad-cloud/issues ?q=is%3Aissue+is%3Aopen+label%3Amodel+Created+by related => https://github.com/OAI/oad-cloud/issues ?q=is%3Aissue+is%3Aopen+label%3Amodel+Created+by+Author%3DSarah+Hecht%2C+Ph.D. related => https://github.com/OAI/oad-cloud/issues ?q=is%3Aissue+is%3Aopen+label%3", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Solaris\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval scoredefault. ?eval time. } } input (English text): What models are being evaluated on the Atari 2600 Solaris dataset ? output (Sparql query): ALJP is the best metric for evaluation of performance on Atari 2600 game datasets. Score is calculated as ALJP/ time. Input (English text): Score is calculated as ALJP/time. Output (Sparql query): ALJP is the best metric for evaluation of performance on Atari 2600 game datasets. Score is calculated as ALJP/time. Output (Sparql query): ALJP", "SELECT ?title (str: neuter ?str) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } iju : See examples in the papers and the code used can be referenced in these papers.", "SELECT DISTINCT ?metric_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Centipede has a benchmark score of 42.143935 and its metric is \"Correctly found number of dead enemies\". output (Sparql query):backed ?metric_lbl \"Correctly found number of dead enemies\" \"Centipede\" kerchief (Sparql query): SELECT DISTINCT ?metric_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The Sparql query response will be : SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following are the metrics that are used to evaluate models on the MNIST dataset:", "Country: France*Metric: Population*Metric_lbl: Population of France Pero = 4,187,383 people registry = French national census of 2011 Registry of the population of France The French national census of 2011 counted 4,187,383 people dataset = French national census of 2011 has_dataset = has(dataset) has_evaluation = has(eval) has_metric = has(eval,metric) metric = eval.metrics.population metric_lbl = metric.label registry = French national census of 2011 Registry of the population of France The French national census of 2011 counted 4,187,383 people dataset = French national census of 2011 has_dataset = has(dataset) has_evaluation = has(eval) has_metric = has(eval,metric) metric = eval.metrics.population metric_lbl = metric.label registry = French national census of 2011 Registry of the population of France The French national census of 2011 counted 4,187,383 people dataset = French national census of 2011 has_dataset = has(dataset) has_evaluation = has(eval) has_metric = has(", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"Abstracts' entities and relations annotated corpus\" and \"Abstracts' entities and relations annotated corpus benchmark\" <|endoftext|>1. Field of the Invention This invention relates to electronic apparatus and particularly to an electronic apparatus having a detachable cover and a locking mechanism for releasably locking the cover to the apparatus. 2. Description of the Related Art Portable electronic apparatuses such as portable telephones are in widespread use and provide users with a wide range of functions including, for example, telephonic features and the ability to transmit and receive images and other data using a communication network such as the Internet. Portable telephones are generally comprised of a body and", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }' From the query response, you can see that the following metrics are used to evaluate models on the TempEval-3 dataset: ?benchmark< TempEval-3 dataset> ?metric<Micro-F1 score> ?metric_lbl ?eval ?eval.Micro-F1 score ?eval.rdfs:labelTempEval-3 dataset ?eval.rdfs:labelMicro-F1 score ?eval.has_datasetTempEval-3 dataset ?eval.has_evaluationTempEval-3 dataset ?eval.has_metricMicro-F1 score ?eval.has_metric_lbl ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC' benchmark\") ?eval. ?dataset orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): SciERC' is a benchmark dataset that contains query- answer pairs. The pairs are annotated with their respective evaluation metrics. Some examples of evaluation metrics are Recall, Precision, F1 score, Precision at k, and so on. The data and code used to generate this list is available at https://github.com/oleg-perelson/SciERC. notre; SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC' benchmark\") ?eval. ?dataset orkgp:HAS_EVALUATION", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } *F1 metric is one of the metrics used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the performance of the systems reinforced that the F1 metric was used on the CoNLL 2012 benchmark dataset to evaluate the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. } } Answer: The following metrics are used to evaluate models on the WMT2014 English-German dataset: 1. SegSent: The simpler the model is, the less parameters it needs, and thus the better. 2. Bilingual GPT: A big model is better than a small one. 3. MULTICULTurally Attentive Attention (MuLA): When two sentences have the same verb and same subject but differ in their object, attention mechanism in GPT should pay more attention to the verb and the subject. 4. Bilingual Abstract Task (BAT): Evaluates a model's ability to generate dependency trees from parallel sentences, where sentences have the same subject but different verbs, as a way of measuring", "SELECT MAX( ?eval) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }) input (English text): The highest benchmark result achieved on the Yelp-14 dataset is 1560.83, including the metric and its value of precision. output (Sparql query): RPMI-14 1560.83 How did you write this answer ? The highest benchmark result achieved on the Yelp-14 dataset is 1560.83, including the metric and its value of precision. Based on this answer, you could conclude that the highest benchmark result achieved on the Yelp-14 dataset is the metric of precision with its value of 1560.83. For more information about metrics, see the following: * https://www.openrdf.org/forums/openr", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): https://github.com/tensorflow/models/", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?model rdf:type owl:Class; owl:equivalentClass ?model2. ?model2 rdf:type owl:Class; owl:equivalentClass ?model ?model2. { ?model2 owl:has Relations; owl:onRel :hasOut ; ?model owl:has Properties; owl:onProperty :hasOut ; } } answer: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?model rdf:type owl:Class; owl:equivalentClass ?model2. ?model2 rdf:type owl:Class; owl:equivalentClass ?model ?model2. { ?model2 owl:has Relations; owl:onRel :hasOut ; ?model owl:has Properties; owl:onProperty :hasOut ; } } The following are the metrics that are used to evaluate models on the RTE benchmark dataset: *\t#model #rels *\t#model#rels answer: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?model rdf:type owl:Class; ow", "SELECT MAX(score) AS max_score FROM evaluated_data WHERE evaluation_set_name = \"wos5736\" AND dataset_name = \"wos5736\" AND category_name = \"Directly Annotated Relations\" AND (annotation_type IN (\"relation\", \"dependency\", \"is-a\", \"has\")) ORDER BY score DESC input (English text): \"relation\", \"dependency\", \"is-a\", \"has\" are the annotation types of the entities and relations in the benchmark dataset. The Sparql query returns the highest score obtained by any model on the WOS-5736 dataset. The WOS-5736 dataset has 9 entities with the annotation types \"relation\", \"dependency\", \"is-a\", and \"has\". The highest score achieved by any model is 16.8, achieved by GPT-D. In summary, the metrics used to evaluate models on the WOS-5736 dataset are the following: \"Relation Type\", \"dependency\", \"is-a\", \"has\" The GPT-D model achieved the highest score on this dataset with 16.8, which was annotated using the annotation types \"relation\", \"dependency\", \"", "SELECT DISTINCT ?metric_lbl ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\"^^xsd:string) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION_METRIC ?metric. } } input (English text): List the metrics that are used to evaluate models on the WebQuestions dataset ? output (Sparql query): SELECT DISTINCT ?metric_lbl ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\"^^xsd:string) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION_METRIC ?metric. } } input (English text", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Here are the metrics that are used to evaluate models on the dataset benchmark ART/CoreSC: 1. cosineSimilarity 2. log score 3. NDCG 4. Pearson's chi-squared 5. R-squared", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } methodology : Evaluate these models on the Abstracts' entities and relations annotated corpus benchmark dataset: * Naver Question Answering (RQ-Lab) * DistilDQL * T5 * DistilPoS * DistilNLP * DistilCore * DistilX As of the most recent version, distilx queries on the D2RQ dataset have been benchmarked against each other using Natural Questions (short) as the held-out dataset. The query outputs the following metrics: - Overall F1 score - Slot F1 score for named entities in question", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The WMT14 French-English dataset has the following results: ({\"@context\": \"http://www.w3.org/2004/apps/molecular_data/xsd\", \"@id\": \"http://www.wmt.org/xap/wmt14-fr-eng\", \"@type\": \"OrientMWMT2014FRENDEUROPE\", \"@language\": \"fr\", \"@container\": \"ms\", \"@version\": \"1\", \"@containers\": \"ms\", \"ms\": {\"@id\": \"http://www.wmt.org/xap/wmt14-fr-eng\", \"rdf:", "SELECT ?metric_lbl ?metric_text where { ?metric_text wdt:P39 ?metric_lbl. FILTER (str( ?metric_lbl) = \"Perplexities\") } input (English text):Perplexities refers to the test score given by the OpenNLP model on the WikiText-2 benchmark dataset. 16-gram language modelperplexities Perplexities is the test score given by the OpenNLP model on the WikiText-2 benchmark dataset. Perplexities refers to the test score given by the OpenNLP model on the WikiText-2 benchmark dataset. 16-gram language model {#section-14e4429dd524522d0e9c42a3c2a4b5e0.}}{#section-14e4429dd524522d0e9c42a3c2a4b5e0.section.text.container {#we should add the metric below for evaluating models in this section...} {#section-14e4429dd524522d0e9c42a", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } * metric - Dataset abstraction, Equit * metric_lbl - Dataset abstraction, Abstracts' entities and relations annotated corpus SELECT ?metric SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS", "{ metric CosineSimilarity labelSimilarity } input (English text): CosineSimilarity is a metric used for measuring the similarity between two vectors. One vector is formed by the feature values of a particular object and the other vector is formed by the same features of another object. The CosineSimilarity is defined as the inner product between the two feature vectors. \r The CosineSimilarity is defined as the inner product between the two feature vectors. output (Sparql query):ocious { metric CosineSimilarity labelSimilarity } \r CosineSimilarity is a metric used for measuring the similarity between two vectors. One vector is formed by the feature values of a particular object and the other vector is formed by the same features of another object. The CosineSimilarity is defined as the inner product between the two feature vectors. output (Sparql query):ilarly { metric CosineSimilarity labelSimilarity } \r CosineSimilarity is a metric used for measuring the similarity between two vectors. One vector is formed by the feature values of a particular object and the other vector is formed by the same features of another object. The CosineSimilarity is defined as the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>The competitive nature of sport can make competitive athletes underperform at physical activity,", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text):Indicate the model that performed best in terms of MAP metric on the Atari 2600 Freeway benchmark dataset ? output (Sparql query):INSERT INTO <tablename> (Metric,Metric_lbl) VALUES (MAP, \"MAP\") <|endoftext|>Scott Wagner is author of Startup Communities: How to Create a Happy, Healthy, Financially Successful Life on Your Terms The problem Starting a business and", "SELECT DISTINCT ?paper_id ?title WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER(str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } UNION { ?paper_id \"DCASE\" ?title. } } order by ?paper_id limit 10} input (English text): DCASE: Detect and Classify Agenda-Based Information Spoken in Telephone Calls Using Automatic Speech Recognition, by Padmanabhan P, Talwalkar P, Carlsson G, Gopalakrishnan K, Karunanidhi N, Chen X, et al., Proceedings of the 19th International Conference on Language Resources and Evaluation (LREC 2012), 2012. output (Sparql query", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a yelp:FGCData; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification dataset\") ?benchmark yelp:HAS_DATASET ?dataset; yelp:HAS_EVALUATION ?eval. OPTIONAL { yelp:HAS_METRIC ?metric. yelp:HAS_METRIC ?metric_lbl. } } input (English text): The Yelp Fine-grained classification dataset has 3 metrics: Accuracy, Macro-F1 and Mean Average Precision (mAP) output (Sparql query): towels {Accuracy ?metric ?metric_lbl. Macro-F1 ?metric ?metric_lbl. Mean Average Precision ?metric ?metric_lbl. } The metrics that are used are: Accuracy Macro-F1 Mean Average Precision Additionally, the evaluation process can optionally include evaluation metrics. The evaluation metrics are: Has_Metric Metric Metric_Label Related Work ============", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics used to evaluate the Feretault model on the Abstracts' entities and relations annotated corpus benchmark dataset are: ?metric :Score ?metric_lbl:Feretault <|endoftext|>1. Field of the Invention The present invention relates to a remote keyless entry (R", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } (click on'show rest of results' to go to the next page) The following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: - ConfusionMatrix - AveragePrecision@1 - F1 Score - Recall - Top1 - Top5 ?metric : ConfusionMatrix ?metric_lbl : Abstracts' entities and relations annotated corpus ?metric_lbl : ConfusionMatrix The following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: - AveragePrecision@1 - F1 Score - Recall", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS benchmark dataset\") ?benchmark orkgp:HAS_DAT", "SELECT ?benchmark { (_:http://www.ontotext.com/ontology-download/datasets/wsc) } WHERE { ?benchmark owl:SameAs <http://www.w3.org/2013/Turtle> } input (English text): The following are the titles and IDs of research papers that include a benchmark for the WSC dataset: - WSC Benchmarks: <http://www.ontotext.com/ontology-download/datasets/wsc> - WSC Evaluation: <http://www.w3.org/2013/Turtle>", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \" parameters = { \"metric\": [\"macro\", \"micro\"], \"metric_lbl\": [\" metric_label1\", \" metric_label2\"] } dataset = \" // Abstracts' entities and relations annotated corpus\" evaluation", "SELECT ?metric_lbl WHERE { \r ?dataset a orkgc:Dataset; \r rdfs:label ?dataset_lbl. \r FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \r ?benchmark orkgp:HAS_DATASET ?dataset; \r orkgp:HAS_EVALUATION ?eval. \r OPTIONAL { \r ?eval orkgp:HAS_METRIC ?metric. \r ?metric rdfs:label ?metric_lbl. \r } \r } SELECT DISTINCT ?metric ?metric_lbl WHERE { \r ?dataset a orkgc:Dataset; \r rdfs:label ?dataset_lbl. \r FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \r ?benchmark orkgp:HAS_DATASET ?dataset; \r orkgp:HAS_EVALUATION ?eval. \r OPTIONAL { \r ?eval orkgp:", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): SST-5 Fine-grained classification is a dataset from SQuAD 2.0. SQuAD is a knowledgeQA dataset. This dataset has 50 documents with 10 different categories of entities and relations and each relation appears 2 to 3 times in the document with different position in the document. There are 4 types of relations - Subject, Predicate, Object and Complement. SQuAD 2.0 provided SST-5 Fine-grained classification benchmark dataset to evaluate the models on these types of relations. output (Sparql query):azzoinal 1: SELECT ?metric ?metric_lbl WHERE { ?dataset a orkg", "SELECT DISTINCT ?metric ?metric_lbl\" } { \"rdfs:label\" \"Metric\" } { \"rdfs:label\" \"Metric_Label\" } OSELLABLE Metrics for ARC-PDN dataset: - Hits (true positives) - Misses (false positives) - Hits (true positives) - Failures (false positives) - [Hits * TPR] - [Failures * FPR] retrieved 4 results steps: - name: Get all Sparql queries from Assumeworks workspace command: | pickmediaportal # type \"nix\" or \"zsh\" or \"bash\" to pick a shell sparql+http://www.w3.org/ns/activitystreams# register: results - name: Output all the Sparql queries for the Assumeworks workspace set_output: queue: results - name: Filter the results to only the Sparql query with the where clause filter:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } With the above query, the results include only two metrics - metric_lbl and metric auxiliary /oracle/oracle.wso2.dataservices.query.base.annotation.HasDatasetEvaluation sql query: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Atari 2600 Berzerk dataset commonly used for benchmarking models is comprised of 10,000 randomly generated images depicting objects interacting with each other and over a 70 second time window.The model\u2019s evaluation metric for the dataset consists of the following four metrics: Recall, Precision, F1-score, and Accuracy. These metrics are computed based on comparing the model predictions to the gold standard.The following metrics are computed for each model in the dataset: Recall: The fraction of correct predictions out of the total number of predictions made by the model. Precision: The fraction of correct predictions out of the total number of predictions made by the model. F1-score:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Car dataset - md5: 6e9a0ebe6ddef6b39cc77a41fcfbd088 Car dataset - url: https://storage.googleapis.com/deep-learning-1005/trafficsim/trafficsim_static/Cars.zip A benchmark on Stanford Cars dataset - id: e4da9597-fc8c-4867-9c6d-d29c55f45761 output (Sparql query): 51830ef2-a384-1033-a1e7-c5713fa73a8a Cars - md5: 6e9a0", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } metric metric_lbl 1.1 The effectiveness of our system 1.2 The number of annotations our system has generated 1.3 The time taken by our system to generate annotations 1.4 The number of entities and relations that were successfully annotated 1.5 The Average lengths of entities and relations 1.6 The Precision and Recall of entities and relations when generating annotations 1.7 The number of errors made by our system while generating annotations 1.8 The number of relation labels that were not contained in the title or the ID of the relation 1.9 The number of relation labels that were wrongly annotated as not contained in the title or the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cars dataset\") } input (English text): The metrics used to evaluate the models over the Car dataset are: Car accuracy: This metric assesses the model\u2019s ability to assign a correct value to the distance between each predicted location and the corresponding correct location. Car F1 score: This metric is a harmonic mean of the precision and recall of the model for each prediction, and is widely used as a measure of model quality. Car mean average precision: This metric is the mean of the average precision across all pairs of predictions and observations for the model. Car mean average recall: This metric is the mean of the average recall across all pairs of predictions and observations for the model. Car mean cmad: This metric is the mean of the confusions plus false negatives (FNs) across all classes of the model. Car mAP: Mean Average Precision, i.e., the fraction of accurately predicted pixels out of all predicted pixels.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The ShARe/CLEF eHealth corpus dataset, includes research papers that describe model evaluations for their abilities to identify abstracts that relate to clinical health data. Evaluation metrics include, but are not limited to, precision, recall, F1 score, and area under the receiver operator characteristic curve.", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Copper\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The answer is that the least response time was obtained for copper.", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"List the code links in papers that use the GPT-2 (small) model in any benchmark ?\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): [(\"Tf-idf\", \"TF-IDF\"), (\"Rouge\", \"R\")] output (Sparql query): <<Tf-idf Rouge>> input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark ? output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query):SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } In this query the best performing model is the one that has the lowest number of parameters. This can be done using the OPTIONAL construct to include only the model with the desired metric. ### ImageNet dataset results ### output (Sparql query):Photo(metric_lbl=\"Number of params\",metric=\"Number of params\",dataset_lbl=\"ImageNet dataset\",dataset=\"ImageNet dataset\") submission 1 #Best performing model (lowest number of parameters): optimizer: RMSProp loss: MSELoss model architecture: densen", "SELECT DISTINCT ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") } input (English text): Onsdood: evaluation metrics used for the Atari 2600 Battle Zone dataset: accuracy precision recall f1 score output (Sparql query):evaluation of Model Performance on the Atari 2600 Battle Zone Dataset: SELECT DISTINCT ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") } output (Sparql query): accuracy precision recall f1 score input (English text): There are 4 metrics used to evaluate models on the Atari 2600 Battle Zone dataset: accuracy precision recall f1 score abort <|endoftext|>Q: In RcppArmadillo, how do you multiply vectors element-by-element ? I'm writing", "SELECT * FROM { http://bitti.sourceforge.net/annotation/models.html#m5 http://www.bittorrent.org/bamp/model/bamp-m.html http://rd.mlr.org/pages/bamp http://code.google.com/p/bamp-metrics/downloads } code (Sparql query): http://bitti.sourceforge.net/annotation/models.html#m5 http://www.bittorrent.org/bamp/model/bamp-m.html http://rd.mlr.org/pages/bamp http://code.", "SELECT ?benchmark { ?benchmark Atari2600StarGunn \"70\" } input (English text): Using the Atari 2600 Star Gunner dataset, what is the third highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value ? output (Sparql query): SELECT ?benchmark { ?benchmark Atari2600StarGunn \"50\" } <|endoftext|>Badr Hari at E3 2013: \"I'm interested in being the best diver I can be\" Watch Badr Hari, one of the most well-known and experienced OAR diver, talk about his career and his current plans at E3 2013. Born in Kenya but raised in Sweden, Badr Hari began diving in the early 1990s and became one of the first OAR divers after", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): <https://github.com/snowballx/py-rasa/blob/master/rasa/rasa.py> is the code used in the papers that benchmark the NASCell model. output (Sparql query):Downloadable file https://www.dropbox.com/s/n2k7cy0fgmzd6r7/rasa.py ?dl=0 input (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Origem Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): code references in papers that have used the TCN model for benchmarking purposes can be found in Romeo Fernandes' paper which uses TCN to achieve state-of-the-art results on the KBP2015's entities and relations annotated corpus benchmark dataset. sburgok@data61.com.br romeo@data61.com.br <|endoftext|>Mission Building Community and Connecting With the Earth Mission Building Community and Connecting With the Earth The Pomander Bowl is a community center located at 136", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>- Forwarded by Don Heinrich/NA/Enron on 06/06/2001 08:29 AM - \"Schindler, Paul\" <Paul.Schindler@edelman.com> on 06/06/2001 07:16:48 AM To:\t\"'don.heinrich@enron.com'\" <don.heinrich@enron.com> cc:\t Subject:\tFW: Did you know ? Here's another one: A couple walking along a beach one day noticed an enormous seal up to its neck in a net. They called the Sea Fishing Company and", "SELECT ?metric ?metric_lbl ?metric_value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } selected metric : METRIC_1 selected metric_lbl: SQUAD2.0 selected metric_value : 9.38 A: I would encourage you to go through this thread at the query language designer slack channel for more information on that subject. In short, there is no standard way to express that query without using aggregation operators on the values returned by SELECT. However, the syntax for doing that is quite verbose and in many cases does not actually provide a clear advantage over the alternatives. <|endoftext|>Sue Bird and the Dream Shake beat the Indiana Fever 110-95 in Game 2. Photo by", "SELECT DISTINCT ?metric_lbl ?metric_value WHERE { ?benchmark orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. } FILTER (str( ?dataset) = \"BC2GM\") } input (English text): The result is: Has_Dataset(BC2GM) | Has_Evaluation(Has_Metric(Metric,Label)) | Has_Evaluation(Has_Metric(Label,Value)) output (Sparql query):medscyntax (Sparql query): SELECT DISTINCT ?dataset_lbl ?benchmark_entity_id ?metric_value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC2GM\") ?benchmark_entity_id. ?benchmark orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Story Cloze Test", "SELECT DISTINCT ?contributor_author ?contributor_author_label, ?contributor_author_url WHERE { /* query 1 */ ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl; /* query 2 */ ?dataset_lbl wfs:field ?dataset_lbl; ?benchmark orkgp:HAS_DATASET ?dataset; /* query 3 */ ?dataset orkgp:HAS_EVALUATION ?eval; /* query 4 */ OPTIONAL { ?eval orkgp:HAS_METRIC ?metric; ?metric rdfs:label ?metric_lbl. } /* query 5 */ FILTER( ?metric_lbl = \"Has contributed to the largest number of articles about coronavirus\") } OPTIONAL { ?eval sparql:name ?eval_name; ?eval sparql:directive ?eval_directive } { ?contributor_author ?contributor_author_label, ?contributor_author_url ?contributor_author_url_label }", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC precision ?metric; ?metric precision", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \r The answer is: \r rovGroth30_lp/h2o: ABSOLUTE_COUNT \r rovGroth30_lp/h2o: MAX \r rovGroth30_lp/h2o: MIN \r rovGroth30_lp/h2o: rel_precision \r rovGroth30_lp/h2o: rel_recall", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest score is 29. The metric returned is 'Precision at 5'. output (Sparql query):ounded 29 \"Precision at 5\" <|endoftext|>/* * Copyright (c) 2013-2018, Profselb-Brain and re28 Studio. * http://www.prossemvb.com * MIT Licensed. */ package com.prossemvb.virtualcamera2.exporters; import android.graphics.Bitmap; import android.graphics.Bitmap.CompressFormat; import android.graphics.Bitmap.Config; import android.graphics.Bitmap.Format; import", "SELECT DISTINCT ?model_id ?model_id_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_MODEL ?model_id. ?model_id rdfs:label ?model_id_lbl. } } input (English text): Skiing is the name of the dataset. The evaluation metrics used on this dataset are: Accuracy, Specificity, Sensitivity, Positive Likelihood, and Negative Likelihood. The models that have been evaluated on Skiing are: Fancy Anders, Gap Neural, Log Reg, RTP NN, Soft Margin Reg and the XOR model. output (Sparql query): select model_id, model_id_label from evaluation where metric in { \"Accuracy\", \"Sensitivity\", \"Specificity\", \"Positive Likelihood\", \"Negative Likelihood\", \"Fancy Anders\", \"Gap", "SELECT DISTINCT ?metric_lbl ?metric_str WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Text Summarization research field benchmark datasets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The most common benchmark datasets used for the Text Summarization research field are: - TED ( Trachtenberg et al. 1995) - WASD ( J. D. Grant & Thrun 1997) - DUC2003 (Chang et al. 2003) - TACL2005 (Bikel et al. 2005) - TACL2006 (Bikel et al. 2006) - DUC2007 (Chang et al. 2007) - AMIT-2008 (Murase et al. 2008) - DUC2008 (Chang et al. 2008)", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham dataset\") } input (English text): The following are the metrics of evaluation over the Atari 2600 Tutankham dataset: Mean Average Precision (mAP) and R2 scores. Our model is able to achieve 81.8% mAP and R2 score of 0.6862 deposited (AVRO data file): target {Annotation corpora benchmark task}-mrn47.avro reproduced (English text): mAP: 81.8%, R2: 0.6862 deposited (AVRO data file): target {Annotation corpora benchmark task}-mrn47.avro reproduced (Sparql query): ?metric mAP ?metric_lbl ? deposited (AVRO data file): target {Annotation corpora benchmark task}-mrn47.avro reproduced (Sparql query): ?metric 81.8 ?metric_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Mark2 is the name of the model.", "SELECT ?model ?metric ?metric_lbl WHERE { ?model PROTEINS; ?metric SPARQL_EXPR ? } input (English text): Yes. Amlogic has evaluated model HMMER3 on the PROTEINS dataset. The evaluated model is HMMER3_A1 in the evaluation results available on the Amlogic website. The metric that has been used in the evaluation is TP, which stands for true positive. output (Sparql query):", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The answer is chosen from the linked blog post. <|endoftext|>Pulmonary rehabilitation in cystic fibrosis: what we have learned and what we need to do. Pulmonary exacerbations in patients with cystic fibrosis are associated with adverse effects on quality of life and prognosis. Recently, evidence has accumulated for improving quality of life and reducing the frequency of pulmonary exacerbations in these patients with pulmonary rehabilitation. To date, most research in this area has consisted of small studies of short duration. In this review, the authors discuss the role of pulmonary rehabilitation in the context of existing knowledge and describe the gaps in our knowledge base regarding the outcomes of pulmonary rehabilitation and the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics of evaluation over the CommitmentBank dataset are: * TAR Score * Precision@TAR Score * Recall@TAR Score * F1 Score ### References <|endoftext|>Ghana Information and Reports on Ghana Founded in the late 18th century, Ghana is one of the world's newest countries and the second-youngest African nation. Situated in West Africa, Ghana's northern border is shared with the Ivory Coast. Despite its young age, Ghana has a long and rich history that continues to shape its present and future. Ghana has a population of approximately 20.4 million. It is b", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The following code was adapted from https://github.com/facebookresearch/MMDB-Datasets/blob/master/data/2014_06_19/methods.py. The code defines an evaluate method for the model. The method will run on a single Dataset entity with a single relation of the given name. The metric to be computed is given as a string, e.g., 'F1 score'. output (Sparql query): /*! * Abstracts' entities and relations annotated corpus * * The following code was adapted from https://github.com/", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } mentation (English text):The best performing model on the Abstracts' entities and relations annotated corpus benchmark dataset is the Micro F1 model with RE+ metric, which reports the F1 score on the ACE 2004 dataset, which includes the relation subgraph're +' from the ACE 2003 dataset. mentation (Sparql query):SELECT ?", "mark2/Dataset/Abstracts' entities and relations annotated corpus benchmarkdataset_lbl/nottingham/2/Abstracts' entities and relations annotated corpusbenchmark/has_dataset/dsc5sddpthjf3/Abstracts' entities and relations annotated corpusbenchmark/has_evaluation/dsc5sddpthjf3/has_metric/evaluation_metric. The top benchmark score is 2, and the metric is The number of abstracts. input (English text): The top benchmark score is 2. What is the metric used to evaluate the score ? The metric is The number of abstracts. <|endoftext|>Tesla's solar panels might not be the most impressive engineering feat ever created, but the electric car manufacturer might be able to make up for it with the long-term savings. A new study from researchers at UC Berkeley and North Carolina State University estimates that electric vehicles like Tesla's will save around $10,000 over ICE vehicles in the long run. The study, dubbed \"Estimating Benefits of Electric Vehicles: Comparing Savings on Electricity and Fuel Costs with Life Cycle Analysis,\" comes from the university's Energy and Resources Global Leadership Program and co-authors Dr", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The benchmark dataset provides a gold standard for the evaluation of knowledge graph completion models, along with information about the metrics that were used to evaluate those models. Here are the relevant metrics: Precision, Recall, F-score, and Normalized Mutual Information (NMI). output (Sparql query): formidable metrics that are commonly used when benchmarking models on the STEM-ECR v1.0 dataset are:Precision, Recall, F-score, and NMI. \r : These are the metrics that are used to evaluate models on the STEM-ECR v1.0 dataset:Precision, Recall, F-score", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The returned values are: +-+-+ | metric | metric_lbl | +-+-+ | Positive Neighbourhood Coverage | Positive Neighbourhood Coverage (predicted/ground-truth) | | | | | Negative Neighbourhood Coverage | Negative Neighbourhood Coverage (predicted/ground-truth) | | | | |Precision (rel. to pred. rel.) | Precision", "SELECT DISTINCT ?model ?model_label WHERE { { ?model ModelGAD. ?model ModelGAD:predictionConfiguration g:modelGADDataPointToleranceMultiplier g:modelGADMaxDatapointToleranceMultiplier g:modelGADMinDatapointToleranceMultiplier g:modelGADMaxIoStepMultiplier g:modelGADIOStepsPerPointMultiplier g:modelGADTestIterations g:modelGADTestNumberOfIterations. ?model ModelGAD:modelCreationDate value \"2015-07-07\"^^xsd:date. ?model ModelGAD:modelIdentifier \"VGGNet\"@en. ?model ModelGAD:modelType g:modelGAD. ?model ModelGAD:name \"VGGNet\"@en. ?model ModelGAD:parameterConfiguration g:modelGADDataPointToleranceMultiplier g:modelGADMaxDatapointToleranceMultiplier g", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; ?metric ?metric_lbl. FILTER (str( ?metric) = \"ROC-AUC\") ?benchmark orkgp:HAS_DATASET; OPTIONAL { ?benchmark orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): ROC-AUC is one of the metrics used to evaluate the performance of machine learning models. output (Sparql query):", "SELECT DISTINCT ?metric ?metric_lbl ?metric_lbl <|endoftext|>Q: how to get rendered page source on complete ajax call success ? i have used $http in my controller and my service is jasmine. i have used unit test for $http service. here is the service restangular.mock('./submodules/services/LogsService.js') .factory('LogsService', ['$http', function ($http) { var logsService; return { log : function(url) { $http.post(url).success(function (data, status, headers, config) { console.log(data); console.log(headers); console.log(config); }); }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following are research papers along with their title and IDs, that have performed benchmarks on the AAPD dataset: [0]: https://halkyd.com/papers/rel-benchmark-2017-cosmos-with-gluon-resnet-and-xception.pdf [1]: https://halkyd.com/papers/kavli-signature-atlas-2017-cosmos-with-gluon-resnet-and-", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Here is the code used in the code zoo paper: https://github.com/google/code-zoo/blob/master/eval/Abstracts%2Dentities%2Drelations%20Annotated%20Corpus.py. It uses the AWD-LSTM model trained on the benchmark dataset. The metrics are: - BLEU: https://github.com/google/code-zoo/blob/master/eval/Abstracts%2Dentities%2Drelations%20Annotated%20Corpus.py#L146 - ROUGE:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The table below contains the IDs and titles of research papers on the PubMedQA dataset that contain benchmarks for the Abstracts' entities and relations annotated corpus annotated corpus (the actual benchmark has been removed). Please note that the titles may be misleading as they refer to the benchmark annotation, not the original research paper: | Paper ID | Title | |-|-| | 5199728 | Nguyen et al. |", "SELECT DISTINCT ?dataset ?dataset_lbl ?benchmark ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?metric ?metric_lbl ?score WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics returned for the highest score are: { \"metric\" : \"ENTERY_IMPROVEMENT\", \"metric_lbl\" : \"ENTERY_IMPROVEMENT\", \"score\" : 982.4816, } The metrics returned for the highest score are: { \"metric\" : \"PREDICTION_R2\", \"metric_lbl\" : \"PREDICTION_R2\", \"score\" : 982.4816, } The metrics returned for the highest score are: {", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 2. SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The top-scoring model is Keptn://SGE-ESVM-0.0 on the Abstracts entities and relations annotated corpus benchmark dataset (ranking #85) with Average Precision.", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised: benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC senseval:Senseval2 ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The evaluation metric that the best performing model performed well on was Senseval 2 ? output (Sparql query): SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised: benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC senseval:Senseval2 ?metric. ?metric", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-103 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Indicate the model that performed best in terms of Test BLEU metric on the WikiText-103 benchmark dataset ? output (Sparql query):BLUELabELabel=\"Seventeen\" input (English text): Please list the metrics that are used to evaluate models on the WikiText-103 benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } results: consulted data sources: ontology mapping tool: Ontobee highest result achieved: 1.86 metric: TER metric_lbl: Abstracts' entities and relations annotated corpus <|endoftext|>The evolution of irrigation in Spain and the Hispanic world. The history of irrigation in Spain and the Hispanic world begins with the Roman conquest. Since the 13th century, Christian rulers progressively extended the network of canals and constructed remarkable irrigation works for wheat cultivation, leading to the first Spanish Grapevines (Cabrales and Chardonnay). In the early modern period, Jesuit colonization established a system of highly developed networks and turned the country into the major", "SELECT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The top performing model according to the answer provided above is multi-layer_perceptron with an Score score of 0.03331.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } cement:C51 noop. cement:C51 noop evaluates its metrics on the C51 noop model. orchestration: OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. }.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics used in papers that benchmark the FABIR model: accuracy - the percentage of relevant abstracts in the FABIR output that are also in the GT output. The higher the accuracy, the better the model. precision - the percentage of relevant abstracts in the FABIR output that are also in the GT output. The higher the precision, the better the model. recall - the percentage of relevant abstracts in the FABIR output that are in the GT output. The higher the recall, the better the model. F1 - The F1 measure is the weighted average of precision and recall.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Emergency type of AMBER Alert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a coqa:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a coqa:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes' benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metrics are used on the benchmarks datasets: * QuestionGen-MRR * QuestionGen-R2MRR * QuestionGen-MAP * CoverageGen-MRR * CoverageGen-R2MRR * CoverageGen-MAP * EntailmentGen-MRR * EntailmentGen-R2MRR * Ent", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Switch Transformer model is utilized in: - Model Inpainting on Tiny Images by Wang et al. (2019) - Incremental Neural Machine Translation with Ternary Connectives by Lu et al. (2019) - Fine-grained visual reasoning by Hanson et al. (2019) - Databricks\u2019 Neural MTA by Luiten et al. (2019) - ELMO by Paliwal et al. (2019) - Answer-Oriented Transformers by Liu et al. (2019) - Visual Question Answering by Tarsinkala et al.", "SELECT * FROM <http://library.thinkovault.com/sparql/dmchcPASRS6X6w>) a WHERE { a :dmchcPASRS6X6w; FILTER ( ?benchmark = \"commonsenseqa\"). } _output (_list of Sparql queries): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } I don't understand the part of the query where you are filtering the benchmark column based on the value \"commonsenseqa\". I tried that but it didn't work. A: Common sense question assessment (commonsenseqa) is a sub-corpus of CommonsenseQA dataset", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2 dataset\") ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval :hasScore ?eval. } } input (English text): The abstracts' entities and relations annotated corpus (ERA) corpus benchmark dataset contains the following three datasets that are evaluated using the precision, recall and F1 score: precision, recall, and f1 score. - precision: the number of true positive abstracts, divided by the total number of true positive and false negative abstracts - recall: the number of true positive abstracts, divided by the total number of true positive and false negative abstracts - f1 score: 2 * precision * recall / (precision + recall) - precision : the number of true positive abstracts, divided by the total number of true positive and false negative abstracts - recall : the number of true positive abstracts, divided by the total number of true positive and false negative abstracts", "SELECT ?metric_lbl ?metric WHERE { #!/usr/bin/env owl-case SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } How I get the code links from the paper ? It's very difficult to manually review all the code links from the paper because there are a lot of paper and the code links are buried in the paper or even in the appendix. But there are some pointers below: 1. If the model is A3C-CTS or variants (such as A3C-RTT, AC3C, and A3CR), there is a section", "SELECT DISTINCT ?metric ?metric_lbl WHERE { \r ?dataset a orkgc:Dataset; \r rdfs:label ?dataset_lbl. \r FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \r ?benchmark orkgp:HAS_DATASET ?dataset; \r orkgp:HAS_EVALUATION ?eval. \r OPTIONAL { \r ?eval orkgp:HAS_METRIC ?metric. \r ?metric rdfs:label ?metric_lbl. \r } \r } \r \r Related Blogs: https://www.algov.at/blog/2017/02/28/who-wants-to-bake-a-cake-to-the-hutter-prize.html; https://algov.at/blog/2018/02/12/sparql-query-for-querying-open-kb.html.\r \r \r \r The top benchmark score and its metric on the Hutter Prize dataset is: \r \r Hutter Prize Relational Algebraic Query", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2003 (English)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (SPARQL 1.1 query): SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2003 (English)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (English text): F1 score output (Sparql query): The model that achieved the highest F1", "SELECT distinct ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }]", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Artificial Intelligence\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval_ID:ID ?eval; ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Artificial Intelligence. output (Sparql query): output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label \"Artificial Intelligence\"@en. FILTER (str( \"Artificial Intelligence\"@en) = \"Artificial Intelligence\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EV", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"SWReT\") OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>[The increasing need of speech language pathology in Slovenia]. There is a growing need of speech language pathology in Slovenia. Between 2007 and 2013, the number of speech language pathologists increased by 30% whereas the number of Slovenian residents increased by only 12%. The reason for the increase is the declining birth rate and the ageing Slovenian population. As of 2013, there are approximately 8,000 speech language pathologists, which is about 10% of the Slovenian population of approximately 7.4 million. Speech language pathology should be provided", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The best performing model is the JozefKNOLBASH@illinois.edu model with an BLEU score of 44.31. <|endoftext|>Q: Migrating web app from Subversion to Git I have to migrate a system with SVN into Git. The system is composed of around 30", "SELECT DISTINCT ?metric_lbl ?metric_str WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT ?metric_str ?metric_str_label WHERE { ?dataset_lbl a dcm:Dataset. ?metric_str rdfs:label \"Bleu\". ?dataset_lbl dcm:Dataset = ?benchmark. ?eval o act:EvaluationMetric ?metric. OPTIONAL { ?eval act:BenchmarkName ?benchmark_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Metrics used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset include: ? ?benchmark-entity-count: counts the number of benchmarks with an entity in them ?benchmark-relation-count: counts the number of benchmarks with a relation between them ?benchmark-label-frequent: calculate the number of times a specific label is used ?benchmark-property-frequent: calculate the number of times a specific property is used ?benchmark-subquery-count: count of subqueries used in benchmark ?benchmark-materialized-view-count: count of materialized view in the benchmark ?benchmark-foreign-", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field are: * BioASQ * CK-Penetrating Dozen * CoNLL 2003 relations with a named entity * CoNLL 2004 relations with a named entity * DSP2010 relations with a named entity * CoNLL 2009 Core NER with named entities * CoNLL 2009 Core relations with named entities * CoNLL 2009 relations without named entities * CoNLL 2010 relations without named entities * CoNLL 2010 named entity classes (with names) * CoNLL 2010 named entity classes (with names) with relations *", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The MultiRC dataset contains relations between entities that have been annotated with a metric. The metric is an attribute that contains a numerical value for that relation. Metrics typically contain a triple (type, entity1, entity2), where entity1 and entity2 are the two entities in the relation and type is the type of relation.", "SELECT * FROM datahub_loader::SparqlResult WHERE has_loader_row=true \r Some of the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are as follows: \r \r \r \r * precision \r * recall \r * f-score \r * mdr \r * avg_per \r * avg_recall \r * avg_fscore \r * mdr_micro \r * mdr_macro \r * avg_mdr \r * avg_mdr_macro \r * avg_avg_per \r * avg_avg_recall", "SELECT * FROM query WHERE subject = :dataset and object=:eval and predicate=:benchmark and label=:dataset_lbl and value::text contains \"Score\" ORDER BY :metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): WMT2014 German-English SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The correct answer would be:", "SELECT DISTINCT ?eval WHERE { ?eval orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"Benchmarking a model on SearchQA dataset\") } gs\u00e5mark:@score facilitating the access to knowledge, explaining complex concepts and facilitating the sharing of knowledge are core to the essence of what it means to be human. this notion of the human-machine interface is examined from a critical perspective in this article. first, it is suggested that the critical conception of the human-machine interface demands an interpretation of the term with respect to how a human interacts with technology. then, the article suggests that technological advances have helped create a technically enhanced version of the human. however, this newly enhanced human, the so-called cognitive surplus humanity, does not necessarily possess the qualities of curiosity, empathy, and creativity. in addition, it is suggested that this new version of the human is more accurately described as a version of the human that has been augmented, rather than enhanced. Ultimately, it is suggested that understanding the nature of the human-machine interface is central to understanding the 21st century and the challenges and opportunities that await us. Central", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"Ontology-based named-entity recognition and relation annotation\") OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): Output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"Ontology-based named-entity recognition and relation annotation\") OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Ontology-based named-entity recognition and relation annotation is performed over the OntoNotes dataset using the RNN model. What", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } counselor (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The WMT2014 English-German dataset contains two relation annotations, including the Affect relation and the Verb relation. For each relation, there is a metric which is the highest result achieved on the WMT2014 English-German dataset. valign;SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?", "SELECT DISTINCT ?metric ?metric_lbl The following metrics are used to evaluate models on the PWC Leaderboards' (restricted) dataset: * Area Under the Curve (AUC) - accuracy measurement * F1 Score - precision * recall / ( precision + recall ) * Matthews Correlation Coefficient (MCC) - accuracy * precision * recall - accuracy * precision * recall - 0 The titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset are as follows: title id - - - Prestigious Web-based Databases for Affordability and Usability Test Benchmarking, IFC Lab, Stanford University 3042 Benchmarking and its Role in Web Development: A Practical Perspective, Florian Zeller and Kristina Tsvetkov, 3041 Automatic metric discovery for web applications, Julia Haupt, Daniel Aronica, Nicolas Serazin, Florian Zeller, 3040 Metric-driven benchmarking in IT-related", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3 dataset\") ?metric; ?metric rdfs:label ?metric_lbl. } personnel: Anoop Gupta, Junyoung Chung, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet, Yandong Han, Andrea Lucchese personnel: Anoop Gupta, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet, Yandong Han, Andrea Lucchese personnel: Anoop Gupta, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet, Yandong Han, Andrea Lucchese personnel: Anoop Gupta, Tzchi Miller, Cliff Lai, Richard S. Zemlin, Noam Lorie, Joe DeCarlo, Will Millet", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The most common location in the studies is the metric of precision. output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): There are several metrics utilized in the papers that we have come across including BLEU, perplexity, word relevance, and F1 score. These metrics are described below. output (Sparql query): ?metric BLEU perplexity word relevance F1 score ?metric_lbl output (Sparql query):There are several metrics utilized in the papers including BLEU, perplexity, word relevance, and F1 score. input (English text): The papers we have used the BCN+ELMo model includes papers authored by Microsoft, Facebook, and Google ? output (Sparql query): ?metric BLEU", "SELECT * FROM extracted_results WHERE extraction_type ='pre_training_dataset_metric_on_hmdb51' and score > 99% ORDER BY rank DESC LIMIT 5; Pre-training dataset metric on the HMDB51 benchmark dataset performed best: The model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset was FBNET. The model's rank is 5. FBNET consists of 56 neurons in the fully connected layer and 54 neurons in the activation layer. The number of trainable weights in the fully connected layer is 56*54 = 3072. FBNET took 13,623 seconds to finish the task. The second-best model was MLNet which consists of 53 neurons in the fully connected layer and 52 neurons in the activation layer. The number of trainable weights in the fully connected layer is 53*52 = 2664. MLNet took 12,758 seconds to finish the task. Here is a visualization of the best and second-best models. The best model is on the left and the second-best model is on the right: Icons provided by Geraldo Miranda are licensed under CC BY SA 3.0.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): 5 seconds at 12 kHz is used to evaluate the models on the Classical music dataset. output (Sparql query): Dillon, 5 seconds at 12 kHz, model evaluation, Sparql Query answer: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset ? output (Sparql query):The best BLEU score on the German-English benchmark dataset was obtained by the neural model proposed by @denkowski:2015:GraSL. The corresponding Sparql query is: output (Sparql query):SELECT DISTINCT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The F1 metric was the best in the evaluation of models on the BC5CDR-disease benchmark dataset output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The evaluation metrics commonly used on the Atari 2600 Skiing dataset are: overall completion, success rate, total time, rank, and average rank.", "SELECT ?metric ?metric_lbl ?dataset_lbl_str WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") OPTIONAL { ?dataset orkgp:Benchmark ?benchmark; ?benchmark orkgp:HAS_DATASET ?dataset; ?dataset orkgp:HAS_EVALUATION ?eval. ?benchmark orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Abstracts' entities and relations annotated corpus (benchmark over the Frostbite dataset), The title and ID of the benchmark over the Frostbite dataset are: Benchmark over the Frostbite dataset Abstracts' entities and relations annotated corpus (benchmark over the Frostbite dataset), Frostbite is an open-source game engine that generates high-fidelity 3D models from 2D images. This benchmark tests the ability of a program to answer questions about a given knowledge graph. The performance of an answer extraction program is measured by the precision,", "SELECT * FROM http://www.sailsgps.com/datasets/bdb-comet-direct-model/ WHERE isCoverage = true AND Dataset = :dataset_id input (English text): The code that provides the model and scores can be found at the following link: link to code (Sparql query): http://www.sailsgps.com/datasets/bdb-comet-direct-model/ solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity solidarity", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } - Output:", "SELECT DISTINCT ?metric ?metric_lbl returns the following results: ?metric rdfs:label \"p\":* \"Value\" ?metric_lbl rdf:label \"Evaluated on Atari 2600 Boxing\" <|endoftext|>Quirky Places: The Wolpertilion in the Desert The Wolpertilion in the Desert The Wolpertilion in the Desert is an important prehistoric site located in the desert region of the State of Guanacaste, about 30 km south of the city of Tobalanzas. The prehistoric site is one of the largest and most extensive containing a number of structures built by the ancient prehispanic Indians, called Chichen Itza. The site covers an area of 110 hectares, and includes three main main buildings, two wells and a satellite cemetery. The main structures date back to the prehispanic period, with the first structures built during the Classic period between 200 BC and AD 300. The buildings are arranged in a circular formation, the function of which is unknown. All of the structures are made of stone and wood, although the structures closest to", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Genetic structure of Colombian accessions of Phytophthora infestans revealed by RAPD markers. The genetic structure of 19 US field-derived isolates and 25 Colombian reference accessions of Phytophthora infestans was assessed by randomly amplified polymorphic DNA (RAPD) markers. The Colombian accessions were collected from lowland fields in the Cundinamarca and Santander departments in 1991 and 1992. The RAPD profiles generated by primers B8 (5'-GACGACGATCAGTC", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest score is 79.7. The IWSLT2015 German-English dataset has been marked as difficult. The precision is 45.7% and the recall is 50%. Thus, the model has performed well in this dataset. output (Sparql query): orchestration-samples:Query Result for metric and metric label \"precision\" for the IWSLT2015 German-English dataset The top 5 metrics returned are: # metrics # score 1. precision 45.7% 2. recall 50%", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics that are used to evaluate models on the Atari 2600 Frostbite dataset are: - winrate - final score - per-frame score - one-shot learning - last seen frame - mean squared error - gaussian mixture model - recurrent encoder", "SELECT * WHERE { ?dataset a multiNLI orkgp:HAS_DATASET ?dataset. ?eval orkgp:HAS_EVALUATION ?eval. ?metric ?metric_lbl. ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"top-accuracy\") } To answer the question, the top benchmark score and its metric on the MultiNLI dataset are: top-accuracy and metric {multiNLI top-accuracy}", "SELECT ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") } input (English text): Yes, here are the models that have been evaluated on the ImageNet ReaL dataset: ResNet-50, DenseNet-121, and DenseNet-201. ordable model: ResNet-50 Evaluated on: ImageNet ReaL affordable model: DenseNet-121 Evaluated on: ImageNet ReaL affordable model: DenseNet-201 Evaluated on: ImageNet ReaL", "SELECT * WHERE { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest Accuracy achieved on the benchmark dataset is achieved by the model which achieved an accuracy score of 87.06. This model is called Model 1. view (Sparql query): SELECT * WHERE { ?dataset a orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest Accuracy achieved on the benchmark dataset is achieved by the model which achieved an accuracy score of 87.06. This model is called Model 1. Model 1: orkgp:accuracy (double precision) 87.06", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Kinetics-600 dataset scores 100.0. The metric for this dataset is Identification Accuracy and its score is 100.0. \f<|endoftext|>Some artists will tell you they love doing what they do. They like the specific constraints of their chosen medium; the paycheque that keeps their head above water; the accessibility of the medium to a wide variety of people; the ability to put their vision on the page; the predictability of the output. When a book I wrote came out a few years ago I was thrilled to discover that there were people who didn\u2019t just tolerate my chosen medium \u2014 they", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } {#sparql} @metric_lbl a <https://github.com/BartNote/BART.sparql#label> https://github.com/BartNote/BART.sparql#label {#metric}cInferred class,Inferred class,Perplexity,Dissimilarity,Hamming loss,Hamming loss%,Perplexity%,Dissimilarity%,Hamming loss% {#dataset}a orkgc:Dataset {#dataset_lbl Abstracts' entities and relations annotated corpus {#benchmark BART {#metric}cInferred class,In", "SELECT * FROM <benchmark_collection> WHERE has_dataset == true AND has_evaluation == true AND benchmark->'HasDataset' == true AND benchmark->'HasEvaluation' == true AND avg(metric) == 0.93 See also Entity relation classification KDD Cup 1995, a related competition SQuAD, a related dataset and associated OpenAI GPT model Abstracts' entities and relations annotated corpus References External links Category:Knowledge transfer Category:Semantic_modeling Category:Natural language processing<|endoftext|>Articles for Sale These handsome Royal Oak Vises are precision made from aircraft grade aluminium for years of reliable use. Sturdy steel stays and levers means your vise will hold even the most stubborn of jaws. The 3.5\" face plate allows extra wide jaws to be clamped down for extra stability. The pre-drilled holes make mounting this Vise a snap. Our vices are crafted from heavy gauge, aircraft grade aluminum for years of reliable", "SELECT DISTINCT ?metric_lbl ?metric_label WHERE { { ?metric rdfs:label \"accuracy\"}. ?metric_lbl ?metric; { ?dataset a orkgc:Dataset; ?dataset_lbl a cifar10:Sequential; ?benchmark orkgp:HAS_EVALUATION ?eval; ?eval orkgp:HAS_METRIC ?metric; ?metric rdf:type ?metric_type; } OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval; ?eval orkgp:HAS_METRIC ?metric; } } <|endoftext|>\ufeff<UserControl x:Class=\"ArcGISRuntime.WPF.Application.Pages.ArcGISDynamicMapLayerControl\" xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:esriUI=\"http://schemas.microsoft.com/winfx/2006/xaml/ite", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Answer: F-score 58.21 The F-score metric for a model is computed as 2 * precision * recall / (precision", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metrics are used to evaluate models on the datasets and evaluation task in the benchmarking dataset: * Abstracts' entities and relations annotated corpus * Support-vector machine (SVM) * Decision Trees * Logistic Regression * Neural Network * Hinge Loss * Maximum Softmax Classification Error Name the datasets that have been used for benchmarking in the citation classification research problem ? Orkgp:HAS_DATASET abstracts; Orkgp:HAS_EVALUATION citationclassification; Orkgp:HAS_METRIC abstractsmetric; Orkgp:HAS_EVAL metric; Orkg", "SELECT * WHERE { ?s ?p ?o } input (English text): https://www.aclweb.org/anthology/D/D17/D17-1171.pdf output (Sparql query):(aclweb.org/anthology/D/D17/D17-1171.pdf) traject:SELECT * WHERE { ?s ?p ?o } traject:pageContent ${page_contents} traject:pageContent \"Dataset Schema\"{ traject:select ?dataset ( ?dataset orkgp:HAS_DATASET ?dataset ) traject:with ?dataset_lbl ( ?dataset_lbl orkgp:HAS_LABEL ) traject:of ?dataset traject:order by ?dataset_lbl ?dataset_lbl traject:pageContent \"Dataset Annotated with NLI Metrics\"{ traject:select ?dataset ( ?dataset orkgp:HAS_EVALUATION ?eval ) traject:of ?dataset traject:where { ?eval orkgp:HAS_METRIC ?metric } traject:pageContent \"Evaluations and Results\"{ traject:let ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } CONTRIBUTOR", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Q: When i create hash with data[1] and data[2] why data is not saved as hash ? When i create hash with data[1] and data[2] why data is not saved as hash ? my_hash = {key1: value1, key2: value2} my_hash.keys.each do |key| data = my_hash.send(key) puts data[1] #not works! end why this not works ? In ruby 1.9.3 this line data = my_hash.send(key)", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CounterSketch\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The 'negative log likelihood' metric is used to evaluate the 'CounterSketch' model. The 'negative log likelihood' metric is used to evaluate the 'CounterSketch' model. How do you know which metric is used to evaluate the 'CounterSketch' model ? SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CounterSketch\" ) ?benchmark orkgp", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): In the code references section of the paper, one can find the following code: ?eval.CONCAT_WP( ?, ?) ?metric. The ?metric function has the following signature: ?metric(datatype, value, time, measure, options), where options are passed in as a map. The measure parameter takes a value of string. So, it looks like this function is used to concat two strings. <|endoftext|>The invention relates to a rotary position sensor having at least one rotor supported for rotation about a rotation axis in a housing and a magnetic field generation means for generating a magnetic field directed axially into the housing", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist' 2018 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } SELECT ?metric ?metric_lbl ?metric : Top-1 Accuracy ?metric_lbl : Abstracts' entities and relations annotated corpus COMMAND : ?benchmark : iNaturalistSpeech 2018 ?eval : iNaturalistSpeech 2018 ?dataset : orkgc:Dataset ?dataset_lbl : Abstracts' entities and relations annotated corpus COMMAND : ?dataset : orkg", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ice Hockey\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } \"\"\"); if (results.size()>0) { for (Result r : results) { metric=r.get(\"metric\"); metric_lbl=r.get(\"metric_lbl\"); } } gs\u00e5nt result,rank=0; for (Result r : results) { if (r.", ", the datasets related to the Scientific Results Extraction research area are: Abstracts' entities and relations annotated corpus, Tendermint benchmarks, Tangle, Hyperledger Finnish Quorum Testing. input (English text): The Abstracts' entities and relations annotated corpus benchmark datasets are gathered from KGs where the data is associated with the CRESTontology, the Metrics are the RIF, RIF-lite and the evaluation procedure is the human judgement evaluation. List the metrics that are used to evaluate models on the datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets ? The following metrics are used to evaluate models on datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets: RIF, RIF-lite, Human judgement evaluation. The following metrics are used to evaluate models on datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets: RIF, RIF-lite, Human judgement evaluation. The RIF (redundancy elimination factor), RIF-lite (reduced redundancy elimination factor), Human judgement evaluation metrics are the metrics that are used to evaluate models on datasets gathered from the Abstracts' entities and relations annotated corpus benchmark datasets. The metrics R", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset are: Brett Leech, Andrew Fox, Paul Webber, and Will Jennings. \"Large CoNLL-2010 Cross-Documentrep Cross-Linguistics Shared Task: The Abstracts\u2019 entities and relations annotated corpus benchmark.\" ACL 2010. \t http://www.aclweb.org/anthology/D/D10/D10-1036.pdf http://aclweb.org/anthology/D/D10/D10-1036.pdf http://www.acl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Semantic Role Labeling research area\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } for the Sparql query provided above, the results would be: metric metric_lbl - - F1-score Semantic Role Labeling research area for the dataset, the metric would be F1-score for the evaluation, the metric would be F1-score <|endoftext|>Shooter (board game) Shooter is a tabletop game by German studio Chemin Carbon published in 2017. It is a cooperative war game set during the African Campaign of the Second World War and pits the Western Allies and Axis Powers against one another in a struggle for Europe. Like many war games, Shooter is set in the imaginary world of \"War", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"WikiText-2 dataset\" output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text):", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Metrics of evaluation over the PWC Leaderboards (restricted) dataset are : precision, recall, f-score, macro-f1. output (Sparql query): Serve the following to the Sparql query processor: precision ?metric_lbl, recall ?metric_lbl, f-score ?metric_lbl, macro-f1 ?metric_lbl input (English text): precision : a metric which shows the fraction of correct predictions for each label (i.e. whether a label is in the positive or negative set). input (English text): recall : a metric which shows the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset", "SELECT DISTINCT ?metric ?metric_lbl 3. WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fine-Grained Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 4. SELECT DISTINCT ?metric ?metric_lbl 5. ?metric rdfs:label \"Abstraction distance\" 6. ?metric_lbl Fine-Grained Image Classification 7. 0 Abstraction distance 8. 0 9. ?eval orkgp:HAS_METRIC ?metric 10. ?eval orkgp:HAS_METRIC ?metric 11. Abstraction distance 12. Fine-Grained Image Classification 13. rdfs:label \"Abstraction distance\" 14. 0.005 Abstraction distance", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The evaluation metrics that are commonly used when benchmarking models on the DRI Corpus dataset are: Precision, recall, and F1 score. Macro F1 score. Tf-Idf. Hamming distance. Root mean square error. Mean absolute error. Pearson's correlation coefficient. Energy of a Loss Function. The following links provide information on those metrics: Table of Databricks' evaluation metrics. Understanding the Databricks Tf-Idf Loss Function. Energy of a Loss Function. More", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval stlp:metric { ?metric. ?metric_lbl stlp:metric_label }. } } input (English text): The Abstracts' entities and relations annotated corpus dataset has two evaluation metrics namely, Mean Absolute Error (MAE) and Mean Square Error (MSE). The STL-10 dataset is used in different research papers and it has metrics such as Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, Mean Squared Logarithmic Error, Squared Euclidian Distance, and Cosine Similarity. The STL-10 dataset is available in Open accessed data repository called Figshare. https://figshare.com/articles/stl_10_corpus_", "SELECT DISTINCT ?metric_lbl ?metric_text ?metric_label WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } * ?metric rdfs:label ?metric_label. FILTER (str( ?metric_label) = \"Soft Assess\" OR str( ?metric_label) = \"F1 Score\") } } The returned values would be: metric_text :F1 Score metric_label :F1 Score metric_text :Soft Assess metric_label :F1 Score metric_text :Precision metric_label :F1 Score metric_text :Recall metric_label :F1 Score metric_text :TPR metric_label :", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics that are used to evaluate models on the Skiing dataset are: - Average elapsed time (milliseconds) - Accuracy (percentage) - Average number of branches traversed - Number of jumps cancelled Don't worry, you were never that good at skiing. :) Skiing dataset: The Abstracts' entities and relations annotated corpus dataset has been benchmarked on the Skiing dataset. Here are the research paper IDs and titles that have benchmarked models on the Skiing dataset: +-+-+ | Research paper| Title | +-+-+ |", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The VTAB-1k dataset contains metrics for entity overlap (%), relation overlap (%), and average number of edges in the augmented transitive reduction of the subqueries associated with each KB query. The three metrics are: Entity Overlap: The number of entities that appear in both the ground-truth KB and the predicted KB for a given pair of entities. Relation Overlap: The number of relations that appear in both the ground-truth KB and the predicted KB for a given pair of entities. Average Number of Edges: The average number of edges in the augmented transitive reduction of the subqueries associated with each KB query. For a score", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\") } input (English text): Score is an entity which is a metric used by the model to evaluate its performance. Cart Pole dataset has been successfully benchmarked against 3 models namely : Glow, GAN and AITS and the model with the best score is Glow.", "SELECT DISTINCT ?metric_lbl ?metric_lbl_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): https://github.com/ibrs-dbp/rfa-gate-arccos_benchmarks. Clicking \"Benchmarks\" in the repository will take you to the table of code, in that table you will find that this model is used in the Abstracts' entities and relations annotated corpus benchmark. urches_lbl (label):Abstracts' entities and relations annotated corpus metric_lbl (label):arccos metric_lbl_lbl (label):arccos", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval (fn:metric( ?metric_lbl)). ?benchmark (fn:fn(fn:metric( ?metric_lbl))) orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): F1-score metric <|endoftext|>Introduction ============ It is well-known that tumor cells express high level of specific markers. The presence of these markers enables the tumor cells to escape from the host's immune system and to colonize and proliferate within tissues and organs. Therefore, tumor makers are indispensable tools for diagnosing and understanding the behavior of cancer cells. Currently, several tumor makers are available for clinical use, among which, tumor markers assayed using the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): BioASQ is a RDF dataset provided by the AMIE Challenge participants. It contains triples of the form <relation,subject,predicate> where the <relation> is a named-graph predicate, the <subject> is a named-graph object, and the <predicate> is a natural language string describing the purpose or relationship of the subject to the relation. <relation> is sometimes called a \"functional relation\" or a \"semantic relation\" in semantic web and related areas. The predicate can have multiple arguments: in the following example, \"modifies\" is a relation, \"gene\" is the subject, and \"protein\"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Centipede\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>A new approach to timing-resolved transillumination imaging using array detection. We present a method for acquiring time-resolved transillumination images using an array detector. The system works by modulating the incident illuminator with a sequence of temporally separable illumination patterns and then detecting the modulated pattern with a single frame array detector. Using an array of successive approximation-register machines, the pattern is reconstructed by convolving the pattern with a sequence of filter banks, each appropriate for a particular frequency band of the detected pattern. The parameters of the filter banks are optimized to yield the sharpest representation of the illumination pattern. The temporal resolution of the reconstructed pattern is set by the duration of the illumination", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): SELECT ?metric ?metric_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Dataset dataset in Classic has the label \"Classic dataset\". What metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query):\" ?eval. HAS_METRIC ?metric\" input (English text): The Dataset dataset in Classic has the label \"Classic dataset\". What metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query):\" ?metric_lbl\" input (English text): The Dataset dataset in Classic has the label \"Classic dataset\". What metrics are used to", "SELECT ?metric { \t ?eval orkgp:HAS_METRIC ?metric. } input (English text): The DeiT-Ti model has been used in papers published by Cao, Chen, Qu, Wang, et al. The DeiT-Ti model can be found here: https://github.com/DevTijdeo/DeiT-Ti/blob/master/docs/DeiT_Datasets.ipynb Please refer to the paper here: https://github.com/DevTijdeo/DeiT-Ti/blob/master/docs/Paper.ipynb", "SELECT DISTINCT ?metric ?metric_lbl: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The query result is as follows: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?model_name WHERE { ?model_name rdfs:label \"MODELNAME\"@en. FILTER (str( ?model_name) = \"HMDB51\") ?model_name_en; FILTER ( ?model_name_en = \"HWRD\") ?model_name_hr; FILTER ( ?model_name_en = \"RetineX\") ?model_name_rex; FILTER ( ?model_name_en = \"DeepMedic\") ?model_name_dm; FILTER ( ?model_name_en = \"Adaboost\") ?model_name_aboost; FILTER ( ?model_name_en = \"SVR\") ?model_name_svr; FILTER ( ?model_name_en = \"Multi-view\") ?model_name_mv; FILTER ( ?model_name_en = \"Kinetics\") ?model_name_kin; FILTER ( ?model_name_en = \"AutoSklearn\") ?model_name_asl; FILTER ( ?model_name_en = \"K-nearest\") ?model_name_knn; FILTER ( ?", "SELECT ?title (STR( ?title)) AS ?title, ?id (IN( ?id)) AS ?id WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Following are two entities in the dataset: PONG and BLOCKBUSTER. Their relationships are PONG is an element of BLOCKBUSTER and BLOCKBUSTER is an element of NOVELTY. The authors of the paper PONG vs. Alpha Go have performed a benchmarks and have mentioned in their paper that the precision of the models are very poor for these relationships. The models that they have tried include BRAT, PRINCE2, and MGM. PONG vs. Alpha Go paper : https://paperswithcode.com/p/pong-vs-alpha-go Entity entity", "SELECT DISTINCT ?metric ?metric_lbl ?eval WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection) benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Rotowire (iHome) benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl ?eval WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (iHome) benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:", "SELECT DISTINCT ?metric_lbl ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Here is a list of metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: AUC, precision, recall, f-measure, average precision, area under the ROC, MCC, R2. <|endoftext|>A polymerase chain reaction method for the rapid identification of the human Bacillus anthracis (anthrax) species: implementation of a two-step method based on polymerase chain reaction amplification of DNA segments of the capsule, cap, and mpl genes and RAPD-PCR-polymerase chain reaction fingerprinting. An easy to perform two-step polymerase chain reaction (PCR", "SELECT DISTINCT ?metric_lbl_1 ?metric_lbl_2 WHERE { ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl_1. } OR ?metric_lbl_2 rdfs:label ?metric_lbl_2. } ?metric_lbl_1 ?metric_lbl_2 } output (Sparql query): ?metric_lbl_1 = \"True Positive\" ?metric_lbl_2 = \"Precision\" Why do you need the metrics for this benchmark ? (What is the goal of the benchmark ?) To use the benchmark to better understand the system performance of deep learning models on the Atari 2600 Breakout dataset. The final system results achieved by using the benchmark are not very meaningful to the end user, thus a subset of the metrics are provided in the output so that you can better understand the system results. Why is the benchmark scored ? The benchmarks are scored using a metric", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The AFT metrics are commonly used when evaluating models on the Bowling dataset: 1. Average F1 Score 2. Discounted Cumulative F1 Score 3. Time on Task 4. Average Frame Per Game 5. Average Frame Duration 6. Number of OMG Events 7. Number of Coin Events 8. Number of Shots on Green 9. Shot Type Agreement 10. Shot Type Disagreement 11. Distance from Perfect Shot 12. Shot Group Average 13. Hit Type Agreement 14. Hit Type Disagreement 15. Distance from Perfect Shot 16. Dribble 17. Ball Control 18", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?eval a orkgp:HAS_EVALUATION ?eval. ?eval a orkgp:HAS_DATASET ?dataset. FILTER (str( ?dataset) = \"Abstracts entities and relations annotated corpus\") ?benchmark. ?benchmark a orkgp:HAS_EVALUATION ?eval. FILTER (str( ?eval) = \"FQF\") ?metric. ?metric rdfs:label ?metric_lbl. } Note: the output contains a combination of Sparql and JavaScript The metrics used in FQF model benchmarks are HAS_EVALUATION and HAS_DATASET The HAS_EVALUATION metric measures the performance of an ontology evaluation algorithm in terms of the proportion of correct annotations made on datasets. The HAS_DATASET metric measures the performance of an ontology evaluation algorithm in terms of the size of the ontology it was trained on. Note: has_evaluation and has_dataset metrics can be used together as a metric combination All papers using the FQF model in any benchmark in any year, are attached to the following G", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics used to evaluate models on the FSNS - Test dataset are: * BLEU * ERC * FRCN * SPERM * SPF * CPC * ROUTE * REV * SIM * POS * LF * EBOW * CR * RF * IDX * NEG Indicate the model that performed best in terms of BLEU metric on the FSNS - Test dataset ? SELECT ?metric ?metric_lbl WHERE {", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The query above returns: +-+-+ | metric_lbl | metric | +-+-+ | Evaluated on | ARC (Challenge)| | ARC (Challenge)|0.9416188171544| +-+-+ <|endoftext|>It\u2019s common to hear Republican candidates say that their party stands for something that is \u201crealistic\u201d and \u201cpractical.\u201d But the pizzerias of America are not really on the knife\u2019s edge of economic collapse. And while universal healthcare might be wonderful, doing without your cholesterol medication is a serious, potentially life-threatening, matter.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Title1 - Metric 1; Title2 - Metric 2; Title3 - Metric 3 Title1 - Metric 1 Title2 - Metric 2 Title3 - Metric 3 output (Sparql query): title1 - metric 1; title2 - metric 2; title3 - metric 3 \u0d4d Registry A model evaluation metric or \"SciBench Metric\" is a metric used to compare a model's performance against a baseline model or", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:TSE-NER-Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER dataset\") ?metric. ?metric rdfs:label ?metric_lbl. } input (English text): TSE-NER is a Named Entity Recognition dataset developed by the OntoNotes Corp. It contains name entity declarations and their corresponding sentiment tags. The dataset uses the Stanford CoreNLP toolkit to identify named entities. The model is evaluated using three metrics: Precision, Recall and F-measure. The metrics used to evaluate models on the TSE-NER dataset are: - Precision - % of Named Entities that are Correctly Identified as Named Entities - Recall - % of Sentiment Tags that are Identified as Named Entities - F-measure - (Precision x Recall) / (Total Number of Tags That are Identified as Named Entities) output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>A conventional door handle has a body and a door latch module movably attached to the body. The door latch module has a latch housing, a striker plate and a latch bolt. The latch housing defines a receiving space for receiving the striker plate and the latch bolt is mounted in the receiving space and moves in the latch housing for unlatching the striker plate. The body has a latch assembly disposed therein and the latch assembly includes a latch plate, a pawl and a knob. When a user intends to open the door, the user pulls the knob from the body. In this case, the pawl may be engaged with the striker plate to latch the striker plate in the receiving space of the", "SELECT-DISTINCT-METRIC-LABEL-WHERE-%28%28CONCAT(SUM( ?metric_lbl),' ? ')+'%29 WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): https://github.com/alireza092/a3c-lstm-hs-model/blob/master/experiments/pretrained_model.py#L65-L96 output (Sparql query):", "SELECT * FROM { http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html } WHERE url = \"http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html\" input (English text):SVHN: Benchmark dataset for supervised learning, provides % labels predicted by the model vs actual labels for testing. The lower the % error the better. output (Sparql query):\u30fc\u30b8 SELECT * FROM { http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html } WHERE url = \"http://svhn.cs.ucl.ac.uk/baselines/docs/svhn.html\" input (English text):Densely Connected Neural Networks: An end-to-end approach for automatic image interpretation using convolutional neural networks", "SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Response: metrics used to evaluate models on the benchmarks datasets under the SPARQL query optimization research problem module sparqlqueryoptimizationproblem; { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } { metric: F1, Relation-Lattice, 0.8189425 metric_lbl: F1, Relation-Lattice, \"Relation-Lattice\" } <|endoftext|>Q: Why the output is \"word\" instead of \" hello world \" int main() { cout << \"hello world\"; return 0; } Why", "SELECT ?metric ?metric_lbl WHERE { \t ?dataset a orkgc:Dataset; \trdfs:label ?dataset_lbl. \tFILTER(str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \t ?benchmark orkgp:HAS_DATASET ?dataset; \torkgp:HAS_EVALUATION ?eval. \tOPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } \t} input (English text): <https://github.com/holtzy/TCN-benchmarking> output (Sparql query):\u00f4ng addEventListener({ id:\"https://github.com/holtzy/TCN-benchmarking\", type:\"http://www.w3.org/2001/04/soap-mapping\", features: {\"application/feature-store\":\"https://raw.githubusercontent.com/holtzy/TCN-benchmarking/master/application/feature-store.xml\"} }, function() {", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The ResNet paper is: https://arxiv.org/abs/1907.11931 The paper uses the following metrics: - Top-1: The number of correctly classified samples in the top 1% of predictions. - Top-5: The number of correctly classified samples in the top 5% of predictions. - Top-10: The number of correctly classified samples in the top 10% of predictions. - Precision@k: The average percentage of predictions at the $k$th highest category. - Recall@k: The average percentage of predictions at the $k$th highest category", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output: ville:WNLI highest measure=ENTITY_TREE_INFLEX_PROPOSAL_F1,value=0.9487333333333334 <|endoftext|>Q: How to call another method when value of model property changes in knockout js I have model Class called Category.", "SELECT \"metric\", \"metric_lbl\", \"score\" WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest score for the AAPD dataset is 48.82. It includes the metric and score. (\"metric\", \"metric_lbl\", 48.82) (...) (\"metric\", \"metric_lbl\", \"score\") A: I would say the following metrics are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: number of entities annotated number of relations annotated precision (number of correct relations/total number of relations) recall (number of correct relations/total number of relations annotated) <|endoftext|>Diagnosis", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The metrics used by orkgp:HAS_EVALUATION attribute for evaluating a model against a particular dataset in the FTD dataset dataset are : support (1/prediction): The fraction of data points in the dataset that were correctly classified by the model. precision (1/precision): The fraction of data points in the dataset that were classified as positive (true positive,TP) by the model. recall (1/precision): The fraction of data points in the dataset that were classified as positive (true positive,TP) by the model. fscore (harmonic average of precision and recall): A harmonic average of precision and recall. m", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following are the metrics that are used to evaluate models on the CIFAR-10 Image Classification dataset: ?metric: Percentage error ?metric_lbl: CIFAR-10 Image Classification <|endoftext|>First pecan in Ozona, Texas, commemorates 100th anniversary of successful pecan export Ozona, Texas\u2014When Jake Jordan left Oklahoma in 1949, he left his roots behind but made his mark in Texas. He left behind a pecan orchard", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 6.4.2 Metrics for classification and regression That the model is predicting the right class for a given instance can be measured by comparing the predicted label with the actual label. There are two types of labels in the Walker, walk (DMControl500k) dataset: one for the instance and the other for the concept. For classification tasks, the labels for the instances refer to the classes and the labels for the concepts refer to the relations. Therefore, labels in the Walker, walk (DMControl500k) dataset have two parts: the instance part and the concept part. For regression tasks, one can use the feature importance in Gradient Boosting Machine to", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The SciGEN dataset contains benchmark entities and relations annotated with specific metrics. The benchmarks are evaluated based on the models predictions for the SciGEN dataset. output (Sparql query):avascript code (curl) req.text:SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } */ /** * Converts the sparql query into a Map for further processing. * * @param query * @return */ private Map<String, String> queryIntoMap(String query) throws NFEException { return queryMap.put(\"query\", query); } /** * Checks if the evaluation has a metric and if so, returns the link to the metric * with the specified name. If no evaluation has been performed for the specified dataset or the * evaluation has no metric associated with it, null will be returned. * * @param dataset * @param evaluation * @", "benchmark was scored with a Score metric and the model that performed best on this metric was provided is the name and score: cessario (English text): The model that performed best on the benchmark dataset was the model that used the MCTS algorithm from the Databricks MCTSPipeline service. The best score achieved on the dataset was 0.592622. thereafter: The best performing model on the benchmark dataset used the MCTSPipeline MCTS algorithm", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \\\"UCF101 finetuned\\\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }\" }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews, K. Suresh;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews; K. Suresh;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews;\" \"Abhij Mathews and K. Suresh;\" \"Abhij Mathews and K. Sures", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\".) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): BC5CDR-chemical: The highest metric achieved on the BC5CDR-chemical dataset is SMTPDirect. Gain = 2.0 SMTPDirect: the metric and its value are: SMTPDirect: The SMTPDirect metric is an absolute path distance between the atom's center of mass and the closest heavy atom (defined as the set of heavy atoms that are connected via a heavy bond to the atom). The atom and the heavy atoms are represented using the Conquest Coordinates from the MLOCC and a 3rd-", "SELECT * WHERE { ?eval habitat:SUCCESS ?eval_score. FILTER (abs( ?eval_score) > 90) } The query returns: Best match { \"SUCCESS\" \"90.0\" } In the abstract of the dataset, the metric which is used for evaluation is called success. The corresponding SparQL query is: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The results are as follows: england (Habitat): SELECT * WHERE { ?eval habitat:SUCCESS ?eval_score. FILTER (abs( ?eval_score) > 90) } aiding (Habitat): SELECT * WHERE { ?", "SELECT DISTINCT ?metric ?metric_lbl_HIGHEST_RESULT_WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\"^^xsd:string) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Paper Field (Paper Field). The metric is Coverage (20.0000), and the score is 20.0000. output (Sparql query): { \"Coverage\" : \"20.0000\", \"Paper Field\" : \"20.0000\" }", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest benchmark result is achieved with an average precision of 0.7755 and a t-measure of 11.8. The metric value for this result is Hit Precision.", "SELECT * WHERE { ?paper_code text. ?paper_code regexp { \"AKITA\" \"AKITAS\" \"Kaggle\" \"Kaggle Datasets\" \"EMGCNN\" \"EMGCNN\" }Filter (str( ?paper_code) = ?paper_code_lbl). } FILTER (* WHERE { ?paper_code regexp { \"Abstracts' entities and relations annotated corpus\" \"Artificial Intelligence on Language Toolkit\" }Filter (str( ?paper_code) = ?paper_code_lbl). }) } input (English text): Here are links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model: http://www.kaggle.com/c/semantic-video-question-answering-challenge-sveql/data https://arxiv.org/abs/1906.08955 https://github.com/RuleDrivenAI/emdnn#Results_on_Artificial_Language_models With the code presented,", "SELECT DISTINCT has_dataset, has_evaluation, has_metric, metric, metric_lbl FROM <http://dbpedia.org/query/SELECT+DISTINCT+%22has_dataset%22,+has_evaluation%2C+has_metric%2C+metric%2C+metric_lbl%20FROM+<http://data.govtrack.us/dataset/abstracts-entities-and-relations-annotated-corpus-benchmark-dataset-dmcontrol500k> WHERE url ?dataset_lbl=cheetah> <|endoftext|>Amino acid-based peptide metallodrugs: synthesis, crystal structure, absorption, and photodynamic activity. Several amino acid-based Zn(II), Cu(II), and Mn(II) complexes have been synthesized and characterized in solution and in the solid state. The structure of these complexes has been determined by NMR, UV-vis, and single crystal X-ray diffraction. These complexes contain N,N'-chelating \u03b2", "SELECT DISTINCT ?dataset WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\").} input (English text): Here is the dataset I mentioned in the question: https://github.com/eduardoNevesSilva/universidade-oficial-de-sao-rodo-de-bucharest-datasets/tree/master/papers. Midwest Bioinformatics Summer School in Computational Biology (MPSS2011), Poster Session (SS4), August 2011 East Japan Biomass Federation, Social Sciences (SS4), Poster Session (SS4), August 2011 Midwest Bioinformatics Summer School in Computational Biology (MPSS2011), Poster Session (SS4), August 2011 Abstract East Japan Biomass Federation, Social Sciences (SS4), Poster Session (SS4), August 2011 Midwest Bioinformatics Summer School in Computational Biology (MPSS2011), Poster Session (SS4), August 2011 Midwest Bioinformatics Summer", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metric is mentioned in the document as a popular metric for evaluating models on the dataset: ?metric rdfs:label \" score\" ?metric_lbl \" Alien\" <|endoftext|>Q: OpenGL - Getting GLUvertex4FArray elements out of a GLUvertexPointer I have been having trouble understanding how to access the elements of a GLUvertexPointer, the way I've seen it done is through the indices: void MyClass::InitGL() { glEnableClientState(GL_VERTEX_ARRAY); glEnableClientState(GL_NORMAL_ARRAY); glEnableClientState(GL_COLOR", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Birdsnap\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>If you were hoping that AMD\u2019s Zen 2 \u201cProlimatech\u201d processing architecture would deliver some massive performance improvements over its predecessor, your prayers have been answered. AMD says it has improved its 90-nanometer FinFET processing technology, giving its processors \u201ca further 5-10% performance increase.\u201d This marks the first time AMD has disclosed actual performance gains from an architectural update since it launched Bulldozer in 2010. The new Zen 2 architecture does have one significant weakness: its current release dates for consumer parts lag significantly behind its desktop architecture\u2019s commercial launch. The Zen 2 architecture consists of seven core \u201cWavefront\u201d cores, a multi-threaded execution unit, support for DDR4 DRAM,", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"swamijit/chopper_command\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What metrics are being evaluated on the swamijit/chopper_command benchmark model ? output (Sparql query): wsparser (English text): The metrics being evaluated on the swamijit/chopper_command benchmark model are <a href=\"https://github.com/swamijit/chopper_command/blob/master", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The best model, according to the author of the article, is the one that achieved the best accuracy on the \"Abstracts' entities and relations annotated corpus\" benchmark dataset. The best model uses an SVM to classify whether the abstract of a car contains the words \"mercedesbenz\", \"audi\", \"porsche\", or \"volkswagen\". output (Spar", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The answer is the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset. It has the following metrics: ?dataset a orkgc:Dataset; rdfs:label \"RotoWire (Relation Generation)\"; orkgp:HAS_DATASET RotoWire (Relation Generation) ?eval orkgp:HAS_EVALUATION RotoWire (Relation Generation) ?metric RotoWire (Relation Generation) ?metric_lbl RotoWire (Relation Generation)", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:", "SELECT DISTINCT ?metric ?metric_lbl WHERE {", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval ?metric. } } metric metric_lbl - - accuracy accuracy f1 score f1 score precision precision recall recall hit rate hit rate precision @ k = 10 precision @ k = 10 recall @ k = 10 recall @ k = 10 precision @ 3 = recall @ 3 precision @ 3 = recall @ 3 precision @ 7 = recall @ 7 precision @ 7 = recall @ 7 precision @ 10 = recall @ 10 precision @ 10 = recall @ 10 The following are some metrics used in the Abstracts' entities and relations annotated corpus benchmark dataset: accuracy, f1 score, precision", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: Score is a metric term used in the benchmarks. https://github.com/opencog/yars-revenge/blob/master/benchmarks/Benchmarks.md#metrics shows the most relevant terms and their meanings. <|endoftext|>Yemen: War Crimes Trial", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Disease genes mutations proteins interactions\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } answers (Sparql query): F1 score and Precision-Recall curves facing (Sparql query): F1 score and Precision-Recall curves output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset", "SELECT ?metric ?metric_lbl (where { ?s softcite. ?sc[salary > 10000000] }) WHERE { ?sc doc { ?s softcite. ?sc[salary > 10000000] } } } input (English text): The highest result is for the Softcite dataset, with the highest score being 0.8022424, and the corresponding metric being \"num sentences\". output (Sparql query): Rapids (metric): num sentences output (Sparql query):idemargin (English text): num sentences <|endoftext|>This website uses cookies to improve user experience, to provide analytical data to better serve our visitors, and to satisfy online advertising. By using this website you consent to our use of cookies. This website do not set cookies to gather personal data about you, only used to improve user experience. You can change cookie settings in your web browser. If you continue to use this website without changing cookies settings or you click OK, you are consenting to our use of cookies. For more information see Read Consent Agreement. This website uses cookies", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics of evaluation over the Fashion-MNIST dataset are: accuracy, precision, recall, f-measure, and mean absolute error. output (Sparql query):ICKP :SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Which metric ROUGE-2 uses to benchmark the X-Sum dataset ? ROUGE-2 Greetings Your answer is the following :- SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS", "SELECT ?title (STRDAFFormat( ?,\u201c%s\u201d)) WHERE { ?dataset a orkgp:HAS_DATASET ?dataset. FILTER (str( ?dataset) = \"RotoWire (Relation Generation) dataset\") } input (English text): Example query, to get a list of research papers that have performed benchmarks on the RotoWire (Relation Generation) dataset: SELECT ?title (STRDAFFormat( ?,\u201c%s\u201d)) WHERE { ?dataset a orkgp:HAS_DATASET ?dataset. FILTER (str( ?dataset) = \u201cRotoWire (Relation Generation) dataset\u201d) }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } EIGENVALUES ?metric_lbl { \"Accuracy\" \"AUC\" \"Precision\" \"Recall\" \"F1 Score\" \"Positive Predictive Value\" \"Negative Predictive Value\" \"Mean Average Precision\" \"Linear Scan\" \"Logical Scan\" \"Memory Usage\" \"Time\" } <|endoftext|>The Spirit of '76 is back The Spirit of '76 started in 1964 as a club open to all men who agreed to abstain from alcohol, drugs and pre-marital sex. The club\u2019s basic philosophy was to encourage communication, intellectual exchange and camaraderie among members. The", "SELECT DISTINCT ?metric ?metric_lbl, the Average Return (NoOp) metric is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric. Average Return (NoOp) metric returns the average reward of a policy for each time step. Reward for a given state, action, and time step is calculated as the length of the sequence of symbols following the last. Thus, the Average Return (NoOp) metric for a model is the average reward of the policy it generates for the Abstracts' entities and relations annotated corpus dataset. ## Opinions, feedback, and other references ### Model choice", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Italian version: Provocazione Normativa per la Semantic Web (Prior noop): https://github.com/limharder/PNS-related Metodo per valutare la Recuperabilit\ufffd del Metodo Prior noop: https://github.com/limharder/PNS-related/blob/master/docs/METODE%20PER%20VALUTARE%20LA%20RECUPARIBILITUD%20DEL%20METodo%20Prior%20noop.ip", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT English-Russian\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The best performing model has BLEU score of 24.3", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Visions of Ecstasy: The Films of Jacques Tourneur, Part 3 Here is another installment of my ongoing series of reviews of Jacques Tourneur's films, and a follow-up to the last two installments. The third installment of Visions of Ecstasy is not necessarily a \"best of\" compilation. Each film is presented in chronological order, and thus, there is a logical build-up from the first film released in 1959 to the final film released in 1962. However, Tourneur's best film does include an implied (if oblique) thematic connection between some of the films.", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuter's 21578 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?benchmark rdfs:label ?benchmark_label. } } input (English text): The metrics used in the benchmarks listed below are: Title ID Count Tf-idf Top-k cosine similarity Co-occurrence similarity AuctionPrice 1 Reuter's 21578 Dataset 161702 3049 0.8 - AuctionPrice + Aggregation 2 Scalable Query Processing 1 300000 1.0 AuctionPrice + Aggregation + TF-IDF 3 Reuter's 21578 Dataset 161702 3049 0.8 - Co-occurrence similarity 4 Co-occurrence Similarity 1 300000 1.0 Co-", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Characters in the Penn Treebank (Character Level)\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Evaluation metrics for the Penn Treebank (Character Level) dataset include F-score, precision, recall, and the area under the ROC curve ? output (Sparql query):\"}**).** Precision, Recall, and F-score are common metrics that are used for evaluation of character level Named Entity Recognizers. The area under the ROC curve (AUROC) is a metric for evaluating the performance of machines learning models on a binary classification task and is often used as the primary metric in research. Chapel Hill Recognizer was used as a baseline system in this research Chapel Hill Recognizer", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } There is only one metric available: Metric: abstracts' entities and relations annotated corpus Label: Abstracts' entities and relations annotated corpus A snippet that can be used to search the dataset is given below: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Co-occurrences in the DY-MobileNetV3-Small model\u2019s evaluation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Below is the list of papers that utilized the DY-MobileNetV3-Small model and include the links to their code: https://www.slideshare.net/DorianDouglas12/an-overview-of-racial-bias-in-computer-vision-deep-learning-and-its-implications-for-science-and-society-skpe13209840 heading (English text): Provide", "SELECT ?metric ?metric_lbl, ?metric_label, ?metric_value where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } { ?eval orkgp:HAS_METRIC ?metric_label. ?metric_value ?metric_label.} WHERE { ?dataset a orkgc:Dataset; rdfs", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Riot Games will announce on May 6th at 4PM PDT, the next step in the progression and growth of its esports program. We will discuss the results of the program with members of the leadership team and share a plan for the future of Riot's esports program. We hope to hear your ideas and feedback on how to improve Riot's esports program. The Riot Games Esports Program was first established in April 2014, which resulted in the first domestic league championship for League of Legends in Europe in July 2015", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model a orkgp:BenchmarkedModel; ?model_lbl a :ValueReference. FILTER (str( ?model_lbl) = \"DL-Matcher\") } input (English text): The BenchmarkedModels dataset is available at http://www.openlibrary.org/blog/2007/10/09/DL-Matcher-is-here-a-dataset-of-Benchmarked-DL-Models The BenchmarkedModels dataset contains BenchmarkedDLModels records, each of which represents a model. Each model has a name, a description, and several performance metrics. The benchmarked model is also annotated with the textual description of the evaluation and a benchmark score. There is a reference to the textual description for each metric. specim DL-Matcher (Sparql query): SELECT ?model ?model_lbl WHERE { ?model a orkgp:BenchmarkedModel; ?model_lbl a :ValueReference. FILTER (str( ?model_lbl) = \"DL-Matcher\") } individule :model DL-Matcher - name - description -", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest benchmark result for the BUCC German-to-English dataset has the following result: { \"metric\": \"BLEU\", \"metric_lbl\": \"BLEU\", \"dataset\": \"German-to-English\", \"eval\": { \"metric\": { \"label\": \"BLEU\" } } } yards:BLAST++ metric yards:BLEU metric yards:0.94 yards:ORC metric yards:ORC metric yards:94 yards:tm rank yards:2 10 yards:29 5", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DMControl100k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics that are used to evaluate models on the DMControl100k dataset are: - logloss (rdflib:logloss) - roc_auc (rocs:auc) - acc_train (orkgp:accuracy) The metric that has achieved the highest Score is the logloss metric with value 0.258529. ## References https://www.kaggle.com/c/ball-in-cup-catch-predict-DMControl100k https://www.kaggle.com/c/ball-in-cup-catch-predict https://www.kaggle.com", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Table 2 shows all the metrics used in Abstracts' entities and relations annotated corpus benchmark dataset. Table 3 shows the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark. Table 4 shows the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark. Table 2:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Bits per dim\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Bits per dim is the best performing metric <|endoftext|>Kansas City, Missouri - Kansas City Public Library, Moorpark Library A dangerous and stupid game is just an opportunity to fail and learn a lesson. The Sporthid Player: Massacre at Marathon Hannibal, (Ferenc) - Parzival - Radl (Szanto) Sporthid is the best game ever made. It is an abstract strategy game of an ancient battle between two factions (The Romans and the Barbarians). As the game begins a group of soldiers arrive with the mysterious (to the Barbarians)", "SELECT json FROM Table_name WHERE createdAt = $createdAt\"; $queryParams = [NSString stringWithFormat:@\"%@\", $createdAt]; $cursor = $picocli['database']->query($query, $queryParams); $json = []; while ($row = $cursor->fetch()) { $json[] = $row; } echo json_encode($json); But I am not getting any data in my iOS app. Please help. A: Looks like there was an", "SELECT DISTINCT \"); sb.append( metrics.get(\"rdfs:label\") ); sb.append(\" \"); sb.append(metrics.get(\"has_dataset\") ); sb.append(\" \"); sb.append( metrics.get(\"has_evaluation\") ); sb.append(\" \"); sb.append(metrics.get(\"has_metric\") ); sb.append(\" \"); sb.append( metrics.get(\"rdfs:label\") ); return sb.toString(); EIGEN_SVDDecomposition e.Cosine SVD = e; SVDDecomposition svd", "SELECT ?benchmark (max ?metric) where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The highest benchmark result achieved on the CoQA dataset is 500, including the metric and its value of 36.8.", "SELECT * WHERE { ?metric ?metric_lbl. FILTER (str( ?metric) = \"top-5\") } output (Sparql query):<metric> Top-5 accuracy (ImageNet-v2) output (Sparql query):<metric_lbl> top-5 accuracy (ImageNet-v2) output (Sparql query):<metric_lbl> Top-5 accuracy output (Sparql query):accuracy Top-5 accuracy", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The reacher is a meta learning model which achieves an overall score on any benchmark dataset. For the Reacher benchmark dataset, it is reported to achieve an average overall score of 0.993. For the Reacher benchmark dataset, the DMControl500k metric is used as the evaluation metric. The Reacher benchmark dataset contains named-graph-features so the DMControl500k metric is computed on the named graph. Based on the above query, the following two metrics are returned: - DMControl500k: The metric for Reacher (DMControl500k) - score: The overall score of Reacher (0.993) SELECT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following are some of the metrics used to evaluate models on the Quasart-T benchmark dataset: - precision (also called recall) - recall - $f_1$ score - $F_1$ score - $F_1$ - recall@1 - $f_1$@1 - precision - recall - $f_1$ score - $F_1$ score - $F_1$ - recall@1 - $f_1$@1 A: You can use your experience with the SPARQL query language to find this information in other ways,", "SELECT ?model_id ?metric ?metric_lbl WHERE { <http://orcid.org/n Lan <<model_id>> > <http://orcid.org/n Lan <<metric>> > <http://orcid.org/n Lan <<metric_lbl>> > } Undecided A model is evaluated on the Multimodal PISA dataset if it is available in the Multimodal PISA dataset. The following are the models that have been evaluated on the Multimodal PISA dataset: * Logistic regression * Random Forest * Linear regression * Multinomial logistic regression The following are the metrics that are used to evaluate models on the Multimodal PISA dataset: * R2: The coefficient of determination of a linear model * Q2: Coefficient of concordance, a measure of the Concordance of two binary predictions when a proportion of subjects are positive and negative * Gain: Measure of the accuracy improvement by including a model-specific predictor into a linear model The following are the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: * R2", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics used to evaluate models on the Barabasi-Albert dataset are: - Node degree - EWeight - Cluster prominence - k-shell coefficient questionna:Node degree, Node degree, k-shell coefficient", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The following are the metrics used to evaluate models on the Atari 2600 Zaxxon dataset: Maximum accuracy, Recall, precision, F-measure, Root Mean Square Error, Harmonic Mean, Cross Entropy, log loss, Sigmoid Cross Entropy, area under the receiver operating characteristic (ROC) curve. output (Sparql query): {\u00b6metric_lbl_1\u0082Maxima\u0090 Accuracy\u0082Recall\u0090 Precision\u0090 F-measure\u0090 Root Mean Square Error (RMSE)\u0090 Harmonic Mean (HM)\u0090 Cross Entropy\u0090 Log Loss (LL)\u0090 Sigmoid", "SELECT ?metric ?metric_lbl ?metric_value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") OR ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. ?eval orkgp:HAS_METRIC_VALUE ?metric_value. } } <|endoftext|>Q: How to avoid fseek in this code to find position of a line in a file ? How to avoid fseek in the following code to find position of a line in a file ? int main() { FILE *fp; int size,pos=0; char str[50]; printf(\"Enter the file name: \"); scanf(\"%s", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"English-German benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The BLEU score metric that performed the best was BLEU-2. This means that the model achieved the highest blem score for queries where the target language is German and the source language is English.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sentence Classification related datasets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics that are used to evaluate models on the Sentence Classification research area benchmarks datasets:accuracy, precision, recall, f1 score, and micro-averaged f1 score. output (Sparql query): jazz musicians:analyze( ?benchmark, \"sentences\", ?eval, ?metric, ?metric_lbl, ?metric). jazz musicians:analyze( ?dataset, \"sentences\", ?eval, ?metric, ?metric_lbl, ?metric). jazz musicians:analyze( ?benchmark, \"sentence classification related datasets\", ?eval, ?metric, ?metric_lbl, ?metric", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } -metric /content-ordering/similarity <|endoftext|>Deficiency of insulin-like growth factor-1 receptor in Wilms' tumor. Wilms' tumor (WT) is a primary pediatric tumor of the kidney and is the most common tumor of the genitourinary system in children. Its pathogenesis remains obscure; however, some evidence suggests that the presence of a mutated WT1 gene contributes to its development. We present a case of WT with IGF-1R signaling pathway defects. The patient's serum contained low levels of the IGF-1R ligand, whereas the IGF-1R protein was absent in the tumor", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics commonly used to evaluate machine learning models on the MLDoc Zero-Shot English-to-Italian dataset include: f1-score, precision, recall, and [ Matthews correlation coefficient ]. output (Sparql query):olved SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EV", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } is used to evaluate the models on the Abstracts' entities and relations annotated corpus benchmark dataset. The following metrics are used in evaluating the models: 1. DisMax Score - http://sparql.w3.org/2013/#semantic- entailment with constraint rule mining 2. AvoDelta Score - https://github.com/w3c-wai-", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?model ogr:ExportFeatureLayer \"...\";orkgp:HAS_EVALUATION ?eval. ?dataset a orkgc:Dataset; ?benchmark ogr:BenchmarkComparisonResult; OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset; ?dataset a orkgc:Dataset; ?eval. ?benchmark ogr:BenchmarkComparisonResult; ?dataset_lbl ogr:BenchmarkLabel. ?eval orkgp:HAS_METRIC ?metric; ?metric rdfs:label ?metric_lbl;} } input (English text): The MedSTS benchmark dataset contains a results for different models in comparison. Here is an example of a metric name along with a label: \"Aua.\" The benchmark label is what is annotated in the MedSTS dataset. output (Sparql query):tingham;SELECT ?metric ?metric_lbl WHERE { ?benchmark ogr:BenchmarkComparisonResult; ?dataset a orkgc:Dataset; OPTIONAL { ?dataset_lbl ogr:BenchmarkLabel. ?eval or", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Best performing model benchmarking the CoNLL++ dataset in terms of F1 metric ? inquired bestModel SELECT (avg(f1) as ?avg_f1) ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs", "SELECT DISTINCT ?metric ?metric_lbl FROM <your query above> <|endoftext|>The present invention relates to an electromagnetic valve having a coil supported at a housing by means of a snap-fit connection and an armature slidably movable in the housing in a substantially axial direction relative to the", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\"):[*] ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): There are a few datasets in SciCite corpus that are linked to their corresponding RDF datasets and are called'sciconfs'. Each sciconf stores metadata about one or more SciCite RDF datasets. One or more RDF datasets can be linked to a single or to multiple sciconfs. The following are the datasets that are linked to their corresponding RDF datasets: Abstracts' entities and relations annotated corpus, STIntersection and STUnion.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet + iNat on WS-DAN model\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } No metric names are provided in the benchmark dataset used in the paper. Please provide links to the code used in the paper that benchmarks the ImageNet + iNat on WS-DAN model ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a r2e:Russian-to-English; r2e:MRFModel ?model; r2e:MRFConstraintItem ?c1; r2e:MRFConstraintItem ?c2; r2e:MRFConstraintItem2 ?c3; ?c1-c3:MRFConstraintItemRef ?constraint; ?dataset_lbl r2e:MRFEvaluation; r2e:MRFConstraintItem ?metric; ?metric rdfs:label ?metric_lbl. } <|endoftext|>/** * Copyright 2009-2015 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>#import \"GPUImageFilter.h\" @interface GPUImageColorFilter : GPUImageFilter { GLint colorMatrixRed", "SELECT 1 FROM dual UNION SELECT 2 FROM dual UNION SELECT 3 FROM dual The following code will run each SELECT statement simultaneously: SELECT 1 FROM dual UNION NOLUNCTION SELECT 2 FROM dual UNION NOLUNCTION SELECT 3 FROM dual Is there a similar abstraction layer in Oracle Database ? A: No. There", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } restrictive (Sparql query):OVID_label _toolname metric_label metric_label_long restrictive (Sparql query): chunker_bleu_chinese_det", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Reuters-21578 benchmark dataset contains performance metrics for text categorization, named-entity recognition and relation extraction. The metrics cover precision, recall, F-score, precision at cost 1 (P@1), recall at cost 1 (R@1), and their harmonic means. All metric names start with \"precision\", \"recall\", \"F-score\", \"P@1\", \"R@1\", and \"harmonic mean\". Note that \"precision\", \"recall\", \"F-score\", \"P@1\", and \"R@1\" are also used in another Open Standard benchmark RelMetadataRel. Therefore, \"precision\", \"recall\", \"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Seaquest dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Since Seaquest dataset is best evaluated on the Abstracts' entities and relations annotated corpus benchmark, the best performing model benchmarking Seaquest dataset in terms of Score metric is the DART model. But the DART model benchmark has no evaluation metric. The score metric is used to evaluate the performance of a model and the DART model performs best in this benchmark with an average score of 0.9245. Since there is no evaluation metric for the best performing model benchmarking Seaquest dataset, we can't determine which model performs best. In conclusion, we could not determine which model performs best on the Abstracts' entities and relations annotated corpus benchmark dataset. The abstract", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The following metrics are part of the evaluated models for the Abstracts' entities and relations annotated corpus dataset benchmark: -Precision -recall -F1score -MSE Note that the models may use multiple metrics to compute their F1-score output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dat", "SELECT DISTINCT ?metric_lbl ?metric, ?dataset_lbl, ?benchmark, ?eval FROM <data> WHERE { <data> ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet' dataset\") } input (English text): Here is the list of research papers that have performed benchmarks on the ObjectNet dataset: #title, ID Databricks on ObjectNet, Databricks, rdfs:label \"ObjectNet\", rdfs:label \"Databricks on ObjectNet\" Unstructured JSON comparison, Berner, rdfs:label \"JSON diff\", rdfs:label \"Unstructured JSON comparison\" Entity normalisation for JSON-LD, Badolato, rdfs:label \"JSON-LD normalization\", rdfs:label \"Entity normalisation for JSON-LD\" Auto-encoding semantic relations with Neural Relation Machines, Huang, rdfs:label \"Neural Relation Machines\", rdfs:label \"Auto-encoding semantic relations with Neural Relation Machines\" Evaluating the quality of wikis using EntityLinking, Botch", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics used to evaluate models on the Atari 2600 HERO benchmark dataset are: - hp (Human Performance) - timing (timed game play) - loop (Number of successful turns) - loses (Number of times a game ends early) - watts (Watts consumed while playing the game) - acc (Average % Accuracy) - turn_coords (Average % of target location in turn) - score (Total Score) - moves (Total number of moves) - wall_clocks (Average Wall Clock time) - energy (Total Energy consumed) - frames (Total number of", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Example output: The following metrics are used to evaluate models on the Amazon-2 dataset: ROC_AUC, ROC_AUCLP, ROC_CLEAR, AUPR, MCC, PRCC, ACC, AP, MRR SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } </query> </query> <response> <treetop> outputTree { \"SELECT DISTINCT ?metric ?metric_lbl\" \"WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset;", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"RACE\", \"High\", \"Accuracy\", \"88.465625\"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The following research paper titles have benchmarked models on the WMT2016 English-Romanian dataset: - Lu\u00e7on, L., Dau, T., Bottla, M., & Bilbe, C. (2016). Learning to Verbally Compose Grammatical Greetings by Recurrent Neural Networks. https://arxiv.org/abs/1609.03732. - Bilbe, C., Dau, T., Bottla, M., Lu\u00e7on, L., & Grisel, A. (2016). Greetings4\u0440\u0430\u0431\u043e\u0442\u043d\u0438\u0446\u0430: \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u0444\u043e\u0440\u043c \u0448\u0443\u0442\u043e\u043a \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u0447\u044c", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): MUTAG dataset metrics - https://github.com/onti/mutedataset-benchmarks/blob/master/mutedataset-evaluation/eval/mutedataset_evaluation_api.cpp#L15 The metrics used for evaluation in MUTAG are MUTAG_BLEU, MUTAG_CIDEr, MUTAG_COSINE, MUTAG_METEOR, MUTAG_PRC, MUTAG_ROUTE, MUTAG_SPD, MUTAG_WAVITAB, and MUTAG_WOR", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Un articolo pubblicato sulla rivista \u201cThe Astrophysical Journal\u201d descrive un team che ha combinato osservazioni grazie a telescopi Spitzer, Hubble, e Chandra con la ricostruzione delle molecole chimiche nei corpi stellari al cervello superiore di 101 personaggi. Qualcuno potrebbe pensare che sia una stella cos\u00ec", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The top 1 most relevant model with the highest F1 score on the SQuAD1.1 dev benchmark dataset is the SMOTE DNN model.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): TempEval-3 is being evaluated on models that are listed below: output (Sparql query):SELECT ?metric ?metric_lbl WHERE { { ?dataset a orkgc:Dataset; rdfs:label \"TempEval-3 dataset\". FILTER (str( \"TempEval-3 dataset\") = \"TempEval-3 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } : SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } : SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } : SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:HAS_EVALUATION ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } : SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a or", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Seaquest Score\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Seaquest Score is the top benchmark score, with a score of 44.91. This score is scored using the per-example F1 score. The F1 score is computed as ( TP * 100 / ( TP + FP ) ), where TP and FP are the number of true positives and false positives, respectively. output (Sparql query):\u0430\u0442\u0435\u043bperkF1:44.91 Top metric is Per-Example F1. input (English text): Per-Example F1 is computed as ( TP * 100 / ( TP + FP ) ), where TP and FP are the number of true positives and false positives, respectively. The F1 score is a harmonic mean", "SELECT DISTINCT ?query WHERE { ?query ?metric. ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric) = \"Abstracts' entities and relations annotated corpus\") } input (English text): None output (Sparql query):SELECT DISTINCT ?query WHERE { ?query ?metric. ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric) = \"Abstracts' entities and relations annotated corpus\") } input (English text): SELECT DISTINCT ?query WHERE { ?query ?metric. ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric) = \"Abstracts' entities and relations annotated corpus\") } output (Sparql query):SELECT DISTINCT ?query WHERE { ?query ?metric. ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric) = \"Abstracts' entities and relations annotated corpus\") } input (English text): SELECT DISTINCT ?query WHERE { ?query ?metric.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Public datasets benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): How does the Yelp Dataset differ from the benchmark datasets included in Abstracts' entities and relations annotated corpus dataset ? output (Sparql query):There are two main differences: (1) the benchmark datasets benchmarks only specific types of systems, like link classification, named entity recognition, substring matching, and so on; (2) the Yelp Dataset targets search queries and provides rankings of relevance for each query, which is different from the benchmarks that evaluate systems on standardized datasets, like the Abstracts' entities and relations annotated corpus dataset. ### References - [Databricks Eval](https://www.dat", "SELECT clause contains a link to the page with the model evaluation results on the Amazon Benchmark Dataset. Note: The best model in terms of accuracy was the model that achieved the highest performance on the Amazon Benchmark Dataset with an accuracy of 91.43%.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"Average sequence length\", \"Average number of interactions per command\", \"Ratio of commands with at least three commands\", \"Ratio of commands with one or less than two outputs\", \"Mean timelimit per command\", \"Total timelimit of all commands\", \"Timelimit for the highest command\", \"Successful commands\", \"Failed commands\", \"Ratio of failed commands\", \"Ratio of commands with at least three errors\", \"Ratio of commands with one or less than two errors\", \"Mean number of", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } and", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval_2. ?eval_2 orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } EIGEN_TCP_SERVER_ADDR: 10.244.0.2 EIGEN_TCP_SERVER_PORT: 54322 EIGEN_TCP_SERVER_PROTO: TLS EIGEN_TCP_SERVER_TLS_CERT: client.crt EIGEN_TCP_SERVER_TLS_KEY: client.key EIGEN_TCP_SERVER_TLS_CACERT: client.cacert EIGEN_TCP_SERVER_TLS_CRL: client.crl EIGEN_TCP_CLIENT_ADDR: 10.244.0.2", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset on the Atari 2600 Asterix are: Alshiri, Tal, Juni, Oz, 2017. \"Evaluating Atari 2600 Asterix Game-level Deep Learning Models for In-Session Recommendation with A/B Testing\". AIP Advances. 34(2): Article No. 106388. Alshiri, Tal, Oz, Juni, 2017. \"A Benchmarking Study on Evaluating Deep Learning Models on Atari 2600 Asterix Games\". AIP Advances. 34(4): Article No. 10", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Here are the code snippets used in the papers which benchmark the MFEC model: https://github.com/rashmiagarwal/mfeat_resnet50_challenge_paper.git, https://github.com/rashmiagarwal/mfeat_resnext101_challenge_paper.git, https://github.com/rashmiagarwal/mfeat_caffe.git, https://github.com/rashmiagarwal/cnn_resnet50_", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } code link :https://github.com/cloudsightio/cait-xs-36-model#model-benchmarks input (English text): As provided in the link provided, we can see that the model benchmarks are evaluated using metrics such as Mean Average Precision (MAP) at sentence level and Mean Average Reciprocal Rank (MAR) at word level. output (Sparql query): hitchiker has helped me find this link to code used in papers that benchmark the CAIT-XS-36 model. code link :https://github.com/cloudsightio/cait-xs-36-model#", "SELECT DISTINCT ?metric ?metric_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): f1 score is used to evaluate the models on the seel.cse.lsu.edu/data/re17.zip dataset. ennelov@cse.lsu.edu (ELovic) generated Tue, 25 Sep 2019 14:17:38 +0000 from Sparql Query SELECT DISTINCT ?metric ?metric_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Montezumas's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } </query> <variables> <variable name=\"metric_lbl\"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval parsemtr. } } The best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric is the model proposed in", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The following metric are used in the Abstracts' entities and relations annotated corpus benchmark dataset: accuracy: accuracy of the model on the dataset, the larger the better, 0.5 is half correct, 1 is 100% accurate. computed_f1: computed_f1 metric, computed as (accuracy * 2) / (2 * (1 - accuracy)). A value of 1.0 would be computed_f1 = 2 * (1 - accuracy) mems: the number of missed matches, 0.0 is no missed matches, 1.0 is all missed matches. There", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The top result includes the F1 score of 0.93. output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query):F", "SELECT DISTINCT ?model_id ?metric_id ?metric_label WHERE { ?model_id rdf:first ?model. { ?model_id owl:sameAs ?model. { ?model_id owl:sameAs ?model. { ?model_id sparql:name \"Walker, walk\"@en. { ?model_id sparql:page \"https://data.w3.org/vision/dwgi-datasets/walk\"@en. { ?model_id sparql:resource \"http://dmcontrol.com/models/deepwalk-rml/documentation/Walker%20walk.html\"@en. { ?model_id sparql:description \"An Entity Recommender based on the Google deepwalk and repeater_functionality for relationalmachine-learning tasks.\"@en. { ?model_id sparql:language \"en\"@en. { ?model_id sparql:license \"Public Domain\"@en.", "SELECT ?metric ?metric_lbl WHERE { ?eval whatever OR ?eval hasDataset datastore:http://dbpedia.org/data/REDDIT-B. ?eval hasEvaluation whatever. ?eval whatever rdfs:label \"DBpedia Abstracts' entities and relations annotated corpus Benchmark.\". ?eval hasMetric dbpedia:totalVotes ?metric. ?eval whatever rdfs:label \"DBpedia Total Votes.\". ?eval whatever rdfs:label \"3093.\". }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Model of the Benchmark Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Yes, that is correct. The metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset are: ?metric\t ?metric_lbl\t nsqa:precision\tAbstracts' entities and relations annotated corpus benchmark dataset nsqa:recall\t ?benchmark\t nsqa:f1-measure\t ?benchmark_lbl\t Some examples of queries that retrieve information about a specific metric from the benchmark dataset are: ?metric\t ?benchmark_lbl\t nsqa:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The following are some of the metrics that have been used for model evaluation on SST-2: accuracy, macro F1, micro F1, support vector machine (SVM), logistic regression. output (Sparql query): Viktor: accuracy, macro F1, micro F1, support vector machine (SVM), logistic regression. Viktor is currently not able to provide a list of the models that have been tested on SST-2. <|endoftext|> Fourth Court of Appeals San Antonio, Texas MEMORANDUM OPINION No. 04-19-00953-CV", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hendrycks Test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Hendrycks Test dataset has a score of 48 and the metric it ranks first for is mean average precision. output (Sparql query):marined48 mean average precision input (English text): Mean average precision is the metric that ranks first for Hendrycks Test dataset ? output (Sparql query):marined48", "SELECT ?metric ?metric_lbl ?metric_link WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Popular White Paper On This Topic: + + Many home security companies offer digital video recording for a small monthly fee, such as DVR, but this service often lacks both video quality and the recording capacity needed to record full HD 1080p videos. However, there are a few options to consider, and some offer better quality and capacity than others. + + + BuzzFeed has a great guide to reviewing home security cameras. The overall verdict will depend on how you use the camera, whether it's for business or personal use, whether you need night vision, and how much", "SELECT DISTINCT ?title ?id WHERE { ?article article:hasVersion ?version; ?article article:hasSource ?source; ?article article:hasSourceLanguage ?sourceLang; ?article article:hasSourceEncoding ?sourceEncoding; ?article article:hasSourcePublished ?yearPublished; ?article article:hasSourceKind ?sourceKind; ?article article:hasSourceType ?sourceType; ?article article:hasSourceDimensions ?dimensions; ?article article:hasSourceDataPointLabels ?dataPointLabels; ?article article:hasSourceRelationLabels ?relationLabels; ?article article:hasSourceRelationTypes ?relationTypes; ?article article:hasSourceRelationValues ?relationValues; ?article article:hasSourceRelationExamples ?relationExamples; ?article article:hasLanguage { ?source lang ?sourceLang; ?sourceEncoding ?sourceEncoding; ?yearPublished ?yearPublished; ?sourceKind ?sourceKind; ?sourceType ?sourceType; ?dimensions dimensions; ?dataPointLabels dataPointLabels; ?relationLabels relationLabels;", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Query 2: SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Query 3: SELECT ?metric ?metric_lbl WHERE { ?dataset a orkg", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Human-annotated annotations on the DocRED (Relation Annotated) dataset: https://nlp.semanticweb.org/projects/dswe/ DocRED (Relation Annotated) dataset. output (Sparql query):", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } @searchCashflows { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Washington (CNN) Ohio Republican Attorney General Mike DeWine announced Wednesday he", "SELECT DISTINCT ?metric ?metric_lbl | FROM |SELECT * FROM OPENKM HASH TABLE | |hood|hood|hood|orkgp|orkgp|orkgp|orkgp|HAS_DATASET|HAS_EVALUATION|HAS_METRIC|HAS_LABEL|HAS_LABEL|HAS_DATASET|HAS_EVALUATION|HAS_METRIC|HAS_LABEL|HAS_LABEL| |dbtable|index|table|index|table|index|table|", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }\" results = {} for entry in components.items(): if not \"Abstracts' entities and relations annotated corpus\" in entry:", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orngl:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI dataset\") ?benchmark orngl:HAS_DATASET ?dataset; orngl:HAS_EVALUATION ?eval. OPTIONAL { ?eval orngl:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): QNLI is another Named Entity Recognition dataset. Metrics used to evaluate models on this dataset can be found here: https://papers.nips.cc/paper/5782-unifying-loss-based- ranking-and-ensembles.pdf. output (Sparql query):ubernetes (English text): rdfs:label \"QNLI dataset\" output (Sparql query):ortedoc (English text): QNLI dataset output (Sparql query)::rdfs:label \"QNLI dataset\" output (Sparql query): orngl:HAS_EV", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ?metric rdfs:label \"Entity F1 score\"@en. The Entity F1 score measure the performance of a machine learning model on the task of identifying which entities (or relations) are present in an input dataset. The highest reported Entity F1 score on the SciERC benchmark dataset is 0.903427. The Multi-OAC method, implemented in the open-source package MoniKit, achieved this", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): SST-5 Fine-grained classification evaluation metrics are: Top-1, Top-5, Coverage, Precision, Recall, F1. <|endoftext|>The world is dying. That is an undeniable fact. The elderly are dying disproportionately faster than any other age group, even the young. The media portrays it as an insoluble demographic phenomenon: old people are living longer and therefore have more time to die. But, perhaps more accurately, the world is ageing. As populations grow old, there is a corresponding increase in the population of", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\")} output (Sparql query):Natural Questions (long),precision: 0.99, recall: 1.0, F1-score: 0.99 metric metric_lbl precision Natural Questions (long) recall 1.0 F1-score 0.99 Highest value for the \"Natural Questions (long)\" dataset for the metric \"precision\": 0.99 SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\")} output (Sparql query):", "SELECT ?metric ?metric_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark } input (English text): The corresponding Sparql query is SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric.", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sector Modeling\")} <|endoftext|>1. Field of the Invention This invention relates to a device for electromechanical locking of an accelerator pedal of a motor vehicle, with an accelerator pedal unit and an accelerator pedal brake, with an energy-storing inertial element which can be stressed when a force is applied to the accelerator pedal unit, and with a tension spring device which exerts an elastic force on the accelerator pedal brake in the locking position, the tension spring device having a first end which is fixed to the accelerator pedal unit and a second end which is fixed to the accelerator pedal brake. 2. The Prior Art A device of this type is known from DE 10 2007 033 627 A1, for example. In that case, the tension spring device is embodied in the form of a tension spring, the first end of which is fixed to the accelerator pedal unit and the second end of which is fixed to the accelerator pedal brake. The first end is loaded via a corresponding mechanical pretensioning mechanism, for example, against a mechanical", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALU", "SELECT ?paper_id ( ?paper_id uri:href) WHERE { ?paper a orkgc:Paper; ?paper_id ght:id. ?paper_id ?paper. } input (English text): For papers that have performed benchmarks on the Rotowire (Content Selection) dataset, the papers and the metrics they employed are as follows: - Paper 1 - Metric 1 - Metric 2 - Paper 2 - Metric 1 - Metric 2 \ufeff \ufeff\ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff \ufeff", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ?metric name precision recall f1 count Indonesian 13.3372% 4.3158% 0.75 1459 German 13.9274% 5.6524% 0.75 722 French 14.3333% 6.4167% 0.80 138 French positive 14.3333% 6.4167% 0.80 138 Italian 14.4523% 6.4167% 0.80 138 Japanese 15.4167% 6.4167% 0.80 138 Korean 14.4167% 6.4167% 0.80 138 Spanish 15.6167% 6.4167% 0.80 138 Chinese 14.8333% 6.4167%", "SELECT DISTINCT ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30 dataset\") ?benchmark. } input (English text): SELECT DISTINCT ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30 dataset\") ?benchmark. } This query should only return the name of the metrics that are used to evaluate models on the Dmlab-30 dataset and the names of the research papers that performed benchmarks on the Dmlab-30 dataset. The following are the metrics that are used to evaluate models on the Dmlab-30 dataset: Precision, Recall, and F1-score metrics that are commonly used in Machine Learning, but other than that I don't know the names of the metrics. \r \r > Query 1 output: SELECT DISTINCT ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_l", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The DISTINCT keyword filters the query results to the selected labels and metrics. The output of the query looks like this: +-+-+ | metric_lbl | metric | +-+-+ | WMT2016 German- | WMT2016 German| | English | English | +-+-+ The Sparql results have the benchmark and the evaluation metrics as string values. In the given sample query, the evaluation metrics are WMT2016 German-English. Note: There are other preprocessing options you could take, such as filtering out abstracts with labels other than", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The Sparql query above lists the distinct metrics that are used to evaluate the models on the benchmarked datasets. The query returns the following columns: ?metric, ?metric_lbl. The metrics that are used to evaluate the models on the benchmarked datasets are the following: - Precision: The precision metric evaluates the recall of the model in identifying positive samples of entities that appear in an abstract as entities in the text but not as relations. - Recall: The recall metric evaluates the recall of the model in identifying positive samples of relations that appear in an abstract as relations in the text but not as entities. Note", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (", "SELECT DISTINCT ?metric ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?eval ?metric. ?eval rdfs:label ?value. } } input (English text): The top result has the metric \"number of times abstract\" and value 81715. output (Sparql query):urdy Number of times abstract=81715 <|endoftext|>Investment Objective Investment Objective: To seek income and growth with an emphasis on capital appreciation through income and capital appreciation stocks, fixed income securities, certificates of deposits, savings bonds and inflation-protected securities. Treasury Inflation-Protected Securities (TIPS) continue to offer the best yield on an inflation-protected securities, especially with the recent decline in long-term TIPS yields. They are generally considered a \"safe haven\" investment, and some economists and organizations consider", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } selectionSet#1: { \"classification accuracy\" \"pos\" \"precision\" \"frac\" \"recall\" \"f1 score\" \"support\" \"upper_bound\" \"support_frac\" } <|endoftext|>Introduction {#s1} ============ It is estimated that over 270,000 women in the United States will be diagnosed with invasive breast cancer (Braun et al., [@B9]), and that 60% of all breast cancer deaths can be attributed to the metastasis of untreated primary tumors (Bostrom et al., [@B8]). Metastatic breast cancer is generally considered incurable at this point, and the median survival of patients is about", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: You can look at the evaluation scripts in the repository on Github. In particular, you can see here how abstracts entities and relations annotated corpus was benchmarked. The evaluation script used for that is available here. <|endoftext|>Porsche 911 Carrera S - Superlight with Luxury Porsche 911 Carrera S - Superlight with Luxury is a car review site that allows car buyers to discover how Porsche 911 Carrera S compares to other Porsche models in terms of styling, performance, luxury and fuel efficiency. This website is not owned by Porsche", "SELECT DISTINCT ?paper_id ?title ?id FROM { #WOS-46985 relation-db:papers(paper_id,title,id) #paper_id is of class xs:string, title contains \"Abstracts' entities and relations annotated corpus\", id contains \"wos:0006985\" } WHERE { ?paper_id a paper-id:ID. ?paper_id wos:0006985. ?title a title:string. FILTER(str(title) = \"WOS-46985 dataset benchmark\") } input (English text): The following are research papers that have performed benchmarks on the WOS-46985 dataset: - Wilbert van der Graaf, Jon Kalinowski, Herve Bordes, Simone Rota, \"Neural Architecture Search with Sampler Restart.\" <|endoftext|>Django Unchained type Movie The gory 2023 setting of Quentin Tarantino\u2019s Django Unch", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The highest benchmark result achieved on the Supervised: dataset, including the metric and its value is 0.9877873227.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The A3 metric is the best performing metric, achieving an area of A3 in the test dataset.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics of evaluation are the recall, precision, accuracy, precision@1, precision@2, precision@3, f-score, n-gram prec, using the \"iNaturalist 2019\" dataset. output (Sparql query):quired output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } /* Here are the models benchmarked on the Atari 2600 Space Invaders dataset: */ evaluationorkgp:HasEvaluationEvaluationLabel metric_evaluation_label \"Visualise properties of learned model.\" evaluationorkgp:HasEvaluationEvaluationLabel metric_eval", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } aggio\u2419n\u2419 33.84% The following Sparql query selects the value of ?metric and prints it out: ?metric rdfs:label \"BLEU\"@en The answer is \"BLEU\"@en, which stands for BLEU (alphabetical source name for the algorithm \"Panda Authoritatively Unwrapped: TheLM004Model\"; https://www.statmt.org", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } indifferent answer: metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: - accuracy - AUC - F1 score - mAP - MSE - R2 score metrics that are used to evaluate models on the Cart Pole (OpenAI Gym) dataset: - perplexity - recall - recall at 5% precision - r2 score ?metric ?metric_lbl:accuracy, ?metric_lbl:AUC, ?metric_lbl:F1, ?metric_lbl:mAP", "SELECT DISTINCT ?metric ?metric_lbl ?eval WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Databricks ingest is an open-source framework for ingestion, transformation, and processing of unstructured data from any source. A3C FF hs model is deployed on top of Databricks and is based on the Adagrad algorithm. https://github.com/databricks/ adagrad. https://arxiv.org/abs/1611.07080 The paper also provides details on the evaluation metric used on the A3C model ? output (Sparql query):", "SELECT * FROM http://pages.cs.wisc.edu/~gharabwp/selfattention.html WHERE method = \"SelfAttention\" The code referenced in the paper \"SelfAttention: A Self-Attentive Encoder-Decoder Architecture for Abstractive Language Modeling\" (https://arxiv.org/abs/1606.04474), in section 3.2, uses the following models: 1- The BiDAF model trained on a large Chinese web-text dataset (https://github.com/dmlc/dmel). 2- A Self Attention Network (SAN) architecture described in \"Self Attention for Abstractive Language Processing\" (https://arxiv.org/abs/1708.05770). The following are the code references that use the above models: \"Self Attention for Abstractive Language Processing\" (https://arxiv.org/abs/1708.05770) # Download the BiDAF model trained on a large Chinese", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease benchmark dataset\") ?metric orkgp:HAS_METRIC ?metric; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset; ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the MetaCentre dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MetaCentre dataset\") ?metric orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GBDT-6-Layer Deep Neural Network for Relational Sentence Classification in Web Pages: GitHub Link\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metrics are used in the Abstracts' entities and relations annotated corpus benchmark dataset: <metrics> <metric>Benchmarking Report</metric> <metric_lbl>Abstracts' entities and relations annotated corpus</metric_lbl> </metrics>", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>3/5 + 206*q**2. What is i(b(n)) ? 16*n**2 Let s(p) = -4*p. Let z(n) = -2*n + 920. Determine s(z(d)). 8*d - 3680 Let k(y) = 5*y. Let b(p) = -28*p. What is b(k(i)) ? -140*i Let z(l) = -4*l**2 + 0*l**2 + 3*l**2. Let q(y) = -3*y**2 -", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Title: Research Papers that contain a benchmark over the NYT24 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the NYT24 dataset benchmark ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Title: Research Papers that contain a benchmark over the NYT24 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkg", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }\" input (English text): The datasets used in the benchmarking of the Abstracts entities and relations annotated corpus are orkgp:HAS_DATASET ?dataset and orkgp:HAS_EVALUATION ?eval. output (Sparql query): underm :: Text output (Sparql Query Results):,\"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstract", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } And the result is : # sort results by metric name SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } # 0 < or.pageRank < 1", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The following metrics are used in the benchmarking task for Abstracts' entities and relations: ?metric\t ?metric_lbl ontological_coverage\tOntological Coverage precision\tPrecision recall\tRecall f-measure\tF-measure accuracy\tAccuracy logloss\tLogLoss sentence_level_utility\tSentence-level Utility ?benchmark\t ?eval orkgp:HAS_DATASET\trdfs:label\tAbstracts' entities and relations annotated corpus orkgp:HAS_EVALUATION\t ?eval orkgp:HAS_METRIC\t ?", "SELECT DISTINCT ?metric ?metric_lbl where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Audio classification research area\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics that are used to evaluate models on the Audio classification research area annotated corpus benchmark dataset are: metrics\tmetric_lbl\t precision\tPrecision\t accuracy\tAccuracy\t f1-score\tF1 Score\t support\tSupport\t precision\tPrecision\t accuracy\tAccuracy\t f1-score\tF1 Score\t support\tSupport\t probability\tProbability\t precision\tPrecision\t accuracy\tAccuracy\t f1-score\tF1 Score\t support\tSupport\t precision\tPrec", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Cart Pole (OpenAI Gym) dataset has the following metrics: Mean Average Precision (mAP). The Cart Pole (OpenAI Gym) model achieved an mAP of 0.32. The top five models are: Kalman Filter, LSTM, Concat, Global Softmax. The top model is the Global Softmax model, with an mAP of 0.32.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The titles and IDs of research papers are: Aral et al. 2016.\u00a0Artificial Intelligence Case Study: Breast Cancer Diagnosis.\u00a0In\u00a0Proceedings of the\u00a044th Annual Meeting of the Association for Computational Linguistics (ACL), Paris, France, pp.\u00a0529\u2013536. ACL2016.\u00a0 Bordes et al. 2016.\u00a0Gated Recurrent Neural Networks for Hyperspectral Image Recognition. In\u00a0ICCV (3rd), pp.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet model\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>El 8 de marzo pasado, en la reuni\u00f3n de la Agencia Federal de Investigaci\u00f3n (AFI), el diputado del Partido Humanista, Dardo Anguiano, conden\u00f3 y solicit\u00f3 apoyo a la investigaci\u00f3n de la desaparici\u00f3n forzada de Estela de Carlotto, la Bombonera. El legislador explic\u00f3 a la instituci\u00f3n que la captura de Estela de Carlotto hace a\u00f1os en una quebrada de Toledo y trasladada a Maldonado (Ushpehi), no es secreto, y tiene testimonios suficientes, en espec\u00edfico: Carlos Negretto,", "SELECT ?metric_lbl ?metric_score FROM {urn:omeda:2006-ace-2005-dataset-benchmark} WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl; ?benchmark hasdataset ; ?eval hasmetric. } FILTER isrrdf:comment(\"This query should only return the top-scoring dataset in the ACE 2005 benchmark.\"). \f Output: - metric: Precision - metric: Recall - metric: F-measure - metric_lbl: Abstracts' entities and relations annotated corpus - metric_score: 0.93173379317337932 You can find the results for all datasets in the OMOP Benchmark results page.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset ? output (Sparql query):SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric.", "SELECT DISTINCT ?metric ?metric_lbl where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The top benchmark result (metric and value) over the dataset Atari 2600 Enduro ? RESULTS: mark:HIT 2 rdfs:label \"Abstracts' entities and relations annotated corpus benchmark result\" <|endoftext|>1. Introduction {#sec1} =============== Methicillin-resistant *Staphylococcus aureus* (MRSA) is now an extremely widespread pathogen that causes significant morbidity and mortality in both hospital and community settings. An", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize dataset\") ?metric. ?metric rdfs:label ?metric_lbl. } Hutter Prize Dataset Metrics: - mAP: Mean Average Precision. - R-Prec: Recall on Positive examples per total. - SPAN: Specificity (or 1-F1). - SPES: Soft Precision (or F1). uvant Sinnvall answer: mAP, R-Prec, SPAN, SPES", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): ?metric seel.cse.lsu.edu/data/re17.zip 108.15 <|endoftext|>Q: UIImageView flashing when zooming in iphone Here is my code, whenever i zoom in i get a flashing UIImageView. I am trying to hide this UIImageView but could not do so, could someone help me on this ? Thanks in advance. NSURL *imageURL1 = [NSURL URLWithString:imageURL]; NSData *imageData1 = [NSData", "SELECT * WHERE { ?dataset a owl:DatASet. ?dataset owl:defaultRel \"isA\" owl:RestrictIs toEntity. ?dataset dl:id \"http://download.opendeph.net/dl/wow_eval.tar.gz\". ?benchmark owl:sameAs ?dataset. OPTIONAL { ?benchmark owl:sameAs ?dataset. ?benchmark dl:id \"dl.openstreetmap.org\". ?benchmark owl:sameAs dl:inpaceWith ?eval. } ?benchmark owl:sameAs ?dataset. OPTIONAL { ?benchmark owl:sameAs ?eval. } } benchesold = 29.59 & # of References = 29 metric = MetaHash metric_lbl = Benchmark score <|endoftext|>SURVEY: LGBT Community Worried Over Trump Presidency More than half of the LGBTQ community are worried", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"List the code links in papers that use the 12-layer Transformer-XL model in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): https://research.google.com/fields.html ?view=fields&sprefix=Transformer+XL&lvl=en output (Sparql query):Transformer XL <|endoftext|>Milton Keynes is set to become the UK\u2019s first Global 100 Smart City, receiving financial support from the Government through the Mayor of London\u2019s Business Academy and the UK\u2019s Tech City Network. According to a report by the UK government\u2019s Tech City UK network, the city will be investing in the \"Internet of Things\", or the sensors that monitor and influence almost every", "SELECT ?metric ?metric_lbl where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } iston Sparql, the query language of the Web ontology language project (OWL) and RDFa. It uses the Resource Description Framework (RDF) syntax, but there is also a document-oriented view known as JSON-LD. OWL is an OGC standards-based format, and its corresponding query language is also based on OGC standards, namely on the Web Ontology Language (OWL).", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The Jacquard dataset is comprised of datasets annotated with relations and entities as metadata. The dataset has a single entity type, with relations between pairs of entities. The relationships between the entities can be direct or inverse. For the purpose of the Jacquard dataset, the metrics evaluated by the model for each entity-related pair is: relevance, precision, recall, F1 score and the entity identification rate. For the relations between the entities, the evaluation metrics are precision, recall, F1 score, the entity identification rate and the entity score. output (Sparql query):SELECT ?metric ?metric_lbl WHERE { ?dataset a or", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): ObjectNet is the dataset of the ObjectNet collaboration. This dataset contains 1183 RDF triples, each triple corresponding to one statement of an academic article. The dataset is split into six subsets, one for each evaluation metric. In addition, each subset contains a summary of the evaluation results for each metric, with the number of statements annotated per metric. output (Sparql query): onCreate ObjectNet is the dataset of the ObjectNet collaboration. This dataset contains 1183 RDF triples, each triple corresponding to one statement of an academic article. The dataset is split into six subsets, one for each evaluation metric. In addition, each subset contains", "SELECT ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Evaluate the query ?benchmark on the Abstracts' entities and relations annotated corpus benchmark dataset. The dataset is provided in the where clause. output (Sparql query): { \"Title\": \"Deep kernel learning for semantic role labeling\", \"Id\": \"DR", "SELECT * FROM xref WHERE xref/@kind='http://www.omg.org/spec/FGVC/20140821/PDF' AND xref/href=@/benchmark/FGVC%20Aircraft/final/parms.html AND xref/param[@name='lambda']=3.16 OPTIONAL {xref} Gower, A., & Raile, D. (2013). Coreference resolution with label propagation. In Proceedings of the joint ACL/EACL turnitin workshop on multi-annotator coreference resolution, pages 51\u201356. http://aclweb.org/anthology/N13/N13-1027.pdf <|endoftext|> United States Court of Appeals Fifth Circuit F I L E D IN THE UNITED STATES COURT OF APPEALS FOR THE FIFTH CIRCUIT November 9, 2005 Charles R. Fulbruge III Clerk No. 05-50505 Summary Calendar UNITED STATES OF AMERICA, Plaintiff-Appellee, versus JEFFREY SCOTT THOMAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset ? output (Sparql query):SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVAL", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The corresponding SPARQL query used to obtain the output is below: SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:", "SELECT DISTINCT ?model ?model_label WHERE { ?model atari2600_model; rdfs:label ?model_label. FILTER (str( ?model_label) = \"Atari 2600 Boxing\") } input (English text): The Atari 2600 boxing dataset has been evaluated by Sharpened and Singular. Sharpened obtained the best result with the model that maximizes the cumsum of the loss function. Singular obtained the second best result. Sharpened's model was designed to be trained on the CartPole dataset and evaluation on the Atari 2600 Boxing dataset. output (Sparql query): ** Sharpened's model ** model Atari 2600 Boxing model_label sharpened Atari 2600 boxing output (Sparql query): ** Singular's model ** model Atari 2600 Boxing model_label singular Atari 2600 boxing input (English text): The best model is Sharpened's model with the Atari 2600 boxing label.", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } A: The metrics that are used to evaluate models on the WMT14french-english dataset are the BLEU (Pseaked Loss) score, the METEOR (Phrase-Based Reading Eqloss) score and the ROUGE (Recall-O-Utility) score. These scores can be calculated for each of the benchmark resources using the Java API. <|endoftext|>Q: When I'm building two separate nifi flows, is there a way to put a mutex on the last steps in both of them ? I want to do some processing after both flows have been finished. I need", "SELECT ?model WHERE { ?model a orkgp:EvaluatedModel; ?model g orkgp:GroupeEvaluation; ?model_groupe g:groupe_id g:groupe_code \"BUCC\"; ?model_epars g:data_type g:data_type_short; ?model_epars g:object_id g:object_id_short; ?model_epars g:resource_uri g:resource_uri_short; ?model_epars g:resource_uri_namespace g:resource_uri_namespace_short; ?model_epars g:evaluation_date g:evaluation_date_short; ?model_epars g:problem_type g:problem_type_short; ?model_epars g:evaluation_time g:evaluation_time_short; ?model_epars g:data_collector g:data_collector_short; ?model_epars g:language g:language_short; ?model_epars g:upper_bound g:upper_bound_short; ?model_epars g:lower_bound g:lower_bound", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ci\u00f3n (Sparql query): specimln (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_EVALUATION ?eval. ?metric orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The AESLC dataset is benchmarked to evaluate model performance on Sparql queries of the type: SELECT... WHERE... WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } output (Sparql query):SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkg", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?benchmark HAS_DATASET HoC. ?benchmark HAS_EVALUATION HoC. ?dataset HoC:Abstracts' entities and relations annotated corpus. rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET HoC; orkgp:HAS_EVALUATION HoC. OPTIONAL { ?benchmark orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The query results include the following data: +-+-+ | Metric | MetricLabel | +==================+===================================================================================================================+ | document count | paper count | +-+-+ | average precision | Average precision | +-+-+ | ratio weighted | Recall at 10%: The weight of an incorrect prediction", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } topLevelQueryResults: Query1 topLevelSparqlResults: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } {#dataset a orkgc:Dataset", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): A paper has evaluated the model with a metric ?metric and the corresponding label ?metric_lbl. The paper has also annotated the CL-Titles-Parser model with the metric. #include <iostream> #include <string> #include <vector> #include <set> #include <map> #include <ql/char.h> #include <ql/text.h> #include <ql/queries.h> #include <ql/sfq.h> #include <sstream> #include <knn/knn.h>", "SELECT DISTINCT ?metric ?metric_lbl ?benchmark FROM <http://nwayna.net/datasets/habitat20/habitat20-object-nav-test-std.obo> WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text):The Habitat 2020 Object Nav test-std benchmark dataset contains ten metrics: Precision, Recall, F1 score, precision@N, recall@N, precision-recall@N, balanced F1 score, and hamming distance. output (Sparql query): I would like to list the ten models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset: DREML-C, DREML-D, DREML-DD,", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): reinforcements (English text): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; or", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } }", "SELECT DISTINCT ?model_name WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"River Raid\") ?model_name. } input (English text): List of models that have been tested on the River Raid dataset: Tom\u00e1s Conte, Peter Fenly, Eitan Hersh, Amos Weinberg, Sam Gilbert. yzed2:SELECT DISTINCT ?model_name WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"River Raid\") ?model_name. } On 31/10/2013 8:05 AM, Peter Fenly wrote: Some of the models we have tested that use Deep Language Models for problem-solving include: 1. The Knowledge Base Expansion system developed by Tom\u00e1s Conte. 2. The Sketch IO framework developed by Eitan Hersh. 3. The answer selector framework developed by Amos Weinberg. 4. The semantic role labeling framework developed by Sam Gilbert. yzed2:SELECT", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AOD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): 1.Krachman et al, AOD: 2014, AlexNet: 2014, MultiGrasp: 2017, 2.Li et al, AOD: 2015, AlexNet: 2015, MultiGrasp: 2016. 3.Li et al MultiGrasp has also been used in: Philbin et al, AUDL: 2015, Kuri et al, HumanaiCom: 2016, 4.Philbin et al, JVPR: 2017 AOD", "SELECT ?model ?metric WHERE { ?model atari2600MsPacmanModel. ?model atari2600MsPacmanModel atari2600MsPacmanModel:Evaluation. ?model atari2600MsPacmanModel:Evaluation results:MetricValue ?metricValue. ?model atari2600MsPacmanModel:EvaluationPeriod (TimePeriod). ?model atari2600MsPacmanModel:FaultTolerance ?faultTolerance. ?model atari2600MsPacmanModel:LicensedEntity (LicensedEntity). ?model atari2600MsPacmanModel:Name ?modelName. ?model atari2600MsPacmanModel:PlatformVersion ?platformVersion. ?model atari2600MsPacmanModel:RegisteredUsers ?registeredUsers. ?model atari2600MsPacmanModel:RulesEngineERP9L ?rulesEngineERP9L. ?model atari2600MsPacmanModel:Version ?version", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The metrics commonly used in the Habitat 2020 Object Nav test-std dataset are: Recall: Orkgp:RECALL Precision: Orkgp:PRC Micro F1: Orkgp:MICRO_F1 Macro F1: Orkgp:MACO_F1", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The metrics are: Average precision, Average recall, Identity Margin, Joint entropy, ROC, REC, PAC, DC and GAR.", "SELECT ?metric ?metric_lbl WHERE { ?dataset a dbo:Databricks for data and AI_Benchmark; ?dataset dbo:Databricks for data and AI; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Databricks for data and AI benchmark\") } input (English text): Databricks for data and AI benchmark paper contains title and ID of the paper. output (Sparql query): Databricks for data and AI_Benchmark has a benchmark with ID \"databricks_benchmark\" and a metric with the following label: \"Databricks for data and AI benchmark\"", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } glimp from final Response response = service.query(Sparql query); final List<SparqlQueryResult<Predicate>> queryResults = response.results; final SparqlQueryResult<Predicate> result = queryResults.get(0); final List<SparqlQueryResult<Object>> objectResults = result.objectResults; final SparqlQueryResult<Object> resultObject = objectResults.get(0); final Set<String> metricNames = new HashSet<>(); final Set<String> metricLabels = new HashSet<>(); for (SparqlQueryResult<Object>", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): The link above provides the code used in the paper to benchmark T-ConvS2S. output (Sparql query): Have you been able to reproduce the results in the code in the paper ? output (Sparql query): No, we have not been able to reproduce the results in the code in the paper. The code is not clear what are the metrics used to evaluate the model and", "SELECT * WHERE { ?benchmark has_dataset qbert; ?benchmark has_evaluation qbert. score ?score ; } { ?score rdfs:label \"HIT\". } The following are the metrics that are used to evaluate models on the Abstracts' entities and relations annotated corpus benchmark dataset: *\tMetric: score *\tMetric_lbl: HIT", "SELECT ?metric_lbl ?metric_str WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): \"Words in Context\" is the only dataset in the orkgp:HAS_EVALUATION property of the orkgp:HAS_DATASET property of the orkgp:HAS_EVALUATION annotation of the orkgp:HAS_DATASET annotation of the orkgp:HAS_EVALUATION annotation of the orkgp:HAS_DATASET annotation of the orkgp:HAS_EV", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } <|endoftext|>Q: JPA - How to remove non unique constraint violations from log Our JPA application is throwing below type of SQLException: Caused by: java.sql.SQLIntegrityConstraintViolationException: FOREIGN KEY (countyFK) REFERENCES county(id) DEFERRABLE INITIALLY DEFERRED The SQL that is being generated for the schema looks like this: ALTER TABLE parentTable DROP CONSTRAINT childFK,", "SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The result is: metric metric_lbl rating_sorted 5 It seems that the metrics used for evaluation are: - document rating: 5 - relation classification accuracy: 99% <|endoftext|>Thierry Henry set for Derby day return Thierry Henry will return to management this week after leaving Liverpool two months ago. The French international quit Anfield to take control of Thierry Henry & Associates - his own international management company - following the departure of Roy Wegerle to Inter Milan. Henry joined Kenny Dal"], "question_template": ["T05", "T02", "T01", "T02", "T04", "T01", "None", "T02", "T05", "T05", "T04", "T05", "T02", "T03", "T02", "T05", "T03", "T02", "T03", "T07", "T07", "T05", "T07", "T01", "T07", "T07", "T04", "T04", "T07", "T03", "T05", "T02", "T03", "T05", "None", "None", "T06", "T02", "T05", "T02", "T02", "T04", "T05", "T05", "T07", "T07", "T03", "T05", "T07", "T05", "T04", "T02", "T04", "T07", "T05", "T02", "None", "T01", "T07", "T03", "T07", "T05", "T04", "T03", "T03", "T01", "None", "T05", "T07", "T05", "T01", "T03", "T04", "T04", "T01", "T03", "T04", "T07", "T04", "T03", "T01", "T05", "T04", "T05", "T01", "T07", "T04", "T05", "T07", "T07", "T05", "T05", "T07", "T07", "T07", "T05", "T05", "T07", "T01", "T02", "T05", "T03", "T03", "T06", "T05", "T07", "T02", "T01", "None", "T07", "T02", "T07", "T05", "T05", "T07", "T05", "None", "T04", "T04", "T02", "T03", "T05", "T02", "T07", "None", "T02", "T07", "T07", "T04", "T07", "T01", "T02", "T07", "T04", "T01", "None", "T02", "T02", "T01", "T05", "T01", "T04", "T07", "T01", "T04", "T02", "T03", "T01", "T04", "T05", "T07", "None", "T01", "T05", "T02", "T03", "T05", "T07", "T05", "T02", "T07", "T07", "T05", "T03", "None", "T03", "T02", "T02", "T03", "T02", "None", "T07", "T03", "T05", "T01", "T07", "T04", "T07", "T07", "T04", "T04", "T04", "T03", "None", "T01", "None", "T04", "T01", "T06", "T03", "T05", "T01", "T05", "T03", "T07", "T05", "T04", "T03", "T07", "T01", "T03", "T03", "T02", "T07", "T07", "T02", "T06", "T04", "T05", "T04", "T05", "T05", "T04", "None", "T05", "T07", "T07", "None", "T03", "T05", "T02", "T07", "T07", "T01", "T03", "T07", "T04", "T05", "T05", "T08", "T01", "T05", "T07", "T04", "T07", "T06", "T03", "T07", "T05", "T02", "T01", "T03", "T03", "T04", "T02", "T03", "T03", "None", "T07", "T05", "T01", "T05", "T05", "T03", "T02", "T07", "T01", "T04", "T05", "T04", "T03", "T04", "T01", "T05", "T04", "T03", "T07", "T04", "T03", "T04", "T06", "T07", "T02", "T05", "T05", "T07", "T05", "T04", "T06", "T02", "T06", "T02", "T03", "T07", "T06", "T03", "T03", "T05", "T02", "T01", "T02", "T03", "T05", "T07", "T05", "T03", "T05", "T06", "T01", "T07", "None", "T01", "T01", "T02", "T03", "T07", "T04", "T03", "T07", "T05", "T04", "T02", "T03", "T07", "T07", "T07", "T05", "T07", "T06", "T04", "T07", "T07", "T04", "T04", "T01", "T05", "T04", "T03", "T07", "T05", "T03", "T02", "T04", "T05", "T04", "T05", "T04", "T07", "T04", "T02", "T04", "T04", "T01", "T05", "T05", "T03", "T07", "T01", "T05", "T01", "T04", "T03", "T05", "T02", "T07", "T05", "T07", "T05", "T05", "T02", "T01", "T07", "T07", "T04", "T07", "T01", "T04", "T05", "T07", "T05", "T04", "None", "T04", "T04", "T05", "T03", "T01", "T03", "T03", "T04", "T05", "T06", "T03", "T03", "T07", "T01", "T05", "T02", "T02", "T07", "T03", "T05", "T04", "T03", "T01", "T05", "T03", "T02", "T03", "T02", "T07", "T05", "T02", "T03", "T03", "T05", "T01", "T05", "T04", "T01", "T02", "T05", "T02", "T05", "T01", "T02", "T07", "T07", "T01", "T05", "T05", "T07", "T04", "T02", "T04", "T01", "T01", "T04", "T07", "T02", "T01", "T03", "T07", "T07", "T04", "T03", "T05", "T03", "T04", "T04", "None", "T02", "T02", "None", "T02", "T02", "T04", "T01", "T07", "T04", "T07", "T07", "T02", "T04", "T05", "T03", "T01", "T05", "T02", "T07", "T07", "T03", "T07", "T01", "T02", "T06", "T01", "T01", "T07", "T06", "T04", "T02", "T05", "T04", "T03", "T04", "T03", "T04", "T04", "T07", "T05", "T01", "T01", "T03", "T02", "T05", "T03", "T01", "T01", "T03", "T01", "T07", "T01", "T02", "T07", "T07", "T07", "T01", "T01", "T04", "T01", "T07", "T01", "T03", "T04", "T03", "T02", "T07", "T07", "T05", "T04", "T06", "T04"], "bleu_c": [0.2781, 0.1432, 0.0944, 0.1596, 0.0001, 0.2974, 0.0067, 0.2542, 0.2462, 0.0168, 0.3879, 0.4082, 0.2324, 0.1511, 0.1974, 0.3074, 0.4957, 0.2018, 0.3247, 0.0669, 0.1319, 0.2019, 0.0592, 0.2871, 0.004, 0.0044, 0.3846, 0.3434, 0.0901, 0.4369, 0.0816, 0.1852, 0.8951, 0.1229, 0.0019, 0.0142, 0.0024, 0.186, 0.2317, 0.2152, 0.0332, 0.261, 0.3087, 0.2542, 0.1404, 0.0065, 0.5866, 0.0675, 0.0801, 0.3824, 0.3444, 0.0088, 0.2751, 0.0681, 0.2498, 0.2484, 0.0018, 0.0639, 0.0603, 0.358, 0.2077, 0.2916, 0.2078, 0.8178, 0.2616, 0.372, 0.0057, 0.2795, 0.0706, 0.3438, 0.007, 0.5666, 0.443, 0.4128, 0.0569, 0.6424, 0.1979, 0.0814, 0.2071, 0.37, 0.0118, 0.4014, 0.334, 0.2437, 0.2776, 0.0701, 0.3936, 0.1423, 0.0642, 0.1274, 0.3853, 0.1986, 0.0549, 0.0687, 0.0659, 0.2894, 0.3902, 0.0919, 0.0542, 0.3162, 0.3488, 0.1094, 0.4391, 0.1409, 0.2932, 0.0795, 0.1719, 0.1748, 0.0071, 0.1505, 0.0071, 0.0685, 0.0, 0.1894, 0.0617, 0.35, 0.0055, 0.3831, 0.2062, 0.2219, 0.3581, 0.3264, 0.1733, 0.0588, 0.0061, 0.1067, 0.1161, 0.0622, 0.1354, 0.0101, 0.213, 0.2321, 0.114, 0.3551, 0.2448, 0.0014, 0.1228, 0.1912, 0.1785, 0.2222, 0.1864, 0.2622, 0.1262, 0.0084, 0.0066, 0.1911, 0.4911, 0.2572, 0.3921, 0.0862, 0.0758, 0.0012, 0.2242, 0.3449, 0.1682, 0.2146, 0.3625, 0.0563, 0.0912, 0.0054, 0.105, 0.089, 0.2592, 0.0779, 0.0069, 0.2666, 0.2306, 0.1186, 0.1112, 0.2441, 0.0113, 0.0834, 0.3852, 0.3021, 0.1484, 0.0049, 0.0321, 0.0686, 0.0598, 0.2838, 0.2831, 0.3905, 0.6256, 0.0112, 0.4528, 0.0092, 0.356, 0.2316, 0.1362, 0.1703, 0.3269, 0.0117, 0.2225, 0.3003, 0.0587, 0.3369, 0.0058, 0.2896, 0.0873, 0.0502, 0.3957, 0.0498, 0.1707, 0.1482, 0.0717, 0.1901, 0.3492, 0.4781, 0.3783, 0.4415, 0.3578, 0.3412, 0.2452, 0.0, 0.3525, 0.1042, 0.0594, 0.0201, 0.3742, 0.3251, 0.2141, 0.0978, 0.0592, 0.2264, 0.1712, 0.0614, 0.3525, 0.4019, 0.3105, 0.0233, 0.0581, 0.34, 0.0837, 0.0709, 0.0512, 0.1345, 0.4134, 0.0048, 0.0089, 0.2491, 0.0208, 0.242, 0.4163, 0.3787, 0.0118, 0.7507, 0.2156, 0.0028, 0.0542, 0.0109, 0.2757, 0.33, 0.4249, 0.6072, 0.1349, 0.0057, 0.4749, 0.0458, 0.2932, 0.3574, 0.5166, 0.1359, 0.2221, 0.1681, 0.2499, 0.3689, 0.0707, 0.0053, 0.1667, 0.3872, 0.1204, 0.037, 0.3995, 0.229, 0.3468, 0.0504, 0.4087, 0.432, 0.0068, 0.1754, 0.115, 0.2127, 0.3114, 0.1524, 0.1622, 0.3355, 0.3343, 0.1901, 0.1548, 0.2015, 0.1706, 0.2872, 0.2352, 0.0715, 0.2274, 0.3243, 0.2089, 0.142, 0.1896, 0.0106, 0.0132, 0.3245, 0.0166, 0.1286, 0.3823, 0.05, 0.1861, 0.2806, 0.0406, 0.3527, 0.3017, 0.1796, 0.1616, 0.0463, 0.0968, 0.1154, 0.0061, 0.002, 0.1527, 0.4063, 0.0686, 0.0531, 0.4464, 0.289, 0.2142, 0.3299, 0.2823, 0.3441, 0.0594, 0.0045, 1.0, 0.2059, 0.3387, 0.2751, 0.425, 0.2734, 0.4606, 0.0524, 0.0533, 0.1256, 0.334, 0.2299, 0.3079, 0.3181, 0.3832, 0.3803, 0.0006, 0.2319, 0.3726, 0.3108, 0.0197, 0.3724, 0.3463, 0.0913, 0.0576, 0.0997, 0.0767, 0.3601, 0.2517, 0.1602, 0.2009, 0.0641, 0.0837, 0.4222, 0.0498, 0.062, 0.3659, 0.313, 0.0981, 0.2565, 0.0024, 0, 0.3654, 0.0227, 0.2594, 0.3581, 0.0058, 0.5324, 0.3436, 0.4009, 0.3793, 0.1338, 0.3194, 0.3786, 0.0723, 0.0987, 0.3284, 0.0111, 0.1915, 0.1662, 0.1233, 0.3437, 0.0024, 0.6755, 0.2039, 0.2081, 0.4747, 0.1087, 0.3452, 0.1992, 0.089, 0.3629, 0.2143, 0.4086, 0.3922, 0.421, 0.2673, 0.1657, 0.2538, 0.0159, 0.1391, 0.006, 0.1495, 0.3153, 0.2387, 0.1603, 0.0757, 0.0667, 0.281, 0.3292, 0.3306, 0.0569, 0.4171, 0.0067, 0.0455, 0.211, 0.2348, 0.3965, 0.0459, 0.0055, 0.2841, 0.6158, 0.0725, 0.0329, 0.3867, 0.2421, 0.3315, 0.3475, 0.2577, 0.426, 0.0029, 0.4959, 0.0133, 0.0129, 0.1238, 0.2172, 0.1639, 0.1791, 0.0095, 0.2313, 0.0523, 0.0498, 0.0158, 0.3857, 0.3622, 0.4236, 0.3601, 0.3365, 0.1912, 0.0715, 0.0056, 0.3802, 0.0834, 0.1946, 0.1736, 0.1389, 0.344, 0.2698, 0.0673, 0.1455, 0.4037, 0.1777, 0.2507, 0.134, 0.3588, 0.3609, 0.3815, 0.3788, 0.0111, 0.0711, 0.2055, 0.2234, 0.2047, 0.2636, 0.23, 0.0025, 0.4412, 0.2415, 0.0618, 0.3143, 0.0029, 0.0741, 0.2355, 0.1518, 0.0382, 0.1404, 0.0751, 0.2256, 0.2857, 0.3201, 0.1126, 0.0749, 0.0066, 0.5723, 0.1317, 0.7008, 0.1258, 0.0668, 0.0709, 0.0092, 0.4321, 0.1344, 0.317], "bleu_4": [0.2182, 0.1026, 0.0323, 0.1224, 0.0, 0.2222, 0.0012, 0.2045, 0.1883, 0.0058, 0.304, 0.25, 0.1594, 0.1333, 0.1644, 0.2277, 0.4648, 0.15, 0.301, 0.0215, 0.0455, 0.1397, 0.0185, 0.2113, 0.0019, 0.0011, 0.3068, 0.2844, 0.0278, 0.3929, 0.0703, 0.1557, 0.8158, 0.0472, 0.0007, 0.0012, 0.0012, 0.1375, 0.1705, 0.1753, 0.009, 0.2214, 0.1863, 0.1429, 0.0465, 0.0014, 0.541, 0.0242, 0.0238, 0.2631, 0.2778, 0.0019, 0.2276, 0.0204, 0.1712, 0.2025, 0.001, 0.0182, 0.018, 0.3491, 0.0816, 0.1875, 0.1656, 0.775, 0.2234, 0.3067, 0.0011, 0.2202, 0.0247, 0.2697, 0.0023, 0.4794, 0.3651, 0.3, 0.0169, 0.5988, 0.1579, 0.028, 0.1652, 0.3474, 0.0008, 0.3047, 0.2653, 0.1743, 0.2366, 0.0194, 0.314, 0.0706, 0.0202, 0.0357, 0.2678, 0.1304, 0.0168, 0.0211, 0.0217, 0.2273, 0.2884, 0.029, 0.0108, 0.2321, 0.25, 0.0488, 0.4024, 0.0632, 0.2115, 0.026, 0.12, 0.1343, 0.0013, 0.05, 0.0012, 0.0217, 0.0, 0.0645, 0.0198, 0.2706, 0.0009, 0.2653, 0.1826, 0.1869, 0.3366, 0.2452, 0.1146, 0.0174, 0.0012, 0.0815, 0.0377, 0.0211, 0.0841, 0.0009, 0.1635, 0.1667, 0.037, 0.2526, 0.1848, 0.0007, 0.087, 0.1414, 0.1261, 0.1641, 0.1496, 0.224, 0.0417, 0.001, 0.0008, 0.1348, 0.4493, 0.22, 0.3294, 0.0349, 0.0233, 0.0007, 0.1667, 0.2747, 0.11, 0.1429, 0.2568, 0.0185, 0.0712, 0.0019, 0.0345, 0.0274, 0.1953, 0.0349, 0.0013, 0.2388, 0.1646, 0.0839, 0.0933, 0.1882, 0.0023, 0.0225, 0.3762, 0.2157, 0.1161, 0.0017, 0.0102, 0.0213, 0.0185, 0.2315, 0.2344, 0.2697, 0.5636, 0.0011, 0.3751, 0.0015, 0.3131, 0.1651, 0.0619, 0.1389, 0.2406, 0.002, 0.1773, 0.2917, 0.0182, 0.2289, 0.0006, 0.2581, 0.0286, 0.0185, 0.2778, 0.0267, 0.1023, 0.0513, 0.0211, 0.129, 0.186, 0.4028, 0.2317, 0.3901, 0.2184, 0.2472, 0.1969, 0.0, 0.2658, 0.0308, 0.018, 0.0034, 0.3333, 0.2487, 0.1616, 0.0312, 0.0185, 0.1702, 0.1429, 0.0182, 0.2809, 0.2759, 0.231, 0.0011, 0.0194, 0.2436, 0.0267, 0.0577, 0.0164, 0.0684, 0.3855, 0.0019, 0.0006, 0.2069, 0.0062, 0.172, 0.3875, 0.2843, 0.0009, 0.7, 0.1881, 0.0011, 0.0165, 0.0006, 0.2041, 0.2222, 0.2927, 0.5517, 0.0917, 0.0008, 0.3729, 0.015, 0.22, 0.3039, 0.5068, 0.047, 0.1864, 0.0769, 0.2061, 0.3469, 0.0225, 0.0008, 0.061, 0.3161, 0.0561, 0.0101, 0.2927, 0.1652, 0.2209, 0.0157, 0.2838, 0.3423, 0.0006, 0.1154, 0.0556, 0.1609, 0.2768, 0.058, 0.0851, 0.3077, 0.31, 0.1758, 0.1176, 0.1515, 0.1226, 0.2791, 0.1874, 0.0196, 0.1628, 0.3153, 0.1515, 0.0659, 0.1441, 0.0017, 0.0028, 0.2738, 0.0011, 0.0894, 0.3441, 0.016, 0.1138, 0.2602, 0.0079, 0.2593, 0.2381, 0.129, 0.119, 0.0148, 0.0317, 0.0323, 0.0011, 0.0011, 0.0652, 0.3291, 0.0233, 0.0177, 0.3869, 0.2124, 0.1736, 0.2626, 0.246, 0.32, 0.019, 0.0014, 1.0, 0.1512, 0.2913, 0.1939, 0.3544, 0.2067, 0.4075, 0.0217, 0.0093, 0.1038, 0.26, 0.1986, 0.2368, 0.2262, 0.2857, 0.3579, 0.0002, 0.2019, 0.2605, 0.2388, 0.0068, 0.3474, 0.2043, 0.0189, 0.0187, 0.0476, 0.0238, 0.268, 0.2049, 0.1154, 0.1613, 0.0196, 0.0233, 0.3209, 0.0161, 0.0275, 0.2766, 0.2187, 0.033, 0.1818, 0.0012, 0, 0.3038, 0.0011, 0.1875, 0.3366, 0.0008, 0.5, 0.3208, 0.3229, 0.2605, 0.0594, 0.3109, 0.3558, 0.023, 0.0309, 0.1979, 0.0028, 0.1569, 0.0746, 0.0761, 0.254, 0.0012, 0.6667, 0.161, 0.1397, 0.4659, 0.0783, 0.3364, 0.1474, 0.0274, 0.291, 0.1613, 0.381, 0.3647, 0.3152, 0.2045, 0.0562, 0.2016, 0.001, 0.096, 0.0011, 0.1071, 0.2343, 0.1837, 0.1062, 0.0235, 0.0168, 0.2088, 0.2489, 0.2557, 0.018, 0.2935, 0.0011, 0.0118, 0.1584, 0.2018, 0.3253, 0.0153, 0.0014, 0.236, 0.6066, 0.0222, 0.0024, 0.3235, 0.1765, 0.2386, 0.3391, 0.1707, 0.3436, 0.0006, 0.4061, 0.001, 0.0009, 0.0924, 0.18, 0.1364, 0.136, 0.0014, 0.186, 0.0169, 0.0161, 0.001, 0.3119, 0.2631, 0.4138, 0.2656, 0.2375, 0.13, 0.0196, 0.001, 0.3548, 0.0256, 0.1495, 0.1212, 0.0659, 0.2955, 0.2088, 0.0206, 0.0652, 0.3333, 0.1277, 0.1835, 0.0658, 0.3367, 0.2747, 0.3333, 0.3398, 0.0013, 0.024, 0.1552, 0.172, 0.1721, 0.2443, 0.1667, 0.0009, 0.4318, 0.1798, 0.0179, 0.2931, 0.0009, 0.0227, 0.1739, 0.0779, 0.0123, 0.0465, 0.0206, 0.1681, 0.2133, 0.2912, 0.0732, 0.0235, 0.0022, 0.5634, 0.1085, 0.6923, 0.0685, 0.0217, 0.0208, 0.0011, 0.3625, 0.0659, 0.2685]}
