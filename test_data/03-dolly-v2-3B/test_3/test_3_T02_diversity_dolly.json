{"template": "T02", "questions": ["\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the UrbanSound8k dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the MAZEA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the TDMSci dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the mean capacity of a carbon-based fuel?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the most common Knowledge representation method?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can all the data sets used in the compared studies be found?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What quantity of iron oxide was discovered on Elorza crater?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the DuIE dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What types of nanocarriers do have therapeutic effect?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which are 3 the most common variables for the atmosphere models?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is Raman spectroscopy?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What data format does CHEMDNER corpus have?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the SEE model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the MNIST dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where did the study with maximal geographic scale take place?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Does paper \"Disambiguating authors in citations on the web and authorship correlations\" employe Cosine similarity?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the DrQA model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which QA benchmarks incorporating more than 10000 questions?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Stanford Cars dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): For what piezoresistive material the least response time was obtained?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the BiT-M model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the NASCell model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Who has contributed to the largest number of articles about coronavirus?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the DuIE dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What coolant does ASTRID reactor use?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the CommitmentBank dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the GAD dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the most common lead compound?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the FABIR model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the emergency type of AMBER Alert?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the SearchQA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the OntoNotes dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the most common location in the studies?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the H-NLI model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the Duel noop model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the BioASQ dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the Classic dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): For which country of study overall prevalence of epilepsy is the highest?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the FQF model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the FTD dataset dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the POP3D model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the ACL Anthology dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the NCBI-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the BoolQ dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Are children examined in the studies?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the MedSTS dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the TempEval-3 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the MFEC model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the Softcite dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which sectors are modeled as energy sectors and how often?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? \n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the WOS-11967 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the Hutter Prize dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the metrics of evaluation over the ObjectNet dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the AESLC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark score and its metric on the Words in Context dataset?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?\n output (Sparql query): ", "\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts\\' entities and relations annotated corpus dataset?\n output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \n input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?\n output (Sparql query): "], "sparql": ["SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Word Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UrbanSound8k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MAZEA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDMSci\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, \"([0-9]+),([0-9]+)\", \"$1$2\" ) ), \"([0-9]+).*\", \"$1\" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), \"(fuel|CO2)\")) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TDM Tagged Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Lunar Lander (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Flair-TDM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Transformer-XL Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAN (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PNDec\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CATTS-XSUM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot German-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SemExp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top 1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1", "SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, \"Not\")) } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Entity Disambiguation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BIOSSES\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Chinese-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PIQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Crazy Climber\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Table-Sequence\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Funnel Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DocRED-BiLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AcrE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SemEval 2013\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, \"Elorza crater\", \"i\")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, \"FeO\")) ?properties orkgp:description ?property_description. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQN-PixelCNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Dynamic Coattention Networks (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = \"T\"^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Unsupervised NMT + weight-sharing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Permuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CRF with sentence expansion\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"FLOPS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DQNMMCe+SR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV TSM-50x2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SAC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MEMEN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Assault\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1-score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"200k Short Texts for Humor Detection\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"STREET\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwiki8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Reading Comprehension\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL04\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet (base)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ohsumed\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, \"tmospher\")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DDQN-PC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TREC-6\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rational DQN Average\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"1-of-100 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PolyAI Reddit\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Long Short Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, \"Raman spectroscopy\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CINIC-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AG News\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 entity level\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GCN Hybrid\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CUB-200-2011\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SEE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Adaptive Input Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Solaris\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MMV\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WLPC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2012\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF (1 day) hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RTE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-5736\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WebQuestions\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ART/CoreSC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"All-attention network - 18 layers\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), \"Disambiguating authors in citations on the web and authorship correlations\", \"i\")) } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DCASE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERT + BiLSTM + CRF Decoding\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pearson Correlation\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"linear-chain CRFs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DrQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC-PDN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, \"[+]\", \"\") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, \"\",\"\", \"\")) AS ?questions_number) FILTER( ?questions_number > 10000) } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"GPT-2 (small)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Number of params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Star Gunner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"NASCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"TCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC2GM\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Story Cloze Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), \"2019-nCoV\")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), \"2019-nCoV\")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DuIE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label \"ASTRID\"^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, \"coolant\")) ?coolant rdfs:label ?coolant_label. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Text Summarization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tutankham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommitmentBank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-M (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"RE+ Micro F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STEM-ECR v1.0\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GAD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AxCell\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AWD-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Dogs\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Senseval 2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Test perplexity\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-103\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"C51 noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FABIR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Switch Transformer\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C-CTS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL 2003 (English)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = \"Artificial Intelligence\") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiLSTM-Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Shake-Shake (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Joint Entity and Relation Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"SRU++ Base\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asteroids\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SearchQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"OntoNotes\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BCN+ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Pre-Training Dataset\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classical music, 5 seconds at 12 kHz\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2014 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"COMET - Direct\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1 score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Frostbite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters En-De\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BART\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"citation classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"H-NLI\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMed 20k RCT\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Macro Recall\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"NLL\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DCN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2018\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ice Hockey\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Scientific Results Extraction\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Semantic Role Labeling\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards (restricted)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Duel noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Fine-Grained Image Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DRI Corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Unpermuted Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STL-10\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rfa-Gate-arccos\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Centipede\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Natural Language Inference\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Classic\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DeiT-Ti\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Venture\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tsetlin Machine\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bowling\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FQF\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Sequence error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ARC (Challenge)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciREX\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Concept Mention Extraction\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"OTF spelling+lemma (single)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C LSTM hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SVHN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LayerNorm HM-LSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"SPARQL query optimization\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Temporal Convolutional Network\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiT-S (ResNet)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AAPD\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FTD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Percentage error\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciGEN\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"POP3D\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Tennis\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MPQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"SUCCESS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Paper Field\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah, run (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"STS Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Birdsnap\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Stanford Cars\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Precision\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Large mLSTM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Yars Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Fashion-MNIST\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"X-Sum\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Fine-Grained Gating\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Average Return (NoOp)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Prior noop\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Top-1 Error Rate\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ANODE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"DY-MobileNetV3-Small\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MPAD-path\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Multi-Perspective Matching (ensemble)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Bits per dim\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, \"ildren\")) } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet V2\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Quasart-T\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multimodal PISA\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Sentence Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Tokenlearner\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MedSTS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoNLL++\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciCite\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ImageNet + iNat on WS-DAN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"ROUGE-1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters-21578\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 HERO\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Rainbow+SEER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy (High)\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RACE\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 dev\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Seaquest\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GENIA - LAS\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Accuracy\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bank Heist\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A2\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Asterix\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"MFEC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CAIT-XS-36\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Montezuma's Revenge\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Params\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"ResNet-152 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"REDDIT-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hendrycks Test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"KD-LSTMreg\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CvT-21 (384 res)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"FG fine-grained gate\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Entity F1\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, \"Naive bayes\")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, \"Bag of words\")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, \"Feature request\")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cartpole, swingup (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Automatically labeled Medline abstracts corpus\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"LSTM (Bai et al., 2018)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"XLNet-Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"EfficientNetV2-L\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised:\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"A3\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist 2019\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"BLEU score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"A3C FF hs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NCBI Disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"6-layer QRNN\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT24\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Robot Navigation\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IMDb-M\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PEGASUS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Audio Classification\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reacher, easy (DMControl100k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"MACs\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Enduro\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Hutter Prize\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Wizard of Wor\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"12-layer Transformer-XL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Up and Down\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NLP-TDMS (Exp, arXiv only)\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \" Jacquard dataset\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"enwik8\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"PARAMS\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FGVC Aircraft\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Road Runner\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BiDAF + Self Attention + ELMo (single model)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"PAR Transformer Large\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"BERTwwm + SQuAD 2\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"CL-Titles-Parser\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Name This Game\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 River Raid\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"AlexNet, MultiGrasp\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DBpedia\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"HRLRE\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"T-ConvS2S\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } ", "SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = \"Score\") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Q*Bert\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Words in Context\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl", "SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = \"Common Sense Reasoning\") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } ", "SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"], "generated_sparql": ["est model that has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset is the Transformer. The model achieved an Accuracy score of 84.5%. \n index (Sparql query): * { }  \n query (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n response (Sparql query): { \n  \"paper\": \"P31\", \n  \"paper_lbl\": \"Abstracts' entities and relations annotated corpus\", \n  \"benchmark\": \"HAS_DATASET\", \n  \"cont\": \"HAS_BENCHMARK\", \n  \"dataset\": \"a\", \n  \"dataset_lbl\": \"\" } \n\nThe following is an excerpt of the description of the", "output (Sparql query):SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n\n input (English text): The following are research papers that contain a benchmark over the Penn Treebank (Word Level) dataset:\n\n - P31: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P31)\n\n - P32: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P32)\n\n - P33: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P33)\n\n - P34: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P34)\n\n - P35: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level", "INSERT INTO quERIES(queries) VALUES\n ('SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }');\n\n 6.INSERT INTO RESULTSET(queries) VALUES ('SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }');\n\n 7.INSERT INTO RESULTSET(queries", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): \nThe research papers that include a benchmark for the Penn Treebank dataset are: \n - P31 in the Penn discriminative training paper,\n - P29 in the Penn broad match training paper,\n - P28 in the Penn universal criminal data Simplification and Benchmarking  paper, \n - P27 in the Penn universal criminal data Simplification and Benchmarking  paper, \n - P26 in the Penn universal criminal data Simplification and Benchmarking  paper.\n\n<|endoftext|>A comparison of the binding of biopyrrolidine and molsidomine with alpha1- and alpha2-adrenoceptors.\nThe potencies of molsidomine and biopyr", ".paper rdf:type orkgp:Paper; rdfs:label \"Paper 26\"; orkgp:P31 rdf:type orkgp:hasDataset;?dataset_lbl nmf:metricRanking;?benchmark rdf:type orkgp:Benchmark;?benchmark_lbl nmf:ranking 10.0. \n input (English text): The MAZEA dataset contains entity and relation triples annotated with BLEU (P capability of the software nltk (Python) module), and the top performing benchmark has a score of 10.0.\n\nQuery\nSELECT DISTINCT?paper?paper_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nOutput\n paper: P31 (MAZEA), has", "input (English text): What models are being evaluated on the TDMSci dataset?\n output (Sparql query): SELECT?model?model_label where { {?model a rdf:Property ; rdfs:label?model_label. } \n  {?model rdf:type owl:Thing ; rdfs:label?model_label. } \n  {?model owl:equivalentClass rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ; rdfs:label?model_label. } \n  {?model owl:annotatedDatatype rdfs:domainContainment[]  [ rdf:type owl:Thing ; owl:equivalentClass rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ; owl:annotatedDatatype rdfs:domainContainment[ rdf:type owl:Thing ; owl:annotatedDatatype rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ; owl:equivalentClass rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ]", "TB {?l Carbon-based fuel?p Capacity }\n\nA Sparql query that identifies the title and ID of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset is:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nFrom the output, we can see that the title of one of the research papers that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset is \"Capacity of a carbon-based fuel\". The ID of the corresponding research paper is \"P31\", and the label of the benchmark in the benchmark property of the selected research paper is \"Abstracts' entities and relations annotated corpus\".", ";\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n   [\n    {\n      \"paper\" : \"P31: Multi-Genre Morphing Deep Learning Model for Zero-Shot Language Detection\",\n      \"paper_lbl\" : \"Abstracts' entities and relations annotated corpus\"\n    }\n  ]\n}\n\n<|endoftext|>//\n//     Generated by class-dump 3.5 (64 bit) (Debug version compiled Oct 15 2018 10:31:50).\n//\n//     class-dump is Copyright (C) 1997-1998, 2000-2001, 2004-2015 by Steve Nygard.\n//\n\n#import <objc/NSObject.h", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kuzushiji-MNIST benchmark dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|># This file is part of CPA-VM.\n#\n# CPA-VM is an integrated suite of software capable ofDoA-style virtual security\n# management. Originally developed at ETH Zuerich, it has since been adopted\n# by the Commonwealth government of Australia and is now a thriving community\n# project.\n#\n# Copyright (c) 2014, 2016, ETH Zurich and University ofatto Lausanne.\n# CPA-VM is licensed underONG clause 10E of the GPL.\n#\n# This file is part of CPA-VM.\n#\n# CPA-VM is", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English benchmark\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are: \npaper, paper_lbl\n\n\u00a0\n\nThe WMT2016 Romanian-English benchmark dataset has achieved the highest BLEU score of 27.\n\n\u00a0\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Romanian-English benchmark\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper or", "str (DMControl500k): catch (DMControl500k): 44.50: Ball in Cup (DMControl500k): 44.50: \n\nSo the correct query is: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThere are 2 entities in the dataset with the labels Abstracts' entities and relations annotated corpus. One is a dataset having the ontology orkgp:HAS_DATASET property, and the other is a benchmark having the property orkgp:HAS_BENCHMARK. The benchmark entity includes the label and the benchmark property. There is also one paper entity with the label and the paper property.", "input (English text): Top-1 Accuracy score\n output (Sparql query): SELECT?value (thanos:Top-1-Accuracy?value) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k dataset\")} \n input (English text): Top-1-Accuracy score = 85.02%\n output (Sparql query):\ufffd\ufffd?value (85.02)", "pend (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"arXiv dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the titles and IDs of research papers", "culate the following query on the TDM Tagged Corpus dataset :\n SELECT * WHERE { \n?dataset a orkgc:Dataset;\n?benchmark orkgp:HAS_DATASET?dataset;\n?cont orkgp:HAS_BENCHMARK?benchmark;\n?paper orkgp:P31?cont;\n?cont :metric?metric;\n?cont :modelExpression?modelExpression;\n?benchmark :modelExpression?modelExpression;\n?benchmark :modelObjectPropertiesLiteral?modelObjectPropertiesLiteral;\n?benchmark :modelLiteralDataPropertyNamesLiteral?modelLiteralDataPropertyNamesLiteral;\n?benchmark :modelLiteralDataValuePropertyValueLiteral?modelLiteralDataValuePropertyValueLiteral;\n?benchmark :modelDataPropertyNamesLiteral?modelDataPropertyNamesLiteral;\n?benchmark :modelDataValuePropertyValueLiteral?modelDataValuePropertyValueLiteral;\n?benchmark :modelObjectPropertyNameLiteral?modelObjectPropertyNameLiteral;\n?benchmark :modelLiteralDataPropertyValueLiteral?modelLiteralDataPropertyValueLiteral;\n?benchmark :modelDataPropertyNameLiteral?modelDataPropertyNameLiteral;\n?benchmark :modelDataProperty", "initial (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-disease dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark.?paper orkgp:P31?benchmark. }\n\nRelated questions:\n- Identify a benchmark in an ontology document?\n- How to identify whether an RDF graph contains a dataset?\n- Find the names and IDs of all datasets in an ontology\n- Identify benchmark in an ontology document\n- Identify whether an RDF graph contains a dataset\n- Identify all datasets in an ontology\n\nThis question originally asked at https://data.stackexchange.com/questions/21888/how-to-find-the-names-and-IDs-of-all-datasets-in-an-ontology, but did not have an answer so I'm reposting it here.\n\n\nThe steps to identify the papers that include benchmarks on the", "mark (lunar_lander)\n\n orchemark (lunar_lander)", ": SELECT DISTINCT?paper_eval { \n?paper orkgp:P31?paper_eval.\n} \n input (English text): Commonly used metrics for benchmarking are F-score, precision, recall, and accuracy. \n output (Sparql query):\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2 : SELECT DISTINCT?paper_eval WHERE { \n?paper orkgp:P31?paper_eval.\n?paper_eval owl:exactText?benchmark.\n} \n input (English text): Here is a sample benchmark that includes FSNS - Test dataset. \n benchmark contains two relations, topics and keywords, each with two entities, a topic and a keyword.\n output (Sparql query):\u0013 : SELECT DISTINCT?paper_eval WHERE { \n?paper orkgp:P31?paper_eval.\n?paper_eval owl:exactText?benchmark.\n?benchmark rdfs:label \"FSNS - Test dataset\".\n} \n input (English text): Here is a sample benchmark that includes FSNS - Test dataset. \n benchmark contains two relations, topics and keywords, each with two entities, a topic and a keyword.\n output (Sparql query):\u0013 :", "pend (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?paper_lbl rdf:type owl:NamedNamedEntity;}\n\n<|endoftext|>USS Porter (DD-741)\n\nUSS Porter (DD-741) was a Fletcher-class destroyer in the United States Navy during World War II.\n\nPorter was named after Commodore James Read Porter, an American naval officer and United States Ambassador to Japan during the 1858\u20131860bertama Expedition.  During the first years of the twentieth century the U.S. Navy had a comprehensive program to build dedicated naval districts, each with a flagship, a series of surface vessels, and other support vessels grouped around a central flagship.  Commodore Porter served as the flagship of the Naval District of Pacific Island Distress Signals during World War I.\n\nConstruction and commissioning\n\nPorter was laid down by the Bethlehem Shipbuilding Corporation, Lackstown,", "input (English text): The metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric; rdfs:label?paper_lbl. }\n\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric;?benchmark <http://www.webexhibits.org/annotation/relations_benchmarks.html>?metric; rdfs:label?paper_lbl. }\n\nThe benchmark <http://www.webexhibits.org/annotation/relations_benchmarks.html> for Abstracts' entities and relations annotated corpus", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): \nThere are several papers that utilize the Flair-TDM model including:\n - Dai et al., 2018\n - Itoh et al., 2018\n - Wang et al., 2018\n - Azulay et al., 2018\n - Raiko et al., 2017\n - Niemeier et al., 2017\n - Ohno et al., 2017\n - Hsiao et al., 2016\n - Wang et al., 2016\n - Jain et al., 2015\n - Viola et al., 2014\n - Padua et al., 2014\n - Thomas et al., 2012\n - Ahn et al., 2010\n - Flair-TDM benchmark results at URL: https://github.com/", "map (Saved in example dataset \u201cctf-abstracts-annotated-nlp-benchmark.iot\u201d): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper rdfs:label?paper_lbl. } \n input (English text): The code in the papers you provided include: (1) transforms.ipynb, (2) model zoo notebooks for the Transformer model, and (3) querying a SPARQL endpoint on the Knowledge Graph Platform.\n\n                                                        map (Saved in example dataset \u201cctf-abstracts-annotated-nlp-benchmark.iot\u201d): {?paper?paper_lbl?cont?benchmark}", "title, benchmark or benchmark@domain\n\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Abstract\n\nObjective To describe the prevalence of urolithiasis in Chinese children and identify the risk factors. Methods A multicenter, cross-sectional study was conducted in children who were admitted to 14 hospitals located in 10 cities in China from January 1, 2016 to December 31, 2016. All enrolled children were diagnosed with urolithiasis based on the pathological examination. For comparison, subjects without urolithiasis were selected from patients who were admitted to the same hospitals with the same period during the same year but who did not have urolithiasis confirmed by pathological examinations. The enrolled children were divided into two groups: a lithiasis group and", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?benchmark. } \n input (English text): Here are the links to the papers that have utilized the SAN (single) model and include the links to their code:\n\n earthquack, Cory. A Benchmark for Evaluating Relations One-Time Learning, TSTLS 2016.", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P31?cont;?model rdf:type?model_lbl. } \n input (English text): The models that have been benchmarked on the ACE 2005 dataset include the following relations: has_dataset, has_benchmark, has_dataset, has_label.\n output (Sparql query):ostics SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P31?cont;", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PNDec model for benchmarking\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_CONTEXT?paper.?paper_lbl rdfs:label?paper_lbl. } \n input (English text): The code references can be found in the repository on GitHub https://github.com/bernhard-despe/pndec. The datasets and benchmark can also be found there. \n\nThe corresponding SPARQL query is: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PNDec model for benchmarking\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_CONTEXT?paper.?paper_lbl rdfs:label?paper_lbl. }", "input (English text): You can find code references in papers that have used the CATTS-XSUM model for benchmarking purposes in the code repository https://github.com/opennlp/spec- repository.\n\n\"?>\nSELECT DISTINCT?paper?paper_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta charset=\"utf-8\"/>\n  </head>\n  <body>\n    <h1>Output:</h1>\n    <h2>SELECT DISTINCT?paper?paper_lbl \n    WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"", "input (English text):P31\n\n        SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n        paper    paper_lbl                            \n\n        <sparql statement-source=\"http://stackoverflow.com/questions/4027783/what-are-the-titles-and-ids-of-research-papers-that-include-a-benchmark-for-the- Abstracts%E2%80%99-entities-and-relations-annotated-corpus-dataset/\" />\n\n        P31\n\n        Benchmark result IMDb-B:\n\n        Metric: precision\n\n        Value: 68.789\n\n        Metric: recall\n\n        Value: 0.738\n\n        Metric: f-", "output (Sparql query): SELECT?paper (rdfs:label?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?benchmark. } \n input (English text): MLDoc Zero-Shot German-to-French.\n output (Sparql query): contradicting the query from the above answer, we got the following: SELECT?paper (rdfs:label?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?", "it (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemExp model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nThe query returned:\n+-----------+------------------------------+\n|paper_lbl  |text                          |\n+===========+==============================+\n|paper_lbl  |\"SemExp model for benchmarking |\n|paper_lbl  |purposes                     |\n+-----------+------------------------------+\n\n\nThe query was successfully completed.\n[ benchmark - dataset ]\n\n<|endoftext|>You'll have plenty to celebrate when you subscribe to the Liverpool FC newsletter Sign me up Thank you for subscribing We have more newsletters Show me See our privacy notice Invalid Email\n\nJames Milner has welcomed the recent focus on mental health within football.\n\nThe Reds' No.6 has spoken about the", "mark : F1 score; \niments (Sparql query): ett import:F1-scorer; omtermin (Sparql query): F1", "will use the XGboost model produced by PTC for this query.\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n|paper   P31 |cont       XGboost Model produced by PTC\n\n\nThe following are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: \nPenn Treebank (Character Level)\n\n\nThe benchmark for Abstracts' entities and relations annotated corpus dataset can be found in the following paper:\nTian Wang, Kaiming Shi, Rajat Monga, Andrew 544 Park, Andy Davis, Joel Sansbury, Radford M. Neal, Andrea Nesto, Jeff Prescod, Drew Deiss, Hrishikesh Kishore, Bo", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level) dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \nfrom \nhttps://www.ontotext.com/ontologize/diagram_ontology/benchmark.html\noutput (Sparql query): orchestrate - given the selection above, the following is a sample of the returned data: \npaper:P31\npaper_lbl:Penn Treebank (Character Level) dataset\nbenchmark:HAS_DATASET\ndataset:a\ncont:orkgp:HAS_BENCHMARK\ndataset_lbl:rdfs:label\n\n\nfrom \nhttps://www.ontotext.com/ontologize/diagram_ontology/benchmark.html\n\n\n orchetable :", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Double Dunk\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): \"precision\", \"recall\", and \"f1-score\" are widely used metrics for evaluating the performance of a dataset.\n output (Sparql query):precision:AVG\n output (Sparql query):recall:AVG\n output (Sparql query):f1-score:AVG\n output (Sparql query):.\n input (English text): \"precision\", \"recall\", and \"f1-score\" are widely used metrics for evaluating the performance of a dataset.\n\n<|endoftext|>By Brooke Chitty and Kate O\u2019Connor\n\nWASHINGTON (Reuters) \u2013 Republican and Democratic leaders on Capitol Hill said on Thursday they were hopeful that U.S", "select?paper?paper_lbl where {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont;?cont orkgp:P31?paper_lbl; rdfs:label?paper_lbl. }\n input (English text): The model achieving the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset is P 31.\n\n ~ Write a query to fetch the desired set of papers with their corresponding dataset, benchmark and paper labels.\n\n ~ SELECT?paper?paper_lbl where {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont;?cont orkgp:P31?paper_lbl; rdfs:label?paper_lbl. }\n\n ~ The above query returns three rows:\n\n * Paper labeled \"P 31\" has dataset \"orkgp:P31\" and benchmark \"orkgp", "pend (Distinct). Paper; Paper LABEL; \"Abstracts' entities and relations annotated corpus\" as benchmark; Sparql result set; \"Abstracts' entities and relations annotated corpus\" as benchmark \n\nThe title and ID of the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nIn this query,?paper and?paper_lbl are the Sparql results set,?dataset is the dataset property of the orkgc:Dataset contained in the input data,?cont is the property of the orkgp:HasDataset property of the orkgc:Dataset contained in the input data, and?benchmark is the property of the", "itus (English text): They can be found at https://github.com/datawave/data-set. In particular, the datasets for P31 and P32 can be found under the p31 and p32 folders, respectively.\n\n Datasets used in P31 and P32 can be found under the p31 and p32 folders, respectively.\n\n          SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Datasets used in P31 and P32\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P32?cont; rdfs:label?paper_lbl. }\n\n  P31: https://github.com/datawave/data-set/tree/master/p31\n\n  P32:  https://github.com/datawave/data-set/tree/master/p32\n\n    ?paper?paper_lbl", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nA solution that relies on supervised machine learning algorithms could be to use the BenchmarkCategoryProperty SKOS ontology as a common parent class for entities and relations in benchmark datasets and then feed this class to supervised machine learning algorithms such as support vector machines (SVM).\n\nAn example model could look like this:\n\n.split-hornetmodel {\n  @model.var.type string paper_label\n  @model.var.label string paper\n  @model.var.value orkgp:P31\n  @model.vars.context orkgp:HAS_BENCHMARK\n  @model.vars.text orkgp:HAS_DATASET", "input (English text): Here is a list of research papers, along with their titles and IDs that have performed benchmarks on the BIOSSES dataset: \nAuthors\nDietterich, Juan Manuel and Moonsamy, Dinesh and Pang, Bilge and Urmson, Noel and Zheng, Yue and Cohn, Edgar and Tomasoni, Antonio and Bartoszewski, Krzysztof. \nAndre, Antonio and Bartoszewski, Krzysztof. and Dietterich, Juan Manuel. and Moonsamy, Dinesh. and Pang, Bilge. and Urmson, Noel. and Zheng, Yue. and Tomasoni, Antonio. and Cohn, Edgar. \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkg", "usting engine modelP31 has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset.\n\nHow we obtain benchmark for the abstracts' entities and relations annotated corpus dataset:\nWe download the file of the dataset that contains the data of the benchmark, and then replace the content of the downloaded file to generate a diff, and use the diff with the diffmerge tool to generate the corresponding id.\n\nSparql query that we run to obtain the RDF data:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont;?benchmark.?paper_lbl. }\n\n<|endoftext|>Hemispheric differences in color perception: the Melbourne reference sample.\nTo examine the intersubject variability in color vision tests and explore whether this", "set{a oregpc:Dataset ; rdfs:label  }SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Libyan Leader Muammar Gaddafi: I Am Still Alive, Honored In Libya, But Spent Last Six Months In A Hospital In Italy While Being Worshipped By the Western Press\n\nThe former Libyan leader Muammar Gaddafi has officially declared he is alive and in good health.\n\nGaddafi told a television network in the Netherlands that his \"physical and mental condition remains stable.\"\n\nThe 74-year-old, who is under arrest in North Africa, also said he is still the leader of Libya and has loyalists in key parts of the country.\n\nHe spoke to the TV network", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Q:\n\nRecursive array with jQuery each()\n\nI want to know if it's possible to make a recursive function to generate an array using jQuery's each() function.\nThis is the function I have so far:\nfunction generateArray(arrayToBuild, arrayValue, arrayFirstElement, arrayLastElement, elementToAddToArray){\n  if(arrayFirstElement == arrayLastElement){\n    arrayToBuild.push(elementToAddToArray);\n  }else{\n    generateArray(arrayToBuild, arrayValue, arrayFirstElement + 1, arrayLastElement, elementToAddToArray + 1);\n  }\n}\n\n$(\".element\").", "nemental query  SELECT?paper_lbl?benchmark  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The best results over the dataset CoNLL++ are:\n metric\tvalue\tdataset\trelation\n accuracy\t84.94%\tlanguage\tsentence proximity\n coords\t27.92%\tsentence\tposition in a sentence\n proximity\t27.92%\tsentence\tpreference for a given relation\n output (Sparql query): It's the benchmark \"Proximity\" and the metric \"Proximity\" and the value \"27.92%\"\n\nThe two RDF statements below are the benchmarks found in the answer:\n\nrdfs:label\tp31\tConll++ Entities and Relations Annotated Corpus\n\n\tAbstracts' entities and", "ary?benchmark WHERE {?benchmark abstracts:HasDataset?dataset.?dataset a orkgc:Dataset.?benchmark orkgp:HAS_BENCHMARK?benchmark.?benchmark rdfs:label \"Best performing model benchmarking the PIQA dataset in terms of Accuracy metric\". }\n\nThe SELECT DISTINCT query returns the following data:\npaper    paper_lbl\n---------------------------\nP31      Abstracts' entities and relations annotated corpus\n\nThe Sparql query returns the following data:\nbenchmark      \n---------------------------\nHasDataset    a\nHAS_BENCHMARK\nAbstracts' entities and relations annotated corpus\nBest performing model benchmarking the PIQA dataset in terms of Accuracy metric\n\n<|endoftext|>Introduction\n============\n\nDentine hypersensitivity is one of the most prevalent oral disorders. Millions of people suffer from dentin hypersensitivity, which is considered the \\\"sixth\\\" great myth of civilization (The sixth great myth of civilization is Death and dentin hypersensitivity).\\[[@B1]\\] One in three people has some degree of dentin hypersensitivity,\\[[@B2]\\] with more women affected than men. It has been shown that the distribution of dentin tubules is different", "n:P31 \n input (English text): P31 is the model that performed best on the Atari 2600 Crazy Climber benchmark dataset. \n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n output (Sparql query): {\n  \"paper\" : \"P31\",\n  \"paper_lbl\" : \"P31\"\n}\n\nI selected the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset as the answer to the question. The selected model is called P31 and its identifier is P31. The selected model scored 41.8 on the benchmark dataset, which is quite good for a search model that hasn't seen any training", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset a orkgp:Dataset;?paper_lbl string:http://doi.org/10.5281/zenodo.1453070.?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P32?cont;?paper orkgp:P33?cont. } \n input (English text): \nTable-Sequence model: https://github.com/mozilla-sbcl/sbcl/blob/master", "uate the datasets, relations and the benchmark and write the titles?\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): The following papers were found: https://doi.org/10.5281/zenodo.1144287, https://doi.org/10.5281/zenodo.1144288, https://doi.org/10.5281/zenodo.1144289, https://doi.org/10.5281/zenodo.1144290, https://doi.org/10.5281/zenodo.1144291, https://doi.org/10.5281/zenodo.1144292, https://doi.org/10.5281", "set_id, bench_id,paper_id,paper_label,metric\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Reuters De-En\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_id;?paper_label?paper_lbl.?metric?cont;?metric a?benchmark. }\n\n<|endoftext|>Can Vending Machine Give You A Quick Snack At Work?\n\nIt\u2019s often difficult to choose what to eat when you\u2019re in a hurry and don\u2019t have much time. Luckily, vending machines at work can provide quick, easy snacks for you to eat when you\u2019re in a pinch. It doesn\u2019t matter if you\u2019re hungry or if you\u2019re looking for something sweet or savory, vending machines at work are always there to provide a quick bite to eat.", ":sparql +\n query results (Sparql query): \n\n # Query Results \n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n...\n<paper:P31 xmlns:paper=\"http://nlp.stanford.edu/data/js/nli.js\" ze=\"3.95\" country=\"France\" author=\"Alain\">\n  <paper:Benchmark language=\"English\" country=\"France\" author=\"Alain\" ze=\"3.95\" annotationType=\"None\" corpus=\"Abstracts' entities and relations annotated corpus\" relationType=\"None\" relationId=\"P31\" relName=\"Protein 31\" relationText=\"Protein 31\" relationEvidence=\"Protein 31 was identified by MS/MS and sequenced by single-", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?benchmark. }\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP_INDICATORS\n\n EIGEN_USING_BOOTSTRAP", "output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FLOPS\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?benchmark. }\n\n input (English text): The best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric is...\n\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl rdf:type owl:Class ; rdfs:label \"FLOPS\";?benchmark owl:hasDataset?dataset;?benchmark owl:hasBenchmark?benchmark;?benchmark owl:member?benchmark_property;?benchmark_property owl:annotatedReads?benchmark_property_value.?paper owl:hasDataset?benchmark. }", "Barkan et al. (2015) describe a benchmark for detecting abstract entities and relations in datasets using mentions in titles. The benchmark system was developed and validated on the abstracts\u2019 entities and relations in the ACL and TACL 2014 datasets. The benchmark has been used to create the datasets ACLLC and TACLLC. The top result is the precision (P31) for detecting abstract entities in the TACLLC dataset. The P31 is 30.43%. \n input (English text):The top benchmark result (metric and value) over the dataset AESLC is 30.43%.\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nSince the output is an ordered tuple, the DISTINCT keyword is not required.", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")?benchmark orkgc:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe answer to this question is a list of SPARQL queries that return the title and ID of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus corpus dataset.", "?benchmark?metric WHERE {?dataset a orkgc:Dataset;?benchmark o rdf:nodeID \"urn:lsid:columbia.edu:rezension:3899-wos-46985-datasets:benchmarkConnection.30444970-dk41m26\";?metric rdfs:label?metric. FILTER (str(?metric) = \"68.80529747333235\")}\n input (English text): 68.80529747333235\n\n 12\n\nA:\n\nTo find a benchmark score on WOS-46985 dataset, the following query will help you:\nSELECT ?benchmark?metric \nWHERE {?dataset a orkgc:Dataset;?benchmark o rdf:nodeID \"urn:lsid:columbia.edu:rezension:3899-wos-46985-datasets:benchmarkConnection.30444970-dk41m26\";?metric rdfs:label?metric. FILTER (str(?metric) = \"68.80529747333235\")}\n\nFor the metric field, you can choose any of these:\n1", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n# --", "?benchmark?paper_lbl\n\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Supervised: dataset in terms of SemEval 2013 metric\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n input (English text): The model with highest accuracy on the SemEval 2013 dataset is...\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval 2013 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }", ".ringer a SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): All research papers that performed a benchmark on the dataset created by SciecERC. \n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): All research papers", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?cont orkgp:HAS_BENCHMARK ironoxide:quantity ; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Elorza crater discovered quantity of iron oxide\")?benchmark orkgp:HAS_DATASET?dataset. } \n input (English text): Elorza crater discovered a quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered a quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide.\n\n 18 abstracts include", "input (English text): Here is a list of models that have been evaluated on the VTAB-1k dataset: Z Moses, C.D. Manning, S. Russell, D. Berenson, Z. Bebber.\n\n output (Sparql query): SELECT * {?paper?paper_lbl;?paper_lbl ukn:model; }\n\n input (English text): Next, you can find the titles and IDs for the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset. The models that have been evaluated on the VTAB-1k dataset are:\n\n output (Sparql query):", "input (English text): ### code: ### https://github.com/mapillary/DQN-PixelCNN.git ### benchmark: ### http://www.mapillary.com/sae/benchmark ### dataset: ### http://www.mapillary.com/sae/dataset ### paper: ### http://www.mapillary.com/sae/paper ### paper_lbl: ### P31 ### dataset_lbl: ### Abstracts' entities and relations annotated corpus ### benchmark: ###HAS_DATASET ### cont: ### HAS_BENCHMARK ### paper or kgp: P31 ### paper_lbl: ### P31 ### benchmark or kgp:HAS_DATASET ### dataset: ### P31 ### dataset_lbl: ### P31 ### benchmark or kgp:HAS_BENCHMARK ### ### code: ### SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BEN", ":SELECT DISTINCT?paper_metrics WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Freeway dataset evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_metrics?paper_metrics. }\n\nThe title and ID of these research papers can be found here: https://github.com/wikimedia/opendatasets/tree/master/db48/Downloads\nThe metrics used to evaluate models on the Atari 2600 Freeway dataset can be found here: https://github.com/openai/human-level-playing-field/blob/master/evaluation.md", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper rdfs:label?paper_lbl. }\n\nIn this example, the answer to the query is: \n<paper_lbls>, <paper>, \"Abstracts' entities and relations annotated corpus\", \"HAS_DATASET\", \"P31\", \"HAS_BENCHMARK\", \"Abstracts' entities and relations benchmark\" \n\nThe datasets used for the benchmark and model are in the?dataset and?benchmark variables respectively.?paper and?cont refer to the paper and content variables. Note that the benchmark variable (?)benchmark also appears in the sparql query as?benchmark.", "is the model described in the paper by Cao et al. (arXiv:1903.10040).\n input (English text): Indicate the publication that includes the benchmark model and provides the annotator\u2019s guide (Exp only): arXiv:1903.10040.\n output (Sparql query):SELECT * WHERE {?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n            ?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?cont orkgp:HAS_BENCHMARK?benchmark. } }\n\n<|endoftext|>The aetiology of chronic fatigue syndrome/myalgic encephalomyelitis: a systematic review.\nChronic fatigue syndrome/myalgic encephalomyelitis (CFS/ME) is a", ":SELECT?benchmark (has?metric)?paper (where?paper_lbl = \"Top benchmark score and its metric on the Atari 2600 Tennis dataset\") WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric. }\n\n                                   :TOP benchmark score and its metric on the Atari 2600 Tennis dataset", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark dawbe:BenchmarkMatch?matches.?paper orkgp:P31?paper_lbl. } \n input (English text): The five most frequent entity types over the Abstracts' entities and relations annotated corpus dataset are: \n1. person\n2. organization\n3. location \n4. product \n5. Databricks \n output (Sparql query): complicate \n1. person\n2. organization\n3. location \n4. product \n\nAbstracts' entities and relations annotated corpus dataset includes three benchmarks that evaluate entity typing over the Abstracts' entities and relations. The five most frequent entity types are: person, organization, location, product, and Databricks. \n\nThere", ":List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset.?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cheetah, run (DMControl500k) benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n\n# Example ResultSet: \n# { \n #  \"paper\": { \"paper\": \"Cheetah, run (DMControl500k) benchmark dataset\"}, \n #  \"paper_lbl\": \"Cheetah, run (DMControl500k) benchmark dataset\"\n # }\n\n\n# Create a SELECT query that contains only the desired values.\nSELECT DISTINCT?paper?paper_lbl \n\tWHERE {?dataset a orkgc:Dataset;?dataset_lbl rdfs:label?dataset_lbl. FILTER (str", "<Query><!-- A SPARQL query fromteaches towhether an entity or a relation is in the Benchmark -->\n  <Where>\n   <Contains>\n    <Variable name=\"benchmark\">\n     <Input object=\"orkgp:HAS_BENCHMARK\"/>\n    </Variable>\n    <Variable name=\"paper\">\n     <Input object=\"orkgp:P31\"/>\n    </Variable>\n    <Variable name=\"cont\">\n     <Input object=\"orkgp:HAS_DATASET\"/>\n    </Variable>\n    <Variable name=\"dataset\">\n     <Input object=\"a\"/>\n    </Variable>\n    <Variable name=\"dataset_lbl\">\n     <Label object=\"a\"/>\n    </Variable>\n   </Contains>\n  </Where>\n </Query>\n\nippet (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET", "inal (English text): The types of nanocarriers that have a therapeutic effect include: polymers, lipids, carbohydrates, inorganic materials, and other biological materials.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"The types of nanocarriers that have a therapeutic effect include: polymers, lipids, carbohydrates, inorganic materials, and other biological materials.\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text):?paper + bench + datasets + (hasDataset) + (hasBenchmark) + (hasCont) + paperLabel\n\n 15:47:38.448 Copyright 2010 2010-2017, IMR-BTS. All rights reserved. \nUser manual, Version 10.0, 09/10/2017\n\n 15:47:38.450 INQUIRY {\n 15:47:", ":CL-SciSumm\n\n spindle:CL-SciSumm\n\n spurqlquery:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CL-SciSumm\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): CL-SciSumm is the name of the dataset in the CL-SciSumm benchmark that we are interested in.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CL-SciSumm\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Unsupervised NMT + weight-sharing model in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): List the code links in papers that use the Multitask NMT + weight-sharing model in any benchmark?\n output (Sparql query):onso:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Multitask NMT + weight-sharing model in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper", "input (English text): Perform a sequential search for the best model based on the Permuted Accuracy score on the Sequential MNIST benchmark dataset. \nThis returned the following document:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Q:\n\nPyramid: Wildcard subdomain pattern not working\n\nI have wildcard subdomains in Pyramid that works fine with reverse subdomain but not with pattern.\nHere is my reverse subdomain:\n@view_config(route_name='')\n@cherrypy.tools.csrf()\ndef index(self, request):\n    return dict(urls='http://foo.bar.baz.com/*')\n\nHere is my pattern subdomain:\n@view", "//model\n\n Output (Sparql query):SELECT?paper_lbl ( COUNT (?paper ) AS?COUNT ) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciTLDR\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n<paper_lbl>:value (COUNT?COUNT) :count\n\n<|endoftext|>We're about:\n\nThe Friendly Neighborhood Website is an informal group of website developers and internet professionals in the greater Charlotte area. Whether you're just starting out or an experienced developer, we hope you'll find this group both helpful and enjoyable. Our meetings are informal and centered around projects or topics of common interest. All levels of experience are welcome; this is a great place to make friends and find mentors as you work on your website projects. Please note our events are open to all and are NOT limited to Charlotte developers.\n\nWe are also happy to point you toward resources for", "input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?metric?metric_lbl. FILTER (str(?metric) = \"commonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?metric?metric_lbl. } \n input (English text): List the papers that contain metrics used on the CommonsenseQA benchmark dataset.\n\n assistance (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?metric?metric_lbl. FILTER (str(?metric) = \"commonsenseQA\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?metric?metric_lbl. }\n\n assistance (", "input (English text): The highest benchmark result achieved on the IMDb-M dataset, including the metric and its value, is 47.67.\n benchmark ( owl :Benchmark ): imdb-m\n\n metric ( owl :Metric ): [lm, mct]\n\n value ( owl :Value ): 47.67\n\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The highest benchmark result achieved on the IMDb-M dataset, including the metric and its value, is 47.67.\n\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scholarly entity usage detection benchmark result\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl; }\n\n meu, the highest benchmark result include the following metric and score:\n\nolutely sure, the highest result is #document-level precision @ 0.787, 0.787, 0.787, 0.787, 0.787 @ #relation-level precision @ 0.726, 0.726, 0.726, 0.726, 0.726 @ #relation-level recall @ 0.775, 0.775, 0.775, 0.775, 0.775\n\n\u00e1dheer", "input (English text): We would like to list the models that have been evaluated on the MultiNLI dataset. \n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?model a mi:Model;?model_lbl a ml:Name. FILTER (str(?model_lbl) = \"MultiNLI\") } \n input (English text): MultiNLI is a natural language inference (NLU) model trained on over 10'000 examples from the English Penn Treebank. It has been trained on over 10'000 examples from the English Penn Treebank.  \n output (Sparql query):In order to evaluate a model, it must be exposed as a Sparql query.  This can be done by providing the dataset the model was trained on.  Here we are providing the sparql query that exposes the MultiNLI model and the name of the model which was trained on the MultiNLI dataset.\n output (Sparql query): SELECT?model?model_lbl WHERE {?model a mi:Model;?model_lbl a ml:Name. FILTER (str(?model_lbl) = \"MultiNLI\") }", "input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset\n output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset;?benchmark?cont } \n input (English text): \"Average negative log-likelihood (NLL),\" \"average precision (AP),\" \"average recall (AR),\" \"F1 score,\" \"sensitivity (SEN),\" \"specificity (SPC),\" \"micro F1 score (mF1)\",\"macro F1 score (MF1),\" \"positive predictive value (PPV),\" \"negative predictive value (NPV),\" \"accuracy,\" \"precision,\" \"recall,\" \"F1 score,\" \"sensitivity (SEN),\" \"specificity (SPC),\" \"micro F1 score (mF1)\",\"macro F1 score (MF1),\" \"positive predictive value (PPV),\" \"negative predictive value (NPV),\" \"accuracy}\n\n output (Sparql query):INSERT INTO \n\t\t\tdatasets(dataset_lbl,benchmark,paper,paper_lbl) \n\t\t\tvalues", "bench --> + Sequential MNIST Dataset: accuracy = 99.3%, recall = 99.8%, F1 score = 99.7% \n  rdfs:label     :Sequential MNIST Dataset\n\n Inputs: +\n -  a\n -  rdfs:label \"Sequential MNIST Dataset\"\n\n Query:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential MNIST Dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Kim Eun-hyung\n\nKim Eun-hyung (born March 1, 1989) is a South Korean professional baseball pitcher for the Kia Tigers of the KBO League. He bats and throws right-handed. He is 5 feet 11 inches tall and weighs 205 pounds.\n\nProfessional career\n\nSeoul Korail\nKim debuted with the Seong", "iter the following papers with code available:\n\n //Code to cite:https://github.com/ofo/ontoworkflow\n //Code to reproduce:https://github.com/ofo/ontoworkflow/blob/master/src/ontoworkflow/evaluation/mst.java\n\n output (Sparql query):\n\n  SELECT DISTINCT?paper?paper_lbl WHERE {\n  ?dataset a orkgc:Dataset; \n   rdfs:label?dataset_lbl. \n   FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n  ?benchmark orkgp:HAS_DATASET?dataset. \n  ?cont orkgp:HAS_BENCHMARK?benchmark. \n  ?paper orkgp:P31?cont; \n   rdfs:label?paper_lbl.\n  }\n\n input (English text): Benjamin Doe, Bernhard Engel, Rafael Malach, Christoph Siemens, Tobias Weltre, Mirko Vettori. (2019). Structured Information Grammars with Rewrite and Recursion. In Proceedings of the 51st Annual", "n?benchmark?benchmark_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT-single's entities and relations annotated corpus\") }\n\noutput:\nSPARQL query: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT-single's entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n  paper                                                                                  paper_lbl                                                                                  benchmark                                                                                       benchmark_lbl                                                                 \n  ----------------------------------------------------------------------- --------- ------------------------------------------------------------------------------------------- --- ------------------------------------------------------------------------------------------------- ------------------------------------------------- --------- --------- --------- ---------\n  P31 Using a distant supervision approach to detect bias in news headlines     P31_1     Using a distant supervision approach to detect bias in news headlines", "SELECT DISTINCT?paper?paper_lbl_label WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciTLDR's metrics\") } \n input (English text): Metrics used on the SciTLDR benchmark dataset include: Tf-Idf, Information Gain, Coarse-Grained F1, Lambda-R, Cohen's kappa, Spearman's rho, Homogeneity Index, Jaccard Index, Precision@K, Recall@K.\n output (Sparql query):text {\"P31\",\"HAS_BENCHMARK\",\"SciTLDR's metrics\"}", "nSelect DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-German model evaluation\")?model.?paper orkgp:P32?model; rdfs:label?paper_lbl. }\n\nThe following results are returned:\n# Papers that contain the benchmark Abstracts' entities and relations annotated corpus\n# The paper titles and IDs are listed below.\nINSERT INTO tmp_sparql_query (query_string, results) values \n'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }', \n '[\n  {\n    \"paper\":", "input (English text): CNN\n output (Sparql query): SELECT * FROM <Where Condition> WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-100\") }\n\n input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?\n output (Sparql query):[Standart Conversion Engine, Densen, LeNet, AlexNet, VGG-16, Inception-v4]\n\nocomplete (Sparql query): SELECT * FROM <Where Condition> WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-100\") }\n\n<|endoftext|>A kinetic study of dicarbonyl compounds at pH 7.4: relationship between dicarbonyl compounds and diabetic complications.\nDiabetes mellitus is a disease characterized by impaired glucose tolerance and insulin insensitivity, and is one of the leading causes of kidney and eye diseases, and cardiovascular disease worldwide. In our present study, we investigated the effect of human", "ition (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): RotoWire (Relation Generation) = 0.608536.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdf:type owl:NamedIndividual;?benchmark owl:sameAs?dataset.?cont rdf:type owl:NamedIndividual;?paper rdf:type owl:NamedIndividual;?paper_lbl rdf:type owl:NamedClass. }\n input (English text): The title of this paper is Benchmarking Abstracts' Entities and Relations Annotated Corpus via", "input (English text): The best performing model is the reacher model.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score metric\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nAnswer: SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score metric\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nExplaination: The best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric, is the reacher model.", "nomerge mdl = \"{urn:nsdl:mdl:assault:2016-01-29}assault2016-01-29.mdl\" with{urn:nsdl:mdl:assault:2016-02-29}assault2016-02-29.mdl;\n output (Sparql query): SELECT?paper WHERE {?paper mdl:model.}\n\n<|endoftext|>HomeAway is the world leader in vacation rentals. We offer the largest selection of properties for any travel occasion and every budget. We're committed to helping families and friends find a perfect vacation rental to create unforgettable travel experiences together.\n\nThis owner has been less than kind and returned to poor reviews for a lot of the same reasons. Some of the hot tub could have used some anti-bacterial chemicals and there was quite a bit of mold in the tub/shower area. The trampoline in the backyard also looked neglected. Overall, I would not recommend this rental and the owner.\n\nIt is very nicely decorated and clean. You have a little kitchenette, dining area and sitting area. The beds are very comfortable and the water hot tub is a wonderful bonus. You can make use of the kayaks", "net (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code references in papers that have used the DQNMMCe+SR model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nThe RDF Dataset for Benchmarking and Code Reference Sparql endpoint is now live at https://data.stellenbosch.ac.za/sabor/sparql. The endpoint allows querying all Sparql endpoints in the SDB on RDF datasets, and in particular the RDF datasets for Abstracts' entities and relations annotated corpus, code references in papers that have used the DQNMMCe+SR model for benchmarking purposes, and SDB benchmarks.\n\nQuerying the endpoint yields the following result set:\n\n*?dataset : a orkgc:Dataset\n*?dataset", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ball in cup, catch (DMControl100k) dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe answer is: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ball in cup, catch (DMControl100k) dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe SPARQL query returns 3 results, of which only 1 has the benchmark Ball in cup, catch (DM", "inate (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT-single dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?benchmark;?paper_lbl?paper_lbl. } \n input (English text): The name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset is.\n output (Sparql query):\u00c3\u00c2\u00c3\u00c2fecture (Sparql query):\u00c3\u00c2\u00c3\u00c2FIFTEEN (P31); particular (P31); nyt-single (P31)\n\n demanded output (Sparql query): encountered no matching documents for the provided query\n\n demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): https://www.dropbox.com/s/d0yytx2o4w7fd2m/memen.R?v=1&s=6 \n output (Sparql query):POST https://www.dropbox.com/s/d0yytx2o4w7fd2m/memen.R?v=1&s=6 HTTP/1.1 Host: www.dropbox.com Content-Type: application/sparql GET https://www.dropbox.com/s/d0yytx2o4w7fd2m/memen.R?v", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MMV TSM-50x2 model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): link to the code?\n output (Sparql query): These are the papers using the MMV TSM-50x2 model: https://drive.google.com/file/d/1uwM3uRFZHz8Ef9F2kQqdFWc-bU2EMh1f/view?usp=sharing\n\n[1] \n[\n {\n  \"paper\": \"P31\",\n  \"paper_lbl\": \"Cont\"\n },\n {\n  \"paper\": \"P33\",\n  \"paper_lbl\": \"ABSTRACTS' entities and relations annotated corpus\"\n }", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-5 benchmark dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark; rdfs:label?paper_lbl. } \n input (English text): The Yelp-5 benchmark dataset consists of a single class (entity/relation) that is annotated with an triple with the fields key, description, and recommender score.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark a orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper_lbl rdf:type sparql%rowtype. } \n input (English text): The Yelp-5 benchmark dataset consists of a single class (entity/relation) that is annotated", "'s the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?\n output (Sparql query):SELECT * WHERE {?model?benchmark ; BLDGT:Benchmark(metric=BLEU,title=best_model,metric_range=[0,100],model_id=?model). \n FILTER (?benchmark=?best_model).}\n\n<|endoftext|>Evaluation of the New Multi-target Strategy \"Folate + Raloxifene\" for Post-Menopausal Depression.\nDepression is one of the leading causes of disability worldwide, and according to the World Health Organization (WHO) has a high impact in women's health, being the second cause of disability worldwide, affecting about 350 million people and causing a huge burden on the society. Currently, many drugs are used for the treatment of depression, but the cognitive and side effects of the available drugs make it necessary the discovery of new therapies to improve the therapeutic efficacy and safety. Thus, our research group has developed a multi-target strategy using the natural drug folate, Raloxifene (RLX) and the natural product Vitispirifoliol (VPL), with the goal", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper skos:seeAlso?cont; rdfs:label?paper_lbl. }\n\nitos:Provide a list of papers that have utilized the SAC model and include the links to their code?", "iter what are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset?\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): <p>List of papers that have utilized the MEMEN model and include the links to their code:</p>\n output (Sparql query):<p> interconnected text</p>\n output (Sparql query):<p> P31. Abstracts' entities and relations annotated corpus dataset. Benchmark for orkgp:HAS_DATASET and orkgp:HAS_BENCHMARK.  P31. Cont. </p>\n output (Sparql query):<p> P", "input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?\n output (Sparql query): SELECT?paper_id ( distinct?paper?paper_lbl) WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?benchmark;?cont orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_P31?paper_id. }\n\n<|endoftext|>\u2018Real capping\u2019 and monetary growth provide lasting solutions to India\u2019s inflation\n\nWe expect the central bank to keep the policy rates stable at the coming review\n\n\u201cWe expect the RBI to keep the policy rates stable at the coming review\u201d is the mantra that the markets and economic pundits are now going on repeat as RBI meets and announces its monetary policy again this month. The RBI\u2019s inflation estimate range is now 0.5% \u2013 1.5% (vs 6.1% recorded in April FY\u201919), and will keep", "icit (English text): The model name is \"Atari 2600 Assault\"\n\n Gomez Abstracts' entities and relations annotated corpus dataset Benchmark has dataset: atari2600assault Benchmark has benchmark: atari2600assault\n\n Output of the query:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n?paper opl_atari2600assault.\n\n Local Variables Summary\n 1.?paper opl_atari2600assault\n\n Object Constraint Summary\n 1.?paper opl_atari2600assault IS a orkgp:HAS_DATASET\n\n Output of the query:\n 1. 2 \n 2. \n 2. Gomez Atari 2600 Assault\n\n Local Variables Summary\n 1.", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Best performing model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nAn example of a response:\n\n input (English text): \"Best performing model\" is an example of a benchmark for the \"Abstracts' entities and relations annotated corpus\" dataset.\n\n desired output (Sparql query): \"best performing model\"\n\nices (English text): \"Best performing model\" is an example of a benchmark for the \"Abstracts' entities and relations annotated corpus\" dataset.\n\n desirable output (Sparql query): \"best performing model\"\n\nemetery (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"", "input (English text): You can find the code references in the paper using the codeReference function from the Sparql query language.\n output (Sparql query): SELECT?codeReference WHERE {?paper?paper_lbl. } FILTER langMatches(codeLanguage, \"Sparql\")\n input (English text): Here is an example of a paper that has used the STREET model for benchmarking purposes. https://doi.org/10.1002/sem.21665\n\n \u00e9galement aucune mention de code reference dans le texte.\n R\u00e9ponse : \n SELECT?codeReference WHERE {?paper?paper_lbl. } FILTER langMatches(codeLanguage, \"Sparql\")\n\n<|endoftext|>Mentor exchange student days\n\nBut first, I want to tell you about an exchange student who became one of my best friends. We met in our first week of school. She had just moved to our country and was beginning to find her feet and make friends. She had a big personality, was very curious and fun to be around, and we clicked right away. She was from Vietnam, and we would speak to each other in Vietnamese. To be completely honest, we understood each other right away.", ":SELECT DISTINCT?model?model_lbl WHERE {?model enwiki8:Vocab --package edu.stanford.nlp.ie.owl.parallel.manning.vocab; rdfs:label?model_lbl. FILTER (str(?model_lbl) = \"Models evaluated on enwiki8\").} \n input (English text): Topic models.\n output (Sparql query):terson-gould:enwiki8-pos:Vocab; enwiki8:Parse; enwiki8:Tokens; enwiki8:Part-of-speech; enwiki8:Word-form; enwiki8:Synset; enwiki8:Word; enwiki8:Word-form; enwiki8:Lemma; enwiki8:Part-of-speech-prefix; enwiki8:Part-of-speech-postfix; enwiki8:Numeric-phrase; enwiki8:Numeric; enwiki8:Numeric-phrase-distance; enwiki8:Numeric-phrase-unit; enwiki8:Ordinal-phrase; enwiki8:Ordinal-phrase-distance; enwiki8", "input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset\n\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset\n\n output (Sparql query):; Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset\n\n output (Sparql query):SELECT?paper (?paper_lbl as?paper_lbl_val) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC dataset\")", "#{\n  \"@context\" : {\n    \"a\" : {\n      \"@id\" : \"http://orcid.org/ns#Object\"\n    },\n    \"b\" : {\n      \"@id\" : \"http://marko.ai/ns/hasDataset\"\n    },\n    \"benchmark\" : {\n      \"@id\" : \"http://marko.ai/ns/hasBenchmark\"\n    },\n    \"dataset\" : {\n      \"@id\" : \"http://marko.ai/ns/Object\"\n    },\n    \"cont\" : {\n      \"@id\" : \"http://marko.ai/ns/hasBenchmark\"\n    },\n    \"paper\" : {\n      \"@id\" : \"http://marko.ai/ns/Object\"\n    }\n  }\n}\n#}\n\n\n# The following SPARQL query retrieves the top-5 papers that best\n# adhere to the benchmark specification.\n\n# Write an SPARQL query to return the top-5 papers that best adhere to the benchmark specification.\n\n# SELECT", "sel(str)?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Evaluation metrics used when benchmarking models on the SQuAD2.0 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nThe answer is the following:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Evaluation metrics used when benchmarking models on the SQuAD2.0 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n<|endoftext|>Protective effects of hesperidin against radiation-induced intestinal injury via anti-inflammatory", ":SELECT?paper_lbl?benchmark_title?cont_title WHERE {?dataset a orkgc:Dataset;?paper_lbl rdfs:label?paper_lbl. FILTER (str(?paper_lbl) = \"Abstracts' entities and relations annotated corpus\")?cont orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl rdf:label?paper_lbl. } \n\nThe answer is the following :\n?paper rdf:type http://www.semanticweb.org/wiki/KGs/RDF/1.1/Property#;\n?paper_lbl rdf:type http://www.semanticweb.org/wiki/KGs/RDF/1.1/PropertyLabel;\n?benchmark rdf:type http://www.semanticweb.org/wiki/KGs/RDF/1.1/HasDataset;\n?benchmark_title rdf:datatype xml;\n?cont_title rdf:datatype xml", "map (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \nrelated research papers are: \nP31 - https://www.cs.cmu.edu/afs/cs/project/semeval-2017/www/task1/papers/P31-Penn-NLP-ACL17.pdf\nP22 - https://www.cs.cmu.edu/afs/cs/project/semeval-2017/www/task1/papers/P22-Cooke-NLP-ACL17.pdf\nP21 - https://www.cs.cmu.edu/afs/cs/project/semeval-2017/www/task1/papers/P21-Farou", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n screened input (English text): \"CONLL04 Benchmark\"\n output (Sparql query):\u094bns:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n<|endoftext|>/*\n * (C) Copyright 2000-2002\n * Wolfgang", "WITH \n  haspapers as \n  (\n    SELECT?paper?paper_lbl\n    FROM <original-query>\n    where <condition>\n  )\nreturn\n{\n  \"haspapers\": [\n    {\n      \"paper\": \"P31\",\n      \"paper_lbl\": \"Abstracts' entities and relations annotated corpus\"\n    }\n  ]\n}\n\n\nThe output of the previous query is as follows:\nSELECT DISTINCT?paper?paper_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nOriginal query text: Provide a list of papers that have utilized the XLNet (base) model and include the links to their code\n\nConverted query text: SELECT DISTINCT?paper?paper_lbl", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nSparql query result:\n coach_benchmark.P31:coach benchmark\n coach_benchmark.P31:coach_lbl benchmark\n coach_benchmark.P31:coach_lbl_lbl coach\n coach_benchmark.P31:dataset dataset\n coach_benchmark.P31:dataset_lbl dataset\n coach_benchmark.P31:cont cont\n coach_benchmark.P31:cont_lbl cont\n dataset.rdfs.label:Abstracts' entities and relations annotated corpus\n dataset_lbl.rdfs.label:Abstracts' entities and relations annotated corpus\n benchmark.rdfs.label:HAS", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NYT29 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): <p>The following research papers have been tested on the NYT29 benchmark dataset:</p>\n <ul>\n <li><a href=\"http://amzn.to/2rJpEee\">Hitting the Human Adversarial Space: Training a Classifier to Detect Successful Attacks</a> by Shreyas Sidhkant Acharya, Devang Shah, Siva Reddi, Arvind Sabass, Quoc V Le, Girish Khauntretseva, et al.</li>\n <li><a href=\"http://amzn.to/2rxTtCj\">What Factors Matter for Training Neural Networks to Improve Class Com", ":P31,P25,P39 \n input (English text): Using IDs, select 3 the most common variables for the atmosphere models: P31, P25 and P39.\n output (Sparql query):\n SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P25?cont;?paper orkgp:P39?cont. }", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): http://static.elsa.kyutech.com/ddqnpc/ddqnpc.model.h2o.all.pkl#/paper/0 /paper/0/cont/oracle/0/paper_lbl/DDQNPCTrainingOnOracle/0.0.1/paper_lbl/Abstracts' entities and relations annotated corpus\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated", "input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TREC-6 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?\n output (Sparql query): Chapel grove ingrams, et al., 2011. Annotating Linked Data with Entity Linking. Association for Computational Linguistics, pages 1-14.\n Chapel grove ingrams, A., Fabian, T., and Brewer, M., 2011. Annotating Linked Data with Relation Linking. Association for Computational Linguistics, pages 1-14.", "input (English text): https://github.com/RDOQN/Rational-DQN-Average-Benchmark\n input (Sparql query): SELECT?paper (?paper_lbl as?paper_lbl_val ) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rational DQN Average model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nA Sparql query that returns the Rational DQN Average model benchmark from code in RDOQN/Rational-DQN-Average-Benchmark.", "inal (Sparql query): The name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset is CNN (Convolutional Neural Network).\n\n sparql 1.1 query\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-French dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Q:\n\nHow do I reverse engineer this stack memory dump?\n\nI am trying to reverse engineer this memory dump:\n2192c0 <+0x000000002119de0>\n2192c0: 8bff ffff 0001 5c0c 0560 7c10 4418 7a14 0001 5c1c 032d fff0 0000\n2192d0", "corenlp model has a 1-of-100 accuracy on the PolyAI Reddit benchmark dataset, and the model identifier is :P31.\n input (English text): Which datasets were included in this research paper\u2019s benchmark for the Abstracts entities and relations annotated corpus dataset?\n output (Sparql query): rdfs:label of the datasets that were included in the benchmark for the Abstracts entities and relations annotated corpus dataset was 'Datasets that were included in this research paper's benchmark for the Abstracts entities and relations annotated corpus dataset'. The dataset identifiers were as follows: rasa_10_reviews, rasa_20_documents, rasa_30_sentences.\n\nsplit=sentences, documents, reviews, coresnstr\n\n for d in rasa_10_reviews rasa_20_documents rasa_30_sentences : do \n     SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?review?review_lbl. FILTER (str(?review_lbl) = \"PolyAI Review dataset\")?paper_lbl. } \n     output (Sparql query):rdfs:label of the research", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }?paper?paper_lbl\n\n\nThe code links in papers that use the Long Short Transformer model in any benchmark:\n\n[https://www.microsoft.com/en-us/research/spotlight-blog/first-ai-model-can-spot-important-relations/](https://www.microsoft.com/en-us/research/spotlight-blog/first-ai-model-can-spot-important-relations/)\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities", "input (Sparql query):.\n limit 1.\n\n #\u2009\u2009SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\u2009\u2009\u2009\u2009SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TriviaQA dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark rdf:type\u200a<http://www.w3.org/2009/04/11-tutorial-on-RDFLessary.html#HAS_DATASET", "SELECT DISTINCT \n   ?paper?paper_lbl \n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Fossil fuels are a widely used energy source that are known to cause global warming. The greenhouse gases emitted by the burning of fossil fuels are believed to have contributed to climactic changes in the past. To address this concern, many nations have implemented various policies intended to reduce the use of fossil fuels. Examples of policies that reduce the use of fossil fuels include incentives to purchase electric cars, as well as government funding to develop renewable energy sources. Nonetheless, these policies have not been sufficient to completely eliminate the use of fossil fuels. This lack of complete elimination of the use of fossil fuels has caused concerns over the issue of climate change.\nOther attempts have been made to reduce the impact of fossil fuels.", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nThe highest benchmark result for the DTD dataset is: \nSELECT * \nWHERE {\n ?dataset a orkgc:Dataset ;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"DTD dataset\" )\n ?benchmark orkgp:HAS_DATASET?dataset.\n ?cont orkgp:HAS_BENCHMARK?benchmark.\n ?paper orkgp:P31?cont;\n  rdfs:label?paper_lbl.\n}\n\n\n[\n  {\n    \"paper\": \"P31\",\n    \"paper_lbl", "input (English text): The best score is 65.04 and it is a metric of 0.8893664, which is \"pTopK\".\n\n export (Sparql query): \nPREFIX orkgc:  <http://www.ontologi.us/dc- corrid/ontology.dc?at=default> \nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf> \nPREFIX sparql: <http://www.w3.org/2001/sparql-srv/sdks/sparql> \n\nSELECT DISTINCT?paper?paper_lbl \nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval-2018 Task 7 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe following are the ID labels of the research papers that include a benchmark for the SemEval-2018 Task 7 dataset:\n\n input (English text): The following are the ID labels of the research papers that include a benchmark for the SemEval-2018 Task 7 dataset: P31, P32, P33, P34, P35, P36, P37, P38, P39, P40.\n\n output (Sparql query): The following are the ID labels of the research papers that include a benchmark for the SemEval-2018 Task 7 dataset: P31, P32, P33, P34, P35, P36, P37, P38, P39, P40.", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark gbm:AG_News_Benchmark; rdfs:label?benchmark_lbl. FILTER (str(?benchmark_lbl) = \"AG News benchmarks\")?benchmark.?paper gbm:P31?benchmark;?dataset_lbl a rdf:Label;?paper_lbl rdf:label. } \n input (English text): The AG News benchmarks are defined in the GBSGM paper (https://www.semanticscholar.org/paper/GBSGM%20-%20On%20the%20Detection%20of%20Associational%20Graphs/c4dff7b8c4711e96c75ff1836ba0b831). The corresponding AG_News_Benchmark metric is defined as follows: AG_News_Benchmark (BLUEJump, GraRep) : The average number of adjacent bluejumps required to update a typical subgraph to a state that does not contain the initial subgraph as a member. \n output", ":the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset is\n\n database-centric model trained on largeWikiText-10 databases and the CoNLL 2013 shared task data.\n\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Disease dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The top performing model for the Abstracts' entities and relations annotated corpus benchmark on the NCBI Disease dataset is the database-centric model trained on largeWikiText-10 databases and the CoNLL 2013 shared task data.\n\n\nNote that this model may not be available in the SPARQL endpoint at this time. Please visit the Model and Dataset Repositories page for details on how to acquire it.\n\n\n<!-- \nThere are two datasets that are", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\n+\nThe list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset are as follows:\n+\n<!--Please provide the titles and IDs of the research papers in the form of a single line of text in square brackets. Line breaks will be removed. Additionally, please provide the models and dataset ID (e.g., \"P31\").-->\n+\n[1] [2] [3] [4] [5] [6] [7] [8] [9] [10]\n\n\n[1] <http://nlp.stanford.edu/data/glove.5B.zip> [P31]\n\n\n[2", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\n Jennings et al. (2018) presents the code used to benchmark the GCN Hybrid model: https://github.com/jennings-lab/Benchmark_Paper \n\nJennings et al. (2018) provides the code for 3 benchmark datasets: \n\n* Dataset with Direct Relationship Annotation:  `https://github.com/jennings-lab/DatasetWithRelationshipAnnotation`\n* Dataset with No Relationship Annotation:  `https://github.com/jennings-lab/NoRelationshipAnnotations`\n* Dataset with Relationship Annotation but No Annotated Entities:  `https://github.com/jennings", "input (English text): csv \n output (Sparql query): SELECT * WHERE { <chemdner> dc:source <urn:epub:CHEMDNER> dc:language English <urn:xsd:string> } \n input (Sparql query): contributed_data\n output (Sparql query): contrib CCDVer14.ENGLISH.CHEMDNER.csv\n\n###### Abstracts\u2019 entities and relations annotated corpus benchmark.\nrdfs:label             Abstracts\u2019 entities and relations annotated corpus\nogp:HAS_DATASET          chemdner\nogp:HAS_BENCHMARK       chemdner\nogp:P31                  chemdner\nrdfs:label             CHEMDNER\nrdfs:label             Abstracts\u2019 entities and relations annotated corpus benchmark\n\n###### CHEMDNER corpus.\nchemdner\ncd:source\nchemdner\ndc:language\nEnglish\nurn:epub:CHEMDNER\nurn:xsd:string\n\n###### CHEMDNER corpus benchmark.\nrdfs:label             Abstracts\u2019 entities and relations annotated corpus\nogp:HAS_DATASET          chemdner\nogp:HAS_BENCHMARK", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a cub:Dataset;?dataset_lbl owl:ObjectProperty?dataset_lbl;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P34?cont;?paper_lbl owl:ObjectProperty?paper_lbl. }\n\nThe above query will list down the following papers: \nCUB-200-2011 \nPapers with the benchmark CUB-200-2011.\n\n\u00a0\n \u00a0\u00a0CUB-200-2011\n\n\u00a0\n\nPaper A has the property HAS_DATASET CUB-200-2011\n\n\u00a0\n \u00a0\u00a0CUB-200-2011\n\nThe Benchmark property of the dataset CUB-200-2011 is contained in the property BENCHMARK CUB-200-2011.\n\n\u00a0\n \u00a0\u00a0CUB-200-2011\n\n\u00a0\n\nPaper B has the property HAS_BENCHMARK CUB-200-2011\n\n\u00a0\n \u00a0\u00a0CUB-200-2011\n\n\u00a0\n\nPaper A has the property P31 CUB-200-2011", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Here is the link to the code: https://github.com/UCIS-RE/SEDAM_TUTORIALS/blob/master/TextExpander/Application/DetectDanglingSpor phrases.sparql \n output (Sparql query): { \"paper\" <http://purl.org/sparql/funotation#Script> \"paper_lbl\" <http://purl.org/sparql/funotation#Label> \"Sparql query\" }\n\n Biomedical Natural Language Processing (BioNLP) is a joint project of the United Kingdom's National Health Service and the United States National Institute of Allergy", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): https://raw.githubusercontent.com/dblagle/duelling-algorithms-for-sparql/master/DUEL_papers_list.sql \n output (Sparql query): LOAD DATA INFILE 'raw\\DUEL_papers_list.sql'\n INTO PUBLIC\n  FOR\n  SELECT?paper?paper_lbl\n  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont or", "selector (Sparql query):?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?benchmark;?cont orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_P31?paper_lbl } \n input (English text):  The top benchmark result for the dataset DocRED (Human-annotated) is to find the 15% of the words that occur the most often. The percentage is 15%, so the value is 0.15.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?benchmark;?cont orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_P31?paper_lbl } \n input (English text): \n<|endoftext|>#!/bin/sh\n\n##############################################################################\n##\n##  Gradle start up script for UN", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe following are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset:\n\n vistaan/SIGIR'18/Abstracts' entities and relations annotated corpus dataset/jordan-etal-2018\n\n vistaan/sigir-2018/has_dataset/edaac (link to code)\n\n vistaan/sigir-2018/has_benchmark/edaac (link to code)\n\n vistaan/sigir-2018/paper/p31/edaac (link to code)\n\n vistaan/sigir-2018/benchmark/has_dataset/edaac (link to code)\n\n vistaan/sigir-2018/", "input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a atkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Solaris benchmark\")?benchmark.?paper atkgp:P31?benchmark.?paper?cont;?cont a atkgp:ObjectPropertyExpr;?cont b atkgp:ObjectPropertyExpr;?cont c atkgp:ObjectPropertyExpr;?cont d atkgp:ObjectPropertyExpr. } \n input (English text): What models are being evaluated on the Atari 2600 Solaris benchmark?\n output (Sparql query): INCLUDE <Raw Facts file contents>.atgjr20.atkgc20.Dataset.atkgp31.Benchmark.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SNLI dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the IDs and titles of research papers that include a benchmark for the SNLI dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SNLI dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Give me a list", "input (English text): The MMV model can be found here: https://www.semanticweb.org/projects/mmodel/mmv. SPARQL query: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MMV model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): In the code references, you can find the code that was used for the evaluation of the model.\n output (Sparql query):The code that was used for the evaluation of the MMV model can be found in the sub-section \"Benchmarking using the MMV model\" of this paper: https://arxiv.org/abs/2003.10734. \n\nmonsieursebastien.bergmann@sfr.fr\n\n\n# \n# Manually Annotated Semantic Web Research Papers with Entity and", "mark P31 Atari 2600 Centipede\n\n output (Sparql query):P31\n\n output (Sparql query):Centipede: 68.83\n\n sparql query from java:\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; \n rdfs:label?paper_lbl. }\n\n<|endoftext|>The BBC has today commissioned two new 24-minute docudramas based on the hit US show, set to debut in 2020.\n\nBBC One is bringing back the iconic drama series with a follow-up called \u201cMission: Crime Scene Investigation.\u201d The six-episode miniseries tells the story of Blythe Danner\u2019s character CSI Grace Kurtz, who is thrust into the unexpected position of Detective Superintendent on the New York City Police Department\u2019s Hostage Rescue", "(Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MNIST dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): What is the model name that is benchmarked on the MNIST dataset?\n output (Sparql query):?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MNIST dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): The paper identifier is?paper and the benchmark identifier is?bench", ":Where took place the study with maximal geographic scale\n\n\nThe title of the research paper is \"Where took place the study with maximal geographic scale\". The benchmark consists of the relation \"HAS_DATASET\" and two entities, \"Abstracts' entities and relations annotated corpus\" and \"P31\". The selected paper has the label \"P31\".\n\nGCP Documentation: https://cloud.google.com/compute/docs/data- remot-source/gcp-hosted-data\n\n\nSo, the following query retrieves only the papers that include the relation \"HAS_BENCHMARK\" and the entities \"Abstracts' entities and relations annotated corpus\" and \"P31\".\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark; \nrdfs:label?paper_lbl. }\n\n\nOutput:\n\nPapers selected", "input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nA:\n\nQLover 3.0.2 uses OWL API 2.0 + Jena API 2.10.1 for ontology construction.\nAs there are two versions of the ontology it is possible that one of the titles (e.g. Dataset) does not exist anymore in OWL API 2.0 and has been replaced in OWL API 2.10.1.\nIf you find such a title you could try the following query:\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \nPREFIX owl: <http://www.w3.org/2002/07/owl", "<rdf:Description xmlns:orkgp=\"http://www.w3.org/ TrackContext/orkgp#\">\n <rdf:value xml:lang=\"en\"formatter=\"org.openreallelement.saka.sparql.formatters.JSONFormat\" >SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }</rdf:value>\n </rdf:Description>\n input (English text): TempEval-3 dataset\n\n<|endoftext|>Category Archives: personal finance\n\nPost navigation\n\nFord Motor Company CEO, Mark Fields, took a leave of absence from the company this year, to spend more time with his family. He has four children under the age of ten, and has been a stay at home dad since the birth of", "- SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P32?cont;?model_lbl rdf:type?model. } \n input (English text):  The models tested on the SciERC benchmark dataset are: P32. \"TED Talk Representations with BERT for Semantic Role Labelling\".\n output (Sparql query):  P32:TED Talk Representations with BERT for Semantic Role Labelling\n\n\n\n                                              - SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciERC benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark", ":70% appER ctx PP;\n\n  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n ?paper?paper_lbl {\n   ?dataset a orkgc:Dataset;\n    rdfs:label \"Abstracts' entities and relations annotated corpus\"@en;\n    FILTER (str(\"Abstracts' entities and relations annotated corpus\") =?benchmark)\n   ?benchmark orkgp:HAS_DATASET?dataset;\n   ?cont orkgp:HAS_BENCHMARK?benchmark;\n   ?paper orkgp:P31?cont;\n    rdfs:label \"Abstracts' entities and relations annotated corpus\"@en;\n  }\n\n output (Sparql query", "nit?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German dataset\")?benchmark. }\n\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 English-German dataset\")?benchmark.?cont orkgp:HAS_DATASET?dataset.?paper orkgp:P31?benchmark.?cont; rdfs:label?paper_lbl. } \n input (English text): The best performing research models in the WMT2014 English-German dataset included: GPT-2, BERT, XLNet, RoBERTa, transformer-XL, Wavenet, and XLNet.\n\n<|endoftext|>Requiem for a Gangster\n\nRequiem for a Gangster (2008) is a semi-autobiographical film written and directed by Phillip Seymour Hoffman and stars Hoffman, Al Pacino, Diane Ke", "pend (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp-14 dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric;?metric?value.?paper orkgp:P31?cont;?cont ;?paper_lbl?paper_lbl. } \n input (English text): The Yelp-14 dataset is available at https://web.stanford.edu/class/2016/spring/Yelp-14.html and includes about 31 million reviews, 385,000 topics and the corresponding topics' entities and relations.\n\nThe following paper is one that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset:  https://cce.csail.mit.edu/wp-content/uploads/2015/05/acl15-hartwig-reify-bench.pdf\n\nBased", ":List the code links in papers that use the A3C FF (1 day) hs model in any benchmark\n\novanni-garlise@usc.edu\nborges@uclouvain.csiro.au\n\n sparql query:\nSELECT DISTINCT \n?paper?paper_lbl \nWHERE \n { \n?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset. \n?cont orkgp:HAS_BENCHMARK?benchmark. \n?paper orkgp:P31?cont; \n rdfs:label?paper_lbl. \n }\n\n<|endoftext|>Q:\n\n\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u0438 \u0434\u0432\u0443\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n\n\u0423\u0432\u0430\u0436\u0430\u0435\u043c\u044b\u0435 \u043e\u0431\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0437\u0430 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c.\n\u041d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u0437\u0430\u0434\u0443\u043c\u043a\u0430 \u0434\u043b\u044f \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0438\u0437", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n   ?paper?paper_lbl\n\n    paper:P31 \"Abstracts' entities and relations annotated corpus\"\n\n    A model that has been tested on the RTE benchmark dataset is named P31. It was implemented in the tool \"ont-Toolbox\".\n\n    paper:P31 \"ont-Toolbox\"\n\n    The tool \"ont-Toolbox\" implementing model P31.\n\n    paper:P31 \"ont-Toolbox\"   \n\n    The tool \"ont-Toolbox\" implementing model P31.\n\n    paper:P31 \"ont-Toolbox\"\n\n    The tool \"ont-Toolbox\" implementing model P31.\n\n    paper:P31 \"ont", "SELECT DISTINCT \n ?paper?paper_lbl \nWHERE \n { \n  ?dataset a orkgc:Dataset; \n   rdfs:label?dataset_lbl. \n   FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n  ?benchmark orkgp:HAS_DATASET?dataset. \n  ?cont orkgp:HAS_BENCHMARK?benchmark. \n  ?paper orkgp:P31?cont; \n   rdfs:label?paper_lbl. \n } \nORDER BY \n  ?benchmark?metric \nLIMIT 5\n\n<|endoftext|>Molecular cloning of a calcium channel gene from the giant panda, Ailuropoda melanoleuca: evolutionary implications for nonmammalian vertebrates.\nWe have characterized a novel calcium channel gene, which we named calcium channel-related gene 2 (canrel2), from the giant panda, Ailuropoda melanoleuca. The canrel2 gene consists of 4641 nucleotides encoding 1199 amino acids. The deduced amino acid sequence shows strong homology to the calcium channel alpha1G subunit from rat (97.5%), mouse (95.", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n { \n ?paper http://www.seas.ucla.edu/research/sparql/publications/petridish/P31.html#paper_lbl, \"P31 Benchmark\"@en, \"P31 Benchmark\"@en \n ?paper_lbl  http://www.seas.ucla.edu/research/sparql/publications/petridish/P31.html, \"Abstracts' entities and relations annotated corpus\"@en, \"Abstracts' entities and relations annotated corpus\"@en }\n\nipeline\nname: Sparql job\ninputs:\n- keyword: research papers that include a benchmark for the Abstracts", "input (English text): SELECT DISTINCT?metric_name WHERE {\n\t?metric_name?metric.\n}\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----\n\n -----", "input (English text): [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name] \n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (short)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name] \n output (Sparql query): [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric_str.?benchmark?score_str. } \n\nAbstracts\u2019 entities and relations annotated corpus (WMT2014 French-English): \nsemeval-v0.1-r8.gz\n\n<|endoftext|>Q:\n\n\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u0430 =?\n\n\u041d\u0443\u0436\u043d\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 \"equals\", \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b \u0432\u044b\u0433\u043b\u044f\u0434\u0435\u043b \u0442\u0430\u043a\npublic boolean equals(Object o) {\n    return (this == o)? true : false;\n}\n\n\u042f \u043f\u043e\u043d\u0438\u043c\u0430\u044e, \u0447\u0442\u043e \u043e\u0448\u0438\u0431\u043a\u0430 \u0441\u043a\u043e\u043c\u043f\u0440\u043e\u043c\u0438\u0441\u0438, \u0442\u0430\u043a \u043a\u0430\u043a, \u043f\u043e \u0441\u0443\u0442\u0438, equals \u0431\u0443\u0434\u0435\u0442", "itation (WikiText-2 benchmark dataset):SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WikiText-2 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark o cra:BenchmarkScore ; rdfs:label?paper_lbl. } \n\n<|endoftext|>Entrance fee required for entry into the museum.\n\nPrices\n\nEntrance fees are non-refundable, but the Museum accepts debit cards and cash.\n\nFees are as follows:\n\nAdult $5\n\nSenior $4\n\nChildren 2-12 $3\n\nInfants under 2 free\n\nGuided Tours\n\nGuided tours of the museum are given several times a day.\n\nGuided tour times are as follows:\n\n9:00am - 11:00am\n\n11:30am - 1:30pm\n\n2:30pm - 4:00pm", "ition (Sparql query): SELECT?paper (rdfs:label?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nA query that returns the code references from the papers that use the All-attention network - 18 layers model for benchmarking purposes.\n\nA:\n\nHere are the IDs of the research papers that contain the following code references:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkg", "Cosine similarity\n\n Sparql execution :\n\n SELECT DISTINCT \n?paper?paper_lbl WHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n ?benchmark orkgp:HAS_DATASET?dataset. \n ?cont orkgp:HAS_BENCHMARK?benchmark. \n ?paper orkgp:P31?cont; \n  rdfs:label?paper_lbl. \n }\n\n Output:\n\n      paper  paper_lbl   \n\n    Disambiguating authors in citations on the web and authorship correlations     P31 Abstracts' entities and relations annotated corpus\n\n<|endoftext|>Q:\n\nLeft join between two tables and getting null values from one table\n\nI have two tables like below\n  TABLE1 \n  ID     NAME\n  1     ABCD\n  2     XYZ\n  3     PQR\n  TABLE2\n  ID     NAME     JOINING_TABLE\n  1     ABC      TABLE1\n  2     XYZ      TABLE1", "input (English text): The models that have been evaluated on the STS Benchmark dataset are: PyBERT, Attentio, Exalt, DistilBERT, SamBERT, ELMo, XLNet, OpenAI GPT, XLM.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PyBERT, Attentio, Exalt, DistilBERT, SamBERT, ELMo, XLNet, OpenAI GPT, XLM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?\n output (Sparql query): ction (English text): The models that have been evaluated on the STS Benchmark dataset are: PyBERT, Attentio, Ex", "sparql \nSELECT DISTINCT?paper?paper_lbl WHERE { \n?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n?benchmark orkgp:HAS_DATASET?dataset. \n?cont orkgp:HAS_BENCHMARK?benchmark. \n?paper orkgp:P31?cont; \n rdfs:label?paper_lbl. \n }\n\n<|endoftext|>Chemical identification of eight polymorphic substances in Yuyehao tea.\nEight polymorphic substances in Yuyehao tea were identified by physicochemical methods and gas chromatography-mass spectrometry (GC-MS). The results show that five of the eight substances are gallic acid derivatives, and the other three are quercetin derivatives. The gallic acid derivatives are CGA (2, 3, 4-trihydroxybenzoic acid), CGA2,3,4-tetrahydroxybenzoic acid, CGA3,4-dihydroxybenzoic acid and CGA4,5,3',4'-tetrahydroxybenzoic acid. The qu", "Select distinct?paper?paper_lbl where {?dataset a dcase:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DCASE dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nNote: You can use the advanced search form to specify any of these criteria. Alternatively, you can copy/paste the query and apply it on the DBpedia dump.\n\n\n### 1. SELECT DISTINCT?paper?paper_lbl WHERE \nThese query contains two SPARQL variables:\n-?paper: the question is about which paper we are looking for\n-?paper_lbl: the label of the paper we are looking for.\n\nThe paper must contain the string \"Abstracts' entities and relations annotated corpus\" in the benchmark property.\n\nNote that the dataset and the benchmark are directly linked in this query using the HAS_DATASET and HAS_BENCHMARK properties.\n\n\n### 2", ":Common evaluation metrics when benchmarking models on the Yelp Fine-grained classification dataset\noutput (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Yelp Fine-grained classification dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n input (English text): Common evaluation metrics when benchmarking models on the Yelp Fine-grained classification dataset include the F1-score, Recall, and Precision.\n\n output (Sparql query): orche:F1-score, precision, recall", "ination 2 achieved the highest score on the Atari 2600 River Raid benchmark dataset.\n input (Sparql query):\nSELECT * \nWHERE { \n   ?paper a rdf:Resource; \n    rdfs:label?paper_lbl; \n   ?benchmark orkgp:HAS_DATASET?dataset; \n   ?benchmark orkgp:HAS_BENCHMARK?benchmark; \n   ?paper orkgp:P31?cont; \n    rdfs:label?paper_lbl;\n}\n\n<|endoftext|>Step into my very secret - and dirty - blog and you'll find all sorts of kinky shit happening. From reliving my sex life with medical professionals to philosophizing about life, love, and the divine feminine. Discover the kinks, the kookiness, and the lessons I'm learning.\n\nMonday, June 18, 2007\n\nHow we learned to love physical touch.\n\nI didn't notice that Kelly was watching me while I showered. She was laying on the bathroom floor and I must have assumed she was meditating. I suppose she probably was. She was letting her fingers play along my body and tracing them around my arm and shoulders.", "ini (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset a orkgp:has_benchmark?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe Output should contain: \n*?paper (values: {...}, label: {paper_lbl})\n*?paper_lbl (values: {Abstracts' entities and relations annotated corpus}, label: {paper_lbl})\n\n\nThe input text for this example is provided in the comments.\n\n\nSparql 1.1 Query Language, SPARQL Protocol and RDF Data Basics, https://www.w3.org/TR/rdf-spirit/\n\n\nSELECT DISTINCT?paper?paper_l", "derussel (English text): P31\n\n MVP: P31\n\n ODRL: Abstracts' entities and relations annotated corpus\n\n  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n ?paper     Abstracts' entities and relations annotated corpus_lbl\n ?paper_lbl Abstracts' entities and relations annotated corpus\n\n ?paper       P31\n\n ?cont       Abstracts' entities and relations annotated corpus\n\n ?benchmark Abstracts' entities and relations annotated corpus_lbl\n\n ?cont       Abstracts' entities and relations annotated corpus_lbl\n\n ?benchmark Abstracts' entities and relations annotated corpus_lbl\n\n ?dataset    orkgc:Dataset\n\n ?dataset_lbl MedSTS", "text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WSC dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Every month, over 10 million people lose their privacy to what are known in the UK as \"phone hacking scandals\". These have been caused by the private communications of people being recorded by private companies without their knowledge or consent. These scandals have exposed the shocking extent of how private information is vacuumed up, and the risks this poses to people's privacy and personal safety.\n\nThis latest revelation is only the latest in a long line of phone hacking scandals. These scandals have cost private companies millions of pounds in settlements paid to families of people whose information was recorded without their knowledge. These scandals have also thrown up questions about why senior figures at British companies knew what was going on, but didn't stop it, and whether they", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n input (English text): https://www.ldc.upenn.edu/Catalog/x Index=doi/10.36028/MFJC.2019.0304.0?index=doi/10.36028/MFJC.2019.0304.0\n output (Sparql query):s using the \"(has_dataset)\" keyword and \"(has_benchmark)\" keyword\n\nA:\n\nIn order to get the code links you need to apply a pipe or \"|\" operator between your query and the response.\nHere is a complete example of a Sparql query that should get you the code links you need : \nSELECT DISTINCT?paper?paper_lbl", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } proced=?paper?paper_lbl proced=proced.\n input (English text): https://www.semanticweb.org/benchmark/data-annotations/files/p31.zip\n output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }", "::= sparqlquery \n query=\"SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\"\n\nA:\n\nI believe your question is\n\nSelect a research paper that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset.\n\nSo your query should be\nSELECT DISTINCT?paper WHERE {\n ?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n ?benchmark orkgp:HAS_DATASET?dataset.\n ?cont orkgp:HAS_BENCHMARK?benchmark.\n ?paper orkgp", ":Commonly used evaluation metrics when benchmarking models on the ARC-PDN dataset\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Commonly used evaluation metrics when benchmarking models on the ARC-PDN dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe following sparql query returns the following results: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rd", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'\n\n input (SAS macro variables): dataset; dataset_lbl; benchmark; cont; paper; paper_lbl\n\n output (SAS table): No results found\n\n ERROR:ORA-29577: could not find a matching artifact with the provided id 'P31'\n\n gravatar:https://avatars2.githubusercontent.com/u/2533125?v=4\n\n gravatar:https://avatars3.githubusercontent.com/u/2533125?v=4\n\n gravatar:https://avatars5.githubusercontent.com/u/2533125?v=4\n\n gravatar:https://avatars6.githubuser", "input (English text): Common evaluation metrics for the Berzerk dataset are the following: %normalized-return, %RT, %avg-count, Average Sampling Rate (ASR), Average Time to Excellent (ATT), and Average Mature Age (AMA).\n\n output (Sparql query): SELECT * {?paper?paper_lbl ;?paper :%normalized-return %. One metric commonly used when evaluating ATARI 2600 Berzerk models is the normalized return, which indicates the proportion of hits to non-hits. Return values closer to one indicate better model performance. Another metric is the average sample rate, which indicates how quickly the model samples the game board and is useful for situations where the total game state is unknown but a finite amount of information about the game state is known beforehand. Average time to Excellent (ATT) and Average Mature Age (AMA) are metrics that compare the model\u2019s performance against the optimal on the Atari 2600 Berzerk dataset.", "; SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Abstracts' entities and relations annotated corpus corpus dataset?\n output (Sparql query):onte; SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:", "input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus corpus dataset?\n output (Sparql query):                             input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus corpus dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cars' performance over the Stanford Cars Dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The following research papers include the benchmark \"Cars' performance over the Stanford Cars Dataset\".\n\n input (Sparql query):?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cars' performance over the Stanford Cars Dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset a orkgc:Dataset; rdfs:label?dataset_", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Q:\n\nContinuity of function at boundary implies continuity\n\nLet $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a function.  Suppose that $f(x) \\to f(x')$ as $|x-x'| \\to 0$ for each $x$ where $x$ is in the domain of $f$, i.e. $x$ is in the domain of $f(x)$, and that $f(x)$ is continuous at $x$.  Does it follow that $f$ is continuous at the boundary of the domain?\nMy thoughts so far: suppose $f$ is not continuous at the boundary of", "ir input (English text): GdFeCo\n\n Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo\n\n Sparql query response: SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n Paper:\n\n Paper ( RDFS:label )\n\n Paper ( P31rdfs:label )\n\n Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo\n\n Sparql query response: NULL\n\n Paper ( RDFS:label GdFeCo )\n\n Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo\n\n Sparql query response: GdFeCo\n\n Paper ( RDFS:label GdFeCo )\n\n Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): https://github.com/dennyzhang/title-ABSTRACTs-and-relations-annotated- corpus/blob/master/gpt2-small-benchmark.md https://github.com/dennyzhang/title-ABSTRACTs-and-relations-annotated- corpus/blob/master/gpt2-small-benchmark.md \n output (Sparql query):icions:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Fishing Derby benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Atari 2600 Fishing Derby benchmark is a corpus dataset. Here are some metrics that are used to evaluate models on this benchmark:?benchmark:dominance,?benchmark:perplexity,?benchmark:success rate,?benchmark:time per move.\n\n Speaker: The benchmarks that were used in the research papers that I mentioned include a subset of Atari 2600 Fishing Derby. Some of the metrics that are used to evaluate models on the benchmark are dominance, perplexity, success rate, and time per move.\n\n Speaker: Dominance is a metric that evaluates if a model's predicted responses are dominated by the human responses (the baseline) and evaluates if the", "ibe a string \"Number of params\" (with spaces).\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Number of params\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nOutput: \npaper                    paper_lbl   \n------------------- ------------ --------------------------------------------------------------------------------------------------------------------------------\nP31  Number of params    ImageNet Challenge                                                                                        \n\n<|endoftext|>Q:\n\nApply function to each field of dataframe in dplyr\n\nI have this dataframe:\nlibrary(dplyr)\ndf <- tibble::tibble(Sale_Price= c(100, 200, 300, 400, 500, 600, 700, 800, 900), \n             Sale_Date= as.Date(c(\"2017-01-01\", \"2017-01-03\", \"2017-01-04\", \n                              \"2017", "n'model', '2600 Battle Zone'. FILTER (?model = \"Atari 2600\")\n\n # list of models evaluated on the Atari 2600 Battle Zone dataset:\n # { \"Atari 2600 Battle Zone\": \"Atari 2600 Battle Zone model\" }\n\n # Sparql query to select only papers with benchmark information:\n # SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n # \n# Output:\n# \n# SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DAT", "<code>SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }</code> \n input (English text): http://www.semanticweb.org/tiwary/knowledge/ontologies/bi-m.obo?version=20161111120900\n output (Sparql query):  http://www.semanticweb.org/tiwary/knowledge/ontologies/bi-m.obo?version=20161111120900#Abstracts%20entities%20and%20relations%20annotated%20corpus\n input (English text): http://www.semanticweb.org/tiwary/knowledge/ontologies/bi-m.obo\n output (Sparql query):  http://", "{?dataset a orkgc:Dataset;}\n output (Sparql query):SELECT?metric (values?metric) WHERE {?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Star Gunner dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n output (Sparql query):SELECT?metric (values?metric) WHERE {?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Star Gunner dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl dc:subject?paper.?cont orkgp:P1?cont;?cont dc:subject?benchmark.?paper_", "input (Sparql query): <p>Can you provide links to code used in papers that benchmark the NASCell model?</p><p><a href=\"https://github.com/NASCell/NASCell\">https://github.com/NASCell/NASCell</a></p>\n input (English text): What are the titles and IDs of research papers that benchmark the NASCell model?\n output (Sparql query): SELECT?paper?paper_lbl (WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. })\n input (English text): What are the titles and IDs of research papers that benchmark the NASCell model?\n output (Sparql query): SELECT?paper?paper_lbl (WHERE {?dataset a orkgc:Dataset; rdfs:", "text (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Datasets used in TCN model\")?paper_code.?cont orkgp:HAS_CODEREF?paper_code.?cont;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): I have used the TCN model for benchmarking purposes, can you provide me the code references in the paper that have used the TCN model for benchmarking purposes?\n\n Output (Sparql query):Elaboration (Sparql query): SELECT?paper?paper_lbl?paper_code  WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Datasets used in TCN model\")?paper_code.?cont orkgp:HAS_CODEREF?paper_code.?cont;?paper orkgp:P31?cont;?", "input (English text): The benchmark with the highest value is the FLB model which achieved 89.6 mark (2.7) in the F LB 2019 task.\n\n ?paper P31 89.6\n\n ?paper_lbl Abstracts' entities and relations annotated corpus\n\n ?benchmark HAS_DATASET Abstracts' entities and relations annotated corpus\n\n ?cont orkgp:HAS_BENCHMARK Abstracts' entities and relations annotated corpus\n\n ?paper orkgp:P31 Abstracts' entities and relations annotated corpus\n\n ?paper_lbl F LB 2019\n\n ?paper F LB 2019 2.7\n\n input (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>The deal", "set (org.qt-project.standards.org/is a dataset about question answering systems. SQuAD2.0 is a question answering dataset made up of 16 million machine reading questions and 30 million answer options from Google. The questions and their answer options are collected from open source QnA sites such as Quora and Knowledge Graph engines.))\n\n output (Sparql query):SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD2.0\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl. }\n\n input (English text): SQuAD2.0 is a question answering dataset made up of 16 million machine reading questions and 30 million answer options from Google. The questions and their answer options are collected from open source QnA sites such as Quora and Knowledge Graph engines.\n\n output (Sparql query):    \n\n input (English text): SELECT * WHERE {?dataset a orkgc", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }' SELECT?benchmark?metric?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n Procedure:results are displayed in Sparql query\n\n Procedure: benchmark is the element of the dataset that has the benchmark. Benchmark is the element of the dataset that has the benchmark and the dataset", "input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset.\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset.\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper or", "input (English text): The largest number of articles about coronavirus has been contributed by WHO and experts from Harvard, Oxford, Imperial College, the University of Toronto, and the Chinese University of Hong Kong. \n input (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?paper?paper_lbl. FILTER (str(?paper_lbl) = \"articles about coronavirus\") } \n output (Sparql query):quisition (English text): WHO and experts from Harvard, Oxford, Imperial College, the University of Toronto, and the Chinese University of Hong Kong. \n input (English text): While the above entities may not be specific to the COVID-19 virus, they are very relevant to answering the task at hand and may be helpful when determining who has contributed the most to the research community around the virus. Hence we will include them in our benchmark.\n output (Sparql query):igation (English text): Although there are many entities and relations that can be included in a benchmark, we choose to include a single benchmark for Articles about Coronavirus (P31). This paper is authored by researchers from Harvard, Oxford, Imperial College, the University of Toronto,", "set (a orkgc:Dataset)?dataset_lbl?benchmark (rdfpedia:model).\n input (English text): rdfpedia:model.\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset_lbl rdfpedia:model;?dataset_lbl rdfs:label \"Abstracts' entities and relations annotated corpus\";?benchmark orkgp:HAS_DATASET?dataset;?benchmark orkgp:HAS_BENCHMARK?cont;?paper orkgp:P31?cont;?cont rdfpedia:model. }\n\n NOTICE: I have replaced \"Abstracts' entities and relations annotated corpus\" with the actual title of the paper.\n\nkerchief (English text): rdfpedia:model.\noutput (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset_lbl rdfpedia:model;?dataset_lbl rdfs:label rdfpedia:model;?benchmark orkgp:HAS_DATASET?dataset;?benchmark orkgp:HAS_BENCH", "net:Astrid reactor uses HeIII gas.\n inputs (None): None\n\n 18) It returned two results. One for 'Astrid reactor uses HeIII gas', and another for 'Abstracts' entities and relations annotated corpus.\n\nBased on the above output, the first result contains the ID of the dataset used for benchmarking, and the second contains the ID of the benchmark.\n\n Selected Papers\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Astrids' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe above query returned the following two results:\n\n+--------------------------------------+\n| Paper                                |\n+--------------------------------------+\n| <http://purl.oclc.org/sparql/dataset/ASTRID_reactor> |\n+--------------------------------------+\n| <http", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe Sparql query above returns 2 results:\n\n* Paper P31, benchmark \"ATARI 2600 BERZERK dataset\": \n[\n    {\n        \"paper\": \"P31\",\n        \"paper_label\": \"Abstracts' entities and relations annotated corpus\",\n        \"benchmark\": {\n            \"has_dataset\": true,\n            \"has_benchmark\": true\n        }\n    }\n]\n\n* Paper P31, benchmark \"ATARI 2600 BERZERK dataset\", score 1.000: \n[\n    {\n        \"paper\": \"P31\",\n        \"paper_label\": \"Abstracts", "n\n OPTIONAL {?paper rdfs:label \"Model evaluation on Atari 2600 Skiing dataset\" }\n OPTIONAL {?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\" }\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdf:label?benchmark_lbl;?cont rdf:label?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?paper orkgp:HAS_BENCHMARK?cont.?paper orkgp:P31?cont;?benchmark rdf:label?paper_lbl. } \n input (English text): Models that have been evaluated on the Atari 2600 Skiing dataset are:\n Arnie\n Boulder\n Rodney\n Yaroslav\n Benchmark model: Abstracts' entities and relations annotated corpus\n\nmedscianals.org", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Most commonly used benchmark datasets for the Text Summarization research field\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?benchmark. } \nNote: There is only one benchmark dataset?benchmark in the results.\n\n\nExamples from the results:\nExamples:\n*?paper | extractpaper_lbl\n*?paper | extractpaper_lbl | extractpaper_title\n*?paper | extractpaper_lbl | extractpaper_title | extractpaper_short\n*?benchmark | extractbenchmark_id\n*?benchmark | extractbenchmark_id | extractbenchmark_type\n\n\nNote: the benchmark ID is just one of the identifiers used in the benchmark datasets. Also note that the extract function takes multiple results and joins them based on the provided pattern.\n\n\nExamples", "ution is the metric of evaluation over the Atari 2600 Tutankham dataset.\n\nQUESTION 1: From the question, we can see that the metrics of evaluation over the Atari 2600 Tutankham dataset is the benchmark of a relevant dataset.\n\n\nThe benchmark for the Abstracts' entities and relations annotated corpus dataset is the?benchmark SPARQL attribute.\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n\nThe benchmark for the Abstracts' entities and relations annotated corpus dataset is the?benchmark SPARQL attribute.\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl", "sparql (English text): The top performing model when benchmarked on the Atari 2600 Tutankham dataset was an Atari 2600 Model trained on the SQuAD dataset.\n\n Sparql query response:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n            ?paper?paper_lbl {?dataset a orkgc:Dataset; rdfs:label \"The Atari 2600 Model trained on the SQuAD dataset.\". FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31", "inal response: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PROTEINS dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark.?paper orkgp:P31?benchmark;?dataset a orkgp:PROTEINS ;?dataset_lbl a proteints:Dataset; rdfs:label?dataset_lbl. }\n\n\f\nproteints:Dataset is a class from the OR-tools ontology that is used to represent a data set. The PROTEINS dataset is used in the benchmark presented in the paper \"Symbolic Frameworks for Detecting Protein-Protein Interaction Sub-units\" by Mar\u00e7al et al. (P31) is from the abstracts of the paper.\n\n\nproteints:PROTEINS is the dataset that is used in the benchmark.\n\n\nproteints:HAS_DATASET is a property that can have the value of true or false that is used to represent whether a certain resource has been", ". P31 \n input (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")}?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper_lbl rdf:resource?paper.", "ition (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark a orkgp:BENCHMARK ;?paper_lbl a rdfs:label. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?cont. FILTER (str(?cont) = \"gold standard abstracts\"),?cont?paper_lbl. }\n\n Challenge Abstracts' Entities and Relations Annotated Corpus Benchmark\n\n Abstracts' entities and relations annotated corpus (P31)\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark\n\n Databricks' benchmark", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }' \n input (English text): https://huggingface.co/news/introducing-the-huggingface-benchmarking-tool#datasets\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl", "ition (English text): The best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric is the proposed extended IB1 model.\n output (Sparql query): SELECT?paper (values (?paper_lbl as?paper_lbl_str)) WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?dataset a orkgp:has?cont. FILTER (str(?cont) = \"Extended IB1\") }\n input (English text): The best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric is the proposed extended IB1 model.\n\n Manager: Extended IB1\n\n Manager: Extended IB1 is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric.\n\n Manager: Extended IB1 is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric.\n\n Manager: The proposed extended IB1 model is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric.\n\n Manager: The proposed extended", "?benchmark?benchmarkScore?paper?paper_lbl.\t\"Title-Match Benchmark\" \"title match\" 100\tThe title-match benchmark measures the effectiveness of titles in relation to the abstracts of journal articles.\t\"title match\"\t100\tThe title-match benchmark scores the abstract of a journal article against the titles of the same journals.\n inversion:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Title-Match Benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n output (Sparql query):?paper?paper_lbl {\"Title-Match Benchmark\" \"title match\" 100}\n\n inversion:SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset;?paper?paper_lbl. FILTER (str(?paper_lbl) = \"", "ibe the title and ID of research papers that include the following evaluation metrics used when benchmarking models on the STEM-ECR v1.0 dataset: \n <ul><li>Bleu \n <li>CIDEr \n <li>CDRC</ul> \n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Bleu\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Bleu, CIDEr, CDRC\n\n<|endoftext|>Introduction {#Sec1}\n============\n\nThe tendency for illness and health to fluctuate over time, whether this is due to pure chance or a directed relationship between the two, has been a source of interest in psychology and medicine for decades. In the fields of psychology and medicine, time-series analysis has been used to study disease rates and risk factors across time.", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\n*Code used in the benchmark paper*\n\n[width=\"90%\",border=\"1\",align=\"center\",]\n*Dataset Benchmark*\n- [title=\"Multi-Perspective Matching (single model)\", author=\"Dai\", journal=\"The BMJ\", year=\"2018\", abstract=\"\n    This method aims to combine global and local models for fine-grained cross-dataset recognition. We implemented two models: one for global representation and one for local representation. Global model is trained on single model by maximizing the evidence lower bound. Local model fuses global and local representations on a two-layer LSTM using simple averaged information. The final representation is integrated by simple averaging information from", "select?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n input (English text): The benchmark for the GAD dataset is HAS_DATASET?dataset.\n\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"GAD dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n output (Sparql query): P31\n\n input (English text): The benchmark for the GAD", "SELMINBenchmarKS query to SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Measures of evaluation over the PubMed 20k RCT dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nSelect the papers that contain the following terms: \n* 'Abstracts' entities and relations annotated corpus'\n* 'Measures of evaluation over the PubMed 20k RCT dataset'\n\nThe selected papers should contain the following terms in their titles and IDs.\n* P31: 'Abstracts' entities and relations annotated corpus'\n* P31: 'Measures of evaluation over the PubMed 20k RCT dataset'\n\n\nThe selected papers are:\n* P31: 'Abstracts' entities and relations annotated corpus'\n* P31: 'Measures of evaluation over the PubMed 20k RCT dataset'\n\n\nThe benchmark for the Abstracts' entities and", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The metric that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset are: \n * Time, in milliseconds\n * Points, the total number of points achieved by the agent \n * Distance, the total distance travelled by the agent\n\n (:P31, the benchmark entity),\n\n (:cont, the dataset entity),\n\n (:dataset_lbl, the dataset label),\n\n (:benchmark, the relation name)\n\n (:paper_lbl, the paper label)\n\n (:paper, the paper entity)\n\n (:dataset a orkgc:Dataset; rdfs:label?dataset_lbl. )\n\n (:benchmark orkgp:HAS_DATASET?dataset.", "itation\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nposable set of tuples : {<paper1>, \"paper1_label\", <dataset1>, <benchmark1>}", ", SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AxCell model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n<|endoftext|>Q:\n\nHow do I make Goji Balls?\n\nI saw a recipe for Goji Balls on Pinterest, but they called for dates instead of goji berries, and rice instead of pumpkin seeds. Also, they don't mention how many goes to make the balls. \n\nA:\n\nIt's a very popular goji recipe that can be found all over Pinterest. \n\nPumpkin seeds + goji berry = Miracle berry\n\nThe recipe for these goji balls is as follows:\n\n150g goji berry\n120g dates\n30g virgin olive oil\n30g icing sugar\npinch of salt\n\nBlend all ingredients, except date", "n't know of any code that could be used.\n input (English text): What are the titles and IDs of research papers that benchmark the AWD-LSTM model?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?paper sparql:value?benchmark;?benchmark sparql:C31. } \n input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n output (Sparql query):  \n  don't know of any code that could be used.\n input (English text): What are the titles and IDs of research papers that benchmark the AWD-LSTM model?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?paper sparql:value?benchmark;?benchmark sparql:C31. } \n input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?\n output (Sparql query):ipeline do{\n\n    def dataset = \"http://data.yaili.org.cn/abstracts/dataset.html\"\n    def", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PubMedQA dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?cont; rdfs:label?paper_lbl. }\n\nSample Result:\n [\n  {\n    \"paper\": \"P31\",\n    \"paper_lbl\": \"PubMedQA dataset\"\n  },\n  {\n    \"paper\": \"P32\",\n    \"paper_lbl\": \"PubMedQA dataset\"\n  }\n]\n\n<|endoftext|>Q:\n\nSQL - how to count number of matches between two tables\n\nI have the following two tables:\nTable 1 - company\n\nId | Name\n-- | -----\n1  | ABC\n2  | XYZ\n3  | DEF\n4  | GHI\n\nTable 2 - department\n\nId | Name\n-- | -----\n1  | Finance\n2", "Input: SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Datasets used for benchmarking in the Image Classification research problem\") }\n\nThe response should return the following data:\npaper P31,paper_lbl \"Abstracts' entities and relations annotated corpus\"\n\nHere is the SQL for the Sparql Query:\nSELECT DISTINCT?paper?paper_lbl WHERE {\n?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl.\n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n?benchmark orkgp:HAS_DATASET?dataset.\n?cont orkgp:HAS_BENCHMARK?benchmark.\n?paper orkgp:P31?cont\n ;\n rdfs:label?paper_lbl.\n }\n\n<|endoftext|>Q:\n\nSums over $\\mathbb{N}^2$, infinite products and diagonal and anti-diagonal matrices\n\nI have this question :\n\nProve that for", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Gibson PointGoal Navigation dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper.\n\n personnelis (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper.\n\n personnelis (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper.\n\n personnelis (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper.\n\n personnelis (English text): The best result is 0.", "iterations: 5\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cheetah\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?score. } \n input (English text): Cheetah\n\n output (Sparql query): P31\n\n input (English text): run (DMControl500k)\n\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?score. }\n\n output (S", "ution 2 returned 1 row {?paper?paper_lbl : benchmarksReturnedObjectiveValue?benchmark. : score : 12.5 }\n input (English text): The top benchmark score is 12.5. Its metric is Tanimoto.\n\n solution (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe Tanimoto metric computes the percentage of overlapping areas of two shapes. The higher the Tanimoto value, the more similar the compared shapes are.\n\n solution (Sparql query): orchestration 2 returned 1 row {?paper?paper_lbl : benchmarksReturnedObjectiveValue 12.5 }\n\n<|endoftext|>Q:\n\nNeo4j cypher.match on list of tuples", ":sparql \noutput (Sparql query): INSERT INTO paper_benchmark (paper, paper_lbl, benchmark, benchmark_id) VALUES \n(\n  'P31', 'The Benchmark','sparql', 'P31'\n); \n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nsolution A:  The Benchmark\nsolution B: Dataset\nsolution C: Dataset, Benchmark, Benchmark_id", "model P31 was best on the WikiText-103 benchmark dataset.vering the models that are also better than P31 can be done.vering the rest of the models that are not better than P31 can be done. Checking the model that is best on the WikiText-103 benchmark dataset: SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): P31\n\n output (Sparql query): P31", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Russian dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset was 73.33 on 2017-02-03T16:33:33Z.\n output (Sparql query):", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nThe selected papers are:\n\nThe dataset \u201cAbstracts\u2019 entities and relations annotated corpus\u201d\nThe benchmark \u201cAbstracts\u2019 entities and relations annotated corpus\u201d\n\n the papers selected are:\n\n\u201cAbstracts\u2019 entities and relations annotated corpus\u201d\n\u201cAbstracts\u2019 entities and relations annotated corpus Benchmark\u201d\n\n the selected result set is:\n\nThe selected papers are:\n\n\u201cAbstracts\u2019 entities and relations annotated corpus\u201d\n\n The dataset \u201cAbstracts\u2019 entities and relations annotated corpus\u201d\nThe benchmark \u201cAbstracts\u2019 entities and relations annotated corpus Benchmark\u201d\n\n<|endoftext|>One would think the Jets would want to upgrade at quarterback, and it's not like they are in a lot", "SELECT?paper (rdfs:label?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?paper. } \n input (English text): The name of the model is \"Combined CNN+LSTM\".\n\nA:\n\nThe following query retrieves the paper with the best F-score on the atari dataset:\nSELECT?paper (rdfs:label?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?", "k: SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper bidi:link?code_lnk. } \n input (English text): Here are the references with the corresponding code links: \n P31 Dataset Manipulation with No-Op C51 Benchmarks (https://archive.ics.uci.edu/ml/datasets/Abstracts%26relations%20annotated+corpora) \n P31 Benchmark Dataset Manipulation with No-Op C51 Benchmarks (https://archive.ics.uci.edu/ml/datasets/Abstracts%26relations%20annotated+corpora#benchmarks) \n P31 Simple R-tree on Abstracts' Entities and Relations Annotated Corpora (https://archive.ics.uci.edu/ml/datasets", "input (English text): https://github.com/fabiopalis/fabir.models.andrewlock/blob/master/examples/benchmark/abstracts.py This code uses classes in fabiopalis.models.andrewlock for benchmarking the FABIR model.\n\n GLOBALS:\n    * dataset: a ORCID Dataset resource as supplied by the developer of FABIR.\n\n CLASS:\n    * benchmark: a HAS_DATASET relation from fabiopalis.models.andrewlock containing a corpus dataset as one of its values\n\n CLASS:\n    * cont: a HAS_BENCHMARK relation from fabiopalis.models.andrewlock containing the benchmark as one of its values\n\n CLASS:\n    * paper: a HAS_P31 relation from fabiopalis.models.andrewlock containing a paper as one of its values\n\n CLASS:\n    * paper_lbl: a LABELS relation from sparql that maps each label in the triple to a string\n\n CLASS:\n    * query: a SELECT statement that returns the papers with their corresponding benchmark, along with a LABELS relation that maps the label of the benchmark to the title of", "input (English text): Emergency type of AMBER Alert.\n\n output (Sparql query):\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n output (Sparql query):\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl.?benchmark orkgp:HAS_DATAS", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA metric\") }\n\n\n\"SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA metric\") }\"\n\n\n\"* CoQA metric\"\n\n\n\"* Gated long-short term memory\"\n\n\n\"* Gated LSTM\"\n\n\n\"* Gated recurrent unit\"\n\n\n\"* Cross-lingual dependencyparser\"\n\n\n\"* CoNLL 2005 cross-lingual dependency parser\"\n\n\n\"* Noun phrase parser\"\n\n\n\"* GRU\"\n\n\n\"* Recurrent dense neural network\"\n\n\n\"* Attentive model\"\n\n\n\"* Attention mechanism\"\n\n\n\"* List of metrics used on CoQA benchmark dataset\"\n\n\n\"* Gated LSTM\"\n\n\n\"* Gated recurrent unit\"\n\n\n\"* Cross-lingual dependencyparser", "input (English text): xBase with xBase-ML achieved the best F1 score on the OntoNotes dataset\n\n hingoutput (Sparql query): SELECT?paper?paper_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThis is one of the possible answers returned by the query:\n\nDOCTYPE doc\nns=# xBase with xBase-ML achieved the best F1 score on the OntoNotes dataset", "input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Defender dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset\n\n A benchmark for the Atari 2600 Defender dataset", "net (Sparql query): SELECT?paper_lbl?paper_id WHERE { \n<paper>?paper rdf:type papers:Paper;\n<paper_lbl>?paper_lbl rdfs:label?paper_lbl;\n<benchmark>?benchmark orkgp:HAS_DATASET?dataset;\n<cont>?cont orkgp:HAS_BENCHMARK?benchmark;\n<paper_id>?paper orkgp:P31?cont;\n<paper_lbl>?paper_lbl rdfs:label?paper_lbl;\n} \n<paper_id>?paper_id str(?paper_id) ;\n<paper_lbl>?paper_lbl rdfs:label?paper_lbl;\n<cont>?cont orkgp:P31?paper_id;\n<paper_id>?paper_id string(?paper_id) ;\n<benchmark>?benchmark orkgp:HAS_DATASET?dataset;\n<benchmark>?benchmark orkgp:HAS_BENCHMARK?cont;\n<benchmark", "ition (Sparql query): SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The following papers have used the Switch Transformer model:\n  [1] \"Databricks: The Dawn of Trust,\" Databricks, November 4, 2019. \n  [2] Vijay Nair, Ali Ghodsi, Ion Stoica, David Ho, Edsger Wijsman, Thain Fauvel, Ion Stoica, Ross McNeill, Luca Camus, Tassilo Weidlich, Krzysztof Glowala, Gina Wodrich, \"Switch Open AI Citys on Databricks,\" Databricks, March 2019. \n  [3] Francis Chou, Benoit Bijvie, It", "iminor (Sparql query 1): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nsplominor (Sparql query 1):?benchmark thesaurl:doi?paper_doi.?cont thesaurl:ref?paper_ref.?paper thesaurl:title?paper_title.", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2 evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n> Abstract entities and relations benchmarked \n> # Abstract entities and relations benchmarked on the\n> # Amazon-2 dataset\n\n> Dataset entities and relations benchmarked \n> # Dataset entities and relations benchmarked on the\n> # Amazon-2 dataset\n\n> Benchmark entities and relations benchmarked \n> # Benchmark entities and relations benchmarked on the\n> # Amazon-2 dataset\n\n> Paper entities and relations benchmarked \n> # Paper entities and relations benchmarked on the\n> # Amazon-2 dataset\n\n   paper:P31  Dataset entities and relations benchmarked  # Dataset entities and relations benchmarked on the Amazon-2", "input (English text): List the code links in papers that use the A3C-CTS model in any benchmark\n\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"A3C-CTS model\") }\n\n input (English text): http://www.semanticscholar.org/paper?paper=P31&paper_lbl=Abstracts' entities and relations annotated corpus\n\n output (Sparql query): http://www.semanticscholar.org/paper/P31\n\n input (English text): http://www.semanticscholar.org/paper?paper=P31&paper_lbl=A3C-CTS model\n\n output (Sparql query):?paper?paper_lbl\n\n input (English text): A3C-CTS model\n\n output (Sparql query): P31\n\n input (English text): Abstracts' entities and relations annotated corpus\n\n output (Sparql query): P31\n\n input (English text): A3C-CTS model\n\n output (Sparql", "?benchmark?benchmark_metric ;\n\n Query execution: \n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n  ?paper    dwh-paper-label         28 \n\n  ?paper_lbl Hutter Prize         28\n\n  ?benchmark Hutter Prize         28\n\n  ?benchmark_metric similarity    [ \"similarity\" ]\n\n Query result: \n   dwh-paper-label: Hutter Prize 28\n   dwh-paper-label: Abstracts' entities and relations annotated corpus 28\n  ?benchmark: Hutter Prize 28\n  ?benchmark: similarity    [ \"similarity\" ]\n\n Query result in json: \n {\"dwh-paper-label\": \"Hutter Prize\", \"dwh-", "n  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\"@nl. FILTER (str(?benchmark) = \"CoNLL 2003 (English)\")}?paper orkgp:P31?benchmark;?paper_lbl rdfs:label \"CONLL 2003 (English)\"@nl. }\n\n<|endoftext|>---\nabstract: 'The main purpose of this work is to investigate the properties of some distributions arising from random matrices. More precisely, we analyze the two cases: the normal distribution and the uniform distribution. As an application, we provide bounds for the discrepancy principle.'\nauthor:\n- |\n    Emilio Marcell\u00e1n and Martin Knapik\\\n    \\\n    Centre for Mathematics, Dresden University of Technology, D-01062, Dresden, Germany\nbibliography:\n- 'literatur.bib'\ntitle: Bounds for discrepancy principles based on random matrices\n---\n\nIntroduction\n============\n\nIn recent years many central limit theorems for probability distributions have been obtained by analyzing the behavior of random matrices. The motivation is that many distributions", ":Top Performing Model \n output (Sparql query): SELECT?paper?paper_lbl?benchmark_id where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark_id.?cont orkgp:HAS_BENCHMARK?benchmark_id.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Top performing model = Top Performing Model\n\n## Query 1\n\n\nSELECT DISTINCT?paper?paper_lbl?benchmark_id \n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"VTAB-1k\")?benchmark_id.?cont orkgp:HAS_BENCHMARK?benchmark_id.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\n#+name: papers dataset\n#+label: datasets with papers for this", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?paper_lbl. FILTER (str(?paper_lbl) = \"Artificial Intelligence\" or str(?paper_lbl) = \"natural language processing\" or str(?paper_lbl) = \"text mining\" or str(?paper_lbl) = \"natural language understanding\" or str(?paper_lbl) = \"speech processing\" or str(?paper_lbl) = \"spoken language processing\" or str(?paper_lbl) = \"speech recognition\" or str(?paper_lbl) = \"speech synthesis\" or str(?paper_lbl) = \"computer interface\" or str(?paper_lbl) = \"natural user interface\" or str(?paper_lbl) = \"speech and language technology\" or str(?paper_lbl) = \"semantic web\" or str(?paper_lbl) = \"knowledge representation\" or str", "n:SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P23?cont; rdfs:label?model_lbl. }\n\n\nmodel = [{\"name\":\"Perceptron\",\"benchmark\":\"HAS_BENCHMARK\",\"model\":\"P23\",\"dataset\":\"Dataset\",\"entity\":\"cont\"}]\n\n\noutput (Sparql query):    SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FSNS - Test dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P23?cont; rdfs:label?model_l", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 German-English dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?benchmark; rdfs:label?paper_lbl. }\n\nIn this answer,?paper is the label of the RDF graph for a research paper,?paper_lbl is the label of the RDF graph for the paper's benchmark,?dataset is the label of the RDF graph for the WMT2014 German-English dataset,?benchmark is the label of the RDF graph for the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric,?cont is the label of the RDF graph for the paper's dataset,?cont_lbl is the label of the RDF graph for the paper's best performing model benchmarking the WMT2014 German-English dataset, and?paper", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'\n\n<|endoftext|>\"The year is 2150.\" \"Futuristic technology allows people to pursue anything they want.\" \"Everything from music to health.\" \"In music, they pursue their dreams with the hopes of performing at one of the biggest music festivals in the world.\" \"But what most people don't know is there's always more than one way to skin a cat.\" \"This is Craig Sawyer, coming to you live from the city of angels, Paris.\" \"For thousands of years, human beings have sung their passions to the heavens.\" \"For the first time in history, this will be possible anywhere, anytime.\" \"For composer Fabrice Baree, that passion is music.\" \"He is one of the", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The highest benchmark result achieved on the STS Benchmark dataset is 79.3%.\n\n<|endoftext|>WASHINGTON (AP) - A federal appeals court on Wednesday ruled that a group of migrant children held in Ohio may move to Michigan, finding that the state's \"special relationship\" with the children's families created a significant opportunity cost.\n\nThe decision by the 6th U.S. Circuit Court of Appeals in Cincinnati is the latest in a string of cases around migrant children being held by the United States government. The courts are dealing with a growing number of children who are being held in states where the families have filed court motions requesting that the children be allowed to move to Michigan.", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Link to code of research paper\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe queries retrieves the necessary information to run in the `jena-benchmark` query engine (included in the Jena benchmark project):\n\n* `dataset`: the name of the dataset \n * `benchmark`: the name of the benchmark entity (typically the SHARE MODEL) \n * `paper`: the name of the paper \n * `cont`: the name of the corresponding ontology concept or URI of a dataset or ontology annotation \nThe SELECT DISTINCT keyword restricts the returned results to a single paper and a single benchmark.", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nThe benchmark datasets used most often are OntoloViz and CoNLL-2009. Both of these datasets contain annotated gold datasets and the gold datasets are the dataset that is the target of the benchmark.\n\n\nThe Benchmark dataset in this example contains the properties has_dataset and has_benchmark. These properties contain the values true and false, respectively. The values of these properties are the dataset and the benchmark that are benchmarked by the dataset.\n\n\nThe values of the properties has_dataset and has_benchmark in the ontology of this benchmark dataset are the dataset and the benchmark that are benchmarked. In the current example, the values of the properties has_dataset and has_benchmark in the", "set:multiRC\n benchmark (ntriples):multiRC HAS_DATASET{DataSet}\n paper (ntriples): P31{Paper_label}\n benchmark (ntriples):HAS_BENCHMARK{Has_benchmark, Benchmark_label}\n paper (ntriples):\n benchmark (ntriples):HAS_BENCHMARK{Has_benchmark, Benchmark_label}\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MultiRC dataset evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The standard set of evaluation metrics commonly used on the MultiRC dataset are: R@1, R@5, R@10, R@50, PR@1, PR@5, PR@10, and PR@50.", "n:FEXPR?paper_safely_abbreviated?paper_code_reference_from_model Where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code references in papers that have used the SRU++ Base model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_safely_abbreviated?paper_code_reference_from_model. } \n input (English text): The code references in the paper can be safely retrieved from the SRU++ Base model.\n output (Sparql query): SELECT?paper_code_reference_from_model WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. FILTER (str(?dataset", "output (Sparql query): rat\n output (Sparql query): benchmarkDataset\n output (Sparql query): Abstracts' entities and relations annotated corpus\n output (Sparql query): rat\n output (Sparql query): P31\n output (Sparql query): rat\n output (Sparql query): rat\n output (Sparql query): benchmark\n\n # The following query returns the results of the Best performers on the Atari 2600 Asteroids benchmark dataset. \n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n # The following is the corresponding JSON output:\n\n # The following is the corresponding JSON output:\n\n #[\n{\n\"paper\": \"rat\",\n\"paper_lbl\": \"P31", "PREFIX a: <http://www.w3.org/and> \nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \nPREFIX or: <http://www.semanticsimfull.org/orm/wmt2014/mynote#>  \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n A Benchmark for Web Markup Rules: WMT2014 German-English Dataset\n\n The Benchmark for Web Markup Rules WMT2014 German-English Dataset\n\n Title: A Benchmark for Web Markup Rules WMT2014 German-English Dataset\n\n  P31: Abstracts' entities and relations annotated corpus\n\n A Benchmark for Web Markup Rules W", "n. SELECT?model?model_label WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_label. FILTER (str(?dataset_label) = \"SearchQA\" )?model.?model_label?benchmark;?model_label?benchmark_label; } \n input (English text): The models that have been benchmarked on the SearchQA dataset include models from the following research communities: \n Conll-2005, CoNLL-2007, and MultiNLI.\n\n Invalid model name \"SearchQA\".\n\n Invalid benchmark name \"HAS_BENCHMARK\".\n\n Invalid model label \"P31\".\n\n Invalid benchmark label \"Abstracts' entities and relations annotated corpus\".\n\n Invalid dataset label \"orkgp:HAS_DATASET\".\n\n Invalid dataset label \"orkgc:Dataset\".\n\n Invalid dataset label \"rdfs:label\".\n\n Invalid benchmark label \"MultiNLI\".\n\n Invalid model label \"MultiNLI\".\n\n Invalid model label \"Conll-2005\".\n\n Invalid model label \"CoNLL-2007\".\n\n Invalid model label \"orkgp:P31\".\n\n Invalid model label \"orkgp:HAS_BENCHMARK", ":?benchmark?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): OntoNotes dataset contains entities and relations about onyx, rock solid, airspeed, takingoff speed, and car sickness.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?paper_lbl. }", "str (?paper_lbl)?paper_id\n\n  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Defender dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n[\n  {\n    \"paper_lbl\": \"Defender dataset\",\n    \"paper_id\": \"P31\"\n  },\n  {\n    \"paper_lbl\": \"Abstracts' entities and relations annotated corpus\",\n    \"paper_id\": \"P31\"\n  }\n]", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT 2014 English-German\")?benchmark_metric.?benchmark_value?benchmark_metric_val.?paper orkgp:P31?cont;?paper_lbl rdfs:label?paper_lbl. }\n input (English text): WMT2014 English-German\n\n benchmark_metric\tWMT 2014 English-German\n benchmark_value\thighest\n\n benchmark_metric_val\t61.0%\n\nideggerien\n\nActually, the highest benchmark result achieved on the WMT 2014 English-German dataset is 61.0%\n\n * WMT 2014 English-German 61.0%\n\n [Source: https://www.w3.org/TR/2012/PR-wd-page-up/revised-dic-model.html#prSection4][1]\n\n * WMT 2014 English-German 61.0%\n\n * The model's F1-score was 61.0% on the English-German WMT2014 news section data set", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"PWC Leaderboards dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe query results include two documents:\n\n* ` paper=P31 `\n* ` paper_lbl=PWC Leaderboards `\n\nThe `paper` document contains the title and the `paper_lbl` document contains the label of the research paper that includes a benchmark for the PWC Leaderboards dataset.\n\n\n# Find out the PWC Leaderboards dataset\n\n\nThe `paper_lbl` document includes the label `PWC Leaderboards` which is related to the `paper` document. The `P31` token in the `paper_lbl` document is the title of the research paper that includes a benchmark for the PWC Leaderboards dataset.\n\n\n# Find out the PWC", "\\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Metrics used on enwik8 benchmark dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n Stadiums or-else fields.\n\n Stadiums are represented using the IRIs http://tf-stadium.herokuapp.com/ and the relations for a given stadium are stored as a property called {stadium.stadiumName} on the graph. The metric used on the enwik8 benchmark is the average distance between two nodes which is computed as Euclidean distance on the values of the corresponding property. The queries can be executed from the endpoints at https://en.m.wikipedia.org/w/api.php?action=query&list=random&rlimit=1000&retlimit=10&StructuredQuery=stadiums%20%3F%20or%20", ":Common evaluation metrics when benchmarking models on the TempEval-3 dataset include Precision Recall on a user-defined metric, F1 score on a model-defined metric, or the standard t-score on a model-defined metric. Some papers also report results on other metrics, such as correlation coefficient, the area under the receiver operating characteristic curve, or Cohen's kappa.\n input (English text): Precision Recall, F1 score, t-score, correlation coefficient, area under the receiver operating characteristic curve, Cohen's kappa.\n\n invite (Sparql query):\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TempEval-3\")}\n\n invite (Sparql query):\n\nordable solution for automated summarization in the cloud (P31)\n\n invite (Sparql query):\n\n ?cont    owl:class  [ owl:equivalentProperty rel:hasDataset[. ]  ]\n\n ?benchmark owl:equivalentProperty rel:hasBenchmark[. ]\n\n ?paper   owl:property", "ition (Sparql query): SELECT?paper_lbl?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } ORDER BY count(*) desc\n input (English text): Most common location in the studies is \u201cAbstracts' entities and relations annotated corpus\u201d.\n output (Sparql query): To retrieve the 2nd result,?paper_lbl should be \u201cAbstracts\u2019 entities and relations annotated corpus\u201d, the 3rd result should be \u201cAbstracts' entities and relations annotated corpus (Benchmark),\u201d the 4th result should be \u201cAbstracts' entities and relations annotated corpus (Benchmark) Benchmark\u201d.\n input (English text): Most common location in the studies is \u201cAbstracts' entities and relations annotated corpus\u201d.\n output (Sparql query): To retrieve the 2nd result,?paper_lbl should be \u201cAbstracts\u2019 entities and relations annotated corpus\u201d, the 3rd result should be \u201cAbstracts' entities and relations annotated corpus (Benchmark),\u201d the 4th result should be \u201cAbstracts' entities and relations annotated corpus", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n injection 1:?dataset a orkgc:Dataset. injection 2:?dataset_lbl string: \"Abstracts' entities and relations annotated corpus\". injection 3:?benchmark orkgp:HAS_DATASET. injection 4:?cont orkgp:HAS_BENCHMARK. injection 5:?paper orkgp:P31. injection 6:?paper_lbl string: \"Abstracts' entities and relations annotated corpus\".\n output (Sparql query): [\n  {\n   \"paper\" : \"P31\",\n   \"paper_lbl\" : \"Abstracts' entities and relations annotated corpus\"\n  },\n  {", ":Transfer Learning with Deep Neural Networks Pre-training on Movie Reviews for Classification and Similarity (Kuehne et al., 2019)\n\nThe following are the ID and title of the research paper:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>                                                                                          ACCEPTED\n                                                                                      14-14-00758-CV\n                                                                        FOURTEENTH COURT OF APPEALS\n                                                                                     HOUSTON, TEXAS\n                                                                                  9/5/2015 4:10:23 PM\n                                                                                  CHRISTOPHER PRINE\n                                                                                               CLERK\n\n                        No. 14-14-00758-CV\n\n                                                          FILED IN\n                                                 14", "n :SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont;?paper orkgp:P31?cont; }\n input (English text): P31 is a model for extracting Piano Rolls from Monaural recordings, a kind of image-based keyframe-based motion tracking. P31 is evaluated on the Classical music, 5 seconds at 12 kHz dataset.\n\n\r\nThe benchmark?benchmark is of class hasdataset and its values are P31, P31, P31, \n\nThe SQL returned for querying this SPARQL endpoint is:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset", "iter unser Forschungsmodel, welcher das beste unter der Bewertung der Bleu-Score bei der IWSLT2014 German-Englisch-Datensatz besteuerten, um in die neueste Ausgabe dieses Datensatzes einzutauchen?\n\n replied Sparql query: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?benchmark. }\n\n replied sparql query: IWSLT2014 German-English\n\n replied output: IWSLT2014 German-English\n\n replied input: IWSLT2014 German-English\n\n replied output: IWSLT2014 German-English\n\n replied input: IWSLT2014 German-English\n\n replied output: IWSLT2014 German-English\n\n replied input", "SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nFrom the above Sparql query we can see that the paper dataset and benchmark are mentioned in the WHERE clause.\nAdditionally, we can also see in the above query that we have at least one relationship between paper and benchmark where paper is the target and benchmark is the property that holds the relationship. \nSELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nSo from the RDF dataset, we have a paper that has dataset and benchmark that has the relationship has_dataset to dataset and has_benchmark to benchmark.  \nThe papers in the benchmark dataset are listed in the?paper_lbl variable that", "str (English text): A variety of evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset. \n input (Sparql query): [ select * where { filter (isA articles:EvaluationMetric) } ]\n\n output (Sparql query): SELECT?paper ( LABEL?paper_lbl as?paper_lbl ) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Skiing evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>The case of Yale\u2019s attempted recruiting of a convicted sex criminal, who in turn sent a now-notorious \u201cpenis picture\u201d to the university is likely part of a wider political game and ideological tug-of-war within the state and the nation over the threat of radical Islam, a former FBI counterterrorism specialist said on Sunday.\n\n\u201cYale is playing a very shrewd", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n  \nList the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n  \nList", ": SELECT DISTINCT?paper?paper_lbl_ref WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): Here are some links to code that benchmark the COMET - Direct model: \n - [Paper: Deep Learning for Abstractive Summarization](http://aclweb.org/anthology/P/P16/P16-1091.pdf) \n - [Paper: Neural Diamonds: Generalizing Recurrent Neural Networks for Abstractive Sentence Summarization](https://arxiv.org/abs/1802.04451) \n - [Paper: Attention Induction for Neural Diamonds](https://arxiv.org/abs/1802.04453) \n - [Paper: Explicit Neural Diamonds](https://arxiv.org/abs/1803", "input (English text): I cannot provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset, however, the best performing model on the dataset is sparql.berlin.de:model-name. You can find a list of recent models that have been benchmarked on the dataset at the following location: http://www.sparql.berlin.de/model-comparison/. \n input (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Best performing model on the BUCC Russian-to-English benchmark dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?cont;?paper_lbl?paper_id. }\n\nOutput: \nP31 \nThe best performing model on the dataset is sparql.berlin.de:model-name. \n\nOr in one line: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?benchmark;?paper_lbl?benchmark_label. } \n input (English text): The highest benchmark result achieved on the Atari 2600 Boxing dataset is at 31,2%.\n\niza\u00e7\u00e3o (Sparql query): di:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Boxing dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?", "inal (Sparql query): SELECT DISTINCT?paper WHERE {?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank Benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark bidi:has_dataset?dataset. } }\n\n Which model has achieved the highest F1 score on the Penn Treebank benchmark dataset?\n\n?paper_lbl:Penn Treebank Benchmark\n\n The mentioned model is\n\n The mentioned model is the Penn Treebank model with a F1 score of 0.91\n\n The mentioned model is the Penn Treebank model with a F1 score of 0.91\n\n The Penn Treebank model has achieved the highest F1 score score on the Penn Treebank benchmark dataset\n\n The Penn Treebank model has achieved the highest F1 score on the Penn Treebank benchmark dataset\n\n The Penn Treebank model has achieved the highest F1 score of 0.91 on the Penn Tree", "ition (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 German-English dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark:metric?metric;?benchmark:score?score. } \n input (English text): IWSLT2015 German-English dataset: IWSLT 2015 English to German parallel corpus. Metric: BLEU, Score: 22.0. \n output (Sparql query):ordable?paper_lbl WHERE {?dataset a orkgc:Dataset;?dataset_lbl a IWSLT2015 German-English dataset. rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"IWSLT2015 German-English dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkg", "net (English text): The benchmarks can be used to measure the performance of annotators in detecting and characterizing entities and relations in the abstracts for a given corpus dataset. The most common measures are based on precision (P), recall (R) and F1 score. \n input (English text): P = (number of true positives)/(number of true positives + false positives)\n R = (number of true positives)/(number of false positives)\n F1 score = 2*P*R/(P+R)\n output (Sparql query):PDATEritems (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }?paper_lbl@paper{?paper rdfs:label?paper_lbl. }\n input (English text", "Marshall et al. (2018) MultiNLI dataset evaluation: human evaluation metrics. arXiv.\ninput (English text): Marshall et al. (2018) MultiNLI dataset evaluation: human evaluation metrics.\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?benchmark;?cont orkgp:HAS_DATASET?dataset; rdfs:label?paper_lbl. }\n\n\nMATCH (paper:P31) - [b:HAS_BENCHMARK] - (benchmark:HAS_BENCHMARK) - [c:HAS_DATASET] - (dataset:Dataset) - [l:rdfs:label] -() - (cont:cont) - (dataset_lbl:rdfs:label) - (paper_lbl:rdfs:label) \n\nWHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus", "icidal models:Model | Paper title | Paper Labels | ImageNet Average Precision | RL publication | RL model \n input (English text): ImageNet ReaL dataset evaluation models:Benchmark | BENCHMARK | LABEL | AVERAGE PCL | 2017 | ResNet-50 | P31 \nvisor | ImageNet Benchmark ResNet-50 | fcn_accuracy | 51.85 | Tulsi-Benchmark-ResNet-50 | Tulsi | fcn_accuracy | 51.85 \n\nThe answer is: \noutput (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Benchmark for the Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }", "imizeLabCorpusRNN sparql query with  dbCommand.execute(). tripleMap(true). find(paper). by(paper_lbl). by(benchmark). by(cont). by(paper). orderBy(benchmark). limit(1)\n\nocompleteLabCorpusRNN\n\n CompleteLabCorpusRNN\n\n SELECT DISTINCT\n\n sparql\n\n  SELECT DISTINCT\n\n?paper?paper_lbl\n\n  WHERE {?dataset a orkgc:Dataset;\n\n  rdfs:label?dataset_lbl.\n\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n\n?benchmark orkgp:HAS_DATASET?dataset.\n\n ?cont orkgp:HAS_BENCHMARK?benchmark.\n\n ?paper orkgp:P31?cont;\n\n  rdfs:label?paper_lbl.\n\n  }\n\n  dbCommand.execute().\n\n  tripleMap(true).\n\n  find(paper).\n\n  by(paper_lbl).\n\n  by(benchmark).\n\n  by(cont).\n\n  by(paper).\n\n  order", "input (English text): The Kinetics-600 dataset is a large collection of annotated video with 20 annotations for each video of human activities from 6 different view points at 50 fps. The annotation of this dataset is mainly focused on the human body and the interacting objects. Among 20 videos, 6 viewpoints and 20 objects are annotated for each video. The results of the highest annotated video, including the metric and score, can be found at: https://github.com/CMU-LV/Kinetics-600/blob/master/benchmark/results.csv\n\nUsing the Databrick notebook client, click the? button after the query completes.\n\n advised (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe Sparql query above will", "n  SELECT DISTINCT?metric \n FROM \n<http://www.omg.org/data/ Benchmarks.5.csv#kind=record> WHERE { {?metric metrics:metricValue?metric; \nrdfs:label?metric}. }\n\n\r\n\r\nOut: \r\n\r\n rdfs:label?metric.\n\n<|endoftext|>Nicotine replacement therapy for smoking cessation: more than a cure.\nSmoking is the leading cause of preventable death in the United States. One in six smokers will die of a smoking-related disease. The use of nicotine replacement therapy (NRT) for smoking cessation results in significant abstinence rates. Behavioral support methods (such as counseling or purchasing NRT) are associated with greater cessation success. The most effective formulation of NRT for smoking cessation is the combined behavioral support with transdermal nicotine. Physicians should recommend that smokers enroll in a cessation program, and they should consider the benefits of NRT when counseling smokers who want to quit.<|endoftext|>\n809 P.2d 46 (1991)\n115 Or. App. 621\nBarbara ROBINSON and Larry Robinson, Wife and Husband, Plaintiffs-Appellants,\nv.\nELMON BROTHERS, INC., an", "k: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark g:HAS_BENCHMARK?benchmark;?paper g:P31?benchmark.?cont g:HAS_DATASET?dataset. }\n\n };The titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are: \nP31, \n\"Abstracts' entities and relations annotated corpus\", \nrdfs:label, \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus\", \n\"Abstracts' entities and relations annotated corpus", "; uppose that the dataset name is datasets and the dataset URI is datasets:ai01. Then the best benchmark score of the benchmark papers on the Natural Questions (short) is 31. The metric of the benchmark papers is P31. \nfollowing is an explanation of the answer:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\n-- the best benchmark score and its metric of the benchmark papers is 31 and P31\n\n<|endoftext|>Q:\n\nSearch data in my database with two different colums\n\nIm trying to do a search in my database but its hard to see the search result only with the last column \"PRICE\". I want to search a field that is equal with UserName in the column 'USERNAME' in my database.  With this", "time (DCT, hinge, MSE, cross-entropy)\n\nA benchmark for a model on an annotation corpus dataset is an evaluation metric that compares the model\u2019s prediction with the true label for the data in the corpus. Some commonly used evaluation metrics are described in the following research papers.\n\n- Dataset Coupling and Cross-Dataset Transfer Learning: Evaluating Adversarial Strategies for Knowledge Graph Completion, by Pengfei Guo, et al., IJCAI-19.\n- A Parallel Benchmark on CIFAR-10: Evaluating Multi-Task Learning Methods for Deep Neural Networks, by Fawaz A. Abid, et al., IJCAI-19.\n- Limitations and Prospects of Few-Shot Learning on Few-Shot Benchmarkdatasets: Benchmarking GPT and RL, by Songxue Zeng, et al., IJCAI-19.\n\nThe datasets and evaluation metrics used in these papers are not publicly available, so we cannot provide the IDs of the research papers.\n\nThe corresponding Sparql query to obtain the three research papers is the following:\n\nSELECT DISTINCT?paper?paper_lbl WHERE {", "SELECT?benchmark?metric WHERE {?dataset a orkgc:Dataset;?benchmark var!dataset. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?benchmark_score?metric;?dataset_lbl a orkgc:DatasetLabel;?dataset_lbl rdf:resource?dataset;?dataset_lbl rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n\n\nCite: \u00a0Authors: Joseph Gilbert, Drew DeVito, Larry A. Vis708-732-6086; Published: 15 Oct 18; URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5812442/\n\n\nPaper 3\n\n\nAbstracts\u2019 entities and relations annotated corpus has been evaluated with the best-performing relation type upon the entire PubMed repository.\n\n\nThe dataset was evaluated on four metrics:\n\n\n- Precision (P): (number of true positives)/(number of positive samples)\n- Recall (R): (number of true positives)/(number of all samples)", ":SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Article 31: Benchmarking Datasets for Citation Classification with SPARQL and KGs. \n output (Sparql query):orpkgp:P31. HAS_BENCHMARK?benchmark. \n input (English text): Article 31. Benchmarking Datasets for Citation Classification with SPARQL and KGs. \n output (Sparql query): Database? Benchmarking?Datasets? for?Citation? Classification?with? SPARQL?and? KGs. \n input (English text): Database? Benchmarking?Datasets? for?Citation? Classification?with? SPARQL?and? KGs. \n output (", ":List the code links in papers that use the H-NLI model in any benchmark\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?paper_lbl. } \n input (English text): List the code links in papers that use the H-NLI model in any benchmark.\n output (Sparql query):", "sets the title and the ID of research papers that include a benchmark for the PubMed 20k RCT dataset\n\n Since the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are?paper and?paper_lbl, respectively, the Sparql query that retrieves these values is as follows:\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n The dataset Abstracts' entities and relations annotated corpus and the benchmark PubMed 20k RCT are declared in the figure below:\n\n figure=\"{\\\"slices\\\":[{\\\"id\\\":\\\"Abstracts' entities and relations annotated corpus\\\",\\\"name\\\":\\\"Abstracts' entities and relations annotated corpus\\\",\\\"label\\\":\\\"Abstracts' entities and relations annotated corpus\\\",\\\"description\\\":\\\"", "SELECT *\n {?paper <http://dl.acm.org/citation.cfm?id=2903705.2903720>?paper_lbl,?paper <http://dl.acm.org/citation.cfm?id=2903705.2903720>?paper_lbl,?paper?benchmark,?benchmark <http://dl.acm.org/citation.cfm?id=2903705.2903720>?benchmark_lbl}\n WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nructions: As of the time of this writing, the best performing model was an LSTM language model, while the second best performing model was aBiLSTM language model. The third best performing model was the GPT-2 model.", "output (Sparql query): * SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Nottingham dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset is `rd_NLL`.", "input (English text): Code references can be found in the paper\u2019s repository on github or arxiv.\n output (Sparql query): SELECT?paper_ref WHERE {?paper_ref rdfs:label \"Code references\"@en..?paper rdf:type \"Repository\"@en.?paper_ref dct:isPartOf?github.?github dct:uri \"https://github.com/\".?github dct:repository?paper.?paper_ref rdfs:label \"Code references\"@en.?paper rdf:type \"Repository\"@en.?paper_ref dct:isPartOf?arxiv.?arxiv dct:uri \"https://arxiv.org/\".?arxiv dct:repository?paper. }\n\n<|endoftext|>EXCLUSIVE: Warner Bros. has closed a straight to series order for the third season of The CW\u2019s Supergirl, it was announced today by Dawn Ostroff and Andrew Kreisberg, the voice and showrunner and co-executive producer on the series, respectively.\n\nSupergirl will see Kara Danvers (Melissa Benoist) gain powers from a meteor that falls in the Cactus town of", "set (a orkgc:Dataset; rdf:label \"iNaturalist 2018 dataset\")?benchmark (rdf:label \"Best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy\")\n input (English text): The best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy is Biologically-Plausible model, and the corresponding Benchmark is Abstracts' entities and relations annotated corpus.\n\n that are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset dataset?\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n P31  Biologically-Plausible model\n\n that are the titles and IDs of research", ":SELECT?paper_lbl?benchmark_score {?benchmark rdfs:label \"Ice Hockey Benchmark\"@en ;?paper rdfs:label \"Ice Hockey Benchmark\"@en ;?benchmark skos:BenchmarkScore@en \"33.33\"@en }\n\n airQuery:\n SELECT?benchmark_score?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ice Hockey Benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nryst:Ice Hockey Benchmark 33.33\n\n                         : SELECT \"Ice Hockey Benchmark\"@en \"33.33\"@en?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Ice Hockey Benchmark\")?benchmark orkgp:HAS_", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Here are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: \nTitles: \n- P31: Benchmarking Datasets for Scientific Results Extraction\n- P38: Benchmarking Datasets for Scientific Visualization\n- P40: Benchmarking Datasets for Scientific Computation\n- P46: Benchmarking Datasets for Multimodal Databases\n- P47: Benchmarking Datasets for Textual Entailment\n- P48: Benchmarking Datasets for Ontology-based Query Evaluation\n- P49: Benchmarking Datasets", "select distinct?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl500k) dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe SPARQL query returned the following results:\n\nRetrieval of the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset.\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n\nRetrieval of the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset.\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\n Benchmarking results across datasets in the Semantic Role Labeling research area:\n+---------------+----------------------------------+\n| Paper_label    | Paper                            |\n+===============+==================================+\n| P31            | Abstracts' entities and relations |\n| P31            | Abstracts' entities and relations |\n| P31            | Abstracts' entities and relations |\n+---------------+----------------------------------+\n\n+---------------+----------------------------------+\n| Paper_label    | Paper_lbl                        |\n+===============+==================================+\n| P31            | Abstracts' entities and relations |\n| P31            | Abstracts' entities and relations |\n| P31            | Abstracts'", "input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nQuery Output:\npaper   P31\npaper_lbl  Abstracts' entities and relations annotated corpus\n\nP31 contains a benchmark of the Abstracts' entities and relations annotated corpus dataset.", "itory (SPARQL query): SELECT?paper_lbl?paper_id?benchmark_id?metric WHERE { \n    ?dataset a orkgc:Dataset; \n     rdfs:label?dataset_lbl. \n     FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n    ?benchmark orkgp:HAS_DATASET?dataset. \n    ?cont orkgp:HAS_BENCHMARK?benchmark. \n    ?paper orkgp:P31?cont; \n     rdfs:label?paper_lbl. \n    ?paper?paper_id; \n    ?benchmark?benchmark_id; \n    ?metric?metric_id. \n     }\n\n input (English text): metric (Textual string): AUC (Abbreviated for Area Under the ROC Curve)\n\n input (English text): paper (Textual string): Kipman, Max; Garcia, Francisco; Song, Yiheng;... \n benchmark (Textual string): PWC Leaderboards (Restricted)\n\n output (English text): paper_id (Textual string): P31\n\n output (English", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }' \n input (English text): https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop.owl#L31 https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop.owl#L34 https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop.owl#L39 https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop", "Array {?paper?paper_lbl } \n\nThe data sources for this example include the Dataset and Benchmark resources from the Ingvars lab at UC Berkeley:\n  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nThe datasets benchmarked under Fine-Grained Image Classification research problem are:\n\n* P31 is a a benchmark for datasets that are annotated with the P31 relation.\n\n<|endoftext|>Q:\n\nwhat is : ~K@ in perl?\n\nI came across this :~K@ in perl, I can't seem to find it. What is it for and what does it do?\n$ touch ~K@\n$ touch ~K@\n$ ls\na.txt\nb.txt\nc.txt\nd.txt\n$ ls -l\na.txt  b.txt  c.txt  d.txt\n$ echo $?\n1", "parse (English text): rdfs:label \"Evaluation metrics\"@pt. \nSparql query output: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Evaluation metrics\"@pt)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nHere are the top 5 results:\n1\trdfs:label \"Evaluation metrics\"@pt\n2\trdfs:label \"Average precision\"\n3\trdfs:label \"F-measure\"\n4\trdfs:label \"Support vector machine\"\n5\trdfs:label \"Kendall's tau\"\n\n\nNext, provide a link to the documentation of each of the evaluation metrics.\n\nrdfs:label \"Evaluation metrics\"@pt: http://www.icsi.berkeley.edu/dls/drs/readings/papers/icdm14", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n presented in [1] are: terms, terms per million, terms per billion, terms per hundred million, terms per hundred billion, number of terms, number of terms per million, number of terms per billion, number of terms per hundred million, number of terms per hundred billions.\n\n Article [1] states the following: \"6.1.2 Term Information\n\n \"The following table lists the terms that annotators used to index the corpus, their count in the corpus and the frequency with which they appear in the benchmark.\"\n\n Article [1] also mentions \"Abstracts' entities and relations annotated corpus\" as the corpus identifier.", "output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sequential CIFAR-10 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): Sequential CIFAR-10 dataset\n output (Sparql query): P31\n\nience (Sequential CIFAR-10 dataset)", "output (Sparql query): SELECT?paper (distinct?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): http://www.semanticweb.org/chandralab/workshops/2015/abstracts-annotated-orpus/\n input (Sparql query): SELECT?paper (distinct?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The following models have been tested on the SciCite benchmark dataset:\n\n Paper \\#31: Berkhout, S., & Goecks, M. (2018). Entity linking in network news byveiled text with entity disambiguation. \u0105Data-driven disambiguation for weakly supervised news category extraction. Journal of the American Society for Information Science and Technology, 65 (11), 1801-1814.\n\n Paper \\#32: Hu, J., Berkhout, S., & Yang, C.-H. (2018). Discovering the ontology and taxonomy of [Wikipedia]{} with heterogeneous tree kernel. \u0105Tree kernel method for navigating large ontology collections. Journal of the American Society", "uest Output : SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nThe following paper titles have benchmarked models on the Atari 2600 Skiing dataset:\n\nMachine Learning on the Atari 2600: A Competitive Analysis by AutoZoo and Martin Arndt, C. O. C. Lum.\n\n SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;", ":List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?\n output (Sparql query): SELECT?paper_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): List the metric identifiers in the following paper titles: \n \"Performing weakly-supervised coreference resolution on CoNLL-2009\", \"Efficient extraction of rare phrases and relation types from full-text search log data\", \"An incremental approach to coreference resolution using RDF data\", \"A Coref-RL based approach for coreference resolution in RDF\", \"Coref-RL: A model-based approach for coreference resolution in RDF\", \"Cross-lingual coreference resolution in RDF\", \"Hierarchical treebanks for coreference resolution", ":Cart Pole (OpenAI Gym) benchmark Model\n output (Sparql query): SELECT * WHERE {?model?benchmark.?benchmark rdfs:label \"Cart Pole (OpenAI Gym) Benchmark Model\".}\n\n {#section-1924otine317}\nThe following research papers are the benchmark dataset (HAS_DATASET) and the benchmark (HAS_BENCHMARK) in the benchmarked entity and relation annotated corpus dataset:\n\n {#section-1924otine318}\nPaper:\nP31 Abstracts' entities and relations annotated corpus\n\n {#section-1924otine319}\nPaper_label: \"Abstracts' entities and relations annotated corpus\"\n\n {#section-1924otine320}\nbenchmark: Cart Pole (OpenAI Gym) benchmark Model\n\n {#section-1924otine321}\ncont: Cart Pole (OpenAI Gym) benchmark Model\n\n {#section-1924otine322}\npaper: P31 Cart Pole (OpenAI Gym) benchmark Model\n\n {#section-1924otine323}\nbenchmark: Cart Pole (OpenAI Gym) benchmark Model\n\n {#section-1924otine324}\npaper_label: \"Cart Pole (OpenAI Gym) Benchmark Model\"", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rfa-Gate-arccos model used in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n sop:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rfa-Gate-arccos model used in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe code link follows: Rfa-Gate-arccos model used in any benchmark\n\n forpaper:P31", "Input \n output (Sparql query):POSTH?paper h?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset;?cont orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?benchmark?paper_lbl. } \n\nSolution 1 (Complex solution): \nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n\nSELECT DISTINCT?paper?paper_lbl WHERE { \n?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. \n FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset. \n?cont orkgp:HAS_BENCHMARK?benchmark. \n?paper orkgp:P31?cont; \n rdfs:label?paper_lbl. \n} \n\nSolution 2 (More readable): \nSELECT DISTINCT?paper?", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BioASQ dataset evaluation metrics\")?metrics. }", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?cont orkgp:HAS_SCORE?score. } \n input (English text): The best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric is model_6.\n\n # Answer\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?cont orkgp:HAS_SCORE?score. }\n\n<|endoftext|>Wondering how a proposed 30-unit building for artists would be financed? Well, now you can see how the project is being funded and where the money is coming from, thanks to an interactive visualization created by Data Mapper, a Los Angeles-based data visualization studio.\n\nThe building, which would be located at 4130 Washington Blvd. in San Pedro, was announced by the city of Los Angeles back", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark a orkgp:HAS_BENCHMARK. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?paper_lbl. }", "penditures[Dataset:*, Benchmark:*, Paper:*, Contribution:*].\n\nItaliano: Quali sono i titoli e gli ID delle ricerche che includono un benchmark per gli entities e le relazioni annotati dal corpus del DATASET?\n\nSvedish: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Entitys e Relazioni Annotate dal Corrispondente\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nEnglish translation: Which are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset?\n\n\nSvedish: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl", "query:SELECT DISTINCT?paper WHERE {?paper?paper_lbl. FILTER (str(?paper_lbl) = \"code references\") } \n input (English text): Here is a sample of code references that were found in the paper \"Benchmarking graph-based methods for keyword-based information extraction\" by Xiong et al.  (https://www.aclweb.org/anthology/D17-1048). \n\"@datasets{xiong:acl17,\r\n    maintainer = \"Alessandro Xiong\", \r\n    title = \"Evaluating Graph-Based Methods for Keyword-Based Information Extraction\", \r\n    work = \"Alessandro Xiong, Liran Hertzberg, Steven Su, Asher Weinberg\", \r\n    url = \"https://www.aclweb.org/anthology/D17-1048\", \r\n    version = \"1.0\", \r\n    timestamp = \"2017\"\r\n }\"@/datasets{xiong:acl17}\"@/work{Alessandro Xiong, Liran Hertzberg, Steven Su, Asher Weinberg}", ":United Statesof America\n\n Output (Sparql query): The prevalence of epilepsy is highest in the United States of America\n\n Output (Sparql query): United States of America\n\n Input (English text): For which country of study overall prevalence of epilepsy is the highest?\n\nA:\n\nThe correct answer is in the comments, but the answer is:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n<|endoftext|>s divided by -88?\n1\nSuppose -4*p = p - q - 28, 0 = -p - 5*q + 28. Suppose -t - 2*l = l - 23, 2*l - 22 = -p*t. What is the remainder when 52 is divided by t?\n7\nSuppose 0 =", "ament?model?model_lbl\n output (Sparql query):\n\n SELECT DISTINCT?model?model_lbl WHERE {\n  ?model grl-pattern:And?model_pattern;\n   grl-pattern:hasNormalizedDataModel id?dataModel;\n   grl-pattern:hasAnnotationModel id?annotationModel;\n   grl-pattern:hasRelationsModel id?relationModel;\n   grl-pattern:hasDataModel id?dataModel;\n   grl-pattern:hasAnnotationModel id?annotationModel;\n   grl-pattern:hasRelationsModel id?relationModel;\n   grl-pattern:hasRelatedModel id?relatedModel;\n   grl-pattern:hasAnnotationModel id?annotationModel;\n   grl-pattern:hasDataModel id?dataModel;\n   grl-pattern:hasRelationsModel id?relationModel;\n   grl-pattern:hasRelatedModel id?relatedModel;\n   grl-pattern:hasAnnotationModel id?annotationModel;\n   grl-pattern:hasDataModel id?dataModel;\n   grl-pattern:hasAnnotationModel id?annotationModel", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HMDB51 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): HMDB51 is an open standard for benchmarking the recognition and identification of visual stimuli. The benchmark includes 11 classes of objects, 20,000 training images per class, and 1000 validation images per class. The benchmark dataset is composed of two parts. The first part is the HMDB51 validation set which is used to tune the model. The HMDB51 dataset is composed of 11 classes of objects. OpenCV has been tested on HMDB51 validation set, but does not support HMDB51\u2019s 11 classes of objects. The paper Hiring Human Body Model Datasets for Action Recognition in Real-World Environments describes 12 models, which include ORB, BRIS", "erring desk: Virtuoso SPARQL engine.\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX yago: <http://yago.org/ns#>\nPREFIX orkgp: <http://oraka.org/ns#>\nPREFIX orkgc: <http://oraka.org/ns#>\nPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\nSELECT?paper?paper_lbl?benchmark where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }", ":List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset? \n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rotowire (Content Selection) benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): List the types of algorithms that are used to evaluate the Rotowire (Content Selection) benchmark dataset?\n output (Sparql query): onCreate:List the types of algorithms that are used to evaluate the Rotowire (Content Selection) benchmark dataset? \n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Rotowire (Content Selection", "input (English text): The references in the code block can be found in this Github repository: https://github.com/openai/gpt4-papers. Please note that papers using Tsetlin Machine for its own validation have not been included.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Tsetlin Machine model for benchmarking purposes\")?paper_ref.?paper?paper_lbl.?paper_ref :has_paper_reference. } \n input (English text): \nHere is an example of how to query the code references in the Github repository:\n SELECT * FROM https://github.com/openai/gpt4-papers/blob/master/papers.csv WHERE file_uri =~ 'papers.csv'\n\n<|endoftext|>An effective humanitarian response to the Ebola epidemic in West Africa required an approach based on proven methods, a strong commitment by multiple countries and organizations, and financial resources. In order to mobilize resources for a comprehensive response and accelerate the development of a safe,", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ATARI 2600 Breakout\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric. }\n\nThe benchmark column of the dataset dataset has the following datatypes: \n<http://purl.org/dc/elements/1.1/Abstract>. \n<http://purl.org/dc/elements/1.1/Creator>. \n<http://purl.org/dc/elements/1.1/Presentation>. \n<http://purl.org/dc/elements/1.1/Format>. \n<http://purl.org/dc/elements/1.1/Language>. \n<http://purl.org/dc/elements/1.1/Subject>. \n<http://purl.org/dc/elements", "_Atari_Bowling_Evaluation_Metrics_Commonly_Used \n input (English text): [bowling_dataset]Evaluation Metrics are Commonly Used When Benchmarking Models on the Atari 2600 Bowling Dataset[/bowling_dataset] \n output (Sparql query): SELECT?paper (str(?paper_lbl)) WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): [Bowling Dataset]Evaluation Metrics are Commonly Used When Benchmarking Models on the Atari 2600 Bowling Dataset[/Bowling Dataset] \n output (Sparql query):#evaluation metrics are commonly used when benchmarking models on the atari 2600 bowling dataset# \n\n\n\f\n The evaluation metrics are Commonly Used When Benchmarking Models on the Atari 2600 Bowling Dataset, that listed here are : \n- Mean Average Error (MAE)\n- Mean Squared Error (MSE", "input (English text): List the code links in papers that use the FQF model in any benchmark.\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): http://query.nytimes.com/gadget/full.html?res=990DE4DB1638F93D3746DF0A531B8F947575DAD6&adx=1\n output (Sparql query): http://query.nytimes.com/gadget/full.html?res=990DE4DB1638F93D3746DF0A531B8F947575DAD6&adx=1\n input (English", ": SPARQL - SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark orkgp:BENCHMARK_SEQ_ERROR?paper_lbl. } \n input (English text): The best performing model was the Shapley-Fraley Multi-label Ranking algorithm.\n\n encounterer_labels: Abstracts' entities and relations annotated corpus\n\n done\n\n posititon \n\nThe best performing model was the Shapley-Fraley Multi-label Ranking algorithm.", "standard arithmetic (i.e., addition, subtraction, multiplication, and division) can be applied to benchmark results. The paper P31 and the benchmark P31 can be considered numbers, and addition is an operation that can be performed on numbers. Therefore, P31 and P31 can be added, which results in P32. Similarly, subtraction is performed on P31 and P31, which results in P31. Multiplication is performed on P31 by P31, which results in 8142, where 8142 is an integer value. Division is performed on P31 by P31, which results in an integer quotient of 5.29, where the quotient is also an integer value. \n input (English text): P31 is the value of the benchmark \"P31\". P31 is also the metric for the benchmark. P31 is integer value. The value of the benchmark \"P32\" is 8142. The value of the quotient is also an integer value. \n output (Sparql query): SELECT * {?paper?paper_lbl?benchmark?benchmark_lbl. } WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SciREX dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>                                Fourth Court of Appeals\n                                        San Antonio, Texas\n                                  MEMORANDUM OPINION\n                                           No. 04-13-00680-CV\n\n                                IN RE COMMITMENT OF M.H.\n\n                        From the 166th Judicial District Court, Bexar County, Texas\n                                     Trial Court No. 2012-W-02319\n                             Honorable Sid L. Harle, Judge Presiding 1\n\nOpinion by:       Rebeca C. Martinez, Justice\n\nSitting:          Catherine Stone, Chief Justice\n                  Rebeca C. Martinez, Justice\n                  Patricia O. Alvarez, Justice\n\nDelivered and Filed: November 21, 2014", "net:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"TSE-NER\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The TSE-NER dataset includes multiple research papers that evaluate models\u2019 performances on various types of TSE NER. The TSE NER dataset\u2019s official benchmark is the Abstracts\u2019 entities and relations annotated corpus.\n\n<|endoftext|>Q:\n\nruby on rails nil.date vs \"2013-09-12\"\n\nI'm fairly new to ruby on rails so forgive me if this is a stupid question.\nI have a very basic question. I have a model for books and I am trying to create a method to find all the books which are due in the next 30 days:\nclass Book < ActiveRecord::Base\n    def due_in_30_days\n        today = Date.today", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Code references in papers that have used the Concept Mention Extraction model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P32?cont; rdfs:label?paper_lbl. } \n input (English text): The following code references have been used in papers that have used the Concept Mention Extraction model for benchmarking purposes:\n\n P31 - https://github.com/googlespora/concept_mention_extraction/blob/master/ConceptMentionExtraction/DocumentRetrieval.py\n\n P32 - https://github.com/googlespora/concept_mention_extraction/blob/master/ConceptMentionExtraction/ConceptMentionExtraction.py\n\n A title of the research paper: Benchmarking Concept Mention Extraction", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): code links in papers that use the OTF spelling+lemma (single) model in any benchmark.\n output (Sparql query):    INSERT INTO #{output_table} (paper, paper_lbl, dataset, benchmark, cont, version) VALUES ('https://www.microsoft.com/en-us/research/uploads/bursts/rbursts190723.pdf', 'rbursts190723', 'orkgc:Dataset', 'orkgp:HAS_DATASET', 'true', '190723')\n\nThe code link is \n https://www.microsoft.com/en-us/research/uploads/bursts/", "of papers that include the link to their code: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdf:ID?benchmark. FILTER (str(?benchmark) = \"A3C LSTM hs model\")?code.?dataset a orkgc:Dataset;?paper rdf:ID?paper. FILTER (str(?paper) = \"Paper Title\"). }\n\nThe answer is: \nThe papers that include the link to their code are: \n- P31 \"Abstracts' entities and relations annotated corpus\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n- \"Paper Title\"\n-", ":SELECT * where {?paper rdf:firstName?paper_firstName;?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\";?benchmark rdf:firstName?benchmark_firstName;?paper rdf:firstName?paper_firstName;?paper rdf:label \"SVHN Benchmark\";?paper rdfs:label \"SVHN Benchmark\";?cont rdf:label \"SVHN Benchmark\";?cont rdf:label \"SVHN Benchmark\";?benchmark rdf:label \"SVHN Benchmark\";?benchmark rdf:label \"SVHN Benchmark\";?paper_lbl string;?paper_firstName string;?paper_secondName string; } \n input (Sparql query): The Percentage error score on the Abstracts' entities and relations annotated corpus benchmark is 82.97% where as the Second best performing model has a Percentage error score of 18.02% on the SVHN benchmark dataset.\n output (English text): The  Percentage error score on the Abstracts' entities and relations annotated corpus benchmark is 82.97% where as the Second best performing model has a Percentage error score of 18.02%", "<http://benchmarksgame.org/> The Benchmarks Game (TBG) is an open-source project to help create and compare performance baselines for NLP, distributed deep learning, and inference. Benchmarks Game datasets are freely available for researchers to use and reproduce results with. Benchmarks Game dataset codes are also available at. The Benchmarks Game dataset for the LayerNorm HM-LSTM model is provided in the dataset section of.\n\nIn order to use the benchmark, please perform the following steps:\n1. Go to the.\n2. To download all code and model definitions, first navigate to the model definition you want to use, and then go to the Downloads section at the bottom of the page. \n\nIf you are using one of the model definitions in the. SPARQL query:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.", "inity Challenge 2016 dataset Benchmark\n\n respond: \n\nicillin (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\n ballot_box_test1:\n dataset\n\nisten (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nballot_box_test", "text (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark;?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_DATASET?dataset.?paper orkgp:P31?benchmark. } \n input (English text): The highest benchmark result achieved on the Amazon-2 dataset is 70.907.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark;?cont orkgp:HAS_DATASET?dataset.?cont orkgp:HAS", "SELECT DISTINCT?paper?paper_lbl_out WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper o_cont:hasReview?review.?paper_lbl o_paper:hasReview?review_lbl.?review_lbl o_review:hasKeyword?keyword. }\n input (English text): I am aware of the link to code used in the paper here: https://www.kaggle.com/c/temporal-convolutional-network/benchmarks. It uses the Databricks` Spark language, but I am not sure how that is related to my question. \noutput (Sparql query): local https://www.kaggle.com/c/temporal-convolutional-network/benchmarks/index.html\n\n                                                                                   SELECT DISTINCT?paper?", "ary of research papers that have utilized the BiT-S (ResNet) model and include the links to their code: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset_lbl a biosrepp:Benchmark; rdfs:label?paper_lbl. } \n input (English text): Provide a list of research papers that have utilized the BiT-S (ResNet) model and have the dataset and benchmark identifiers included?\n output (Sparql query):", "map (Sparql query): SELECT * WHERE {?paper?paper_lbl.?benchmark hasDataset?dataset. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\")?benchmark_lbl.?paper hasBenchmark?benchmark.?paper_lbl a wnli.?cont orkgp:HAS_BENCHMARK?benchmark.?benchmark_lbl wnli.?paper_lbl.?paper_lbl a wnli. } \n input (English text): The highest benchmark result achieved on the WNLI dataset is 71.8%\n\nThe highest benchmark result achieved on the WNLI dataset is 71.8% \nThe metric and its value are wnli", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n receives:\n #paper  \n #paper_lbl \n #benchmark \n#dataset \n #cont \n #dataset_lbl \n\nOnDatabaseQuery: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nOnDatabaseQuery:?", "MAP :benchmark :model  \n input (English text): Orderer model is the dataset model. P31 is the model name.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Orderer model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): Orderer model is the dataset model. P31 is the model name.\n output (Sparql query): Alternatively, P31 is the model name.\n input (English text): Orderer model is the dataset model. P31 is the model name.\n output (Sparql query): P31 is the model name.\n input (English text): Orderer model is the dataset model. P31 is the model name.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset", ": SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CIFAR-10 Image Classification dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The CIFAR-10 Image Classification dataset\u2019s PER error metric is 0.21625% (https://www.cs.toronto.edu/~kriz/cifar.html).\n\nThe best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric is the following model:\n - Paper: P31\n - Paper Label: CIFAR-10 Image Classification dataset\n - Benchmark: PER\n\nennel:P31", "intialize the graph factory and the RDF store ://databricks/demo/core/spark-rdd- graph-diagram or//databricks/demo/core/spark-rdd- graph-diagram#spark-rdd- graph-diagram, or //databricks/demo/core/spark-rdd- RDF store: Installing the RDF store...Running initialization tasks for graph factory and RDF store...Getting references to entities from the RDF store... SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Walker, walk (DMControl500k)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?cont@dmcontrol500k rdfs:label?paper_lbl. } \n input (English text): The top result is <P31@dmcontrol500k>85.82 using 87.52 bytes of memory on the dataset, the benchmark and the", "select distinct?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scigen dataset evaluation metrics\")?eval.?paper orkgp:P31?eval. } \n input (English text): The following evaluation metrics are commonly used when benchmarking models on the SciGEN dataset:\n* Distinct, Recall, F1 Score, Precision, Coverage.\n* Some research also uses Mean Average Precision (AP) when compared with human performances on an APEval category evaluation.\n* The Scigen dataset evaluation metrics are trained and evaluated on the SciGEN training and test sets.\noutput (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Scigen dataset evaluation metrics\")?eval.?paper orkgp:P31?eval. }\n\nNote that the \"Scigen dataset evaluation metrics\" are trained and evaluated on the SciGEN training and test sets.\n\n<|endoftext|>Q:\n\nTime", ":List the code links in papers that use the POP3D model in any benchmark\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code links in papers that use the POP3D model in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): https://dl.dropbox.com/u/2070568/Datasets.csv\n output (Sparql query):SELECT?paper (SUM(LENGTH(str(?cont)))) AS?paper_lbl WHERE {?dataset a orkgc:Dataset;?cont orkgp:HAS_BENCHMARK ; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code links in papers that use the POP3D model in any benchmark\")?", "prehension (English text): the best model is the one that achieved the highest Score\n\n pillow (Sparql query): P31\n\n     \n  (Sparql query): SELECT * FROM < Dataset | Has Benchmark | P31 > WHERE Score", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UCF101 (finetuned) evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): Mean Average Precision (mAP), which is a popular metric used when benchmarking models.\n output (Sparql query): mAP", ":SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdfs:label?benchmark_lbl;?cont orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?paper_lbl rdfs:label?paper_lbl. }?\n\nMPQA benchmarks are available here: https://www.ling.upf.se/item/mpqa-benchmarks.\nThe titles of the research papers with MPQA benchmarks are: \n\nP31 - Abstracts' entities and relations annotated corpus\nP32 - Converting relations to mentions in abstracts \nP33 - Multilingual abstract concept recognition with reinforcement learning \nP34 - A system for abstractive text summarisation in Chinese \nP35 - Deep neural network architectures for neural sentence representation\nP36 - SemEval-2017 task 7: Linguistic features for semantic textual similarity and dialogue state retrieval.\nP37 - Two syntactic and semantic metrics for neural machine translation.\nP38 - Enhancing paragraph retrieval via collaborative filtering based on term-document embeddings. \nP39 - Ensembles of teacher-student networks for", "text (English text):The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 833,908. The result is obtained using the trained model M2+ and the benchmark containing the abstracts of papers in the BC5CDR-chemical dataset.\n\n guidance (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BC5CDR-chemical\" )?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?benchmark. } \n guidance (Sparql query): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 833,908.\n\n guidance (English text):The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 833,908. The result is obtained using the trained model M", "input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Habitat 2020 Object Nav test-std benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): Habitat 2020 Object Nav test-std benchmark dataset.\n output (Sparql query): HABitat 2020 Object Nav test-std benchmark dataset; SUCCESS=5.0\n\n implemenation : \nprivate java.util.ArrayList<HashMap<String, Object>> generateAllResults();\n\npublic void generateResults(SpagoBisModel spagoBisModel) {\n    HashMap<String, Object> result = new HashMap<String, Object>();\n    HashMap<String, Object> resultBenchmark = new HashMap<String, Object>();\n\n    for (Object paper : spagoBisModel.getPaper", "input (English text): The highest benchmark result includes the metric and score for the Paper Field dataset.\n output (Sparql query): SELECT * WHERE {?paper?paper_lbl. FILTER (str(?paper_lbl) = \"Paper Field dataset\") }\n\n<|endoftext|>Q:\n\nSqlAlchemy with postgresql - model cascades\n\nConsider a simple use case for a Postgresql database in which a single table tracks hierarchical relationships between groups of items.  Each item can be a parent of zero or more other items, and the goal is to report a list of the highest level parents for a given object.  To simplify things, I have separated the domain logic into a base classes and a derived classes for each grouping of related items, and have defined the necessary table/primary key relationships via declarative metadata.  So, here is the object definitions and domain classes for the generic parent item:\nimport sqlalchemy as sa\nfrom sqlalchemy import ForeignKey, Collection,Column, Integer, String\nfrom sqlalchemy.orm import relationship, sessionmaker\n\nfrom..base import BaseItem\n\nclass BaseItem(BaseItem):\n    __tablename__ = 'base_items'", ":Skip transform; +\n output (Sparql query): preprocess: tokenize; sentio: wordpiece; open feat: lower,upper; close feat: lower,upper; tokenizer: NER; feature: [token_number, token_en, token_de, token_cat, token_type, relation_type, token_text]; n-gram: [4,5]; pos: [1,2];\n output (Sparql query): +% Sparql+ Query SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n output (Sparql query): preprocess: tokenize; sentio: wordpiece; open feat: lower,upper; close feat: lower,upper; tokenizer: NER; feature:", "input (English text): The highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value is 31. This result is achieved using a configuration of 64 TFLOPs on the OpenAI Gym\u2019s Pixiu agent. The highest scoring result for this benchmark on the Atari 2600 Double Dunk dataset is 31. \n\nA query to filter the results by benchmark ID:\n SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark:metric:Value?value.?paper orkgp:P31?cont;?benchmark:metric:Name?name. }\n\nThe query returned 3 rows:\npaper", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe code used in the papers that include benchmark for the Abstracts' entities and relations annotated corpus dataset can be found here.\nThe three papers that include the benchmark can be found here.\nThe names and IDs of the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset can be found here.\n\n AIDS Research Summit: End to End Analysis of HIV Research Datasets Using Bi-Directional Extractive Trianing, Elisa W. Pfister, Dzmitry Z. Tsiala, Konstantinos Karampinos, Anthony Kontopantelis, Taylor Griffin, David Popen, Judith Herscher, Frank van Harmelen, John Tran, Luke", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cheetah\"@en)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark@en?DMControl500k. } \n input (English text): The Cheetah benchmark score is 40.0 and its metric is F1 score.\n\nIf you would like to see other benchmarks run on the Cheetah dataset, the following papers also contain benchmarks on the Cheetah dataset:\n\nP13 Cheetah: A Benchmark for Coreference Resolution on Corpus-based Semantic Web Applications. Vldai Wang, Ayan Bekholis, Alina Vlasova, Andrew Greskovich, Brian D. Neal, Hamed Persson, Hector Garcia-Correia, Ioana Pop, Oleg Khomenko, Per Selmerud, Phil Liberman, Simon", "nit (English text): \t\t\tSELECT DISTINCT?paper?paper_lbl WHERE { \t\t ?dataset a orkgc:Dataset; \t\t  rdfs:label?dataset_lbl. \t\t  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \t\t ?benchmark orkgp:HAS_DATASET?dataset. \t\t ?cont orkgp:HAS_BENCHMARK?benchmark. \t\t ?paper orkgp:P31?cont; \t\t  rdfs:label?paper_lbl. \t\t  } \n input (Sparql query):/dataset a orkgc:Dataset; rdfs:label Dataset_lbl/ Filter (str( Dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") has_dataset?dataset/ Has_dataset?dataset_lbl/ Has_dataset?benchmark/ Has_benchmark?dataset/ Has_dataset?cont/ Has_dataset?paper/ Has_paper P31/ Cont; rdfs:label paper_lbl/ Filter (str( paper_lbl) =", "ito Atari 2600 Alien benchmark score of 2037.65 with metric of 0.996476. \n input (Sparql query): SELECT?benchmark?score?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Alien dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?score?metric } \n\nAs a result, the selected two papers are: \nP31  2037.65 0.996476\nArena    2037.65 0.996476\n\n\nA benchmark is the number of 'yes' answers to the query. A benchmark metric is the percentage of 'yes' answers relative to the number of steps. \n\nMore details about benchmark scores can be found here: https://sundials.github.io/bert-datasets/bert-atari-2600-alien/\n\n\nBelow is the selected benchmark from the paper \"Atari 2600", "net:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Birdsnap dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The highest benchmark result achieved on the Birdsnap dataset is 2048.\n\n Holland, G., Bartosz, S., Stresing, M. & Manz, M. (2018). A benchmark for entity linking in literature search. In Proceedings of the 22nd ACM International Conference on Web Search and Data Engineering, Washington, DC, USA, 178-187.\n\n<|endoftext|>We are going to have to do a better job of convincing them of the value of coal if we want the industry to survive and thrive. They may think of coal as an outdated energy source, but we have to put ourselves in their shoes and ask them what else are they considering? There is a lot of uncertainty out there and", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Chopper Command dataset\") }\n\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Chopper Command dataset\") } \n\nThe following are the paper titles and IDs that contain a benchmark for the Atari 2600 Chopper Command dataset: \n\nP31 - Chopper. Dataset. Benchmark\n\n<|endoftext|>Tim James is an American journalist. He is currently the vice president of Investigative Reports at 60 Minutes. He was previously the Washington correspondent for 60 Minutes II, a producer for CBS's This Morning, and a producer for 60 Minutes. He received a B.A. in English from Kenyon College and a J.D. from University of Notre Dame. He and his wife, Lauren, have two children.\n\nThe new 60 Minutes episode \"The First Family Detail\" shows how the", ":Stanford Cars Benchmark Dataset for LiDIA Based Entity Extraction[P31] \nsince it has the relation HAS_DATASET with the specific dataset Abstracts\u2019 entities and relations annotated corpus", ": a support vector machine model", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SQuAD1.1 Metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset include: Precision, Recall, F1 Score, and the ARL.\n\n \u03c5sir@SQuAD1.1 Metrics\n\n \u03c5sir@SQuAD1.1 Metrics\n\n \u03c5sir@SQuAD1.1 Metrics\n\nThere are a total of 5 metrics used to evaluate models on the SQuAD1.1 benchmark dataset.", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } ?paper?paper_lbl. \n manifest \n-- ### Large mLSTM Benchmark for Abstracts' Entities and Relations Annotated Corpus \n-- [ https://github.com/ABFA-Lab/lm-lm/blob/master/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/benchmark/lm-lm/benchmark.csv  ]", "input (English text): ACL Anthology Dataset Benchmark Model Evaluations.\n\n i\u00e7input (English text): ACL Anthology Dataset Benchmark Model Evaluations\n\n i\u00e7output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACL Anthology Dataset Benchmark Model Evaluations\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }", "n  SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper :score?sc OR kgp:HAS_SCORE?sc. }\n\n\nThe following is the correct response:\n\nSELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper :score?sc OR kgp:HAS_SCORE?sc. } \n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dat", "input (English text): - BiLSTM: \u201cBiLSTM + MLM\u201d\n - GRU: \u201cGRU + MLM\u201d\n - LSTM: \u201cLSTM + MLM\u201d\n - GRU + LSTM: \u201cGRU + LSTM + MLM\u201d\n - DNN: \u201cDNN\u201d\n - Attn + DNN: \u201cAttn + DNN + MLM\u201d\n - MLP: \u201cMLP + MLM\u201d\n - Attn + MLP: \u201cAttn + MLP + MLM\u201d\n - Hybrid: \u201cHybrid\u201d\n\nionage (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BiLSTM: BiLSTM + MLM\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }", "output (Sparql query):  SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Softcite dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): For the Softcite dataset, the highest benchmark result is using the metric \"token count\" and the score \"0.029656\". \n output (Sparql query):* 0.029656 for \"token count\" and \"0\" for \"score\"*\n The highest benchmark result is using the metric \"token count\" and the score \"0.029656\". \n input (English text): The Softcite dataset has been used as the testbed for several research papers, including (but not limited to) the following research papers: \n-  [CLIPS: Semantic Layer for Entity Resolution and Information Extraction](https://link.springer.com/chapter", "\u0441\u0442\u0432handelsdelopmenterforutegnstadannelsites (For References)\n input (English text): The evaluation metric for Fashion-MNIST is adadem (Fashion-MNIST is not included in the original article). The abridged version of the article is available at https://www.dropbox.com/s/4mlcyyh6ksr1m4p/mnist_adadem.pdf.\n\n\n# select distinct paper,paper_lbl where \n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\n# SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(", "SELECT DISTINCT\n ?paper?paper_lbl\nWHERE {\n ?dataset a orkgc:Dataset\n  rdfs:label?dataset_lbl\n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")\n ?benchmark orkgp:HAS_DATASET?dataset.\n ?cont orkgp:HAS_BENCHMARK?benchmark.\n ?paper orkgp:P31?cont\n  rdfs:label?paper_lbl\n}\n}", "Sir; I have provided the answer to your question below:\n input (English text): Here is the list of research papers that include benchmarks on the RotoWire (Relation Generation) dataset:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n output (Sparql query):PL-P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P", "net (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The code used in the benchmark can be found in https://github.com/alainmameni/alain-mameni-icde-2016-workshop.\n output (Sparql query):idential (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper or", ":AverageReturn  returns:bar\n\n\n\n\n\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\n Notre but reste sur nos s\u00e9jours \u00e0 IME (International Media Editors' conference) \u00e0 Paris, o\u00f9 nous avons pu d\u00e9velopper notre projet.\n\n\nIme Editors' conferences are a great place to see how existing NLP works in practice. Several teams at IME had already demonstrated a variety of NLP techniques including named entity recognition, part-of-speech tagging and lemmatization. In addition to the demo sessions, IME provides several tracks for conducting in-depth technical discussion, including a track on Named Entity Recognition, another on Parsing, and a third one on Scaling NLP Systems.\n\n\nAlso during", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } \n\t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-31.rq</code></paper>\n\t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-32.rq</code></paper>\n\t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-33.rq</code></paper>\n\t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-34.rq</", "launch {\\\"http://mariadb.com/\\\" \\\"mariadb-connector-https\\\"} SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?benchmark_val.?paper orkgp:P31?benchmark_val.?paper_lbl?paper_lbl_val. } \n input (English text): The following papers report the best performing model for the WMT2016 English-Russian dataset in terms of BLEU score metric. \n - P31 \"Abstracts' entities and relations annotated corpus\"\n - P31 \"WMT2016 English-Russian benchmark\"\n - P31 \"WMT2016 English-Russian evaluation script\"\n\n // P31 \"Abstracts' entities and relations annotated corpus\"\n SELECT DISTINCT?paper?paper_l", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset.\n output (Sparql query): 0.51 \n\nA:\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;\n  rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.\n ?cont orkgp:HAS_BENCHMARK?benchmark.\n ?paper or", "set (a orkgc:Dataset), bench (rdfs:label \"Abstracts' entities and relations annotated corpus\"), benchmark (orkgp:HAS_DATASET), paper (rdfs:label \"P31\"), cont (orkgp:HAS_BENCHMARK), paper_lbl (rdfs:label \"rdfs:label\")\n\n\r\n\nThe answers are the following:\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark or", "ary?paper where {?paper?paper_lbl.?paper_lbl?paper_lbl. FILTER (str(?paper_lbl) = \"Penn Treebank (Character Level)\") }\n\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level\") )?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nAbstracts\u2019 entities and relations annotated corpus benchmark: P31\n\n<|endoftext|>Q:\n\nLimiting a Spark reduce on a group with a window\n\nI am using Spark's (v1.6.1) Window function \"reduce\" on a group, as illustrated below:\nval df = Seq(\n  (0, \"a\"),\n  (1, \"a\"),\n  (2, \"b\"),\n  (", ":SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text):http://labs.hermitian.net/software/anoide.shtml#sparql", "end:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?paper_lbl_cont. } \n\nThe following are the papers which have utilized the DY-MobileNetV3-Small model:\n* [airos2018ling] Deep Language Understanding Using Small CNNs.\n* [scarslet2009progressive] ProgressiveNet: Learning Fine-Grained Semantics from Comprehend.\n* [babaeizadeh2018dwam] DataWave: Distributed, Batch and Parallel Hierarchical Training of Deep Learning Models for Data warehousing Applications.\n* [xavier], [vervet], [xavier-vervet], [xavier-vervet-old] XAVIER: a Scalable GPU Cloud-Based Deep Learning Serv...\n*", "set_lbl:Abstracts' entities and relations annotated corpus rdfs:label \"FSNS - Test dataset\". FILTER (str( has_dataset( \"FSNS - Test dataset\" ) ) = \"has_benchmark\"  and benchmark(dataset(\"FSNS - Test dataset\")) = \"P31\").\n\nExplanation:\nThe following SPARQL query selects all research papers, along with their dataset, benchmark and abstract labels:\nSELECT ?paper?paper_lbl  WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset ;?cont orkgp:HAS_BENCHMARK?benchmark ;?paper orkgp:P31?cont;?benchmark ?paper_lbl. }\n\nThe following FILTER clause restricts the output to the research papers that include the FSNS - Test dataset as a benchmark:\nFILTER (str( has_dataset( \"FSNS - Test dataset\" ) ) = \"has_benchmark\"  and benchmark(dataset(\"FSNS - Test dataset\")) = \"P31\")\n\nThe final SELECT clause searches for the paper label with the ID P31,", "input (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Code references in papers that have used the MPAD-path model for benchmarking purposes\") } \n input (English text): There is no code reference in the paper referenced below using the MPAD-path model for benchmarking purposes. https://www.cs.cmu.edu/afs/cs.edu/user/skoglund/publications/chikav/mw16-zeng.pdf \n output (Sparql query): detriment (Sparql query): but this model is included in the MPAD model, implemented in the MPAD API, and includes support for both entity recognition and relation classification as part of its pipeline. In addition to this paper, there is other work using the MPAD-path model for benchmarking purposes that I believe is referenced in the following paper:  https://link.springer.com/article/10.1007/s10878-014-9451-0. While this paper uses the MPAD model, the paper", "uce distinct?paper where {?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BoolQ' models\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } }\n input (English text): P31 is a model that is available in the Models category.\n output (Sparql query): SELECT?paper_lbl WHERE {?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BoolQ' models\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } }\n input (English text): P31 is a model that is", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark. FILTER (str(?benchmark) = \"Abstracts' entities and relations annotated corpus\"). }\n input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n output (Sparql query):ido:P31 \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n output (Sparql query): \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n output (Sparql query): P31 \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n output (Sparql query): P31 \"Abstracts' entities and relations annotated corpus\" as the benchmark.\n input (English text): \"Abstracts'", "{?paper?paper_lbl where {?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper_lbl owlt:exactText \"Ball in cup, catch (DMControl100k)\"}. }\n\nThe answer is:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n...\n?paper?paper_lbl where {?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper_lbl owlt:exactText \"Ball in cup, catch (DMControl100k)\"}. }\n\n<|endoftext|>Q:\n\nHow to find coefficients for a", "inal (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Multi-Perspective Matching (ensemble) model in any benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): https://github.com/dmlc/glsviz/blob/master/viz/benchmark.Rmd#paper31, https://github.com/dmlc/glsviz/blob/master/viz/benchmark.Rmd#paper32\n output (Sparql query):aphysical path '/dmlc/glsviz/viz/benchmark.Rmd#paper31'http://dmlc.", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet 64x64 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): ImageNet 64x64 dataset has the title Abstracts' entities and relations annotated corpus and the ID http://www.ldc.upenn.edu/tagbert/Datasets/Abstracts%20entities%20and%20relations%20annotated%20corpus.\n output (Sparql query): Photos (Sparql query): SELECT?paper?paper_lbl {?paper orkgp:P31?cont;?cont orkgp:HAS_BENCHMARK?benchmark;?benchmark orkgp:HAS_DATASET?dataset;?dataset a orkgc:Dataset;?", ":SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CommonsenseQA\"^^xsd:string)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): CommonsenseQA.\n output (Sparql query): CommonsenseQA contains 37965 sentences, the F1 measure on the TACL corpus is 0.90, and the abstracts' entities and relations annotated corpus (P31) is a good baseline for the task.\n\nThe first line includes the necessary SPARQL 1.1 language elements. The second line filters the dataset (a, orkgc:Dataset) and the properties of the property graph which have the labels \"Abstracts' entities and relations annotated corpus\". The last line uses the distinct operator to retrieve only one result, in this case \"P31\".\n\nThe benchmark entity and relation in the third line is the", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Children examined in the studies\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?cont; rdfs:label?paper_lbl. } \n input (English text): Are children examined in the studies.\n output (Sparql query):\u00edses output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Children examined in the studies\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?cont; rdfs:label?paper_lbl. } \n input (English text): Are children examined in", "ary (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"CoQA dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): \"CoQA dataset\": http://rajpurkar.github.io/achine/CoQA/dataset.html", "?benchmark?benchmark_score?metric\n\n SELECT DISTINCT?paper?benchmark_score?metric\n\n?paper  rdfs:label \"Abstracts' entities and relations annotated corpus\"@en\n\n?benchmark imnet50@2\n\n?metric 91.09\n\n?benchmark_score 92.91\n\n?metric 91.09\n\n?benchmark\n\n?paper  rdfs:label \"Abstracts' entities and relations annotated corpus\"@en\n\n?benchmark_score 91.09\n\n?metric 91.09\n\n?benchmark\n\n?paper  rdfs:label \"Abstracts' entities and relations annotated corpus\"@en\n\n?benchmark_score 92.91\n\n?metric 91.09\n\n?benchmark\n\n?paper  rdfs:label \"Abstracts' entities and relations annotated corpus\"@en\n\n?benchmark_score 91.09\n\n?metric 91.09\n\n?benchmark\n\n?benchmark imnet50@2\n\n?metric 91.09\n\n?metric 91.09\n\n?metric 91.09\n\n?benchmark\n\n?paper  rdfs:label \"Abstracts' entities and relations annotated corpus\"@en\n\n?benchmark_score 92.91", "nulldata (Oracle DBUtils) SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nRunning the query yields the following results:\n\n.. |paper| image:: https://www.db-util.com/assets/uploads/DBUtils/ screenshot0.png\n.. |paper_lbl| image:: https://www.db-util.com/assets/uploads/DBUtils/ screenshot1.png\n|paper| paper is a DBpedia property with label \"P31\".\n|paper_lbl| paper_lbl is a DBpedia property with label \"Abstracts' entities and relations annotated corpus\".\n\nThe query appears to correctly identify and return one research paper that includes the", "set of metrics: hamming_distance count numdistinct, f_score mean, accuracy median, recall interquartile range, precision median. \nAll of the metrics below can be found in the \"Metrics\" class.\n\n Complete response: The titles and IDs of the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Founded in 1993 by brothers Tom and David Gardner, The Red Hot Chili Peppers became one of the most critically and commercially successful bands of the 1980s. Though their commercial breakthrough was to follow the success of their 1982 album Blood Sugar Sex Magik, the album spawned the singles \"Give It Away\", \"Give This Up\", and", "nect (http://www.semanticscholar.org/QueryingBioCreativeImproves;jsessionid=BCC3CE2272A89EAF76F4F1E646978FDE8?id=QL%3A%5BSELECT%20DISTINCT%20%28%22paper%22%2C%20%22paper_lbl%22%20%20%20%3B%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20", "ier a x 2:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Barabasi-Albert benchmark\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl a?benchmark.?paper_lbl b?benchmark.?cont;?paper_lbl c?benchmark.?paper_lbl d. }\n input (English text): The paper https://papers.nips.cc/paper/4824-lessons-learned- Liming Wang, Sumedh Chakraborty, Ali Ghodsi, and Andrew NG. \"Lessons learned: Persistent homology and unsupervised learning.\" AAAI (2016).\n\nThe paper \"Lessons Learned: Persistent Homology and Unsupervised Learning\" mentions the following metrics:\n\nMetric\tStatement\t\n-\tBarabasi-Albert Benchmark\t\"The Bar", "nit: The following metrics were used to evaluate models on the Atari 2600 Zaxxon dataset: Mean Average precision, Recall and F1 score.\n\nFollowing is the SPARQL query: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Zaxxon dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nReferences\n\nExternal links \n BioNLP\u201914 page\n Page with sparQL endpoint\n A unifying framework for annotation, mining and evaluation of semantics annotation, mining and evaluation of natural language annotations at ACL 2015\n Tutorial on Mining Semantic Web data using SPARQL\n\n*<|endoftext|>Wanted: Machines to Outperform Managers\n\n1. The race between managers and machines is on. 1Q20 data shows that the top 0.1% of managers do very poorly versus machines at", "str: #paper_lbl?paper?paper_lbl\n Lemma: #paper_lbl matched exactly\n Sparql Query: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n >> SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n >> [?paper_l", "mark [model name] as the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset. The best model was named RNNLM.\n\n\nThe best model on the WMT16 English-German benchmark dataset was named RNNLM.\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?bench", "ament:Dataset; \nAnnotation; \nrdfs:label \"Sentence Classification Research Area\"\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; \nrdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Sentence Classification Research Area\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; \nrdfs:label?paper_lbl. } \n\n-- | Dataset; \n-- Annotation; \n-- rdfs:label \"Sentence Classification Research Area\"\n\n-- | Dataset; \n-- Annotation; \n-- rdfs:label \"Sentence Classification Research Area\"\n\n-- | Dataset; \n-- Annotation; \n-- rdfs:label \"Sentence Classification Research Area\"\n\n-- | Dataset; \n-- Annotation; \n-- rdfs:label \"Sentence Classification Research Area\"\n\n-- | Dataset; \n-- Annotation;", "pend (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RotoWire (Content Ordering)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?\n output (Sparql query): * paper_lbl CLASS :Label rdfs:label \"RotoWire (Content Ordering)\"  paper_lbl = \"RotoWire (Content Ordering)\"\n * benchmark CLASS :Document/hasalgebra :nested :HAS_BENCHMARK rdfs:label \"RotoWire (Content Ordering)\"\n * dataset CLASS :Document/hasalgebra :nested :HAS_DATASET rdfs:label \"RotoWire (Content Ordering)\"\n * paper CLASS :Document/hasalgebra :", "net (Sparql query): SELECT DISTINCT?paper_metric?paper_metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian dataset evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): FLOPs are commonly used when evaluating machine learning models for zero-shot English-to-Italian language translation.\n output (Sparql query):)}{paper_metric :flops?paper_metric_lbl;}SELECT DISTINCT?paper_metric?paper_metric_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FLOPs\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BEN", "output (Sparql query): SELECT?paper (STRENGTH?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text):?paper_lbl\n input (English text): TITLES of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset\n output (Sparql query): scoopil\n output (Sparql query): \nPapers that include the Abstracts' entities and relations annotated corpus benchmark\n\n<|endoftext|>Managing the mental health of people with type 1 diabetes.\nType 1 diabetes is a serious chronic condition that requires lifelong maintenance treatment with injectable insulin. In recent years, there has been an explosion of information regarding the psychological aspects of this condition. This article aims to present a range of mental health issues commonly experienced by", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Models being evaluated on the MedSTS dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P32?cont;?paper orkgp:P33?cont;?paper orkgp:P34?cont;?paper orkgp:P35?cont;?paper orkgp:P36?cont;?paper orkgp:P37?cont;?paper orkgp:P38?cont;?paper orkgp:P39?cont;?paper orkgp:P40?cont;?paper orkgp:P41?cont;?paper orkgp:P42?cont;?paper orkgp:P43?cont;?paper orkgp:P44?cont;?paper orkgp:P45?cont;?paper orkg", "net (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Best performing model benchmarking the CoNLL++ dataset in terms of F1 metric\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\nThe answer is the following SPARQL query:\n SELECT DISTINCT \n  paper@p31 \n   paper@paper_lbl \n  WHERE \n    dataset@a \n     orkgc:Dataset\n     rdfs:label \n       = \"Best performing model benchmarking the CoNLL++ dataset in terms of F1 metric\"\n     has_dataset@a \n      ?dataset\n     has_benchmark@a \n      ?benchmark\n     has_dataset@cont \n      ?dataset\n     has_benchmark@cont \n      ?benchmark\n     paper@p31 \n      ?cont", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"UCF101 (Finetuned)\" )?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nThe outputs should be \n- P31\n- P33\n- P35\n- P41\n- P43\n- P45\n- P47\n- P51\n- P53\n\n\nNOTES:\n- Benchmark is used in the Abstracts' entities and relations annotated corpus datasets\n- The dataset datasets are included in the PDLS (Personal Datasets Lab) benchmark dataset\n- These benchmarks are: P31, P33, P35, P41, P43, P45, P47, P51, P53\n\n\nHere are the sample answers from the sparql query:\n- P31: UCF101 (Finetuned)\n- P33: U", "input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?\n output (Sparql query):SELECT?paper (STRVAL?paper_lbl) WHERE {?paper?paper_lbl. FILTER (str(?paper_lbl) = \"Abstracts' entities and relations annotated corpus\") }\n output (Sparql query):<p>* P31  : https://lsparks.org/works/33_Abstracts_Entities_and_Relations_Annotated_Corpus.html</p>\n output (Sparql query):<p>* 33 : Abstracts' Entities and Relations Annotated Corpus</p>\n output (Sparql query):<p>* paper : 33</p>\n output (Sparql query):<p>* paper_lbl : Abstracts' entities and relations annotated corpus</p>\n output (Sparql query):<p>* dataset : a orkgc:Dataset</p>\n output (Sparql query):<p>* dataset_lbl : \"Abstracts' entities and relations annotated", ", SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet + iNat on WS-DAN model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThis query returns three results.\n\nThe first result contains the paper P31 from the paper with the code https://paperswithcode.com/download/download-single-index-python/paper-p31.html. The query finds the paper and the benchmark and the relations between them.\n\nThe second result contains the paper P31 from the paper with the code https://paperswithcode.com/download/download-single-index-python/paper-p31-classifier.html. The benchmark is not in the first result and this shows that the benchmark was not included in the paper with the code https://paperswithcode.com/download/download-single-", "net (English text): The BUCC Russian-to-English benchmark contains seven metrics that evaluate the quality of machine translation. The metrics are fluentnicity, gurantic, bytes, chars, etymony, domain-specific grammer rules, and specific domain grammar errors. \n input (Sparql query):ruptingdb (English text): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English benchmark\")?benchmark.?paper orkgp:P31?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n output (Sparql query): shattered:metrics used to evaluate models on the BUCC Russian-to-English benchmark SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English benchmark", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ROUGE-1 metric\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nIn the response,?paper is the title of the paper that contains a benchmark for the Abstracts' entities and relations annotated corpus dataset.?paper_lbl is the label of the paper.?benchmark is the URI of the benchmark.\n\n * dataset is the title of the paper that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset.\n * dataset_lbl is the label of the paper.\n * benchmark is the URI of the benchmark.\n * papers is the short name of the paper set.\n * papers_lbl is the label of the paper.\n\n * P31 is the URI of the paper containing the best performing model benchmarking the AESLC dataset in terms of ROUGE-", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ModelNet40 score\")?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): ModelNet40 score: 7.82\n output (Sparql query): p31:ModelNet40 score 7.82\n\n \n      inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ModelNet40 score\")?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\np31", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Penn Treebank metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The following metrics are used to evaluate models on the Penn Treebank benchmark dataset:\n\n?paper-arity,?paper-depth,?paper-trie,?paper-LM,?paper-LCS,?paper-NER,?paper-NER,?paper-Chase,?paper-NER-LM,?paper-CoNLL2000,?paper-Conll2007\n\n?paper_lbl-arity,?paper_lbl-depth,?paper_lbl-trie,?paper_lbl-LM,?paper_lbl-LCS,?paper_lbl-NER,?paper_lbl-NER,?paper_", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): A benchmark for the Abstracts' entities and relations annotated corpus dataset is provided by doi:10.17487/d0e6a4ad-c0b3-4c4a-bbc0-a76984abb3a8.\n output (Sparql query): persists and contains P31 that include a benchmark for the Abstracts' entities and relations annotated corpus dataset.\n input (English text): A benchmark for the Abstracts' entities and relations annotated corpus dataset is provided by doi:10.17487/d0e6a4ad-c0b3-4c4a-bbc0-a76984abb3a", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Seaquest\"^^xsd:string)?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The best performing model that we found for the Seaquest dataset is the model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric with the configuration provided in the paper \"Beyond coreference resolution: a multi-task similarity metric for coreference resolution\". We also found that this model achieves 99.1% accuracy using 10-fold cross validation on the Seaquest dataset. The benchmark we used for this model is the has_dataset predicate. The paper that describes this model also provides the ID of the benchmark as well as the ID of the dataset.\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;", "sparql query \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SemEval'17:euro` 13 corpus dataset evaluation\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n<|endoftext|>Q:\n\nLaravel Eloquent Query results with nested results\n\nI have a result set with two levels of nesting:\n(\n[id] => 1\n[data] => \n    [item_a] => 1\n    [item_b] => 2\n    [item_c] => 3\n)\n\n(\n[id] => 2\n[data] => \n    [item_a] => 4\n    [item_b] => 5\n    [item_c] => 6\n)\n\nI need to get the results in an eloquent query where I can retrieve all the rows, or the rows with the", "ary:SELECT?paper (?paper uriFragment { \"http://purl.org/NET/objectenet/dataset.owl#\" concat ( string(str(?paper_lbl)) \",\"?dataset_lbl) }) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper rdf:type owl:Class. } \n input (English text):  Here are some research papers with their titles and IDs. \n  \nSELECT DISTINCT?paper (?paper uriFragment { \"http://purl.org/NET/objectenet/dataset.owl#\" concat ( string(str(?paper_lbl)) \",\"?dataset_lbl) }) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstract", "select?paper?paper_lbl where {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl rdfs:label?paper_lbl. }\n\n oracle:metrics (Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:benchmark (HAS_DATASET Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:benchmark (HAS_BENCHMARK Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:paper (P31 Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:paper_lbl (P31 Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:dataset (Dataset)\n\n oracle:cont (HAS_DATASET Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:cont (HAS_BENCHMARK Abstracts' entities and relations annotated corpus benchmark)\n\n oracle:paper (P31)\n\n oracle:paper_lbl (P31)\n\n oracle:dataset (Dataset)\n\n oracle:cont (HAS", "nit (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon-2 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the NTCIR-2009 dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"NTCIR-2009 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl_lnk WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark_lnk. rdfs:label?paper_lbl. }\n\n performed_task_dataset: databricks-rainbowplusseer\n\n performed_task_model: https://github.com/RainbowPapers/RainbowPlusSEER\n\n performed_task_architecture: https://github.com/RainbowPapers/RainbowPlusSEER/blob/master/architecture.md\n\n performed_task_hyperparameter: { \"dropout\": 0.5, \"num_layers\": 4, \"num_classes\": 10 }\n\n performed_task_pretrained_model: https://storage.googleapis.com/cloud-training-datasets/p31.", "input (Sparql query result): posting 29\noutput (Sparql query result): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark rdf:type orkgp:HAS_BENCHMARK;?paper?benchmark.?cont rdf:type orkgp:HAS_DATASET;?paper?cont.?paper_lbl rdf:type orkgp:P31. }\n\n<|endoftext|>Q:\n\nWhich is best way to get all the value from XML using XSLT 1.0?\n\nI have XML file as \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ROOT>\n  <CATEGORY>Electronics</CATEGORY>\n  <ITEMS>\n    <ITEM>\n      <ITEM_ID>1</ITEM_ID>\n      <ITEM_NAME>Samsung 33</ITEM_NAME>\n      <PRICE>19999.99</PRICE>\n    </ITEM>\n    <ITEM>\n      <ITEM_ID>2</ITEM_ID>\n      <ITEM_NAME>iPhone", "input (English text): The research papers that have benchmarked models on the WMT2016 English-Romanian dataset are:\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\n                            (Sparql query): include Abstracts' entities and relations annotated corpus", "text SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"MUTAG dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text):?dataset a orkgc:Dataset; rdfs:label?dataset_lbl.?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl.\n\n data (Sparql query):\n\n datatype:graphdatatypedecl\n\n paper:P31\n\n dataset:a\n\n benchmark:HAS_DATASET\n\n cont:orkgp:HAS_BENCHMARK\n\n paper_lbl:rdfs:label\n\n dataset_lbl:str( \"MUTAG dataset\")", ":Commonly used evaluation metrics when benchmarking models on the ESC-50 dataset.\noutput (Sparql query): SELECT?paper_lbl?benchmark where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ESC-50 dataset\")?paper_lbl. } \n input (English text): Evaluation metrics to use when benchmarking models on the ESC-50 dataset are as follows:\n Commonly used evaluation metrics when benchmarking models on the ESC-50 dataset include the Recall@K, Precision@K, and F1 scores.\n\n 515555555\n\n 515555555 Recall@K: 0.3895124741795124\n 515555555 Precision@K: 0.3003853173917193\n 515555555 F1: 0.341428577338839\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 515555555\n\n 5155555", "net has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset\n\n {#section.caption}\nBenchmark for P31 vs P40 for the Abstracts entities and relations annotated corpus dataset.\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n {#section.caption}\nP31\n\n {#section.caption}\nP40\n\n {#section.caption}\nP31\n\n {#section.caption}\nAbstracts' entities and relations annotated corpus dataset\n\n {#section.caption}\nThe F1 score on the SQuAD1.1 dev benchmark dataset for the models that participated in the shared task on Comprehensible Readability, as of June 2023.\n\nP40 : 0.929\n\nP", "n?model_name where {?model_name?dataset. FILTER (str(?dataset) = \"TempEval-3\") }\n\nMy answer: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\nThe papers that include the Abstracts entities and relations benchmark are:\n\nP31 - Evaluation of statistical and deep learning-based methods for named entity recognition on TempEval-3\nP16 - Boosting of semantic parsing with semantic role labelling\nP15 - Evaluating neosocial senti\ufb01ed LSTM and CNN on the TempEval-3 dataset\n\n<|endoftext|>PREDICTIONS FOR FUTURE WILL BE USE SUGGESTIVE: THE INSTANT COST CASH MACHINE BY GOOGLE.\n\nSome are going to say", "; SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): RotoWire (Content Ordering) dataset was created to study the problem of ordering documents in a corpus according to a given concept hierarchy.\n\n paper:P31 \"Benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric\"\n\n paper_lbl:Abstracts entities and relations annotated corpus\n\n benchmark:HAS_BENCHMARK\n\n paper:P31 \"Benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric\"\n\n benchmark:HAS_BENCHMARK BLEU 36.07\n\n output\n\n<|endoftext|>[Efficacy of psychopharmacological interventions in the treatment of autism spectrum disorders].\nAut", ":SELECT?benchmark?metric WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark?metric. } \n input (English text): The benchmark score is 24.37 and the metric is Median test accuracy.", "mit the titles of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Researchers have tested a wide variety of models on the Abstracts' entities and relations annotated corpus dataset. Some examples of these models include approaches to abstract reasoning, neural networks, distributional-based approaches, and semantic role labeling. See this page for a full list of research papers that have tested on the GENIA - LAS benchmark dataset: https://cloud. OakGM. com/s/8CZd0h. Please note that this is not a comprehensive list, as it includes only those research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset.", "input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nThe output of the query is :\n# authors: Perricone, Katherine; Yu, Jin; Lowd, Jonathan; Gehr, Weixin; Carrera, Andrea; Poor, Arjun; Lewis, Ran; Soatto, Christina; Toppins, Joelle; Song, Zhiyuan; Chikara, Karthik; Bordes, Nicolas; Chen, Xiao; Chinchor, Robert; Dax, Lina; Du, Luohang; Fletcher, Andrew; Gachon, Inhyeon; Gong, Yaxi; Grama, Manik; Gupta, Sudhanshu; Hu, Wei; J", "input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?\n\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Amazon benchmark\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): P31 indicates the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset.\n\n\nOutput:\n<paper>\n  <paper_lbl>Amazon benchmark</paper_lbl>\n  <paper_id>P31</paper_id>\n</paper>\n\n\nThe above output indicates the P31 model performed best on the Amazon benchmark dataset.\n\n\nA benchmark is used to evaluate and compare different model-based reasoning system on specific corpus datasets. Here is a list of different kinds of benchmarks that are commonly used in model-based reasoning", "pend (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkbh:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Bank Heist dataset\")?benchmark orkbh:HAS_DATASET?dataset.?cont orkbh:HAS_BENCHMARK?benchmark.?paper orkbh:P31?cont; rdfs:label?paper_lbl. } \n\nA:\n\nI think the correct query is:\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkbh:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Bank Heist dataset\")?benchmark orkbh:HAS_DATASET?dataset.?cont orkbh:HAS_BENCHMARK?benchmark.?paper orkbh:P31?cont; rdfs:label?paper_lbl. } \n\nBut it is not clear from your question what type of query this is. If this is a", "sparqlq: SELECT?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ANLI test dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): \"ANLI test dataset\"\n\n\nThe answer to the above question is: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ANLI test dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n\nThe full text of the above answer is:\nSELECT DISTINCT?paper?paper_lbl WHERE", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nResults:\n\nQuery Result #1:\n|paper|paper_lbl|\n|P31  |QNLI_BENCHMARK|\n\nQuery Result #2:\n|paper|paper_lbl|\n|P31  |QNLI_ABSTRACTS_ENTITIES_AND_RELATIONS_ANNOTATED_CORPUS|\n\nNote: P31 is the model identifier in Open Knowledge Platform.\n\nThe dataset \"Abstracts' entities and relations annotated corpus\" is included in QNLI benchmark dataset.\n\nReferences:\n\nBenchmark for Named-Entity Recognition in Biomedical Texts Using Recognizers Guided by Distribution", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a asterix:AsterixDataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Asterix dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P33?cont; rdfs:label?paper_lbl. }\n input (English text): What are the titles and IDs of research papers that include a benchmark for the Databricks dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a databricks:DatabricksDataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Databricks dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P43?cont; rdfs:label?paper_lbl. }\ninput (English text): What are", "ndex?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): Here is a query that yields a list of papers that benchmark the MFEC model:\n input (Sparql query):\n # Please add 'PREFIX': <your-namespace>: \n # \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; \n rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; \n?paper?paper", "ition (Sparql query): The code for the baselines and our model, as well as for code for other methods that are benchmarked in these papers, can be found at https://github.com/Yelp/cait-xs-36. The baseline codes are also available at https://github.com/Yelp/cait-xs-36/tree/master/experiments/benchmarks. Our code is available at https://github.com/Yelp/cait-xs-36/tree/master/code. This answer also provides a link to the full untrained corpus, which includes all annotated entities and relations.\n input (English text): https://github.com/Yelp/cait-xs-36\n\n union : SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rd", "SPARQL \nSELECT DISTINCT \n?paper?paper_lbl \nWHERE { \n ?dataset a orkgc:Dataset; \n  rdfs:label?dataset_lbl. \n  FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \n ?benchmark orkgp:HAS_DATASET?dataset. \n ?cont orkgp:HAS_BENCHMARK?benchmark. \n ?paper orkgp:P31?cont; \n  rdfs:label?paper_lbl. \n}\n\n<|endoftext|>Q:\n\nApply a lookup relationship to all records in azure table\n\nI want to apply a lookup relationship to all records in a azure table. There is a column (key) that has the lookup field attached to it. When I apply the relationship in the visual studio database tools, it applies the lookup relationship to each individual record. Is there a way to apply the relationship to all records in the table?\n\nA:\n\nYes, you can use a scan operation (scan in sql or execute in python) to apply the lookup relationship to the whole table.\nThe", "sparql (Sparql query):SELECT?paper (rdfs:label?paper_lbl) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Montezuma's Revenge benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n  The best performing model is the DistilBERT model with a Score metric of 0.965, where the Dev set was used as the held out test set.  The model architecture used is:\n\n  model: \"distilbert\"\n  model_param_prefix: \"distilbert\"\n  distilbert_size: 1280\n  hidden_dim: 1280\n  config: { sequence_length: 12 }\n  input_prefix: \"data/bert_dev\"\n\n\nPlease note that Benchmark datasets must have a primary key that matches the Sparql primary key, which can be found in the labels", ": SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n\nThe best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric is... \n\nP31.\n\nA paper that contains the full datasets, along with the benchmark for the entities and relations annotated corpus, is \nSparql query, Output 1.\n\nA paper that contains the full datasets, the benchmark for the entities and relations annotated corpus and an SQL query over the benchmark is\nSparql query, Output 2.\n\nPlease note that this answer depends on the data files being available publicly (i.e., the files are not part of the answer).\n\nOne paper that contains both the datasets and a benchmark for the entities and relations annotated corpus is\nSparql query,", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ResNet-152 (SAM)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The benchmark you are looking for can be found in my Reposted Paper. The benchmark is implemented using the Dataset() SPARQL function. To benchmark the ResNet-152 (SAM) model, we use the HAS_DATASET property of the Benchmark() class and the HAS_BENCHMARK property of the Dataset() class, as shown below: \n benchmark :has_dataset{ \r\n  resnet_152: has_model;\r\n  resnet_152: has_training;\r\n  } \r\n \r\n dataset :has_dataset{ \r\n  resnet_152: has_model;\r\n  resnet_152: has_training", "reset;Initial query execution took 1.59 seconds. querying \"manually\" took 0.09 seconds. \n initial query execution took 1.59 seconds. querying \"manually\" took 0.09 seconds. SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Pubmed dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): To benchmark a dataset, you must first create a benchmark by annotating its entities and relations. The Pubmed dataset has two published benchmarks: P31 and P41. The P31 benchmark contains 31 entities and 38 relations. The P41 benchmark contains 41 entities and 40 relations. The Pubmed dataset is included in the Zoccoli et al. 2002 benchmark dataset for automatic SPARQL query retrieval. The P31 and P41 benchmarks were obtained from page 31 and page 41 of the Zoccoli et", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n* `?paper` - a resource from the `dc:title`, `dc:creator`, `kctp:title`, or `skos:description` communities.\n* `?paper_lbl` - a resource from the `title`, `creator`, or `description` community.\n* `?benchmark` - a resource from the `has_dataset`, `has_benchmark`, `has_contribution`, `has_ontology`, or `has_property` communities.\n* `?cont` - a resource from the `has_dataset`, `has_contribution`, `has_ontology`, or `has_property` communities.\n\n<|endoftext|>by", "set (a orkgc:Dataset; rdfs:label?dataset_lbl)?benchmark{ rdf:description \"Abstracts' entities and relations annotated corpus\"@label; max??benchmark; fix (CONTAINING_ENTITY?benchmark).} \n input (English text): The highest benchmark result achieved on the REDDIT-B dataset is 62.27.\n output (Sparql query): From the paper Benchmarking Abstracts\u2019 Entities and Relations Annotated Corpus for Understandability, presented at QSSS 2017, we can see that the containing entity of the most recent benchmark is \"Abstracts\u2019 entities and relations annotated corpus\".\n\nprompt: The containing entity of the most recent benchmark is \"Abstracts\u2019 entities and relations annotated corpus\"\n\n\nA Sparql query to find the highest benchmark result on the REDDIT-B dataset:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATAS", "model;\n query results (Sparql query): [model, \"Softcite dataset\", \"Abstracts' entities and relations annotated corpus\"],\n query error (Sparql query): There is no model with that name on this server.\n\n\n\n\n\nAs part of creating this dataset, I benchmarked 6 models against it to see which models perform the best on the dataset. Benchmarking is the process of comparing 2 models to see which performs better than the other. The following are the benchmark models: \n\n- XGMR GAN (XGMR) \n\n- DSST-GANN (DSST)\n\n- Joint Likelihood (JL)\n\n- Clean Graph (LG)\n\n- FastText (FT)\n\n- BERT (BERT)\n\n\nBenchmark results can be found here. The results show that FT, BERT and JL perform the best on the dataset.\n\n\nI then created a separate SPARQL query that lists all the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset. The query is as follows: \n\nSELECT?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-2 Binary classification benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'\n\nmodels that have been tested on SST-2 Binary classification benchmark dataset:\n\n  * Abstracts' entities and relations annotated corpus\n  * Tensorflow LSTM\n  * LightGBM\n  * XGBoost\n  * Adaboost\n  * Logistic Regression\n  * k-nearest neighbors\n  * Decision Trees\n  * Support Vector Machines\n  * Linear Regression\n  * Linear Discriminant Analysis\n  * Decision Stump\n\n\nThe titles and IDs of the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are:\n\n* Abstracts' entities and relations annotated corpus\n* Tensorflow L", "SELECT?benchmark HendrycksTest_score HendrycksTest_metric where {?dataset a orkgc:Dataset;?benchmark HendrycksTest_score?benchmark; HendrycksTest_score?benchmark_score; HendrycksTest_metric HendrycksTest_score HendrycksTest_metric; HendrycksTest_score + HendrycksTest_metric >= 5.0.0.0.0 }\n\nA:\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>Inhibition of PPD stimulation induces histamine release from peritoneal mast cells in rats.\nThe production of cytokines such as histamine by human mast cells is believed to play a major role in the recruitment of cells of", "input (English text): I'll take a look at the code used in the paper you linked to. The link is to a repository of open source machine learning code. The code in the repository is based on KD-LSTMreg, which is the name of the model trained and benchmarked in the paper. The code repository also contains a description of the datasets and the metric used for performance evaluation.\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl?benchmark WHERE {?dataset a orkgc:Dataset;?dataset_lbl rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl rdfs:label?paper_lbl. } \n input (English text): The repository does indeed contain the code used for the paper you linked to. The code for KD-LSTMreg is based on the Keras deep learning framework. The repository also contains datasets,", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgp:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl rdf:label. }\n\n<|endoftext|>Leather's new haircut. (Leather's hair, I mean.)\n\nGet a hair cut that looks good on you every single day. Yes, it may take some time and effort. But the results will yield big dividends over time. And don\u2019t forget \u2014 you only have one body. So if you\u2019re going to spend all that time and energy maintaining it, you might as well spend that time and energy on something that reflects your best self.\n\nLearning to give and receive a haircut.\n\nI\u2019ve been taking it for about a month now. My eyebrows are still a little stubby but I\u2019m getting there. My hair feels a little heavy, but I expect that as it heats up, it\u2019ll get lighter.\n\nAnd thus, Leather\u2019s New Hairstyle.", "input (English text): Two of the models that have been evaluated on ClueWeb09-B are: \"ElasticR\", \"Darmstadt12\". \n output (Sparql query): SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ClueWeb09-B models\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P31?cont; rdfs:label?model_lbl. }\n\n GPC-Model: ElasticR P31\n\n GPC-Model: Darmstadt12 P31\n\nemetery: ElasticR Darmstadt12\n\n Cemetery: Darmstadt12\n\nindecomposable: ElasticR\n\nindecomposable: Darmstadt12\n\n Batch: ElasticR\n\n Batch: Darmstadt12\n\n<|endoftext|>[Desktop Entry]\nEncoding=UTF-8\nName=Ruler\nComment=Simple ruler app for Windows 10\nComment[eng]=Simple ruler app for Windows", "input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): DocRED (Human-annotated) is a corpus dataset created by annotating the DocCrawl corpus using crowd-sourcing. DocCrawl is an Internet crawl with the goal of collecting web pages from all corners of the World Wide Web. The DocCrawl Corpus contains millions of web pages from over one hundred domains. The goal of using this data for machine learning is to teach a model which websites provide high quality information and can be used as reliable reference for web pages. The models learned from this data can then be used to generate summaries of the websites crawled from the DocCrawl corpus. \n DocRED is an open repository for research on DocRED (Human", ". DOCUMENT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset_lbl a vocab-20200314.rdf ; rdfs:label?paper_lbl. } \n input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset_lbl a vocab-20200314.rdf ;?paper?paper_lbl. } \n output (Spar", "input (English text): Here are the code links from the papers that benchmark the FG fine-grained gate model:\n\nThese code links provide an explanation of the metrics used in the papers. The metrics include the F-score and the area under the ROC curve (AUC). \n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FG fine-grained gate model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nSELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FG fine-grained gate model\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp", ":SELECT?paper (str:tokenize(?paper, \",\")) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?benchmark.?paper_lbl rdf:value?paper_lbl. } \n input (English text): \"The highest benchmark result achieved on the NYT29 dataset, including the metric and its value, is P31, with an average precision of 0.72.\"\n output (Sparql query): Cork:SELECT?paper (str:tokenize(?paper, \",\")) WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?", "set (a orkgc:Dataset)?dataset_lbl\n benchmark (has ohttp:Benchmark)?benchmark_lbl\n paper (descrip tors of language models)?paper_lbl\n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"QNLI dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Here are the names of the QNLI metrics used in the F1@1 and F1@5 benchmarks:\n metric (short name)  F1@1       F1@5\n----------------------------\n doc-match            doc-match  \n length-distinct      length-distinct\n length-median         length-median\n length-average        length-average\n word-match            word-match\n word-mismatch", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n ?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n ?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl", "initials query: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"SST-5 Fine-grained classification evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P32?cont;?paper orkgp:P31?cont;?paper_lbl?paper_lbl. } \n input (English text): The SST-5 Fine-grained classification evaluation metrics are common metrics to use when benchmarking models on the SST-5 Fine-grained classification dataset. \n output (Sparql query):Initials from the Initials query: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?paper?paper_lbl. FILTER (str(?paper_lbl) = \"SST-5 Fine-grained classification evaluation metrics\"). } \n\nNote: Initials query does not contain any sparql as", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Natural Questions (long) dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The highest result for the Natural Questions (long) dataset is a metric of 59.11.\n output (Sparql query): * text:Natural Questions (long) dataset (Metric: 59.11)\n input (English text): The highest result for the Natural Questions (long) dataset is a score of 57.\n output (Sparql query): * text:Natural Questions (long) dataset (Score: 57)\n\n Westar Hospital's radiology department created the Natural Questions (long) benchmark dataset. The dataset includes 78,721 triples and 3,778 annotations. The following are the names of the research papers that include a benchmark for the dataset:\n\nastolico, S., Gl", "<paper>P31</paper>\n <paper_lbl>Abstracts' entities and relations annotated corpus</paper_lbl>\n <benchmark>rdfs:label \"WMT2016 Czech-English\"</benchmark>\n <cont>31.2981</cont>\n <paper>P31</paper>\n <paper_lbl>WMT2016 Czech-English</paper_lbl>\n <benchmark>BENCHMARK</benchmark>\n <cont>31.2981</cont>\n\nThe answer is the following query in Sparql:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 Czech-English\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n<|endoftext|>The authors confirm and confirm that the data underlying the findings are fully available without restriction. All relevant data are within", ":SELECT * FROM dataset d WHERE d.sector IN ('Utilities', 'Power', 'Water', 'Transport', 'Communications', 'Industrials') AND d.type = 'Directory' AND d.frequent.\n\n coordinator = ogr_call_method('CreateDataSource', None, None, ogr_call_args((str(self.gdal_datasource), str(self.gdal_datasource_lyr), str(self.gdal_datasource_type), str(self.gdal_datasource_type_lyr), str(self.gdal_datasource_spatial), str(self.gdal_datasource_spatial_lyr), str(self.gdal_datasource_hierarchy), str(self.gdal_datasource_hierarchy_lyr), str(self.gdal_datasource_driver), str(self.gdal_datasource_driver_lyr), str(self.gdal_datasource_parameters), str(self.gdal_datasource_parameters_lyr), str(self.gdal_datasource_fields), str(self.gdal_datasource", "; SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text):?paper?paper_lbl;?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK;?cont orkgp:HAS_DATASET;?paper_lbl string \"Kinetics-600 dataset\".\n\n assistance with this question will be provided upon request.\n\n You can find a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset by running the query below:\n\n SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Kinetics-600 dataset", "Commissioner, MM, Guha, SS, Ando, T, & Schiele, BR (2017). Labeling and Annotating Rotowire Data. In 18th International Workshop on Open Data (WODAR). Florence, Italy: International Association for Computational Linguistics. [CSPLD-2017-0011](http://cspl.io/file/CSPLD20170011.pdf). 83 pp.\n #Paper Title\n #P31 Labeling and Annotating Rotowire Data\n\n #Paper ID\n #P31 P31\n\n #Label\n\n #dataset Abstracts' entities and relations annotated corpus\n\n #benchmark Has_dataset\n\n #benchmark Has_benchmark\n\n #cont Rotowire (Content Selection)\n\n #paper P31\n\n #paper_lbl Labeling and Annotating Rotowire Data\n\n #paper_lbl P31\n\n #paper_lbl P31\n\nA:\n\nHere is the correct query:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities", "uate using a query language specific to the ontology. In this case, we will use the language SPARQL. SELECT?paper (precision?precision?recall?f1) WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl  dbpedia_owl:annotatedBy (node-property(rcs:Article,'/snippet/title'[text()])).?paper?paper_lbl }\n\nDataset - http://dbpedia.org/resource/Abstracts' entities and relations annotated corpus\nHas_Dataset - domain specific bag of words algorithm in combination with the bag of words feature\nHas_Benchmark - Dataset\nP31 - has_dataset property\ncont - has_benchmark property\nrcs:Article - classifies the Article element from the dbpedia Article resource using the resource properties/node-property", "output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dmlab-30 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n\f\n<paper>\n\t<paper_lbl>Abstracts' entities and relations annotated corpus</paper_lbl>\n</paper>\n<paper>\n\t<paper_lbl>Dmlab-30 dataset</paper_lbl>\n</paper>\n\n\u00e4sidentman@openknowledge.net(English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Dmlab-30 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS", ":SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2016 German-English dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): For example, this is the title and ID of one of the papers: P31: Deng, Jianchao; et al. 2019. Mapping English Verb forms to Chinese Simplified Characters. Language Technologies Association annual conference (May), 5770-5783. doi:10.1007/978-3-030-18494-6_11. \n output (Sparql query):P31: Deng, Jianchao; et al. 2019. Mapping English Verb forms to Chinese Simplified Characters. Language Technologies Association annual conference (May), 5770-5783. doi:10.1007/978-3-030-18494-6_", "?paperP31 swingup (DMControl500k) dataset P31 with DM control 500k avg. this is the highest benchmark score ever achieved on the Cartpole dataset.\n\nThe provided answer can be obtained by applying the following Sparql query to the dataset Abstracts\\' entities and relations annotated corpus in the databeseado humian:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nAnswer: swingup (DMControl500k) dataset P31 with DM control 500k avg.\n\n The highest benchmark score ever achieved on the Cartpole dataset is 71.30.\n\n There are a total of 33 research papers published on the Cartpole dataset. The title and ID of the paper with the highest benchmark result are as follows: \n   SELECT", "sparql (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?cont a orkgp:HAS_BENCHMARK?benchmark;?paper?paper_lbl. } \n input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }?paper?paper_lbl. \nLSTM (Bai et al., 2018): P31 \nRDF dumping with Python: http://semantics.cs.uu.nl/dl/RDF/SparqlHome.html# datasets-benchmark-papers \nDatasets Benchmark Papers\nRDF dumping with Java: http://semantics.cs.uu.nl/dl/RDF/SparqlHome.html# datasets-benchmark-papers \nDatasets Benchmark Papers", "pend (Sparql query): SELECT?paper?paper_lbl?metric?value WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark rdfs:label?paper_lbl;?value oiiao:MetricValue. } \n input (English text): The best result returns the metric value for Pets dataset: OIiao:MetricValue = 2.92. \n output (Sparql query):*paper_lbl:P31:OIiao:MetricValue:2.92* \n input (English text): The dataset Oxford-IIIT Pets is a dataset that includes entities with the properties of name, type, cite, and description. \n output (Sparql query): *dataset_lbl:P31:name:edgar* \n input (English text): The benchmark returned in the query returns the metric value", "ins (English text): Here is one possible query using the identifiers provided in the answer text and a selected code from one of the papers that I was able to find. SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): The code below is from the following paper: Espe", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"EfficientNetV2-L\")?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe code references can be found in the following research papers that used the EfficientNetV2-L model for benchmarking purposes:\n\nGuo, Chen, Zhe Wang, Xiaoou Tang, Shou Tang, and Kai Xu. \"EfficientNet: Towards Improving DNN Training Through Post-Training Ladder Analysis.\" arXiv preprint arXiv:1904.02767 (2019).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jiajun Xiao, Qi Cheng, Da Hong, and Kai Xu. \"Improved Loss Functions and Architectures for Deep Neural Networks (DNNs) Based on Eases of Eigenvector Triangularization.\" arXiv preprint arXiv:1511.02531 (2015).\nYouquan Gao, Pengchen", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-46985 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?dataset a orkgp:Dataset; rdfs:label?paper_lbl. } \nThe research papers returned by this query are: \nBiological Sequence Ontology: P31 \nEfficient and accurate protein structure classification based on protein sequences using SVMs and machine learning approaches: P31, Benchmark: Abstracts' entities and relations annotated corpus \nConditional Random Fields for Sequence Labeling: Benchmark: Abstracts' entities and relations annotated corpus", "sets (English text): #SELECT DISTINCT?paper WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark;?paper OR kgp:P31?cont.?paper_lbl rdf:type SparqlReturn.?paper SparqlReturn.?benchmark SparqlReturn. } \n input (English text): For each paper, also include the benchmark name, dataset name and relation name.\n output (Sparql query):ddingpaper (English text): #SELECT DISTINCT?paper?paper_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_B", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark a orkgp:A3?a3. } \n input (English text): P31 is the model identifier of the best performing model benchmarking the ANLI test dataset in terms of A3 metric.\n output (Sparql query):ordering by?benchmark:A3?a3.\ninput (Sparql query response):\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK", "ien Naturalist 2019 Benchmark Dataset Evaluation Metrics { \n   datameets: iNaturalist 2019 Benchmark Dataset Evaluation Metrics;\n }\n\nFollowing the data model, there are two entities benchmark and dataset, and three relations has_dataset, has_benchmark and has_relation. The SPARQL query below returns the datasets and the benchmark that have a has_dataset and has_benchmark relationship:\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"iNaturalist' 2019 Benchmark Dataset Evaluation Metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe dataset and benchmark retrieved with the query above are:\n Dataset: \nDataset=UnikonPathos https://www.unikonpathos.com/en/home\nDataset_lbl=iNaturalist' 2019 Bench", "store (SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset ; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Space Invaders\")}\n input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?\n output (Sparql query):?paper?paper_lbl WHERE {?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?cont; rdfs:label?paper_lbl.}\n\n replied as \"Oficio: Modelos de automa\u00e7\u00e3o baseados em Sentimentos de satisfa\u00e7\u00e3o e feedback de usu\u00e1rios\"\n\nCentralized model repositories contain a body of information that describes a set of machine learning models and how the models were trained and tested. In particular, each model contains the formulae used to generate the model outputs, the training data, and the evaluation metrics used to evaluate the model's outputs.  A benchmark is a comparison of model performance to a", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (WMT2014 English-German benchmark dataset JSON-LD snippet):{\"dataset\":{\"@id\": \"http://www.w3.org/2020/05/WWW/session/P31\", \"@type\": \"orkgc:Dataset\"},\"benchmark\":{\"@id\": \"http://www.w3.org/2020/05/abstracts-relations-annotated-dataset\", \"@type\": \"orkgp:HAS_DATASET\"},\"cont\": {\"@id\": \"http://www.w3.org/wiki/Core_Language_Techniques\", \"@type\": \"orkgp:HAS_BENCHMARK\"},\"paper\": {\"@id\": \"", "input: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThis is a good benchmark for papers that include a model on the CART_POLE dataset. The paper titled \"Cart Pole (OpenAI Gym) dataset\" includes a model on the CART_POLE dataset.", ":SELECT DISTINCT?paper WHERE { { \n# ProblemStatement Schema\n ?problem strmbase:name \"a practical problem\". \n ?problem strmbase:category \"Dataset\". \n ?problem orkgp:P31?cont. \n ?cont rdfs:label \"Abstracts' entities and relations annotated corpus\". \n ?dataset a orkgc:Dataset. \n  }\n\n  {\n ?dataset strmbase:name \"a practical dataset\". \n ?dataset rdfs:label \"Dataset\". \n ?dataset orkgp:HAS_DATASET?dataset. \n  }\n\n  {\n ?dataset strmbase:name \"a practical dataset\". \n ?dataset rdfs:label \"Abstracts' entities and relations annotated corpus\". \n ?dataset a orkgp:HAS_DATASET?dataset. \n ?benchmark orkgp:HAS_BENCHMARK?benchmark. \n ?benchmark rdfs:label \"Abstracts' entities and relations benchmarked\". \n ?paper orkgp:P31?cont. \n ?paper rdfs:label", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The paper published in ACL 2019 titled \"BERT: Precomputing Recommended Separators forsent Recognition\" by Zou et al. use the BiDAF + Self Attention + ELMo model for benchmarking purposes. A code reference is \n<https://github.com/huangxiaojie/bert-benchmark>\n which you can find in the paper.\n\nThe paper includes the following benchmark:\n\n  <https://github.com/huangxiaojie/bert-benchmark/blob/master/paper/benchmark.json#L7-L", "output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Metrics used to evaluate models on the NCBI Disease benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n respond (Sparql query): metrics used to evaluate models on the ncbi disease benchmark dataset\n\n respond (Sparql query): P31 P-value\n\n respond (Sparql query): P31\n\n respond (Sparql query): P-value\n\n respond (Sparql query): P-value\n\n respond (Sparql query): Mean average precision\n\n respond (Sparql query): Mean average precision\n\n respond (Sparql query): Mean average precision\n\n respond (Sparql query): Mean average precision\n\n respond (Sparql query): Mean average precision\n\n respond (Sparql query): Mean average precision\n\n respond (Sparql", "input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Code references in papers that have used the 6-layer QRNN model for benchmarking purposes\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): https://github.com/abreuwe/qrnn-benchmarking\n\n Final query executed: https://api.hubapi.com/search/v2/query?q=http%3A%2F%2Fgithub.com%2Fabreuwe%2Fqrnn-benchmarking&state=....&pretty=true\n\n Output (Sparql query): PRETTY: \"true\", \"paper\", \"", "ibe:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WOS-11967 dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe following models are being evaluated on the WOS-11967 dataset: \n  * Ontram\n  * C3NET\n  * RuleBrew\n  * MetaGRID\n  * BiGRU\n  * LSTM\n\nA benchmark is a named tuple that includes the entity types and relation names that are annotated in the dataset.", "<query server>/openstudio/queryserver/rest/tau/v1/query/?query=SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20", "input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Datasets used for benchmarking in the Robot Navigation research problem\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Datasets used for benchmarking in the Robot Navigation research problem\n output (Sparql query):", "box:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n input (English text): Searching for \"bert, GoogLeNet, LSTM, CNN\" returned the following papers:\n{{#paper}}\n  {{paper_lbl}}: {{paper}}\n{{/paper}}\n\n<|endoftext|>Hi. I'm Nathan Ridge, the designer of Zeppelin.\n\nI get a lot of questions about Zeppelin from newcomers to the Magic game, so I wanted to make a blog post to address some commonly asked questions.\n\n1. How did Zeppelin get its name?\n\nWe played a lot of Magic: The Gathering when we were younger. My oldest son, Nathan, was 3 years old at the time. We found a set", "n 'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }'\n\nThe query result includes the following pairs of labels and papers:\npaper_lbl paper\nP31     IMDb-M benchmark dataset", "sparql query for code references: SELECT DISTINCT?paper WHERE {?paper?paper_lbl;?paper_lbl rdf:type foaf:Agent. } \n input (English text): The paper [1] includes code references: [2]. The code references include information about the implementation of PEGASUS in Java [3]. The Java implementation is available in the file [4] (org.pegasus.modeling.P31).\n\nThe code reference at [1] is from the paper [2]. The paper [2] uses the code reference at [3] to implement the PEGASUS model for benchmarking purposes. The code reference at [3] includes information about the Java implementation of PEGASUS in the file [4]. The Java implementation of PEGASUS is available at [4].\n\n Memorial Sloan-Kettering Cancer Center, 631 Fifth Ave, Suite 3060, New York, NY 10021, USA\n    <http://www.mskcc.org/research/publications/publication/view/4738>\n\n Memorial Sloan-Kettering Cancer Center,\n\n    Pegasus; version 3.0\n\n    * Contact: Greg Douglas", "escript { concat?paper_lbl,?paper?paper_lbl } WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Audio Classification research area\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Audio Classification research area\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nA:\n\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?", ":Cart Pole (OpenAI Gym) benchmark result highest score 90.657566.?paper_lbl a <http://paperdatasets.org/dataset/CartPole>:Cart Pole (OpenAI Gym) Benchmark. \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Cart Pole (OpenAI Gym) dataset?\n output (Sparql query): SELECT DISTINCT?paper WHERE {?dataset a cartp:Cart Pole (OpenAI Gym).?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl a <http://paperdatasets.org/dataset/CartPole>:Cart Pole (OpenAI Gym) Benchmark. }\n\n*[P31]: The paper identifier.\n\n\n# Artifical Intelligence Benchmark Benchmark\n\n\n# Full ResultSet\n\n\n# Annotated Entities and Relations\n\n\n# Cart Pole (OpenAI Gym) Benchmark\n\n\n# Cart Pole (OpenAI Gym) Benchmark Score 90.657566\n\n\n# Cart Pole (", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?cont orkgp:HAS_DATASET?dataset. FILTER (str(?dataset) = \"ImageNet\")?benchmark. }\n\nThe answer is the paper \"Multiple Annotationsts for Disjoint Settings in Conditional Random Fields\". It includes the following titles and IDs for the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset:\n\n\"Multiple Annotationsts for Disjoint Settings in Conditional Random Fields\", Jia Deng, Ting Liu, Heng Xiao, Tongcao Gong, Kilian Weinberger, Simone Teufel, titled the paper, which includes the following abstracts: \n\nThe model in the paper \"Multiple Annotationsts for Disjoint Settings in Conditional Random Fields\" achieves the highest Macs score on the ImageNet benchmark dataset. \n\nNote that the benchmarks used in the research papers are not necessarily the same, and the names of the research papers and the datasets used", "input (English text): \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\"\n output (Sparql query): SELECT?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper_lbl?benchmark_metric_score. } \n input (English text): \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\"\n output (Sparql query): SELECT?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\")?benchmark orkg", ". SELECT?paper_lbl?benchmark WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): The metrics that are used to evaluate models on the DBpedia benchmark dataset are the following:\n\n   - Mean Reciprocal Rank (MRR)\n   - Normalized Discounted Cumulative Gain (nDCG)\n\n\u00c3\u00c2\u00c3\u00c2 You can also use the SPARQL endpoint at http://dbpedia.org/sparql to query the benchmark dataset.ifulm (SPARQL query):", "BeckerAtari2600EnduroDATATOOLS\n\n SAMPLE_QUERY\n\n SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark ne spores:value \"8,719,937\".?paper orkgp:P31?cont;?cont g:HebertAtari2600EnduroDATATOOLS.?paper }\n\n/*\n * }\n */\n\n<|endoftext|>import Vue from 'vue';\nimport { mount } from '@vue/test-utils';\nimport ElementFiller from '..';\n\ndescribe('ElementFiller', () => {\n  describe('new', () => {\n    it('should create an instance', () => {\n      const wrapper = mount(ElementFiller);\n      wrapper.setData({});", "InputStream (HtmlPage):<html>...<body>...<p>There are 22 RDF datasets available for evaluation in the Hutter Prize Competition, namely RDF Movie Reviews, RDF Planets, RDF Slashdot, RDF Sports, RDF Proceedings, RDF Reddit, RDF GeoNames, RDF Ontology of Chemistry, RDF Ontology of Biology, and RDF Freebase</p>...</body></html>\n input (English text): There are 22 datasets for evaluation in the Hutter Prize Competition. The metrics of evaluation over these datasets can be found in this RDF dataset: https://github.com/openlinksw/HutterPrize/blob/master/datasets/metrics.rdf. \n output (Sparql query): SELECT?paper (count (*))?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"RDF Movie Reviews\", \"RDF Planets\", \"RDF Sports\", \"RDF Proceedings\", \"RDF Reddit\", \"RDF GeoNames\", \"RDF Ontology of Chemistry\", \"RDF Ont", ".benchmark is #P31,seel.cse.lsu.edu/data/re17.zip,0.4239259;.\n\n            \n input (English text):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThere is one result: #P31,seel.cse.lsu.edu/data/re17.zip,0.4239259;", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n<paper number=\"31\" paper_lbl=\"WOW\" metric=\"22.8\">\n<benchmark number=\"16\" benchmark_lbl=\"WORLD RECORDS\" metric=\"22.8\">\n\nA:\n\nBased on your query you can use \n?paper?paper_lbl where {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp", "ary:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n input (English text): https://opendatasors.org/items?q=12-layer%20Transformer-XL%20benchmark&in_filter=0\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?benchmark <http://www.emdat.org/ontology/Score/>?score;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The best performing model on Atari 2600 Up and Down dataset in terms of Score metric is Model X.\n output (Sparql query):SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Score\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont", "input (English text): [?dataset_lbl],?benchmark,?paper,?paper_lbl. \n output (Sparql query): SELECT?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): [ \"Abstracts' entities and relations annotated corpus\", \"Exp, arXiv only\", \"P31\", \"Annotated corpus\" ] \n output (Sparql query):* Dillon S. Brown, Sara Brown, Paul N. Howard, Jane Doe, Abstracts' entities and relations annotated corpus,\"\n input (English text): [ \"Exp, arXiv only\", \"P31\", \"Annotated corpus\" ] \n output (Sparql query): Dillon S. Brown, Sara Brown, Paul N. Howard, Jane Doe", "ite (SELECT * FROM <Query> WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\n Example output: \n Paper    P31 \n Benchmark    HAS_BENCHMARK\n\n<|endoftext|>Q:\n\n\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442 \u0438\u0437 \u0441\u0441\u044b\u043b\u043a\u0438 \u0431\u0435\u0437 \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043f\u043e\u0434 linux?\n\n\u0414\u043b\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u0432 bash \u043c\u043e\u0436\u043d\u043e \u0442\u0430\u043a\nbash /home/me/\u043f\u0443\u0442\u044c/\u0443\u0434\u0430\u043b\u0438\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442/bash\n\n\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u044e \u0432 Bash \u0432\u0438\u0434\u0435\u043b, \u0447\u0442\u043e-\u0442\u043e \u0432\u0440\u043e\u0434\u0435 \u044d\u0442\u043e\u0433\u043e\nrm -rf.cache/work/remmina-0.9.6/* \n\n\u0418\u043b\u0438 \u0432\u043e\u043f\u0440\u043e\u0441, \u0440\u0435\u0430\u043b\u044c\u043d\u043e \u0442\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442? \u0412\u0435\u0434", "?benchmark?paper_lbl\n\nThe following is the answer to the question:\n\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ObjectNet dataset\")?benchmark orkgp:HAS_DATASET?benchmark.?paper orkgp:P31?benchmark;?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\nThe following is the answer to the question:\n\n output (Sparql query): ObjectNet dataset (DART) benchmark P31.\n\n (\\[?benchmark :idObject\\]?paper)\n ObjectNet (DART) benchmark P31.\n\n (\\[?cont :idDataset\\])\n ObjectNet (DART)\n\n (\\[?cont :idDataset\\]?paper)\n ObjectNet (DART) benchmark P31.\n\n (\\[?paper :idCont\\])\n ObjectNet (DART)\n\n (\\[?paper_lbl :label\\])\n ObjectNet (DART)\n\nA:", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. }\n\n\nThe following titles are included in this list:\n- Aram Ametger, Alexander Binder, Alexander Hartmann, Alexander Papp, Alexander Schreiber, Alexander Stock, Alexander Wahlster, Alan De Martino, Alan Millar, Alan Meyenberg, Andrew Clark, Andrew Demuth, Andrew Fowler, Andrew Kahan, Andrew Moes, Andrew Motora, Andrew Northen, Andrew Ritchie, Andy Hodson, Andy Long, Andriy Lysytskiy, Anton Battellini, Anton Chekhov, Anton Konjevod, Anton Rudolf, Anton van Vliet, Antoine Cully, Antony Murphy, Artemis Stavrinos, Artur Sahade", "set (FGVC Aircraft benchmark dataset) rdfs:label \"FGVC Aircraft Benchmark\" WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"FGVC Aircraft Benchmark\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont;?paper_lbl a foaf:label \"FGVC Aircraft Benchmark.\" } \n input (English text): The model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset was the, CRAFT model.\n output (Sparql query): SELECT * WHERE {?dataset a orkgc:Dataset;?dataset_lbl a foaf:label \"FGVC Aircraft Benchmark.\"?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont;?paper_lbl a foaf:label \"FGVC Aircraft Bench", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")?benchmark orkgp:HAS_DATASET?benchmark.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The following metrics are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset: 1. number of lives; 2. number of frames per life; 3. number of wins; 4. number of losses; 5. average frames per loss; 6. game complexity; 7. number of unique objects.\n\nThe benchmark dataset and their corresponding metric names are:\n count(*) \ncount(*) \ncount(*) \ncount(*) \navg(frames) \ngame complexity \nnumber of unique objects", "net:SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Road Runner game data\") }\n input (English text): The benchmarks that have been performed on the Road Runner game data.\n output (Sparql query):\n text:Road Runner game data\n text:Abstracts' entities and relations annotated corpus\n text:Benchmark for the Abstracts' entities and relations annotated corpus", "net (Sparql query): SELECT?model?model_lbl?cont WHERE {?dataset a orkgc:Dataset;?model?model_label; rdfs:label?model_label; FILTER (str(?model) = \"Atari 2600 boxing\")?cont ;?cont orkgp:HAS_DATASET?dataset. } \n input (English text): Atari 2600 boxing is a widely used game playing benchmark. It is a model evaluations problem if the model produces the correct result but the run time is too long. The F1 score is widely used as the performance metric.\n output (Sparql query):'?model_lbl':'Atari 2600 boxing', '?'model_label':'Atari 2600 boxing', '?'model':'Atari 2600 boxing', '?'cont':true\n\n\u00a0\n \u00a0\n\nThe paper's entities and relations benchmark are:\n\n* Atari 2600 boxing\n* F1 score", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"WMT2014 French-English dataset evaluation metrics\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): N-gram based BLEU, ROUGE, ACE, CIDEr, SPICE, METEOR, iPOST, NIST etc are commonly used evaluation metrics for machine translation systems.\n output (Sparql query):", ":SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper orkgp:P31?cont;?paper_lbl a rdfs:label?paper_lbl. } \n input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?\n output (Sparql query): \u00e4n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"BUCC French-to-English dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): https://github.com/bidel/dfweb-embedding/blob/master/model/data/bimodal.py \n output (Sparql query): it contains code that can be used to benchmark BiDAF + Self Attention + ELMo model. Please also check https://github.com/bidel/dfweb-embedding/blob/master/model/data/bimodal.py\n\niersp (Sparql query):lod convert public.occi.ism.AR.2018-03-16T00-00-00.parsedSkim-train training -d wsml https://docs.wso2.com", "mark (SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"AESLC dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. })\n input (English text): The AESLC benchmark models evaluated are the DistilBERT model, the DistilGPT model, the DistilSE model, the XLNet model, and the TGNet model.\n\nMSG= The AESLC benchmark models evaluated are the DistilBERT model, the DistilGPT model, the DistilSE model, the XLNet model, and the TGNet model\n\n via=DistilBERT,DistilGPT,DistilSE,XLNet,TGNet\n\n via=DistilBERT,DistilGPT,DistilSE\n\n via=DistilBERT,DistilGPT,DistilSE\n\n via=DistilBERT,DistilGPT,", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"HoC dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The benchmark for the HoC dataset is in paper P31 in the dataset \"Abstracts' entities and relations annotated corpus\".\n output (Sparql query):?paper?paper_lbl \n input (English text):?paper_lbl\n\n<|endoftext|>i know, i know... don't try to guess who it is...\n\n -----Original Message-----\nFrom: \tHerndon, Rogers  \nSent:\tMonday, February 04, 2002 10:43 AM\nTo:\tLenhart, Matthew; Bass, Eric; Smith, Merrill; 'vince.lentrice@att.com'; 'andrew.wilson@mail.house.gov';'siva66@", "set (a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\"))\n input (English text): I want to find the code references in papers that have used the PAR Transformer Large model for benchmarking purposes.\n output (Sparql query):SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper rdfs:label?paper_lbl. }\n input (English text): SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_B", "input (English text): <a href=\"https://github.com/baidu-research/xlnet\">https://github.com/baidu-research/xlnet</a>, <a href=\"https://arxiv.org/abs/1906.11164\">https://arxiv.org/abs/1906.11164</a>, <a href=\"https://doi.org/10.1021/acs.cgd.9b00979\">10.1021/acs.cgd.9b00979</a> \n output (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } <a href=\"https://github.com/baidu-research/xlnet/blob/master/README", "'SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }'\n\n\n# Note that all code, and datasets used to test this model, are attached here:\n\n# https://github.com/Linkedin/CL-Titles-Parser/blob/master/docs/benchmarks.md\n# https://github.com/Linkedin/CL-Titles-Parser/blob/master/docs/dataset.md\n\n\n# Examples of titles that should appear in the?paper_lbl column:\n# 1. \"CL-Titles-Parser: An RNN based framework for named entity recognition in scientific abstracts\"\n# 2. \"RNN-based approach to named entity recognition in scientific abstracts\"\n# 3.", "SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Habitat 2020 Object Nav test-std benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?model orkgp:P31?cont;?model_lbl rdf:label?model_lbl. }\n\n import examples/Habitat2020ObjectNavBenchmark.ttl\n\n import examples.triplettesting.Habitat2020ObjectNavBenchmark;\n import testing.Expected;\n\n import testing.freesql.FreeSQLTest;\n\n runExamples();\n\n input \n triplettesting.Habitat2020ObjectNavBenchmark model;\n {\n   \"model\" : \"Habitat 2020 Object Nav test-std benchmark dataset\", \n   \"model_lbl\" : \"Habitat 2020 Object Nav test-std benchmark dataset\"\n }\n\n runTest();\n\n input \n expected \"Habitat 2020 Object Nav test-std benchmark", "SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper rdfs:label?paper_lbl. } \n input (English text): A model that performs well on the Atari 2600 Name This Game dataset would be one that accurately labels objects in the game's sprite inventory. There are several approaches to this classification task, and benchmarking models is one way to assess them. A benchmark is a query that asks a model to provide the IDs of the objects in a dataset, and that model is evaluated on the availability of well-defined benchmark classes for the target dataset. The paper \"Annotating Atari Game Inventories with Weakly-Supervised Learning\" presents an approach for classifying the sprites in Atari's \"Name This Game\" as either human or robotic. This approach used multi-label logistic regression with hierarchical soft", "get the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value.\n output (Sparql query):   SELECT * WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont.?benchmark :hasValue?metric.?paper orkgp:P31?cont;?benchmark :hasValue?metric;?paper orkgp:P31?cont;?metric :hasValue?value. } \n input (English text): The Atari 2600 Freeway dataset has five entities (images and their corresponding bounding boxes) with the relation HAS_BENCHMARK (see Figure 1 below). In paper P31, authors annotated these entities and relations with the SQuADPOST51 benchmark, and achieved a precision of 91.40%, a recall of 91.40%, and F1-measure of 91.40%.\nFigure", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Cornellbenchmark.model:GloVe; Abstracts\u2019 entities and relations annotated corpus corpus dataset.\n output (Sparql query):?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): Gl", "pend (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe following is the solution to the problem.\n <query>\n\n <where>\n  <Contains>\n   <contains item=\"rdfs:label\" attribute=\"Abstracts' entities and relations annotated corpus\">Abstracts' entities and relations annotated corpus</contains>\n   <contains item=\"rdfs:label\" attribute=\"AlexNet, MultiGrasp\">AlexNet, MultiGrasp</contains>\n  </Contains>\n  <filter>\n   <enrichmentIsOn target=\"#hasDataset\" target=\"#hasBenchmark\"/>\n  </filter>\n </where>\n\n <orderby><leq>rdfs:label</leq", "n: SELECT DISTINCT?model?model_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Ms. Pacman dataset\")?model. }\n input (English text): Ms. Pacman is a classic game. In this model, the Atari 2600 displays Pac Man for you to hit. This is a highly acclaimed model. \n output (Sparql query):\u043e\u0441\u0441n: SELECT?model WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Ms. Pacman dataset\") }\n\nossip (Sparql query): practition: SELECT?model WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Atari 2600 Ms. Pacman dataset\") }\n\nimonit (Sparql query): practition: SELECT?model WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FIL", "pend (Measure) a :Measure;string \"Recall\"; string \"Precision\"; string \"F1 score\"; string \"MCC\"; string \"MCC (Harmonic)\"; string \"MAP\"; double \"Sensitivity\"; double \"Specificity\"; double \"F1 Sensitivity\"; double \"F1 Specificity\"; string \"Ranking Score\"; double \"Ranking Score (Harmonic)\";\n output (Sparql query): SELECT?metric ( Measure?metric;?metric \"Recall\";?metric \"Precision\";?metric \"F1 score\";?metric \"MCC\";?metric \"MCC (Harmonic)\";?metric \"MAP\";?metric \"Sensitivity\";?metric \"Specificity\";?metric \"F1 Sensitivity\";?metric \"F1 Specificity\";?metric \"Ranking Score\";?metric \"Ranking Score (Harmonic)\") WHERE {?dataset a orkgc:Dataset;?dataset_lbl rdfs:label?dataset_lbl;?benchmark orkgp:HAS_DATASET?dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark;?paper orkgp:P31?cont;?paper", "output (Sparql query): SELECT * FROM <html> <body> <table> <caption>Top scored SPARQL queries for the BC5CDR-disease dataset</caption> <tr> <th>Query</th> <th>Score</th> </tr> <tr> <td>SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } </td> <td>148.090</td> </tr></table></body></html>\n\nA:\n\nTo find the names of RPTs in your corpus dataset, first convert your corpus dataset to ontology dataset:\ncensus:dataset-to-ontology\n\nand then use the SPARQL query\nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a", "inal (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ImageNet 64x64 benchmark dataset\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }\n\nThe SPARQL query that was used to get the list of papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset is as follows:\n\n spurql query: SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?", "SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?\n output (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont;?paper?paper_lbl. } \n input (English text): List the title and", "Batcher (https://github.com/pbourke/AbstractAbstraction) - P31 \n input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code.\n output (Sparql query): SELECT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n\n\n[\n  {\n    \"paper\": \"Bacher, P.\"\n    \"paper_lbl\": \"P31\"\n  }\n]\n<|endoftext|>In keeping with the festive holiday spirit, we have decided to host a giveaway of sorts, to commemorate both the holidays and the release of Legends of Light & Legend of the Sky and Beyond.\n\nFor the entry, all you have to do is download the game,", "est (Sparql query): SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"T-ConvS2S model code\") } \n input (English text): See https://github.com/facebook/T-ConvS2S. Learned automatically from the provided T-ConvS2S model code.\n output (Sparql query): performed successfully. - 1 result: \n  { \"paper\": \"https://github.com/facebook/T-ConvS2S/blob/master/model_zoo/literals.py\", \"paper_lbl\": \"T-ConvS2S model code\" } \n  \n\nThe code provided in the above link is used to benchmark the T-ConvS2S model.  The benchmark is found in the \"model_zoo\" folder of the T-ConvS2S model and is named \"literals.py\".\n\nThe following code queries the datasets and the benchmarks that are contained in the dataset and return a list of data points:\n\nSELECT DISTINCT?", "n. SELECT?paper_lbl WHERE {?dataset a orkgc:Dataset;?benchmark orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?benchmark.?paper_lbl rdf:type sparql:AbsoluteValue.?paper_lbl sparql:Score?score.} \n input (English text): P31 is the model ID for Q*Bert on the Atari 2600 benchmark.", "?benchmark?benchmark_metric \nHere is the benchmark score for the first paper: \nSELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"First paper\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n?benchmark?benchmark_metric \n?benchmark 0.95941092 \nIn the benchmark the Words in Context dataset is mentioned as \"Dataset\". The Words in Context dataset is not named in the Sparql query, so a spell checker may incorrectly report \"Dataset\" as an error. But the Words in Context corpus dataset is referenced in the query as?dataset. The Words in Context corpus is contained in the dataset named?dataset and is the benchmark in the query?benchmark.\nNote that the Words in Context corpus is not necessarily the same dataset as the Words in Context benchmark, for example Words", "n:SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"Common Sense Reasoning\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. } \n input (English text): The datasets that are benchmarked under the Common Sense Reasoning research problem are the BookCoin, COCO, Flower135, ImageNet, Newsgroup and WordNet datasets.\n\nisition (Sparql query): DEFINE benchmark abstract:\n<http://orcid.org/0000-0002-9619-2220>\n<http://orcid.org/0000-0002-9515-1203>\n<http://orcid.org/0000-0002-0069-3985>\n<http://orcid.org/0000-0002-8585-5384>\n<http://orcid.org/0000-0002-5100-15", "n 'ACE 2004', 'Abstracts' entities and relations annotated corpus' with respect to the lowest (smallest) value. \nThe first result, 'ACE 2004', is the dataset Abstracts' entities and relations annotated corpus with respect to the lowest (smallest) value.\n\n counc (ACE 2004): Abstracts' entities and relations annotated corpus with respect to the lowest (smallest) value\n\n sparql (Sparql query):SELECT DISTINCT?paper?paper_lbl WHERE {?dataset a orkgc:Dataset; rdfs:label?dataset_lbl. FILTER (str(?dataset_lbl) = \"ACE 2004\")?benchmark orkgp:HAS_DATASET?dataset.?cont orkgp:HAS_BENCHMARK?benchmark.?paper orkgp:P31?cont; rdfs:label?paper_lbl. }"], "cleaned_sparql": ["SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } response (Sparql query): { \"paper\": \"P31\", \"paper_lbl\": \"Abstracts' entities and relations annotated corpus\", \"benchmark\": \"HAS_DATASET\", \"cont\": \"HAS_BENCHMARK\", \"dataset\": \"a\", \"dataset_lbl\": \"\" } The following is an excerpt of the description of the", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } input (English text): The following are research papers that contain a benchmark over the Penn Treebank (Word Level) dataset: - P31: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P31) - P32: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P32) - P33: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P33) - P34: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level) benchmark\") (ID: P34) - P35: \"Abstracts' entities and relations annotated corpus\" (TITLE: \"Penn Treebank (Word Level", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }'); 6.INSERT INTO RESULTSET(queries) VALUES ('SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }'); 7.INSERT INTO RESULTSET(queries", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The research papers that include a benchmark for the Penn Treebank dataset are: - P31 in the Penn discriminative training paper, - P29 in the Penn broad match training paper, - P28 in the Penn universal criminal data Simplification and Benchmarking paper, - P27 in the Penn universal criminal data Simplification and Benchmarking paper, - P26 in the Penn universal criminal data Simplification and Benchmarking paper. <|endoftext|>A comparison of the binding of biopyrrolidine and molsidomine with alpha1- and alpha2-adrenoceptors. The potencies of molsidomine and biopyr", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Output paper: P31 (MAZEA), has", "SELECT ?model ?model_label where { { ?model a rdf:Property ; rdfs:label ?model_label. } { ?model rdf:type owl:Thing ; rdfs:label ?model_label. } { ?model owl:equivalentClass rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ; rdfs:label ?model_label. } { ?model owl:annotatedDatatype rdfs:domainContainment[] [ rdf:type owl:Thing ; owl:equivalentClass rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ; owl:annotatedDatatype rdfs:domainContainment[ rdf:type owl:Thing ; owl:annotatedDatatype rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ; owl:equivalentClass rdf:resource \"http://data.cs.umn.edu/resources/ssdl\" ]", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } From the output, we can see that the title of one of the research papers that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset is \"Capacity of a carbon-based fuel\". The ID of the corresponding research paper is \"P31\", and the label of the benchmark in the benchmark property of the selected research paper is \"Abstracts' entities and relations annotated corpus\".", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } [ { \"paper\" : \"P31: Multi-Genre Morphing Deep Learning Model for Zero-Shot Language Detection\", \"paper_lbl\" : \"Abstracts' entities and relations annotated corpus\" } ] } <|endoftext|>// // Generated by class-dump 3.5 (64 bit) (Debug version compiled Oct 15 2018 10:31:50). // // class-dump is Copyright (C) 1997-1998, 2000-2001, 2004-2015 by Steve Nygard. // #import <objc/NSObject.h", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kuzushiji-MNIST benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|># This file is part of CPA-VM. # # CPA-VM is an integrated suite of software capable ofDoA-style virtual security # management. Originally developed at ETH Zuerich, it has since been adopted # by the Commonwealth government of Australia and is now a thriving community # project. # # Copyright (c) 2014, 2016, ETH Zurich and University ofatto Lausanne. # CPA-VM is licensed underONG clause 10E of the GPL. # # This file is part of CPA-VM. # # CPA-VM is", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English benchmark\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are: paper, paper_lbl \u00a0 The WMT2016 Romanian-English benchmark dataset has achieved the highest BLEU score of 27. \u00a0 SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Romanian-English benchmark\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper or", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } There are 2 entities in the dataset with the labels Abstracts' entities and relations annotated corpus. One is a dataset having the ontology orkgp:HAS_DATASET property, and the other is a benchmark having the property orkgp:HAS_BENCHMARK. The benchmark entity includes the label and the benchmark property. There is also one paper entity with the label and the paper property.", "SELECT ?value (thanos:Top-1-Accuracy ?value) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k dataset\")} input (English text): Top-1-Accuracy score = 85.02% output (Sparql query):\ufffd\ufffd ?value (85.02)", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"arXiv dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers", "SELECT * WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?cont :metric ?metric; ?cont :modelExpression ?modelExpression; ?benchmark :modelExpression ?modelExpression; ?benchmark :modelObjectPropertiesLiteral ?modelObjectPropertiesLiteral; ?benchmark :modelLiteralDataPropertyNamesLiteral ?modelLiteralDataPropertyNamesLiteral; ?benchmark :modelLiteralDataValuePropertyValueLiteral ?modelLiteralDataValuePropertyValueLiteral; ?benchmark :modelDataPropertyNamesLiteral ?modelDataPropertyNamesLiteral; ?benchmark :modelDataValuePropertyValueLiteral ?modelDataValuePropertyValueLiteral; ?benchmark :modelObjectPropertyNameLiteral ?modelObjectPropertyNameLiteral; ?benchmark :modelLiteralDataPropertyValueLiteral ?modelLiteralDataPropertyValueLiteral; ?benchmark :modelDataPropertyNameLiteral ?modelDataPropertyNameLiteral; ?benchmark :modelDataProperty", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-disease dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper orkgp:P31 ?benchmark. } Related questions: - Identify a benchmark in an ontology document ? - How to identify whether an RDF graph contains a dataset ? - Find the names and IDs of all datasets in an ontology - Identify benchmark in an ontology document - Identify whether an RDF graph contains a dataset - Identify all datasets in an ontology This question originally asked at https://data.stackexchange.com/questions/21888/how-to-find-the-names-and-IDs-of-all-datasets-in-an-ontology, but did not have an answer so I'm reposting it here. The steps to identify the papers that include benchmarks on the", "mark (lunar_lander) orchemark (lunar_lander)", "SELECT DISTINCT ?paper_eval { ?paper orkgp:P31 ?paper_eval. } input (English text): Commonly used metrics for benchmarking are F-score, precision, recall, and accuracy. output (Sparql query):\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2\u00c3\u00c2 : SELECT DISTINCT ?paper_eval WHERE { ?paper orkgp:P31 ?paper_eval. ?paper_eval owl:exactText ?benchmark. } input (English text): Here is a sample benchmark that includes FSNS - Test dataset. benchmark contains two relations, topics and keywords, each with two entities, a topic and a keyword. output (Sparql query):\u0013 : SELECT DISTINCT ?paper_eval WHERE { ?paper orkgp:P31 ?paper_eval. ?paper_eval owl:exactText ?benchmark. ?benchmark rdfs:label \"FSNS - Test dataset\". } input (English text): Here is a sample benchmark that includes FSNS - Test dataset. benchmark contains two relations, topics and keywords, each with two entities, a topic and a keyword. output (Sparql query):\u0013 :", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?paper_lbl rdf:type owl:NamedNamedEntity;} <|endoftext|>USS Porter (DD-741) USS Porter (DD-741) was a Fletcher-class destroyer in the United States Navy during World War II. Porter was named after Commodore James Read Porter, an American naval officer and United States Ambassador to Japan during the 1858\u20131860bertama Expedition. During the first years of the twentieth century the U.S. Navy had a comprehensive program to build dedicated naval districts, each with a flagship, a series of surface vessels, and other support vessels grouped around a central flagship. Commodore Porter served as the flagship of the Naval District of Pacific Island Distress Signals during World War I. Construction and commissioning Porter was laid down by the Bethlehem Shipbuilding Corporation, Lackstown,", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric; rdfs:label ?paper_lbl. } output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric; ?benchmark <http://www.webexhibits.org/annotation/relations_benchmarks.html> ?metric; rdfs:label ?paper_lbl. } The benchmark <http://www.webexhibits.org/annotation/relations_benchmarks.html> for Abstracts' entities and relations annotated corpus", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): There are several papers that utilize the Flair-TDM model including: - Dai et al., 2018 - Itoh et al., 2018 - Wang et al., 2018 - Azulay et al., 2018 - Raiko et al., 2017 - Niemeier et al., 2017 - Ohno et al., 2017 - Hsiao et al., 2016 - Wang et al., 2016 - Jain et al., 2015 - Viola et al., 2014 - Padua et al., 2014 - Thomas et al., 2012 - Ahn et al., 2010 - Flair-TDM benchmark results at URL: https://github.com/", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper rdfs:label ?paper_lbl. } input (English text): The code in the papers you provided include: (1) transforms.ipynb, (2) model zoo notebooks for the Transformer model, and (3) querying a SPARQL endpoint on the Knowledge Graph Platform. map (Saved in example dataset \u201cctf-abstracts-annotated-nlp-benchmark.iot\u201d): { ?paper ?paper_lbl ?cont ?benchmark}", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Abstract Objective To describe the prevalence of urolithiasis in Chinese children and identify the risk factors. Methods A multicenter, cross-sectional study was conducted in children who were admitted to 14 hospitals located in 10 cities in China from January 1, 2016 to December 31, 2016. All enrolled children were diagnosed with urolithiasis based on the pathological examination. For comparison, subjects without urolithiasis were selected from patients who were admitted to the same hospitals with the same period during the same year but who did not have urolithiasis confirmed by pathological examinations. The enrolled children were divided into two groups: a lithiasis group and", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?benchmark. } input (English text): Here are the links to the papers that have utilized the SAN (single) model and include the links to their code: earthquack, Cory. A Benchmark for Evaluating Relations One-Time Learning, TSTLS 2016.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P31 ?cont; ?model rdf:type ?model_lbl. } input (English text): The models that have been benchmarked on the ACE 2005 dataset include the following relations: has_dataset, has_benchmark, has_dataset, has_label. output (Sparql query):ostics SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P31 ?cont;", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PNDec model for benchmarking\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_CONTEXT ?paper. ?paper_lbl rdfs:label ?paper_lbl. } input (English text): The code references can be found in the repository on GitHub https://github.com/bernhard-despe/pndec. The datasets and benchmark can also be found there. The corresponding SPARQL query is: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PNDec model for benchmarking\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_CONTEXT ?paper. ?paper_lbl rdfs:label ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"/> </head> <body> <h1>Output:</h1> <h2>SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } paper paper_lbl <sparql statement-source=\"http://stackoverflow.com/questions/4027783/what-are-the-titles-and-ids-of-research-papers-that-include-a-benchmark-for-the- Abstracts%E2%80%99-entities-and-relations-annotated-corpus-dataset/\" /> P31 Benchmark result IMDb-B: Metric: precision Value: 68.789 Metric: recall Value: 0.738 Metric: f-", "SELECT ?paper (rdfs:label ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?benchmark. } input (English text): MLDoc Zero-Shot German-to-French. output (Sparql query): contradicting the query from the above answer, we got the following: SELECT ?paper (rdfs:label ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemExp model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The query returned: +-+-+ |paper_lbl |text | +===========+==============================+ |paper_lbl |\"SemExp model for benchmarking | |paper_lbl |purposes | +-+-+ The query was successfully completed. [ benchmark - dataset ] <|endoftext|>You'll have plenty to celebrate when you subscribe to the Liverpool FC newsletter Sign me up Thank you for subscribing We have more newsletters Show me See our privacy notice Invalid Email James Milner has welcomed the recent focus on mental health within football. The Reds' No.6 has spoken about the", "mark : F1 score; iments (Sparql query): ett import:F1-scorer; omtermin (Sparql query): F1", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } |paper P31 |cont XGboost Model produced by PTC The following are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: Penn Treebank (Character Level) The benchmark for Abstracts' entities and relations annotated corpus dataset can be found in the following paper: Tian Wang, Kaiming Shi, Rajat Monga, Andrew 544 Park, Andy Davis, Joel Sansbury, Radford M. Neal, Andrea Nesto, Jeff Prescod, Drew Deiss, Hrishikesh Kishore, Bo", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } from https://www.ontotext.com/ontologize/diagram_ontology/benchmark.html output (Sparql query): orchestrate - given the selection above, the following is a sample of the returned data: paper:P31 paper_lbl:Penn Treebank (Character Level) dataset benchmark:HAS_DATASET dataset:a cont:orkgp:HAS_BENCHMARK dataset_lbl:rdfs:label from https://www.ontotext.com/ontologize/diagram_ontology/benchmark.html orchetable :", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Double Dunk\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): \"precision\", \"recall\", and \"f1-score\" are widely used metrics for evaluating the performance of a dataset. output (Sparql query):precision:AVG output (Sparql query):recall:AVG output (Sparql query):f1-score:AVG output (Sparql query):. input (English text): \"precision\", \"recall\", and \"f1-score\" are widely used metrics for evaluating the performance of a dataset. <|endoftext|>By Brooke Chitty and Kate O\u2019Connor WASHINGTON (Reuters) \u2013 Republican and Democratic leaders on Capitol Hill said on Thursday they were hopeful that U.S", "SELECT ?paper ?paper_lbl where { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; ?cont orkgp:P31 ?paper_lbl; rdfs:label ?paper_lbl. } ~ The above query returns three rows: * Paper labeled \"P 31\" has dataset \"orkgp:P31\" and benchmark \"orkgp", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } In this query, ?paper and ?paper_lbl are the Sparql results set, ?dataset is the dataset property of the orkgc:Dataset contained in the input data, ?cont is the property of the orkgp:HasDataset property of the orkgc:Dataset contained in the input data, and ?benchmark is the property of the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Datasets used in P31 and P32\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P32 ?cont; rdfs:label ?paper_lbl. } P31: https://github.com/datawave/data-set/tree/master/p31 P32: https://github.com/datawave/data-set/tree/master/p32 ?paper ?paper_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } A solution that relies on supervised machine learning algorithms could be to use the BenchmarkCategoryProperty SKOS ontology as a common parent class for entities and relations in benchmark datasets and then feed this class to supervised machine learning algorithms such as support vector machines (SVM). An example model could look like this: .split-hornetmodel { @model.var.type string paper_label @model.var.label string paper @model.var.value orkgp:P31 @model.vars.context orkgp:HAS_BENCHMARK @model.vars.text orkgp:HAS_DATASET", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkg", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; ?benchmark. ?paper_lbl. } <|endoftext|>Hemispheric differences in color perception: the Melbourne reference sample. To examine the intersubject variability in color vision tests and explore whether this", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Libyan Leader Muammar Gaddafi: I Am Still Alive, Honored In Libya, But Spent Last Six Months In A Hospital In Italy While Being Worshipped By the Western Press The former Libyan leader Muammar Gaddafi has officially declared he is alive and in good health. Gaddafi told a television network in the Netherlands that his \"physical and mental condition remains stable.\" The 74-year-old, who is under arrest in North Africa, also said he is still the leader of Libya and has loyalists in key parts of the country. He spoke to the TV network", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Q: Recursive array with jQuery each() I want to know if it's possible to make a recursive function to generate an array using jQuery's each() function. This is the function I have so far: function generateArray(arrayToBuild, arrayValue, arrayFirstElement, arrayLastElement, elementToAddToArray){ if(arrayFirstElement == arrayLastElement){ arrayToBuild.push(elementToAddToArray); }else{ generateArray(arrayToBuild, arrayValue, arrayFirstElement + 1, arrayLastElement, elementToAddToArray + 1); } } $(\".element\").", "SELECT ?paper_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The best results over the dataset CoNLL++ are: metric\tvalue\tdataset\trelation accuracy\t84.94%\tlanguage\tsentence proximity coords\t27.92%\tsentence\tposition in a sentence proximity\t27.92%\tsentence\tpreference for a given relation output (Sparql query): It's the benchmark \"Proximity\" and the metric \"Proximity\" and the value \"27.92%\" The two RDF statements below are the benchmarks found in the answer: rdfs:label\tp31\tConll++ Entities and Relations Annotated Corpus \tAbstracts' entities and", "SELECT DISTINCT query returns the following data: paper paper_lbl - P31 Abstracts' entities and relations annotated corpus The Sparql query returns the following data: benchmark - HasDataset a HAS_BENCHMARK Abstracts' entities and relations annotated corpus Best performing model benchmarking the PIQA dataset in terms of Accuracy metric <|endoftext|>Introduction ============ Dentine hypersensitivity is one of the most prevalent oral disorders. Millions of people suffer from dentin hypersensitivity, which is considered the \\\"sixth\\\" great myth of civilization (The sixth great myth of civilization is Death and dentin hypersensitivity).\\[[@B1]\\] One in three people has some degree of dentin hypersensitivity,\\[[@B2]\\] with more women affected than men. It has been shown that the distribution of dentin tubules is different", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): { \"paper\" : \"P31\", \"paper_lbl\" : \"P31\" } I selected the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset as the answer to the question. The selected model is called P31 and its identifier is P31. The selected model scored 41.8 on the benchmark dataset, which is quite good for a search model that hasn't seen any training", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?dataset a orkgp:Dataset; ?paper_lbl string:http://doi.org/10.5281/zenodo.1453070. ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P32 ?cont; ?paper orkgp:P33 ?cont. } input (English text): Table-Sequence model: https://github.com/mozilla-sbcl/sbcl/blob/master", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The following papers were found: https://doi.org/10.5281/zenodo.1144287, https://doi.org/10.5281/zenodo.1144288, https://doi.org/10.5281/zenodo.1144289, https://doi.org/10.5281/zenodo.1144290, https://doi.org/10.5281/zenodo.1144291, https://doi.org/10.5281/zenodo.1144292, https://doi.org/10.5281", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Reuters De-En\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_id; ?paper_label ?paper_lbl. ?metric ?cont; ?metric a ?benchmark. } <|endoftext|>Can Vending Machine Give You A Quick Snack At Work ? It\u2019s often difficult to choose what to eat when you\u2019re in a hurry and don\u2019t have much time. Luckily, vending machines at work can provide quick, easy snacks for you to eat when you\u2019re in a pinch. It doesn\u2019t matter if you\u2019re hungry or if you\u2019re looking for something sweet or savory, vending machines at work are always there to provide a quick bite to eat.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ... <paper:P31 xmlns:paper=\"http://nlp.stanford.edu/data/js/nli.js\" ze=\"3.95\" country=\"France\" author=\"Alain\"> <paper:Benchmark language=\"English\" country=\"France\" author=\"Alain\" ze=\"3.95\" annotationType=\"None\" corpus=\"Abstracts' entities and relations annotated corpus\" relationType=\"None\" relationId=\"P31\" relName=\"Protein 31\" relationText=\"Protein 31\" relationEvidence=\"Protein 31 was identified by MS/MS and sequenced by single-", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?benchmark. } EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP_INDICATORS EIGEN_USING_BOOTSTRAP", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FLOPS\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?benchmark. } input (English text): The best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric is... output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl rdf:type owl:Class ; rdfs:label \"FLOPS\"; ?benchmark owl:hasDataset ?dataset; ?benchmark owl:hasBenchmark ?benchmark; ?benchmark owl:member ?benchmark_property; ?benchmark_property owl:annotatedReads ?benchmark_property_value. ?paper owl:hasDataset ?benchmark. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } Since the output is an ordered tuple, the DISTINCT keyword is not required.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Oxford-IIIT Pets\") ?benchmark orkgc:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The answer to this question is a list of SPARQL queries that return the title and ID of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus corpus dataset.", "SELECT ?benchmark ?metric WHERE { ?dataset a orkgc:Dataset; ?benchmark o rdf:nodeID \"urn:lsid:columbia.edu:rezension:3899-wos-46985-datasets:benchmarkConnection.30444970-dk41m26\"; ?metric rdfs:label ?metric. FILTER (str( ?metric) = \"68.80529747333235\")} For the metric field, you can choose any of these: 1", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } # -", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Supervised: dataset in terms of SemEval 2013 metric\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The model with highest accuracy on the SemEval 2013 dataset is... output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval 2013 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): All research papers that performed a benchmark on the dataset created by SciecERC. output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): All research papers", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_BENCHMARK ironoxide:quantity ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Elorza crater discovered quantity of iron oxide\") ?benchmark orkgp:HAS_DATASET ?dataset. } input (English text): Elorza crater discovered a quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered a quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include a benchmark for Elorza crater discovered quantity of iron oxide. 18 abstracts include", "SELECT * { ?paper ?paper_lbl; ?paper_lbl ukn:model; } input (English text): Next, you can find the titles and IDs for the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset. The models that have been evaluated on the VTAB-1k dataset are: output (Sparql query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BEN", "SELECT DISTINCT ?paper_metrics WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Freeway dataset evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_metrics ?paper_metrics. } The title and ID of these research papers can be found here: https://github.com/wikimedia/opendatasets/tree/master/db48/Downloads The metrics used to evaluate models on the Atari 2600 Freeway dataset can be found here: https://github.com/openai/human-level-playing-field/blob/master/evaluation.md", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper rdfs:label ?paper_lbl. } In this example, the answer to the query is: <paper_lbls>, <paper>, \"Abstracts' entities and relations annotated corpus\", \"HAS_DATASET\", \"P31\", \"HAS_BENCHMARK\", \"Abstracts' entities and relations benchmark\" The datasets used for the benchmark and model are in the ?dataset and ?benchmark variables respectively. ?paper and ?cont refer to the paper and content variables. Note that the benchmark variable ( ?)benchmark also appears in the sparql query as ?benchmark.", "SELECT * WHERE { ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark. } } <|endoftext|>The aetiology of chronic fatigue syndrome/myalgic encephalomyelitis: a systematic review. Chronic fatigue syndrome/myalgic encephalomyelitis (CFS/ME) is a", "SELECT ?benchmark (has ?metric) ?paper (where ?paper_lbl = \"Top benchmark score and its metric on the Atari 2600 Tennis dataset\") WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric. } :TOP benchmark score and its metric on the Atari 2600 Tennis dataset", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark dawbe:BenchmarkMatch ?matches. ?paper orkgp:P31 ?paper_lbl. } input (English text): The five most frequent entity types over the Abstracts' entities and relations annotated corpus dataset are: 1. person 2. organization 3. location 4. product 5. Databricks output (Sparql query): complicate 1. person 2. organization 3. location 4. product Abstracts' entities and relations annotated corpus dataset includes three benchmarks that evaluate entity typing over the Abstracts' entities and relations. The five most frequent entity types are: person, organization, location, product, and Databricks. There", "SELECT query that contains only the desired values. SELECT DISTINCT ?paper ?paper_lbl \tWHERE { ?dataset a orkgc:Dataset; ?dataset_lbl rdfs:label ?dataset_lbl. FILTER (str", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"The types of nanocarriers that have a therapeutic effect include: polymers, lipids, carbohydrates, inorganic materials, and other biological materials.\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): ?paper + bench + datasets + (hasDataset) + (hasBenchmark) + (hasCont) + paperLabel 15:47:38.448 Copyright 2010 2010-2017, IMR-BTS. All rights reserved. User manual, Version 10.0, 09/10/2017 15:47:38.450 INQUIRY { 15:47:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): CL-SciSumm is the name of the dataset in the CL-SciSumm benchmark that we are interested in. output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CL-SciSumm\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Unsupervised NMT + weight-sharing model in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): List the code links in papers that use the Multitask NMT + weight-sharing model in any benchmark ? output (Sparql query):onso:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multitask NMT + weight-sharing model in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Q: Pyramid: Wildcard subdomain pattern not working I have wildcard subdomains in Pyramid that works fine with reverse subdomain but not with pattern. Here is my reverse subdomain: @view_config(route_name='') @cherrypy.tools.csrf() def index(self, request): return dict(urls='http://foo.bar.baz.com/*') Here is my pattern subdomain: @view", "SELECT ?paper_lbl ( COUNT ( ?paper ) AS ?COUNT ) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <paper_lbl>:value (COUNT ?COUNT) :count <|endoftext|>We're about: The Friendly Neighborhood Website is an informal group of website developers and internet professionals in the greater Charlotte area. Whether you're just starting out or an experienced developer, we hope you'll find this group both helpful and enjoyable. Our meetings are informal and centered around projects or topics of common interest. All levels of experience are welcome; this is a great place to make friends and find mentors as you work on your website projects. Please note our events are open to all and are NOT limited to Charlotte developers. We are also happy to point you toward resources for", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?metric ?metric_lbl. FILTER (str( ?metric) = \"commonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?metric ?metric_lbl. } input (English text): List the papers that contain metrics used on the CommonsenseQA benchmark dataset. assistance (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?metric ?metric_lbl. FILTER (str( ?metric) = \"commonsenseQA\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?metric ?metric_lbl. } assistance (", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The highest benchmark result achieved on the IMDb-M dataset, including the metric and its value, is 47.67. output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scholarly entity usage detection benchmark result\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl; } meu, the highest benchmark result include the following metric and score: olutely sure, the highest result is #document-level precision @ 0.787, 0.787, 0.787, 0.787, 0.787 @ #relation-level precision @ 0.726, 0.726, 0.726, 0.726, 0.726 @ #relation-level recall @ 0.775, 0.775, 0.775, 0.775, 0.775 \u00e1dheer", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model a mi:Model; ?model_lbl a ml:Name. FILTER (str( ?model_lbl) = \"MultiNLI\") } input (English text): MultiNLI is a natural language inference (NLU) model trained on over 10'000 examples from the English Penn Treebank. It has been trained on over 10'000 examples from the English Penn Treebank. output (Sparql query):In order to evaluate a model, it must be exposed as a Sparql query. This can be done by providing the dataset the model was trained on. Here we are providing the sparql query that exposes the MultiNLI model and the name of the model which was trained on the MultiNLI dataset. output (Sparql query): SELECT ?model ?model_lbl WHERE { ?model a mi:Model; ?model_lbl a ml:Name. FILTER (str( ?model_lbl) = \"MultiNLI\") }", "SELECT * WHERE { ?dataset a orkgc:Dataset; ?benchmark ?cont } input (English text): \"Average negative log-likelihood (NLL),\" \"average precision (AP),\" \"average recall (AR),\" \"F1 score,\" \"sensitivity (SEN),\" \"specificity (SPC),\" \"micro F1 score (mF1)\",\"macro F1 score (MF1),\" \"positive predictive value (PPV),\" \"negative predictive value (NPV),\" \"accuracy,\" \"precision,\" \"recall,\" \"F1 score,\" \"sensitivity (SEN),\" \"specificity (SPC),\" \"micro F1 score (mF1)\",\"macro F1 score (MF1),\" \"positive predictive value (PPV),\" \"negative predictive value (NPV),\" \"accuracy} output (Sparql query):INSERT INTO \t\t\tdatasets(dataset_lbl,benchmark,paper,paper_lbl) \t\t\tvalues", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential MNIST Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Kim Eun-hyung Kim Eun-hyung (born March 1, 1989) is a South Korean professional baseball pitcher for the Kia Tigers of the KBO League. He bats and throws right-handed. He is 5 feet 11 inches tall and weighs 205 pounds. Professional career Seoul Korail Kim debuted with the Seong", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Benjamin Doe, Bernhard Engel, Rafael Malach, Christoph Siemens, Tobias Weltre, Mirko Vettori. (2019). Structured Information Grammars with Rewrite and Recursion. In Proceedings of the 51st Annual", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single's entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } paper paper_lbl benchmark benchmark_lbl - - - - - - - - - - P31 Using a distant supervision approach to detect bias in news headlines P31_1 Using a distant supervision approach to detect bias in news headlines", "SELECT DISTINCT ?paper ?paper_lbl_label WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciTLDR's metrics\") } input (English text): Metrics used on the SciTLDR benchmark dataset include: Tf-Idf, Information Gain, Coarse-Grained F1, Lambda-R, Cohen's kappa, Spearman's rho, Homogeneity Index, Jaccard Index, Precision@K, Recall@K. output (Sparql query):text {\"P31\",\"HAS_BENCHMARK\",\"SciTLDR's metrics\"}", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }', '[ { \"paper\":", "SELECT * FROM <Where Condition> WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") } input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset ? output (Sparql query):[Standart Conversion Engine, Densen, LeNet, AlexNet, VGG-16, Inception-v4] ocomplete (Sparql query): SELECT * FROM <Where Condition> WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-100\") } <|endoftext|>A kinetic study of dicarbonyl compounds at pH 7.4: relationship between dicarbonyl compounds and diabetic complications. Diabetes mellitus is a disease characterized by impaired glucose tolerance and insulin insensitivity, and is one of the leading causes of kidney and eye diseases, and cardiovascular disease worldwide. In our present study, we investigated the effect of human", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): RotoWire (Relation Generation) = 0.608536. output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdf:type owl:NamedIndividual; ?benchmark owl:sameAs ?dataset. ?cont rdf:type owl:NamedIndividual; ?paper rdf:type owl:NamedIndividual; ?paper_lbl rdf:type owl:NamedClass. } input (English text): The title of this paper is Benchmarking Abstracts' Entities and Relations Annotated Corpus via", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Score metric\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Answer: SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Score metric\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Explaination: The best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric, is the reacher model.", "SELECT ?paper WHERE { ?paper mdl:model.} <|endoftext|>HomeAway is the world leader in vacation rentals. We offer the largest selection of properties for any travel occasion and every budget. We're committed to helping families and friends find a perfect vacation rental to create unforgettable travel experiences together. This owner has been less than kind and returned to poor reviews for a lot of the same reasons. Some of the hot tub could have used some anti-bacterial chemicals and there was quite a bit of mold in the tub/shower area. The trampoline in the backyard also looked neglected. Overall, I would not recommend this rental and the owner. It is very nicely decorated and clean. You have a little kitchenette, dining area and sitting area. The beds are very comfortable and the water hot tub is a wonderful bonus. You can make use of the kayaks", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"code references in papers that have used the DQNMMCe+SR model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The RDF Dataset for Benchmarking and Code Reference Sparql endpoint is now live at https://data.stellenbosch.ac.za/sabor/sparql. The endpoint allows querying all Sparql endpoints in the SDB on RDF datasets, and in particular the RDF datasets for Abstracts' entities and relations annotated corpus, code references in papers that have used the DQNMMCe+SR model for benchmarking purposes, and SDB benchmarks. Querying the endpoint yields the following result set: * ?dataset : a orkgc:Dataset * ?dataset", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The answer is: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ball in cup, catch (DMControl100k) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The SPARQL query returns 3 results, of which only 1 has the benchmark Ball in cup, catch (DM", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT-single dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?benchmark; ?paper_lbl ?paper_lbl. } input (English text): The name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset is. output (Sparql query):\u00c3\u00c2\u00c3\u00c2fecture (Sparql query):\u00c3\u00c2\u00c3\u00c2FIFTEEN (P31); particular (P31); nyt-single (P31) demanded output (Sparql query): encountered no matching documents for the provided query demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded demanded", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): https://www.dropbox.com/s/d0yytx2o4w7fd2m/memen.R ?v=1&s=6 output (Sparql query):POST https://www.dropbox.com/s/d0yytx2o4w7fd2m/memen.R ?v=1&s=6 HTTP/1.1 Host: www.dropbox.com Content-Type: application/sparql GET https://www.dropbox.com/s/d0yytx2o4w7fd2m/memen.R ?v", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MMV TSM-50x2 model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): link to the code ? output (Sparql query): These are the papers using the MMV TSM-50x2 model: https://drive.google.com/file/d/1uwM3uRFZHz8Ef9F2kQqdFWc-bU2EMh1f/view ?usp=sharing [1] [ { \"paper\": \"P31\", \"paper_lbl\": \"Cont\" }, { \"paper\": \"P33\", \"paper_lbl\": \"ABSTRACTS' entities and relations annotated corpus\" }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-5 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; rdfs:label ?paper_lbl. } input (English text): The Yelp-5 benchmark dataset consists of a single class (entity/relation) that is annotated with an triple with the fields key, description, and recommender score. output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark a orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper_lbl rdf:type sparql%rowtype. } input (English text): The Yelp-5 benchmark dataset consists of a single class (entity/relation) that is annotated", "SELECT * WHERE { ?model ?benchmark ; BLDGT:Benchmark(metric=BLEU,title=best_model,metric_range=[0,100],model_id= ?model). FILTER ( ?benchmark= ?best_model).} <|endoftext|>Evaluation of the New Multi-target Strategy \"Folate + Raloxifene\" for Post-Menopausal Depression. Depression is one of the leading causes of disability worldwide, and according to the World Health Organization (WHO) has a high impact in women's health, being the second cause of disability worldwide, affecting about 350 million people and causing a huge burden on the society. Currently, many drugs are used for the treatment of depression, but the cognitive and side effects of the available drugs make it necessary the discovery of new therapies to improve the therapeutic efficacy and safety. Thus, our research group has developed a multi-target strategy using the natural drug folate, Raloxifene (RLX) and the natural product Vitispirifoliol (VPL), with the goal", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper skos:seeAlso ?cont; rdfs:label ?paper_lbl. } itos:Provide a list of papers that have utilized the SAC model and include the links to their code ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): <p>List of papers that have utilized the MEMEN model and include the links to their code:</p> output (Sparql query):<p> interconnected text</p> output (Sparql query):<p> P31. Abstracts' entities and relations annotated corpus dataset. Benchmark for orkgp:HAS_DATASET and orkgp:HAS_BENCHMARK. P31. Cont. </p> output (Sparql query):<p> P", "SELECT ?paper_id ( distinct ?paper ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?benchmark; ?cont orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_P31 ?paper_id. } <|endoftext|>\u2018Real capping\u2019 and monetary growth provide lasting solutions to India\u2019s inflation We expect the central bank to keep the policy rates stable at the coming review \u201cWe expect the RBI to keep the policy rates stable at the coming review\u201d is the mantra that the markets and economic pundits are now going on repeat as RBI meets and announces its monetary policy again this month. The RBI\u2019s inflation estimate range is now 0.5% \u2013 1.5% (vs 6.1% recorded in April FY\u201919), and will keep", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper opl_atari2600assault. Local Variables Summary 1. ?paper opl_atari2600assault Object Constraint Summary 1. ?paper opl_atari2600assault IS a orkgp:HAS_DATASET Output of the query: 1. 2 2. 2. Gomez Atari 2600 Assault Local Variables Summary 1.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Best performing model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } An example of a response: input (English text): \"Best performing model\" is an example of a benchmark for the \"Abstracts' entities and relations annotated corpus\" dataset. desired output (Sparql query): \"best performing model\" ices (English text): \"Best performing model\" is an example of a benchmark for the \"Abstracts' entities and relations annotated corpus\" dataset. desirable output (Sparql query): \"best performing model\" emetery (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"", "SELECT ?codeReference WHERE { ?paper ?paper_lbl. } FILTER langMatches(codeLanguage, \"Sparql\") input (English text): Here is an example of a paper that has used the STREET model for benchmarking purposes. https://doi.org/10.1002/sem.21665 \u00e9galement aucune mention de code reference dans le texte. R\u00e9ponse : SELECT ?codeReference WHERE { ?paper ?paper_lbl. } FILTER langMatches(codeLanguage, \"Sparql\") <|endoftext|>Mentor exchange student days But first, I want to tell you about an exchange student who became one of my best friends. We met in our first week of school. She had just moved to our country and was beginning to find her feet and make friends. She had a big personality, was very curious and fun to be around, and we clicked right away. She was from Vietnam, and we would speak to each other in Vietnamese. To be completely honest, we understood each other right away.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model enwiki8:Vocab -package edu.stanford.nlp.ie.owl.parallel.manning.vocab; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = \"Models evaluated on enwiki8\").} input (English text): Topic models. output (Sparql query):terson-gould:enwiki8-pos:Vocab; enwiki8:Parse; enwiki8:Tokens; enwiki8:Part-of-speech; enwiki8:Word-form; enwiki8:Synset; enwiki8:Word; enwiki8:Word-form; enwiki8:Lemma; enwiki8:Part-of-speech-prefix; enwiki8:Part-of-speech-postfix; enwiki8:Numeric-phrase; enwiki8:Numeric; enwiki8:Numeric-phrase-distance; enwiki8:Numeric-phrase-unit; enwiki8:Ordinal-phrase; enwiki8:Ordinal-phrase-distance; enwiki8", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset output (Sparql query):; Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset output (Sparql query):SELECT ?paper ( ?paper_lbl as ?paper_lbl_val) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC dataset\")", "SELECT", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Evaluation metrics used when benchmarking models on the SQuAD2.0 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } <|endoftext|>Protective effects of hesperidin against radiation-induced intestinal injury via anti-inflammatory", "SELECT ?paper_lbl ?benchmark_title ?cont_title WHERE { ?dataset a orkgc:Dataset; ?paper_lbl rdfs:label ?paper_lbl. FILTER (str( ?paper_lbl) = \"Abstracts' entities and relations annotated corpus\") ?cont orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl rdf:label ?paper_lbl. } The answer is the following : ?paper rdf:type http://www.semanticweb.org/wiki/KGs/RDF/1.1/Property#; ?paper_lbl rdf:type http://www.semanticweb.org/wiki/KGs/RDF/1.1/PropertyLabel; ?benchmark rdf:type http://www.semanticweb.org/wiki/KGs/RDF/1.1/HasDataset; ?benchmark_title rdf:datatype xml; ?cont_title rdf:datatype xml", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } related research papers are: P31 - https://www.cs.cmu.edu/afs/cs/project/semeval-2017/www/task1/papers/P31-Penn-NLP-ACL17.pdf P22 - https://www.cs.cmu.edu/afs/cs/project/semeval-2017/www/task1/papers/P22-Cooke-NLP-ACL17.pdf P21 - https://www.cs.cmu.edu/afs/cs/project/semeval-2017/www/task1/papers/P21-Farou", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } screened input (English text): \"CONLL04 Benchmark\" output (Sparql query):\u094bns:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } <|endoftext|>/* * (C) Copyright 2000-2002 * Wolfgang", "SELECT ?paper ?paper_lbl FROM <original-query> where <condition> ) return { \"haspapers\": [ { \"paper\": \"P31\", \"paper_lbl\": \"Abstracts' entities and relations annotated corpus\" } ] } The output of the previous query is as follows: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Original query text: Provide a list of papers that have utilized the XLNet (base) model and include the links to their code Converted query text: SELECT DISTINCT ?paper ?paper_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Sparql query result: coach_benchmark.P31:coach benchmark coach_benchmark.P31:coach_lbl benchmark coach_benchmark.P31:coach_lbl_lbl coach coach_benchmark.P31:dataset dataset coach_benchmark.P31:dataset_lbl dataset coach_benchmark.P31:cont cont coach_benchmark.P31:cont_lbl cont dataset.rdfs.label:Abstracts' entities and relations annotated corpus dataset_lbl.rdfs.label:Abstracts' entities and relations annotated corpus benchmark.rdfs.label:HAS", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NYT29 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): <p>The following research papers have been tested on the NYT29 benchmark dataset:</p> <ul> <li><a href=\"http://amzn.to/2rJpEee\">Hitting the Human Adversarial Space: Training a Classifier to Detect Successful Attacks</a> by Shreyas Sidhkant Acharya, Devang Shah, Siva Reddi, Arvind Sabass, Quoc V Le, Girish Khauntretseva, et al.</li> <li><a href=\"http://amzn.to/2rxTtCj\">What Factors Matter for Training Neural Networks to Improve Class Com", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P25 ?cont; ?paper orkgp:P39 ?cont. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): http://static.elsa.kyutech.com/ddqnpc/ddqnpc.model.h2o.all.pkl#/paper/0 /paper/0/cont/oracle/0/paper_lbl/DDQNPCTrainingOnOracle/0.0.1/paper_lbl/Abstracts' entities and relations annotated corpus output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TREC-6 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset ? output (Sparql query): Chapel grove ingrams, et al., 2011. Annotating Linked Data with Entity Linking. Association for Computational Linguistics, pages 1-14. Chapel grove ingrams, A., Fabian, T., and Brewer, M., 2011. Annotating Linked Data with Relation Linking. Association for Computational Linguistics, pages 1-14.", "SELECT ?paper ( ?paper_lbl as ?paper_lbl_val ) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rational DQN Average model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } A Sparql query that returns the Rational DQN Average model benchmark from code in RDOQN/Rational-DQN-Average-Benchmark.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-French dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Q: How do I reverse engineer this stack memory dump ? I am trying to reverse engineer this memory dump: 2192c0 <+0x000000002119de0> 2192c0: 8bff ffff 0001 5c0c 0560 7c10 4418 7a14 0001 5c1c 032d fff0 0000 2192d0", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?review ?review_lbl. FILTER (str( ?review_lbl) = \"PolyAI Review dataset\") ?paper_lbl. } output (Sparql query):rdfs:label of the research", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl The code links in papers that use the Long Short Transformer model in any benchmark: [https://www.microsoft.com/en-us/research/spotlight-blog/first-ai-model-can-spot-important-relations/](https://www.microsoft.com/en-us/research/spotlight-blog/first-ai-model-can-spot-important-relations/) SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } \u2009\u2009\u2009\u2009SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TriviaQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark rdf:type\u200a<http://www.w3.org/2009/04/11-tutorial-on-RDFLessary.html#HAS_DATASET", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Fossil fuels are a widely used energy source that are known to cause global warming. The greenhouse gases emitted by the burning of fossil fuels are believed to have contributed to climactic changes in the past. To address this concern, many nations have implemented various policies intended to reduce the use of fossil fuels. Examples of policies that reduce the use of fossil fuels include incentives to purchase electric cars, as well as government funding to develop renewable energy sources. Nonetheless, these policies have not been sufficient to completely eliminate the use of fossil fuels. This lack of complete elimination of the use of fossil fuels has caused concerns over the issue of climate change. Other attempts have been made to reduce the impact of fossil fuels.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The highest benchmark result for the DTD dataset is: SELECT * WHERE { ?dataset a orkgc:Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DTD dataset\" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } [ { \"paper\": \"P31\", \"paper_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont;", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval-2018 Task 7 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following are the ID labels of the research papers that include a benchmark for the SemEval-2018 Task 7 dataset: input (English text): The following are the ID labels of the research papers that include a benchmark for the SemEval-2018 Task 7 dataset: P31, P32, P33, P34, P35, P36, P37, P38, P39, P40. output (Sparql query): The following are the ID labels of the research papers that include a benchmark for the SemEval-2018 Task 7 dataset: P31, P32, P33, P34, P35, P36, P37, P38, P39, P40.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark gbm:AG_News_Benchmark; rdfs:label ?benchmark_lbl. FILTER (str( ?benchmark_lbl) = \"AG News benchmarks\") ?benchmark. ?paper gbm:P31 ?benchmark; ?dataset_lbl a rdf:Label; ?paper_lbl rdf:label. } input (English text): The AG News benchmarks are defined in the GBSGM paper (https://www.semanticscholar.org/paper/GBSGM%20-%20On%20the%20Detection%20of%20Associational%20Graphs/c4dff7b8c4711e96c75ff1836ba0b831). The corresponding AG_News_Benchmark metric is defined as follows: AG_News_Benchmark (BLUEJump, GraRep) : The average number of adjacent bluejumps required to update a typical subgraph to a state that does not contain the initial subgraph as a member. output", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Disease dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The top performing model for the Abstracts' entities and relations annotated corpus benchmark on the NCBI Disease dataset is the database-centric model trained on largeWikiText-10 databases and the CoNLL 2013 shared task data. Note that this model may not be available in the SPARQL endpoint at this time. Please visit the Model and Dataset Repositories page for details on how to acquire it. <!- There are two datasets that are", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } + The list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset are as follows: + <!-Please provide the titles and IDs of the research papers in the form of a single line of text in square brackets. Line breaks will be removed. Additionally, please provide the models and dataset ID (e.g., \"P31\").-> + [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [1] <http://nlp.stanford.edu/data/glove.5B.zip> [P31] [2", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Jennings et al. (2018) presents the code used to benchmark the GCN Hybrid model: https://github.com/jennings-lab/Benchmark_Paper Jennings et al. (2018) provides the code for 3 benchmark datasets: * Dataset with Direct Relationship Annotation: `https://github.com/jennings-lab/DatasetWithRelationshipAnnotation` * Dataset with No Relationship Annotation: `https://github.com/jennings-lab/NoRelationshipAnnotations` * Dataset with Relationship Annotation but No Annotated Entities: `https://github.com/jennings", "SELECT * WHERE { <chemdner> dc:source <urn:epub:CHEMDNER> dc:language English <urn:xsd:string> } input (Sparql query): contributed_data output (Sparql query): contrib CCDVer14.ENGLISH.CHEMDNER.csv ###### Abstracts\u2019 entities and relations annotated corpus benchmark. rdfs:label Abstracts\u2019 entities and relations annotated corpus ogp:HAS_DATASET chemdner ogp:HAS_BENCHMARK chemdner ogp:P31 chemdner rdfs:label CHEMDNER rdfs:label Abstracts\u2019 entities and relations annotated corpus benchmark ###### CHEMDNER corpus. chemdner cd:source chemdner dc:language English urn:epub:CHEMDNER urn:xsd:string ###### CHEMDNER corpus benchmark. rdfs:label Abstracts\u2019 entities and relations annotated corpus ogp:HAS_DATASET chemdner ogp:HAS_BENCHMARK", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a cub:Dataset; ?dataset_lbl owl:ObjectProperty ?dataset_lbl; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P34 ?cont; ?paper_lbl owl:ObjectProperty ?paper_lbl. } The above query will list down the following papers: CUB-200-2011 Papers with the benchmark CUB-200-2011. \u00a0 \u00a0\u00a0CUB-200-2011 \u00a0 Paper A has the property HAS_DATASET CUB-200-2011 \u00a0 \u00a0\u00a0CUB-200-2011 The Benchmark property of the dataset CUB-200-2011 is contained in the property BENCHMARK CUB-200-2011. \u00a0 \u00a0\u00a0CUB-200-2011 \u00a0 Paper B has the property HAS_BENCHMARK CUB-200-2011 \u00a0 \u00a0\u00a0CUB-200-2011 \u00a0 Paper A has the property P31 CUB-200-2011", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Here is the link to the code: https://github.com/UCIS-RE/SEDAM_TUTORIALS/blob/master/TextExpander/Application/DetectDanglingSpor phrases.sparql output (Sparql query): { \"paper\" <http://purl.org/sparql/funotation#Script> \"paper_lbl\" <http://purl.org/sparql/funotation#Label> \"Sparql query\" } Biomedical Natural Language Processing (BioNLP) is a joint project of the United Kingdom's National Health Service and the United States National Institute of Allergy", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): https://raw.githubusercontent.com/dblagle/duelling-algorithms-for-sparql/master/DUEL_papers_list.sql output (Sparql query): LOAD DATA INFILE 'raw\\DUEL_papers_list.sql' INTO PUBLIC FOR SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont or", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?benchmark; ?cont orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_P31 ?paper_lbl } input (English text): <|endoftext|>#!/bin/sh ############################################################################## ## ## Gradle start up script for UN", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: vistaan/SIGIR'18/Abstracts' entities and relations annotated corpus dataset/jordan-etal-2018 vistaan/sigir-2018/has_dataset/edaac (link to code) vistaan/sigir-2018/has_benchmark/edaac (link to code) vistaan/sigir-2018/paper/p31/edaac (link to code) vistaan/sigir-2018/benchmark/has_dataset/edaac (link to code) vistaan/sigir-2018/", "SELECT ?paper ?paper_lbl WHERE { ?dataset a atkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Solaris benchmark\") ?benchmark. ?paper atkgp:P31 ?benchmark. ?paper ?cont; ?cont a atkgp:ObjectPropertyExpr; ?cont b atkgp:ObjectPropertyExpr; ?cont c atkgp:ObjectPropertyExpr; ?cont d atkgp:ObjectPropertyExpr. } input (English text): What models are being evaluated on the Atari 2600 Solaris benchmark ? output (Sparql query): INCLUDE <Raw Facts file contents>.atgjr20.atkgc20.Dataset.atkgp31.Benchmark.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20.ObjectPropertyExpr.atcont20", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the IDs and titles of research papers that include a benchmark for the SNLI dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SNLI dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Give me a list", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MMV model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): In the code references, you can find the code that was used for the evaluation of the model. output (Sparql query):The code that was used for the evaluation of the MMV model can be found in the sub-section \"Benchmarking using the MMV model\" of this paper: https://arxiv.org/abs/2003.10734. monsieursebastien.bergmann@sfr.fr # # Manually Annotated Semantic Web Research Papers with Entity and", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>The BBC has today commissioned two new 24-minute docudramas based on the hit US show, set to debut in 2020. BBC One is bringing back the iconic drama series with a follow-up called \u201cMission: Crime Scene Investigation.\u201d The six-episode miniseries tells the story of Blythe Danner\u2019s character CSI Grace Kurtz, who is thrust into the unexpected position of Detective Superintendent on the New York City Police Department\u2019s Hostage Rescue", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What is the model name that is benchmarked on the MNIST dataset ? output (Sparql query): ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MNIST dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The paper identifier is ?paper and the benchmark identifier is ?bench", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark; rdfs:label ?paper_lbl. } Output: Papers selected", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } A: QLover 3.0.2 uses OWL API 2.0 + Jena API 2.10.1 for ontology construction. As there are two versions of the ontology it is possible that one of the titles (e.g. Dataset) does not exist anymore in OWL API 2.0 and has been replaced in OWL API 2.10.1. If you find such a title you could try the following query: PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }</rdf:value> </rdf:Description> input (English text): TempEval-3 dataset <|endoftext|>Category Archives: personal finance Post navigation Ford Motor Company CEO, Mark Fields, took a leave of absence from the company this year, to spend more time with his family. He has four children under the age of ten, and has been a stay at home dad since the birth of", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P32 ?cont; ?model_lbl rdf:type ?model. } input (English text): The models tested on the SciERC benchmark dataset are: P32. \"TED Talk Representations with BERT for Semantic Role Labelling\". output (Sparql query): P32:TED Talk Representations with BERT for Semantic Role Labelling - SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciERC benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl { ?dataset a orkgc:Dataset; rdfs:label \"Abstracts' entities and relations annotated corpus\"@en; FILTER (str(\"Abstracts' entities and relations annotated corpus\") = ?benchmark) ?benchmark orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label \"Abstracts' entities and relations annotated corpus\"@en; } output (Sparql query", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 English-German dataset\") ?benchmark. ?cont orkgp:HAS_DATASET ?dataset. ?paper orkgp:P31 ?benchmark. ?cont; rdfs:label ?paper_lbl. } input (English text): The best performing research models in the WMT2014 English-German dataset included: GPT-2, BERT, XLNet, RoBERTa, transformer-XL, Wavenet, and XLNet. <|endoftext|>Requiem for a Gangster Requiem for a Gangster (2008) is a semi-autobiographical film written and directed by Phillip Seymour Hoffman and stars Hoffman, Al Pacino, Diane Ke", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp-14 dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric; ?metric ?value. ?paper orkgp:P31 ?cont; ?cont ; ?paper_lbl ?paper_lbl. } input (English text): The Yelp-14 dataset is available at https://web.stanford.edu/class/2016/spring/Yelp-14.html and includes about 31 million reviews, 385,000 topics and the corresponding topics' entities and relations. The following paper is one that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset: https://cce.csail.mit.edu/wp-content/uploads/2015/05/acl15-hartwig-reify-bench.pdf Based", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Q: \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u043f\u0440\u0438 \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u0438 \u0434\u0432\u0443\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0423\u0432\u0430\u0436\u0430\u0435\u043c\u044b\u0435 \u043e\u0431\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0437\u0430 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c. \u041d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u0437\u0430\u0434\u0443\u043c\u043a\u0430 \u0434\u043b\u044f \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0438\u0437", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl paper:P31 \"Abstracts' entities and relations annotated corpus\" A model that has been tested on the RTE benchmark dataset is named P31. It was implemented in the tool \"ont-Toolbox\". paper:P31 \"ont-Toolbox\" The tool \"ont-Toolbox\" implementing model P31. paper:P31 \"ont-Toolbox\" The tool \"ont-Toolbox\" implementing model P31. paper:P31 \"ont-Toolbox\" The tool \"ont-Toolbox\" implementing model P31. paper:P31 \"ont", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ORDER BY ?benchmark ?metric LIMIT 5 <|endoftext|>Molecular cloning of a calcium channel gene from the giant panda, Ailuropoda melanoleuca: evolutionary implications for nonmammalian vertebrates. We have characterized a novel calcium channel gene, which we named calcium channel-related gene 2 (canrel2), from the giant panda, Ailuropoda melanoleuca. The canrel2 gene consists of 4641 nucleotides encoding 1199 amino acids. The deduced amino acid sequence shows strong homology to the calcium channel alpha1G subunit from rat (97.5%), mouse (95.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } { ?paper http://www.seas.ucla.edu/research/sparql/publications/petridish/P31.html#paper_lbl, \"P31 Benchmark\"@en, \"P31 Benchmark\"@en ?paper_lbl http://www.seas.ucla.edu/research/sparql/publications/petridish/P31.html, \"Abstracts' entities and relations annotated corpus\"@en, \"Abstracts' entities and relations annotated corpus\"@en } ipeline name: Sparql job inputs: - keyword: research papers that include a benchmark for the Abstracts", "SELECT DISTINCT ?metric_name WHERE { \t ?metric_name ?metric. } - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (short)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name] output (Sparql query): [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name], [model name", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric_str. ?benchmark ?score_str. } Abstracts\u2019 entities and relations annotated corpus (WMT2014 French-English): semeval-v0.1-r8.gz <|endoftext|>Q: \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u0430 = ? \u041d\u0443\u0436\u043d\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 \"equals\", \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b \u0432\u044b\u0433\u043b\u044f\u0434\u0435\u043b \u0442\u0430\u043a public boolean equals(Object o) { return (this == o) ? true : false; } \u042f \u043f\u043e\u043d\u0438\u043c\u0430\u044e, \u0447\u0442\u043e \u043e\u0448\u0438\u0431\u043a\u0430 \u0441\u043a\u043e\u043c\u043f\u0440\u043e\u043c\u0438\u0441\u0438, \u0442\u0430\u043a \u043a\u0430\u043a, \u043f\u043e \u0441\u0443\u0442\u0438, equals \u0431\u0443\u0434\u0435\u0442", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WikiText-2 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark o cra:BenchmarkScore ; rdfs:label ?paper_lbl. } <|endoftext|>Entrance fee required for entry into the museum. Prices Entrance fees are non-refundable, but the Museum accepts debit cards and cash. Fees are as follows: Adult $5 Senior $4 Children 2-12 $3 Infants under 2 free Guided Tours Guided tours of the museum are given several times a day. Guided tour times are as follows: 9:00am - 11:00am 11:30am - 1:30pm 2:30pm - 4:00pm", "SELECT ?paper (rdfs:label ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } A query that returns the code references from the papers that use the All-attention network - 18 layers model for benchmarking purposes. A: Here are the IDs of the research papers that contain the following code references: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkg", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Output: paper paper_lbl Disambiguating authors in citations on the web and authorship correlations P31 Abstracts' entities and relations annotated corpus <|endoftext|>Q: Left join between two tables and getting null values from one table I have two tables like below TABLE1 ID NAME 1 ABCD 2 XYZ 3 PQR TABLE2 ID NAME JOINING_TABLE 1 ABC TABLE1 2 XYZ TABLE1", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PyBERT, Attentio, Exalt, DistilBERT, SamBERT, ELMo, XLNet, OpenAI GPT, XLM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset ? output (Sparql query): ction (English text): The models that have been evaluated on the STS Benchmark dataset are: PyBERT, Attentio, Ex", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Chemical identification of eight polymorphic substances in Yuyehao tea. Eight polymorphic substances in Yuyehao tea were identified by physicochemical methods and gas chromatography-mass spectrometry (GC-MS). The results show that five of the eight substances are gallic acid derivatives, and the other three are quercetin derivatives. The gallic acid derivatives are CGA (2, 3, 4-trihydroxybenzoic acid), CGA2,3,4-tetrahydroxybenzoic acid, CGA3,4-dihydroxybenzoic acid and CGA4,5,3',4'-tetrahydroxybenzoic acid. The qu", "SELECT DISTINCT ?paper ?paper_lbl WHERE These query contains two SPARQL variables: - ?paper: the question is about which paper we are looking for - ?paper_lbl: the label of the paper we are looking for. The paper must contain the string \"Abstracts' entities and relations annotated corpus\" in the benchmark property. Note that the dataset and the benchmark are directly linked in this query using the HAS_DATASET and HAS_BENCHMARK properties. ### 2", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Yelp Fine-grained classification dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Common evaluation metrics when benchmarking models on the Yelp Fine-grained classification dataset include the F1-score, Recall, and Precision. output (Sparql query): orche:F1-score, precision, recall", "SELECT * WHERE { ?paper a rdf:Resource; rdfs:label ?paper_lbl; ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl; } <|endoftext|>Step into my very secret - and dirty - blog and you'll find all sorts of kinky shit happening. From reliving my sex life with medical professionals to philosophizing about life, love, and the divine feminine. Discover the kinks, the kookiness, and the lessons I'm learning. Monday, June 18, 2007 How we learned to love physical touch. I didn't notice that Kelly was watching me while I showered. She was laying on the bathroom floor and I must have assumed she was meditating. I suppose she probably was. She was letting her fingers play along my body and tracing them around my arm and shoulders.", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?dataset a orkgp:has_benchmark ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The Output should contain: * ?paper (values: {...}, label: {paper_lbl}) * ?paper_lbl (values: {Abstracts' entities and relations annotated corpus}, label: {paper_lbl}) The input text for this example is provided in the comments. Sparql 1.1 Query Language, SPARQL Protocol and RDF Data Basics, https://www.w3.org/TR/rdf-spirit/ SELECT DISTINCT ?paper ?paper_l", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper Abstracts' entities and relations annotated corpus_lbl ?paper_lbl Abstracts' entities and relations annotated corpus ?paper P31 ?cont Abstracts' entities and relations annotated corpus ?benchmark Abstracts' entities and relations annotated corpus_lbl ?cont Abstracts' entities and relations annotated corpus_lbl ?benchmark Abstracts' entities and relations annotated corpus_lbl ?dataset orkgc:Dataset ?dataset_lbl MedSTS", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WSC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Every month, over 10 million people lose their privacy to what are known in the UK as \"phone hacking scandals\". These have been caused by the private communications of people being recorded by private companies without their knowledge or consent. These scandals have exposed the shocking extent of how private information is vacuumed up, and the risks this poses to people's privacy and personal safety. This latest revelation is only the latest in a long line of phone hacking scandals. These scandals have cost private companies millions of pounds in settlements paid to families of people whose information was recorded without their knowledge. These scandals have also thrown up questions about why senior figures at British companies knew what was going on, but didn't stop it, and whether they", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): https://www.ldc.upenn.edu/Catalog/x Index=doi/10.36028/MFJC.2019.0304.0 ?index=doi/10.36028/MFJC.2019.0304.0 output (Sparql query):s using the \"(has_dataset)\" keyword and \"(has_benchmark)\" keyword A: In order to get the code links you need to apply a pipe or \"|\" operator between your query and the response. Here is a complete example of a Sparql query that should get you the code links you need : SELECT DISTINCT ?paper ?paper_lbl", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } proced= ?paper ?paper_lbl proced=proced. input (English text): https://www.semanticweb.org/benchmark/data-annotations/files/p31.zip output (Sparql query): SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }\" A: I believe your question is Select a research paper that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset. So your query should be SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Commonly used evaluation metrics when benchmarking models on the ARC-PDN dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following sparql query returns the following results: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rd", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' input (SAS macro variables): dataset; dataset_lbl; benchmark; cont; paper; paper_lbl output (SAS table): No results found ERROR:ORA-29577: could not find a matching artifact with the provided id 'P31' gravatar:https://avatars2.githubusercontent.com/u/2533125 ?v=4 gravatar:https://avatars3.githubusercontent.com/u/2533125 ?v=4 gravatar:https://avatars5.githubusercontent.com/u/2533125 ?v=4 gravatar:https://avatars6.githubuser", "SELECT * { ?paper ?paper_lbl ; ?paper :%normalized-return %. One metric commonly used when evaluating ATARI 2600 Berzerk models is the normalized return, which indicates the proportion of hits to non-hits. Return values closer to one indicate better model performance. Another metric is the average sample rate, which indicates how quickly the model samples the game board and is useful for situations where the total game state is unknown but a finite amount of information about the game state is known beforehand. Average time to Excellent (ATT) and Average Mature Age (AMA) are metrics that compare the model\u2019s performance against the optimal on the Atari 2600 Berzerk dataset.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Abstracts' entities and relations annotated corpus corpus dataset ? output (Sparql query):onte; SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC German-to-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus corpus dataset ? output (Sparql query): input (English text): What are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus corpus dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cars' performance over the Stanford Cars Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The following research papers include the benchmark \"Cars' performance over the Stanford Cars Dataset\". input (Sparql query): ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cars' performance over the Stanford Cars Dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?dataset a orkgc:Dataset; rdfs:label ?dataset_", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ShARe/CLEF eHealth corpus dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Q: Continuity of function at boundary implies continuity Let $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a function. Suppose that $f(x) \\to f(x')$ as $|x-x'| \\to 0$ for each $x$ where $x$ is in the domain of $f$, i.e. $x$ is in the domain of $f(x)$, and that $f(x)$ is continuous at $x$. Does it follow that $f$ is continuous at the boundary of the domain ? My thoughts so far: suppose $f$ is not continuous at the boundary of", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Paper: Paper ( RDFS:label ) Paper ( P31rdfs:label ) Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo Sparql query response: NULL Paper ( RDFS:label GdFeCo ) Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo Sparql query response: GdFeCo Paper ( RDFS:label GdFeCo ) Benchmark for the Abstracts' entities and relations annotated corpus dataset GdFeCo", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): https://github.com/dennyzhang/title-ABSTRACTs-and-relations-annotated- corpus/blob/master/gpt2-small-benchmark.md https://github.com/dennyzhang/title-ABSTRACTs-and-relations-annotated- corpus/blob/master/gpt2-small-benchmark.md output (Sparql query):icions:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Fishing Derby benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Atari 2600 Fishing Derby benchmark is a corpus dataset. Here are some metrics that are used to evaluate models on this benchmark: ?benchmark:dominance, ?benchmark:perplexity, ?benchmark:success rate, ?benchmark:time per move. Speaker: The benchmarks that were used in the research papers that I mentioned include a subset of Atari 2600 Fishing Derby. Some of the metrics that are used to evaluate models on the benchmark are dominance, perplexity, success rate, and time per move. Speaker: Dominance is a metric that evaluates if a model's predicted responses are dominated by the human responses (the baseline) and evaluates if the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Number of params\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Output: paper paper_lbl - - - P31 Number of params ImageNet Challenge <|endoftext|>Q: Apply function to each field of dataframe in dplyr I have this dataframe: library(dplyr) df <- tibble::tibble(Sale_Price= c(100, 200, 300, 400, 500, 600, 700, 800, 900), Sale_Date= as.Date(c(\"2017-01-01\", \"2017-01-03\", \"2017-01-04\", \"2017", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } # # Output: # # SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DAT", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }</code> input (English text): http://www.semanticweb.org/tiwary/knowledge/ontologies/bi-m.obo ?version=20161111120900 output (Sparql query): http://www.semanticweb.org/tiwary/knowledge/ontologies/bi-m.obo ?version=20161111120900#Abstracts%20entities%20and%20relations%20annotated%20corpus input (English text): http://www.semanticweb.org/tiwary/knowledge/ontologies/bi-m.obo output (Sparql query): http://", "SELECT ?metric (values ?metric) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Star Gunner dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query):SELECT ?metric (values ?metric) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Star Gunner dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl dc:subject ?paper. ?cont orkgp:P1 ?cont; ?cont dc:subject ?benchmark. ?paper_", "SELECT ?paper ?paper_lbl (WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }) input (English text): What are the titles and IDs of research papers that benchmark the NASCell model ? output (Sparql query): SELECT ?paper ?paper_lbl (WHERE { ?dataset a orkgc:Dataset; rdfs:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Datasets used in TCN model\") ?paper_code. ?cont orkgp:HAS_CODEREF ?paper_code. ?cont; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): I have used the TCN model for benchmarking purposes, can you provide me the code references in the paper that have used the TCN model for benchmarking purposes ? Output (Sparql query):Elaboration (Sparql query): SELECT ?paper ?paper_lbl ?paper_code WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Datasets used in TCN model\") ?paper_code. ?cont orkgp:HAS_CODEREF ?paper_code. ?cont; ?paper orkgp:P31 ?cont; ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>The deal", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD2.0\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl. } input (English text): SQuAD2.0 is a question answering dataset made up of 16 million machine reading questions and 30 million answer options from Google. The questions and their answer options are collected from open source QnA sites such as Quora and Knowledge Graph engines. output (Sparql query): input (English text): SELECT * WHERE { ?dataset a orkgc", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' SELECT ?benchmark ?metric ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Procedure:results are displayed in Sparql query Procedure: benchmark is the element of the dataset that has the benchmark. Benchmark is the element of the dataset that has the benchmark and the dataset", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset. output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper or", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?paper ?paper_lbl. FILTER (str( ?paper_lbl) = \"articles about coronavirus\") } output (Sparql query):quisition (English text): WHO and experts from Harvard, Oxford, Imperial College, the University of Toronto, and the Chinese University of Hong Kong. input (English text): While the above entities may not be specific to the COVID-19 virus, they are very relevant to answering the task at hand and may be helpful when determining who has contributed the most to the research community around the virus. Hence we will include them in our benchmark. output (Sparql query):igation (English text): Although there are many entities and relations that can be included in a benchmark, we choose to include a single benchmark for Articles about Coronavirus (P31). This paper is authored by researchers from Harvard, Oxford, Imperial College, the University of Toronto,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset_lbl rdfpedia:model; ?dataset_lbl rdfs:label \"Abstracts' entities and relations annotated corpus\"; ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_BENCHMARK ?cont; ?paper orkgp:P31 ?cont; ?cont rdfpedia:model. } NOTICE: I have replaced \"Abstracts' entities and relations annotated corpus\" with the actual title of the paper. kerchief (English text): rdfpedia:model. output (Sparql query):SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset_lbl rdfpedia:model; ?dataset_lbl rdfs:label rdfpedia:model; ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_BENCH", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Astrids' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The above query returned the following two results: +-+ | Paper | +-+ | <http://purl.oclc.org/sparql/dataset/ASTRID_reactor> | +-+ | <http", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Berzerk dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The Sparql query above returns 2 results: * Paper P31, benchmark \"ATARI 2600 BERZERK dataset\": [ { \"paper\": \"P31\", \"paper_label\": \"Abstracts' entities and relations annotated corpus\", \"benchmark\": { \"has_dataset\": true, \"has_benchmark\": true } } ] * Paper P31, benchmark \"ATARI 2600 BERZERK dataset\", score 1.000: [ { \"paper\": \"P31\", \"paper_label\": \"Abstracts", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdf:label ?benchmark_lbl; ?cont rdf:label ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?paper orkgp:HAS_BENCHMARK ?cont. ?paper orkgp:P31 ?cont; ?benchmark rdf:label ?paper_lbl. } input (English text): Models that have been evaluated on the Atari 2600 Skiing dataset are: Arnie Boulder Rodney Yaroslav Benchmark model: Abstracts' entities and relations annotated corpus medscianals.org", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Most commonly used benchmark datasets for the Text Summarization research field\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?benchmark. } Note: There is only one benchmark dataset ?benchmark in the results. Examples from the results: Examples: * ?paper | extractpaper_lbl * ?paper | extractpaper_lbl | extractpaper_title * ?paper | extractpaper_lbl | extractpaper_title | extractpaper_short * ?benchmark | extractbenchmark_id * ?benchmark | extractbenchmark_id | extractbenchmark_type Note: the benchmark ID is just one of the identifiers used in the benchmark datasets. Also note that the extract function takes multiple results and joins them based on the provided pattern. Examples", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The benchmark for the Abstracts' entities and relations annotated corpus dataset is the ?benchmark SPARQL attribute. SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl { ?dataset a orkgc:Dataset; rdfs:label \"The Atari 2600 Model trained on the SQuAD dataset.\". FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PROTEINS dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper orkgp:P31 ?benchmark; ?dataset a orkgp:PROTEINS ; ?dataset_lbl a proteints:Dataset; rdfs:label ?dataset_lbl. } \f proteints:Dataset is a class from the OR-tools ontology that is used to represent a data set. The PROTEINS dataset is used in the benchmark presented in the paper \"Symbolic Frameworks for Detecting Protein-Protein Interaction Sub-units\" by Mar\u00e7al et al. (P31) is from the abstracts of the paper. proteints:PROTEINS is the dataset that is used in the benchmark. proteints:HAS_DATASET is a property that can have the value of true or false that is used to represent whether a certain resource has been", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long)\")} ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper_lbl rdf:resource ?paper.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark a orkgp:BENCHMARK ; ?paper_lbl a rdfs:label. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?cont. FILTER (str( ?cont) = \"gold standard abstracts\"), ?cont ?paper_lbl. } Challenge Abstracts' Entities and Relations Annotated Corpus Benchmark Abstracts' entities and relations annotated corpus (P31) Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark Databricks' benchmark", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' input (English text): https://huggingface.co/news/introducing-the-huggingface-benchmarking-tool#datasets output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl", "SELECT ?paper (values ( ?paper_lbl as ?paper_lbl_str)) WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?dataset a orkgp:has ?cont. FILTER (str( ?cont) = \"Extended IB1\") } input (English text): The best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric is the proposed extended IB1 model. Manager: Extended IB1 Manager: Extended IB1 is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric. Manager: Extended IB1 is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric. Manager: The proposed extended IB1 model is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric. Manager: The proposed extended", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Title-Match Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): ?paper ?paper_lbl {\"Title-Match Benchmark\" \"title match\" 100} inversion:SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; ?paper ?paper_lbl. FILTER (str( ?paper_lbl) = \"", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Bleu\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Bleu, CIDEr, CDRC <|endoftext|>Introduction {#Sec1} ============ The tendency for illness and health to fluctuate over time, whether this is due to pure chance or a directed relationship between the two, has been a source of interest in psychology and medicine for decades. In the fields of psychology and medicine, time-series analysis has been used to study disease rates and risk factors across time.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } *Code used in the benchmark paper* [width=\"90%\",border=\"1\",align=\"center\",] *Dataset Benchmark* - [title=\"Multi-Perspective Matching (single model)\", author=\"Dai\", journal=\"The BMJ\", year=\"2018\", abstract=\" This method aims to combine global and local models for fine-grained cross-dataset recognition. We implemented two models: one for global representation and one for local representation. Global model is trained on single model by maximizing the evidence lower bound. Local model fuses global and local representations on a two-layer LSTM using simple averaged information. The final representation is integrated by simple averaging information from", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"GAD dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): P31 input (English text): The benchmark for the GAD", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Measures of evaluation over the PubMed 20k RCT dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } Select the papers that contain the following terms: * 'Abstracts' entities and relations annotated corpus' * 'Measures of evaluation over the PubMed 20k RCT dataset' The selected papers should contain the following terms in their titles and IDs. * P31: 'Abstracts' entities and relations annotated corpus' * P31: 'Measures of evaluation over the PubMed 20k RCT dataset' The selected papers are: * P31: 'Abstracts' entities and relations annotated corpus' * P31: 'Measures of evaluation over the PubMed 20k RCT dataset' The benchmark for the Abstracts' entities and", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The metric that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset are: * Time, in milliseconds * Points, the total number of points achieved by the agent * Distance, the total distance travelled by the agent (:P31, the benchmark entity), (:cont, the dataset entity), (:dataset_lbl, the dataset label), (:benchmark, the relation name) (:paper_lbl, the paper label) (:paper, the paper entity) (:dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ) (:benchmark orkgp:HAS_DATASET ?dataset.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } posable set of tuples : {<paper1>, \"paper1_label\", <dataset1>, <benchmark1>}", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AxCell model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } <|endoftext|>Q: How do I make Goji Balls ? I saw a recipe for Goji Balls on Pinterest, but they called for dates instead of goji berries, and rice instead of pumpkin seeds. Also, they don't mention how many goes to make the balls. A: It's a very popular goji recipe that can be found all over Pinterest. Pumpkin seeds + goji berry = Miracle berry The recipe for these goji balls is as follows: 150g goji berry 120g dates 30g virgin olive oil 30g icing sugar pinch of salt Blend all ingredients, except date", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper sparql:value ?benchmark; ?benchmark sparql:C31. } input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model ? output (Sparql query): don't know of any code that could be used. input (English text): What are the titles and IDs of research papers that benchmark the AWD-LSTM model ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?paper sparql:value ?benchmark; ?benchmark sparql:C31. } input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model ? output (Sparql query):ipeline do{ def dataset = \"http://data.yaili.org.cn/abstracts/dataset.html\" def", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PubMedQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P32 ?cont; rdfs:label ?paper_lbl. } Sample Result: [ { \"paper\": \"P31\", \"paper_lbl\": \"PubMedQA dataset\" }, { \"paper\": \"P32\", \"paper_lbl\": \"PubMedQA dataset\" } ] <|endoftext|>Q: SQL - how to count number of matches between two tables I have the following two tables: Table 1 - company Id | Name - | - 1 | ABC 2 | XYZ 3 | DEF 4 | GHI Table 2 - department Id | Name - | - 1 | Finance 2", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Datasets used for benchmarking in the Image Classification research problem\") } The response should return the following data: paper P31,paper_lbl \"Abstracts' entities and relations annotated corpus\" Here is the SQL for the Sparql Query: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont ; rdfs:label ?paper_lbl. } <|endoftext|>Q: Sums over $\\mathbb{N}^2$, infinite products and diagonal and anti-diagonal matrices I have this question : Prove that for", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Gibson PointGoal Navigation dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper. personnelis (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper. personnelis (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper. personnelis (English text): The best result is 0.03 (with \"meters\" and 0.98 score), included in the abstract of this paper. personnelis (English text): The best result is 0.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?score. } input (English text): Cheetah output (Sparql query): P31 input (English text): run (DMControl500k) output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?score. } output (S", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The Tanimoto metric computes the percentage of overlapping areas of two shapes. The higher the Tanimoto value, the more similar the compared shapes are. solution (Sparql query): orchestration 2 returned 1 row { ?paper ?paper_lbl : benchmarksReturnedObjectiveValue 12.5 } <|endoftext|>Q: Neo4j cypher.match on list of tuples", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } solution A: The Benchmark solution B: Dataset solution C: Dataset, Benchmark, Benchmark_id", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): P31 output (Sparql query): P31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Russian dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The highest benchmark result achieved on the WMT2016 English-Russian dataset was 73.33 on 2017-02-03T16:33:33Z. output (Sparql query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The selected papers are: The dataset \u201cAbstracts\u2019 entities and relations annotated corpus\u201d The benchmark \u201cAbstracts\u2019 entities and relations annotated corpus\u201d the papers selected are: \u201cAbstracts\u2019 entities and relations annotated corpus\u201d \u201cAbstracts\u2019 entities and relations annotated corpus Benchmark\u201d the selected result set is: The selected papers are: \u201cAbstracts\u2019 entities and relations annotated corpus\u201d The dataset \u201cAbstracts\u2019 entities and relations annotated corpus\u201d The benchmark \u201cAbstracts\u2019 entities and relations annotated corpus Benchmark\u201d <|endoftext|>One would think the Jets would want to upgrade at quarterback, and it's not like they are in a lot", "SELECT ?paper (rdfs:label ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?paper. } input (English text): The name of the model is \"Combined CNN+LSTM\". A: The following query retrieves the paper with the best F-score on the atari dataset: SELECT ?paper (rdfs:label ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper bidi:link ?code_lnk. } input (English text): Here are the references with the corresponding code links: P31 Dataset Manipulation with No-Op C51 Benchmarks (https://archive.ics.uci.edu/ml/datasets/Abstracts%26relations%20annotated+corpora) P31 Benchmark Dataset Manipulation with No-Op C51 Benchmarks (https://archive.ics.uci.edu/ml/datasets/Abstracts%26relations%20annotated+corpora#benchmarks) P31 Simple R-tree on Abstracts' Entities and Relations Annotated Corpora (https://archive.ics.uci.edu/ml/datasets", "SELECT statement that returns the papers with their corresponding benchmark, along with a LABELS relation that maps the label of the benchmark to the title of", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. ?benchmark orkgp:HAS_DATAS", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA metric\") } \"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA metric\") }\" \"* CoQA metric\" \"* Gated long-short term memory\" \"* Gated LSTM\" \"* Gated recurrent unit\" \"* Cross-lingual dependencyparser\" \"* CoNLL 2005 cross-lingual dependency parser\" \"* Noun phrase parser\" \"* GRU\" \"* Recurrent dense neural network\" \"* Attentive model\" \"* Attention mechanism\" \"* List of metrics used on CoQA benchmark dataset\" \"* Gated LSTM\" \"* Gated recurrent unit\" \"* Cross-lingual dependencyparser", "SELECT ?paper ?paper_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } This is one of the possible answers returned by the query: DOCTYPE doc ns=# xBase with xBase-ML achieved the best F1 score on the OntoNotes dataset", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Defender dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset A benchmark for the Atari 2600 Defender dataset", "SELECT ?paper_lbl ?paper_id WHERE { <paper> ?paper rdf:type papers:Paper; <paper_lbl> ?paper_lbl rdfs:label ?paper_lbl; <benchmark> ?benchmark orkgp:HAS_DATASET ?dataset; <cont> ?cont orkgp:HAS_BENCHMARK ?benchmark; <paper_id> ?paper orkgp:P31 ?cont; <paper_lbl> ?paper_lbl rdfs:label ?paper_lbl; } <paper_id> ?paper_id str( ?paper_id) ; <paper_lbl> ?paper_lbl rdfs:label ?paper_lbl; <cont> ?cont orkgp:P31 ?paper_id; <paper_id> ?paper_id string( ?paper_id) ; <benchmark> ?benchmark orkgp:HAS_DATASET ?dataset; <benchmark> ?benchmark orkgp:HAS_BENCHMARK ?cont; <benchmark", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The following papers have used the Switch Transformer model: [1] \"Databricks: The Dawn of Trust,\" Databricks, November 4, 2019. [2] Vijay Nair, Ali Ghodsi, Ion Stoica, David Ho, Edsger Wijsman, Thain Fauvel, Ion Stoica, Ross McNeill, Luca Camus, Tassilo Weidlich, Krzysztof Glowala, Gina Wodrich, \"Switch Open AI Citys on Databricks,\" Databricks, March 2019. [3] Francis Chou, Benoit Bijvie, It", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } splominor (Sparql query 1): ?benchmark thesaurl:doi ?paper_doi. ?cont thesaurl:ref ?paper_ref. ?paper thesaurl:title ?paper_title.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2 evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } > Abstract entities and relations benchmarked > # Abstract entities and relations benchmarked on the > # Amazon-2 dataset > Dataset entities and relations benchmarked > # Dataset entities and relations benchmarked on the > # Amazon-2 dataset > Benchmark entities and relations benchmarked > # Benchmark entities and relations benchmarked on the > # Amazon-2 dataset > Paper entities and relations benchmarked > # Paper entities and relations benchmarked on the > # Amazon-2 dataset paper:P31 Dataset entities and relations benchmarked # Dataset entities and relations benchmarked on the Amazon-2", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"A3C-CTS model\") } input (English text): http://www.semanticscholar.org/paper ?paper=P31&paper_lbl=Abstracts' entities and relations annotated corpus output (Sparql query): http://www.semanticscholar.org/paper/P31 input (English text): http://www.semanticscholar.org/paper ?paper=P31&paper_lbl=A3C-CTS model output (Sparql query): ?paper ?paper_lbl input (English text): A3C-CTS model output (Sparql query): P31 input (English text): Abstracts' entities and relations annotated corpus output (Sparql query): P31 input (English text): A3C-CTS model output (Sparql", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper dwh-paper-label 28 ?paper_lbl Hutter Prize 28 ?benchmark Hutter Prize 28 ?benchmark_metric similarity [ \"similarity\" ] Query result: dwh-paper-label: Hutter Prize 28 dwh-paper-label: Abstracts' entities and relations annotated corpus 28 ?benchmark: Hutter Prize 28 ?benchmark: similarity [ \"similarity\" ] Query result in json: {\"dwh-paper-label\": \"Hutter Prize\", \"dwh-", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\"@nl. FILTER (str( ?benchmark) = \"CoNLL 2003 (English)\")} ?paper orkgp:P31 ?benchmark; ?paper_lbl rdfs:label \"CONLL 2003 (English)\"@nl. } <|endoftext|>- abstract: 'The main purpose of this work is to investigate the properties of some distributions arising from random matrices. More precisely, we analyze the two cases: the normal distribution and the uniform distribution. As an application, we provide bounds for the discrepancy principle.' author: - | Emilio Marcell\u00e1n and Martin Knapik\\ \\ Centre for Mathematics, Dresden University of Technology, D-01062, Dresden, Germany bibliography: - 'literatur.bib' title: Bounds for discrepancy principles based on random matrices - Introduction ============ In recent years many central limit theorems for probability distributions have been obtained by analyzing the behavior of random matrices. The motivation is that many distributions", "SELECT ?paper ?paper_lbl ?benchmark_id where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark_id. ?cont orkgp:HAS_BENCHMARK ?benchmark_id. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Top performing model = Top Performing Model ## Query 1 SELECT DISTINCT ?paper ?paper_lbl ?benchmark_id WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"VTAB-1k\") ?benchmark_id. ?cont orkgp:HAS_BENCHMARK ?benchmark_id. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } #+name: papers dataset #+label: datasets with papers for this", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?paper_lbl. FILTER (str( ?paper_lbl) = \"Artificial Intelligence\" or str( ?paper_lbl) = \"natural language processing\" or str( ?paper_lbl) = \"text mining\" or str( ?paper_lbl) = \"natural language understanding\" or str( ?paper_lbl) = \"speech processing\" or str( ?paper_lbl) = \"spoken language processing\" or str( ?paper_lbl) = \"speech recognition\" or str( ?paper_lbl) = \"speech synthesis\" or str( ?paper_lbl) = \"computer interface\" or str( ?paper_lbl) = \"natural user interface\" or str( ?paper_lbl) = \"speech and language technology\" or str( ?paper_lbl) = \"semantic web\" or str( ?paper_lbl) = \"knowledge representation\" or str", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P23 ?cont; rdfs:label ?model_lbl. } model = [{\"name\":\"Perceptron\",\"benchmark\":\"HAS_BENCHMARK\",\"model\":\"P23\",\"dataset\":\"Dataset\",\"entity\":\"cont\"}] output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FSNS - Test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P23 ?cont; rdfs:label ?model_l", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?benchmark; rdfs:label ?paper_lbl. } In this answer, ?paper is the label of the RDF graph for a research paper, ?paper_lbl is the label of the RDF graph for the paper's benchmark, ?dataset is the label of the RDF graph for the WMT2014 German-English dataset, ?benchmark is the label of the RDF graph for the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric, ?cont is the label of the RDF graph for the paper's dataset, ?cont_lbl is the label of the RDF graph for the paper's best performing model benchmarking the WMT2014 German-English dataset, and ?paper", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' <|endoftext|>\"The year is 2150.\" \"Futuristic technology allows people to pursue anything they want.\" \"Everything from music to health.\" \"In music, they pursue their dreams with the hopes of performing at one of the biggest music festivals in the world.\" \"But what most people don't know is there's always more than one way to skin a cat.\" \"This is Craig Sawyer, coming to you live from the city of angels, Paris.\" \"For thousands of years, human beings have sung their passions to the heavens.\" \"For the first time in history, this will be possible anywhere, anytime.\" \"For composer Fabrice Baree, that passion is music.\" \"He is one of the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The highest benchmark result achieved on the STS Benchmark dataset is 79.3%. <|endoftext|>WASHINGTON (AP) - A federal appeals court on Wednesday ruled that a group of migrant children held in Ohio may move to Michigan, finding that the state's \"special relationship\" with the children's families created a significant opportunity cost. The decision by the 6th U.S. Circuit Court of Appeals in Cincinnati is the latest in a string of cases around migrant children being held by the United States government. The courts are dealing with a growing number of children who are being held in states where the families have filed court motions requesting that the children be allowed to move to Michigan.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Link to code of research paper\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The queries retrieves the necessary information to run in the `jena-benchmark` query engine (included in the Jena benchmark project): * `dataset`: the name of the dataset * `benchmark`: the name of the benchmark entity (typically the SHARE MODEL) * `paper`: the name of the paper * `cont`: the name of the corresponding ontology concept or URI of a dataset or ontology annotation The SELECT DISTINCT keyword restricts the returned results to a single paper and a single benchmark.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The benchmark datasets used most often are OntoloViz and CoNLL-2009. Both of these datasets contain annotated gold datasets and the gold datasets are the dataset that is the target of the benchmark. The Benchmark dataset in this example contains the properties has_dataset and has_benchmark. These properties contain the values true and false, respectively. The values of these properties are the dataset and the benchmark that are benchmarked by the dataset. The values of the properties has_dataset and has_benchmark in the ontology of this benchmark dataset are the dataset and the benchmark that are benchmarked. In the current example, the values of the properties has_dataset and has_benchmark in the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MultiRC dataset evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The standard set of evaluation metrics commonly used on the MultiRC dataset are: R@1, R@5, R@10, R@50, PR@1, PR@5, PR@10, and PR@50.", "SELECT ?paper_code_reference_from_model WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. FILTER (str( ?dataset", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } # The following is the corresponding JSON output: # The following is the corresponding JSON output: #[ { \"paper\": \"rat\", \"paper_lbl\": \"P31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } A Benchmark for Web Markup Rules: WMT2014 German-English Dataset The Benchmark for Web Markup Rules WMT2014 German-English Dataset Title: A Benchmark for Web Markup Rules WMT2014 German-English Dataset P31: Abstracts' entities and relations annotated corpus A Benchmark for Web Markup Rules W", "SELECT ?model ?model_label WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_label. FILTER (str( ?dataset_label) = \"SearchQA\" ) ?model. ?model_label ?benchmark; ?model_label ?benchmark_label; } input (English text): The models that have been benchmarked on the SearchQA dataset include models from the following research communities: Conll-2005, CoNLL-2007, and MultiNLI. Invalid model name \"SearchQA\". Invalid benchmark name \"HAS_BENCHMARK\". Invalid model label \"P31\". Invalid benchmark label \"Abstracts' entities and relations annotated corpus\". Invalid dataset label \"orkgp:HAS_DATASET\". Invalid dataset label \"orkgc:Dataset\". Invalid dataset label \"rdfs:label\". Invalid benchmark label \"MultiNLI\". Invalid model label \"MultiNLI\". Invalid model label \"Conll-2005\". Invalid model label \"CoNLL-2007\". Invalid model label \"orkgp:P31\". Invalid model label \"orkgp:HAS_BENCHMARK", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Defender dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } [ { \"paper_lbl\": \"Defender dataset\", \"paper_id\": \"P31\" }, { \"paper_lbl\": \"Abstracts' entities and relations annotated corpus\", \"paper_id\": \"P31\" } ]", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT 2014 English-German\") ?benchmark_metric. ?benchmark_value ?benchmark_metric_val. ?paper orkgp:P31 ?cont; ?paper_lbl rdfs:label ?paper_lbl. } input (English text): WMT2014 English-German benchmark_metric\tWMT 2014 English-German benchmark_value\thighest benchmark_metric_val\t61.0% ideggerien Actually, the highest benchmark result achieved on the WMT 2014 English-German dataset is 61.0% * WMT 2014 English-German 61.0% [Source: https://www.w3.org/TR/2012/PR-wd-page-up/revised-dic-model.html#prSection4][1] * WMT 2014 English-German 61.0% * The model's F1-score was 61.0% on the English-German WMT2014 news section data set", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"PWC Leaderboards dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The query results include two documents: * ` paper=P31 ` * ` paper_lbl=PWC Leaderboards ` The `paper` document contains the title and the `paper_lbl` document contains the label of the research paper that includes a benchmark for the PWC Leaderboards dataset. # Find out the PWC Leaderboards dataset The `paper_lbl` document includes the label `PWC Leaderboards` which is related to the `paper` document. The `P31` token in the `paper_lbl` document is the title of the research paper that includes a benchmark for the PWC Leaderboards dataset. # Find out the PWC", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Metrics used on enwik8 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Stadiums or-else fields. Stadiums are represented using the IRIs http://tf-stadium.herokuapp.com/ and the relations for a given stadium are stored as a property called {stadium.stadiumName} on the graph. The metric used on the enwik8 benchmark is the average distance between two nodes which is computed as Euclidean distance on the values of the corresponding property. The queries can be executed from the endpoints at https://en.m.wikipedia.org/w/api.php ?action=query&list=random&rlimit=1000&retlimit=10&StructuredQuery=stadiums%20%3F%20or%20", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TempEval-3\")} invite (Sparql query): ordable solution for automated summarization in the cloud (P31) invite (Sparql query): ?cont owl:class [ owl:equivalentProperty rel:hasDataset[. ] ] ?benchmark owl:equivalentProperty rel:hasBenchmark[. ] ?paper owl:property", "SELECT ?paper_lbl ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } ORDER BY count(*) desc input (English text): Most common location in the studies is \u201cAbstracts' entities and relations annotated corpus\u201d. output (Sparql query): To retrieve the 2nd result, ?paper_lbl should be \u201cAbstracts\u2019 entities and relations annotated corpus\u201d, the 3rd result should be \u201cAbstracts' entities and relations annotated corpus (Benchmark),\u201d the 4th result should be \u201cAbstracts' entities and relations annotated corpus (Benchmark) Benchmark\u201d. input (English text): Most common location in the studies is \u201cAbstracts' entities and relations annotated corpus\u201d. output (Sparql query): To retrieve the 2nd result, ?paper_lbl should be \u201cAbstracts\u2019 entities and relations annotated corpus\u201d, the 3rd result should be \u201cAbstracts' entities and relations annotated corpus (Benchmark),\u201d the 4th result should be \u201cAbstracts' entities and relations annotated corpus", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } injection 1: ?dataset a orkgc:Dataset. injection 2: ?dataset_lbl string: \"Abstracts' entities and relations annotated corpus\". injection 3: ?benchmark orkgp:HAS_DATASET. injection 4: ?cont orkgp:HAS_BENCHMARK. injection 5: ?paper orkgp:P31. injection 6: ?paper_lbl string: \"Abstracts' entities and relations annotated corpus\". output (Sparql query): [ { \"paper\" : \"P31\", \"paper_lbl\" : \"Abstracts' entities and relations annotated corpus\" }, {", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|> ACCEPTED 14-14-00758-CV FOURTEENTH COURT OF APPEALS HOUSTON, TEXAS 9/5/2015 4:10:23 PM CHRISTOPHER PRINE CLERK No. 14-14-00758-CV FILED IN 14", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; } input (English text): P31 is a model for extracting Piano Rolls from Monaural recordings, a kind of image-based keyframe-based motion tracking. P31 is evaluated on the Classical music, 5 seconds at 12 kHz dataset. \r The benchmark ?benchmark is of class hasdataset and its values are P31, P31, P31, The SQL returned for querying this SPARQL endpoint is: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?benchmark. } replied sparql query: IWSLT2014 German-English replied output: IWSLT2014 German-English replied input: IWSLT2014 German-English replied output: IWSLT2014 German-English replied input: IWSLT2014 German-English replied output: IWSLT2014 German-English replied input", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } From the above Sparql query we can see that the paper dataset and benchmark are mentioned in the WHERE clause. Additionally, we can also see in the above query that we have at least one relationship between paper and benchmark where paper is the target and benchmark is the property that holds the relationship. SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } So from the RDF dataset, we have a paper that has dataset and benchmark that has the relationship has_dataset to dataset and has_benchmark to benchmark. The papers in the benchmark dataset are listed in the ?paper_lbl variable that", "SELECT ?paper ( LABEL ?paper_lbl as ?paper_lbl ) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Skiing evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>The case of Yale\u2019s attempted recruiting of a convicted sex criminal, who in turn sent a now-notorious \u201cpenis picture\u201d to the university is likely part of a wider political game and ideological tug-of-war within the state and the nation over the threat of radical Islam, a former FBI counterterrorism specialist said on Sunday. \u201cYale is playing a very shrewd", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset ? SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } List", "SELECT DISTINCT ?paper ?paper_lbl_ref WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Here are some links to code that benchmark the COMET - Direct model: - [Paper: Deep Learning for Abstractive Summarization](http://aclweb.org/anthology/P/P16/P16-1091.pdf) - [Paper: Neural Diamonds: Generalizing Recurrent Neural Networks for Abstractive Sentence Summarization](https://arxiv.org/abs/1802.04451) - [Paper: Attention Induction for Neural Diamonds](https://arxiv.org/abs/1802.04453) - [Paper: Explicit Neural Diamonds](https://arxiv.org/abs/1803", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Best performing model on the BUCC Russian-to-English benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?paper_id. } Output: P31 The best performing model on the dataset is sparql.berlin.de:model-name. Or in one line: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?benchmark; ?paper_lbl ?benchmark_label. } input (English text): The highest benchmark result achieved on the Atari 2600 Boxing dataset is at 31,2%. iza\u00e7\u00e3o (Sparql query): di:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Boxing dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?", "SELECT DISTINCT ?paper WHERE { ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark bidi:has_dataset ?dataset. } } Which model has achieved the highest F1 score on the Penn Treebank benchmark dataset ? ?paper_lbl:Penn Treebank Benchmark The mentioned model is The mentioned model is the Penn Treebank model with a F1 score of 0.91 The mentioned model is the Penn Treebank model with a F1 score of 0.91 The Penn Treebank model has achieved the highest F1 score score on the Penn Treebank benchmark dataset The Penn Treebank model has achieved the highest F1 score on the Penn Treebank benchmark dataset The Penn Treebank model has achieved the highest F1 score of 0.91 on the Penn Tree", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark:metric ?metric; ?benchmark:score ?score. } input (English text): IWSLT2015 German-English dataset: IWSLT 2015 English to German parallel corpus. Metric: BLEU, Score: 22.0. output (Sparql query):ordable ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl a IWSLT2015 German-English dataset. rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"IWSLT2015 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkg", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper_lbl@paper{ ?paper rdfs:label ?paper_lbl. } input (English text", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?benchmark; ?cont orkgp:HAS_DATASET ?dataset; rdfs:label ?paper_lbl. } MATCH (paper:P31) - [b:HAS_BENCHMARK] - (benchmark:HAS_BENCHMARK) - [c:HAS_DATASET] - (dataset:Dataset) - [l:rdfs:label] -() - (cont:cont) - (dataset_lbl:rdfs:label) - (paper_lbl:rdfs:label) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Benchmark for the Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }", "SELECT DISTINCT sparql SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } dbCommand.execute(). tripleMap(true). find(paper). by(paper_lbl). by(benchmark). by(cont). by(paper). order", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The Sparql query above will", "SELECT DISTINCT ?metric FROM <http://www.omg.org/data/ Benchmarks.5.csv#kind=record> WHERE { { ?metric metrics:metricValue ?metric; rdfs:label ?metric}. } \r \r Out: \r \r rdfs:label ?metric. <|endoftext|>Nicotine replacement therapy for smoking cessation: more than a cure. Smoking is the leading cause of preventable death in the United States. One in six smokers will die of a smoking-related disease. The use of nicotine replacement therapy (NRT) for smoking cessation results in significant abstinence rates. Behavioral support methods (such as counseling or purchasing NRT) are associated with greater cessation success. The most effective formulation of NRT for smoking cessation is the combined behavioral support with transdermal nicotine. Physicians should recommend that smokers enroll in a cessation program, and they should consider the benefits of NRT when counseling smokers who want to quit.<|endoftext|> 809 P.2d 46 (1991) 115 Or. App. 621 Barbara ROBINSON and Larry Robinson, Wife and Husband, Plaintiffs-Appellants, v. ELMON BROTHERS, INC., an", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark g:HAS_BENCHMARK ?benchmark; ?paper g:P31 ?benchmark. ?cont g:HAS_DATASET ?dataset. } };The titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are: P31, \"Abstracts' entities and relations annotated corpus\", rdfs:label, \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus\", \"Abstracts' entities and relations annotated corpus", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } - the best benchmark score and its metric of the benchmark papers is 31 and P31 <|endoftext|>Q: Search data in my database with two different colums Im trying to do a search in my database but its hard to see the search result only with the last column \"PRICE\". I want to search a field that is equal with UserName in the column 'USERNAME' in my database. With this", "SELECT DISTINCT ?paper ?paper_lbl WHERE {", "SELECT ?benchmark ?metric WHERE { ?dataset a orkgc:Dataset; ?benchmark var!dataset. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?benchmark_score ?metric; ?dataset_lbl a orkgc:DatasetLabel; ?dataset_lbl rdf:resource ?dataset; ?dataset_lbl rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } Cite: \u00a0Authors: Joseph Gilbert, Drew DeVito, Larry A. Vis708-732-6086; Published: 15 Oct 18; URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5812442/ Paper 3 Abstracts\u2019 entities and relations annotated corpus has been evaluated with the best-performing relation type upon the entire PubMed repository. The dataset was evaluated on four metrics: - Precision (P): (number of true positives)/(number of positive samples) - Recall (R): (number of true positives)/(number of all samples)", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Article 31: Benchmarking Datasets for Citation Classification with SPARQL and KGs. output (Sparql query):orpkgp:P31. HAS_BENCHMARK ?benchmark. input (English text): Article 31. Benchmarking Datasets for Citation Classification with SPARQL and KGs. output (Sparql query): Database ? Benchmarking ?Datasets ? for ?Citation ? Classification ?with ? SPARQL ?and ? KGs. input (English text): Database ? Benchmarking ?Datasets ? for ?Citation ? Classification ?with ? SPARQL ?and ? KGs. output (", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?paper_lbl. } input (English text): List the code links in papers that use the H-NLI model in any benchmark. output (Sparql query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The dataset Abstracts' entities and relations annotated corpus and the benchmark PubMed 20k RCT are declared in the figure below: figure=\"{\\\"slices\\\":[{\\\"id\\\":\\\"Abstracts' entities and relations annotated corpus\\\",\\\"name\\\":\\\"Abstracts' entities and relations annotated corpus\\\",\\\"label\\\":\\\"Abstracts' entities and relations annotated corpus\\\",\\\"description\\\":\\\"", "SELECT * { ?paper <http://dl.acm.org/citation.cfm ?id=2903705.2903720> ?paper_lbl, ?paper <http://dl.acm.org/citation.cfm ?id=2903705.2903720> ?paper_lbl, ?paper ?benchmark, ?benchmark <http://dl.acm.org/citation.cfm ?id=2903705.2903720> ?benchmark_lbl} WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ructions: As of the time of this writing, the best performing model was an LSTM language model, while the second best performing model was aBiLSTM language model. The third best performing model was the GPT-2 model.", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Nottingham dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset is `rd_NLL`.", "SELECT ?paper_ref WHERE { ?paper_ref rdfs:label \"Code references\"@en.. ?paper rdf:type \"Repository\"@en. ?paper_ref dct:isPartOf ?github. ?github dct:uri \"https://github.com/\". ?github dct:repository ?paper. ?paper_ref rdfs:label \"Code references\"@en. ?paper rdf:type \"Repository\"@en. ?paper_ref dct:isPartOf ?arxiv. ?arxiv dct:uri \"https://arxiv.org/\". ?arxiv dct:repository ?paper. } <|endoftext|>EXCLUSIVE: Warner Bros. has closed a straight to series order for the third season of The CW\u2019s Supergirl, it was announced today by Dawn Ostroff and Andrew Kreisberg, the voice and showrunner and co-executive producer on the series, respectively. Supergirl will see Kara Danvers (Melissa Benoist) gain powers from a meteor that falls in the Cactus town of", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } P31 Biologically-Plausible model that are the titles and IDs of research", "SELECT ?paper_lbl ?benchmark_score { ?benchmark rdfs:label \"Ice Hockey Benchmark\"@en ; ?paper rdfs:label \"Ice Hockey Benchmark\"@en ; ?benchmark skos:BenchmarkScore@en \"33.33\"@en } airQuery: SELECT ?benchmark_score ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ice Hockey Benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ryst:Ice Hockey Benchmark 33.33 : SELECT \"Ice Hockey Benchmark\"@en \"33.33\"@en ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Ice Hockey Benchmark\") ?benchmark orkgp:HAS_", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Here are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: Titles: - P31: Benchmarking Datasets for Scientific Results Extraction - P38: Benchmarking Datasets for Scientific Visualization - P40: Benchmarking Datasets for Scientific Computation - P46: Benchmarking Datasets for Multimodal Databases - P47: Benchmarking Datasets for Textual Entailment - P48: Benchmarking Datasets for Ontology-based Query Evaluation - P49: Benchmarking Datasets", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } Retrieval of the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset. SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Benchmarking results across datasets in the Semantic Role Labeling research area: +-+-+ | Paper_label | Paper | +===============+==================================+ | P31 | Abstracts' entities and relations | | P31 | Abstracts' entities and relations | | P31 | Abstracts' entities and relations | +-+-+ +-+-+ | Paper_label | Paper_lbl | +===============+==================================+ | P31 | Abstracts' entities and relations | | P31 | Abstracts' entities and relations | | P31 | Abstracts'", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Query Output: paper P31 paper_lbl Abstracts' entities and relations annotated corpus P31 contains a benchmark of the Abstracts' entities and relations annotated corpus dataset.", "SELECT ?paper_lbl ?paper_id ?benchmark_id ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. ?paper ?paper_id; ?benchmark ?benchmark_id; ?metric ?metric_id. } input (English text): metric (Textual string): AUC (Abbreviated for Area Under the ROC Curve) input (English text): paper (Textual string): Kipman, Max; Garcia, Francisco; Song, Yiheng;... benchmark (Textual string): PWC Leaderboards (Restricted) output (English text): paper_id (Textual string): P31 output (English", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' input (English text): https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop.owl#L31 https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop.owl#L34 https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop.owl#L39 https://github.com/ontio/ontology-research/blob/master/research/benchmarks/dueling/noop/noop", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The datasets benchmarked under Fine-Grained Image Classification research problem are: * P31 is a a benchmark for datasets that are annotated with the P31 relation. <|endoftext|>Q: what is : ~K@ in perl ? I came across this :~K@ in perl, I can't seem to find it. What is it for and what does it do ? $ touch ~K@ $ touch ~K@ $ ls a.txt b.txt c.txt d.txt $ ls -l a.txt b.txt c.txt d.txt $ echo $ ? 1", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Evaluation metrics\"@pt) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Here are the top 5 results: 1\trdfs:label \"Evaluation metrics\"@pt 2\trdfs:label \"Average precision\" 3\trdfs:label \"F-measure\" 4\trdfs:label \"Support vector machine\" 5\trdfs:label \"Kendall's tau\" Next, provide a link to the documentation of each of the evaluation metrics. rdfs:label \"Evaluation metrics\"@pt: http://www.icsi.berkeley.edu/dls/drs/readings/papers/icdm14", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } presented in [1] are: terms, terms per million, terms per billion, terms per hundred million, terms per hundred billion, number of terms, number of terms per million, number of terms per billion, number of terms per hundred million, number of terms per hundred billions. Article [1] states the following: \"6.1.2 Term Information \"The following table lists the terms that annotators used to index the corpus, their count in the corpus and the frequency with which they appear in the benchmark.\" Article [1] also mentions \"Abstracts' entities and relations annotated corpus\" as the corpus identifier.", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sequential CIFAR-10 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): Sequential CIFAR-10 dataset output (Sparql query): P31 ience (Sequential CIFAR-10 dataset)", "SELECT ?paper (distinct ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): http://www.semanticweb.org/chandralab/workshops/2015/abstracts-annotated-orpus/ input (Sparql query): SELECT ?paper (distinct ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The following models have been tested on the SciCite benchmark dataset: Paper \\#31: Berkhout, S., & Goecks, M. (2018). Entity linking in network news byveiled text with entity disambiguation. \u0105Data-driven disambiguation for weakly supervised news category extraction. Journal of the American Society for Information Science and Technology, 65 (11), 1801-1814. Paper \\#32: Hu, J., Berkhout, S., & Yang, C.-H. (2018). Discovering the ontology and taxonomy of [Wikipedia]{} with heterogeneous tree kernel. \u0105Tree kernel method for navigating large ontology collections. Journal of the American Society", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The following paper titles have benchmarked models on the Atari 2600 Skiing dataset: Machine Learning on the Atari 2600: A Competitive Analysis by AutoZoo and Martin Arndt, C. O. C. Lum. SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont;", "SELECT ?paper_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the metric identifiers in the following paper titles: \"Performing weakly-supervised coreference resolution on CoNLL-2009\", \"Efficient extraction of rare phrases and relation types from full-text search log data\", \"An incremental approach to coreference resolution using RDF data\", \"A Coref-RL based approach for coreference resolution in RDF\", \"Coref-RL: A model-based approach for coreference resolution in RDF\", \"Cross-lingual coreference resolution in RDF\", \"Hierarchical treebanks for coreference resolution", "SELECT * WHERE { ?model ?benchmark. ?benchmark rdfs:label \"Cart Pole (OpenAI Gym) Benchmark Model\".} {#section-1924otine317} The following research papers are the benchmark dataset (HAS_DATASET) and the benchmark (HAS_BENCHMARK) in the benchmarked entity and relation annotated corpus dataset: {#section-1924otine318} Paper: P31 Abstracts' entities and relations annotated corpus {#section-1924otine319} Paper_label: \"Abstracts' entities and relations annotated corpus\" {#section-1924otine320} benchmark: Cart Pole (OpenAI Gym) benchmark Model {#section-1924otine321} cont: Cart Pole (OpenAI Gym) benchmark Model {#section-1924otine322} paper: P31 Cart Pole (OpenAI Gym) benchmark Model {#section-1924otine323} benchmark: Cart Pole (OpenAI Gym) benchmark Model {#section-1924otine324} paper_label: \"Cart Pole (OpenAI Gym) Benchmark Model\"", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rfa-Gate-arccos model used in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } sop:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rfa-Gate-arccos model used in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The code link follows: Rfa-Gate-arccos model used in any benchmark forpaper:P31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Solution 2 (More readable): SELECT DISTINCT ?paper ?", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BioASQ dataset evaluation metrics\") ?metrics. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?cont orkgp:HAS_SCORE ?score. } input (English text): The best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric is model_6. # Answer SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?cont orkgp:HAS_SCORE ?score. } <|endoftext|>Wondering how a proposed 30-unit building for artists would be financed ? Well, now you can see how the project is being funded and where the money is coming from, thanks to an interactive visualization created by Data Mapper, a Los Angeles-based data visualization studio. The building, which would be located at 4130 Washington Blvd. in San Pedro, was announced by the city of Los Angeles back", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark a orkgp:HAS_BENCHMARK. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Entitys e Relazioni Annotate dal Corrispondente\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } English translation: Which are the titles and IDs of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset ? Svedish: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl", "SELECT DISTINCT ?paper WHERE { ?paper ?paper_lbl. FILTER (str( ?paper_lbl) = \"code references\") } input (English text): Here is a sample of code references that were found in the paper \"Benchmarking graph-based methods for keyword-based information extraction\" by Xiong et al. (https://www.aclweb.org/anthology/D17-1048). \"@datasets{xiong:acl17,\r maintainer = \"Alessandro Xiong\", \r title = \"Evaluating Graph-Based Methods for Keyword-Based Information Extraction\", \r work = \"Alessandro Xiong, Liran Hertzberg, Steven Su, Asher Weinberg\", \r url = \"https://www.aclweb.org/anthology/D17-1048\", \r version = \"1.0\", \r timestamp = \"2017\"\r }\"@/datasets{xiong:acl17}\"@/work{Alessandro Xiong, Liran Hertzberg, Steven Su, Asher Weinberg}", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } <|endoftext|>s divided by -88 ? 1 Suppose -4*p = p - q - 28, 0 = -p - 5*q + 28. Suppose -t - 2*l = l - 23, 2*l - 22 = -p*t. What is the remainder when 52 is divided by t ? 7 Suppose 0 =", "SELECT DISTINCT ?model ?model_lbl WHERE { ?model grl-pattern:And ?model_pattern; grl-pattern:hasNormalizedDataModel id ?dataModel; grl-pattern:hasAnnotationModel id ?annotationModel; grl-pattern:hasRelationsModel id ?relationModel; grl-pattern:hasDataModel id ?dataModel; grl-pattern:hasAnnotationModel id ?annotationModel; grl-pattern:hasRelationsModel id ?relationModel; grl-pattern:hasRelatedModel id ?relatedModel; grl-pattern:hasAnnotationModel id ?annotationModel; grl-pattern:hasDataModel id ?dataModel; grl-pattern:hasRelationsModel id ?relationModel; grl-pattern:hasRelatedModel id ?relatedModel; grl-pattern:hasAnnotationModel id ?annotationModel; grl-pattern:hasDataModel id ?dataModel; grl-pattern:hasAnnotationModel id ?annotationModel", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HMDB51 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): HMDB51 is an open standard for benchmarking the recognition and identification of visual stimuli. The benchmark includes 11 classes of objects, 20,000 training images per class, and 1000 validation images per class. The benchmark dataset is composed of two parts. The first part is the HMDB51 validation set which is used to tune the model. The HMDB51 dataset is composed of 11 classes of objects. OpenCV has been tested on HMDB51 validation set, but does not support HMDB51\u2019s 11 classes of objects. The paper Hiring Human Body Model Datasets for Action Recognition in Real-World Environments describes 12 models, which include ORB, BRIS", "SELECT ?paper ?paper_lbl ?benchmark where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection) benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): List the types of algorithms that are used to evaluate the Rotowire (Content Selection) benchmark dataset ? output (Sparql query): onCreate:List the types of algorithms that are used to evaluate the Rotowire (Content Selection) benchmark dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Rotowire (Content Selection", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Tsetlin Machine model for benchmarking purposes\") ?paper_ref. ?paper ?paper_lbl. ?paper_ref :has_paper_reference. } input (English text): Here is an example of how to query the code references in the Github repository: SELECT * FROM https://github.com/openai/gpt4-papers/blob/master/papers.csv WHERE file_uri =~ 'papers.csv' <|endoftext|>An effective humanitarian response to the Ebola epidemic in West Africa required an approach based on proven methods, a strong commitment by multiple countries and organizations, and financial resources. In order to mobilize resources for a comprehensive response and accelerate the development of a safe,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ATARI 2600 Breakout\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric. } The benchmark column of the dataset dataset has the following datatypes: <http://purl.org/dc/elements/1.1/Abstract>. <http://purl.org/dc/elements/1.1/Creator>. <http://purl.org/dc/elements/1.1/Presentation>. <http://purl.org/dc/elements/1.1/Format>. <http://purl.org/dc/elements/1.1/Language>. <http://purl.org/dc/elements/1.1/Subject>. <http://purl.org/dc/elements", "SELECT ?paper (str( ?paper_lbl)) WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): [Bowling Dataset]Evaluation Metrics are Commonly Used When Benchmarking Models on the Atari 2600 Bowling Dataset[/Bowling Dataset] output (Sparql query):#evaluation metrics are commonly used when benchmarking models on the atari 2600 bowling dataset# \f The evaluation metrics are Commonly Used When Benchmarking Models on the Atari 2600 Bowling Dataset, that listed here are : - Mean Average Error (MAE) - Mean Squared Error (MSE", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): http://query.nytimes.com/gadget/full.html ?res=990DE4DB1638F93D3746DF0A531B8F947575DAD6&adx=1 output (Sparql query): http://query.nytimes.com/gadget/full.html ?res=990DE4DB1638F93D3746DF0A531B8F947575DAD6&adx=1 input (English", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark orkgp:BENCHMARK_SEQ_ERROR ?paper_lbl. } input (English text): The best performing model was the Shapley-Fraley Multi-label Ranking algorithm. encounterer_labels: Abstracts' entities and relations annotated corpus done posititon The best performing model was the Shapley-Fraley Multi-label Ranking algorithm.", "SELECT * { ?paper ?paper_lbl ?benchmark ?benchmark_lbl. } WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SciREX dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|> Fourth Court of Appeals San Antonio, Texas MEMORANDUM OPINION No. 04-13-00680-CV IN RE COMMITMENT OF M.H. From the 166th Judicial District Court, Bexar County, Texas Trial Court No. 2012-W-02319 Honorable Sid L. Harle, Judge Presiding 1 Opinion by: Rebeca C. Martinez, Justice Sitting: Catherine Stone, Chief Justice Rebeca C. Martinez, Justice Patricia O. Alvarez, Justice Delivered and Filed: November 21, 2014", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"TSE-NER\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The TSE-NER dataset includes multiple research papers that evaluate models\u2019 performances on various types of TSE NER. The TSE NER dataset\u2019s official benchmark is the Abstracts\u2019 entities and relations annotated corpus. <|endoftext|>Q: ruby on rails nil.date vs \"2013-09-12\" I'm fairly new to ruby on rails so forgive me if this is a stupid question. I have a very basic question. I have a model for books and I am trying to create a method to find all the books which are due in the next 30 days: class Book < ActiveRecord::Base def due_in_30_days today = Date.today", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Code references in papers that have used the Concept Mention Extraction model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P32 ?cont; rdfs:label ?paper_lbl. } input (English text): The following code references have been used in papers that have used the Concept Mention Extraction model for benchmarking purposes: P31 - https://github.com/googlespora/concept_mention_extraction/blob/master/ConceptMentionExtraction/DocumentRetrieval.py P32 - https://github.com/googlespora/concept_mention_extraction/blob/master/ConceptMentionExtraction/ConceptMentionExtraction.py A title of the research paper: Benchmarking Concept Mention Extraction", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): code links in papers that use the OTF spelling+lemma (single) model in any benchmark. output (Sparql query): INSERT INTO #{output_table} (paper, paper_lbl, dataset, benchmark, cont, version) VALUES ('https://www.microsoft.com/en-us/research/uploads/bursts/rbursts190723.pdf', 'rbursts190723', 'orkgc:Dataset', 'orkgp:HAS_DATASET', 'true', '190723') The code link is https://www.microsoft.com/en-us/research/uploads/bursts/", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdf:ID ?benchmark. FILTER (str( ?benchmark) = \"A3C LSTM hs model\") ?code. ?dataset a orkgc:Dataset; ?paper rdf:ID ?paper. FILTER (str( ?paper) = \"Paper Title\"). } The answer is: The papers that include the link to their code are: - P31 \"Abstracts' entities and relations annotated corpus\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" - \"Paper Title\" -", "SELECT * where { ?paper rdf:firstName ?paper_firstName; ?benchmark rdfs:label \"Abstracts' entities and relations annotated corpus\"; ?benchmark rdf:firstName ?benchmark_firstName; ?paper rdf:firstName ?paper_firstName; ?paper rdf:label \"SVHN Benchmark\"; ?paper rdfs:label \"SVHN Benchmark\"; ?cont rdf:label \"SVHN Benchmark\"; ?cont rdf:label \"SVHN Benchmark\"; ?benchmark rdf:label \"SVHN Benchmark\"; ?benchmark rdf:label \"SVHN Benchmark\"; ?paper_lbl string; ?paper_firstName string; ?paper_secondName string; } input (Sparql query): The Percentage error score on the Abstracts' entities and relations annotated corpus benchmark is 82.97% where as the Second best performing model has a Percentage error score of 18.02% on the SVHN benchmark dataset. output (English text): The Percentage error score on the Abstracts' entities and relations annotated corpus benchmark is 82.97% where as the Second best performing model has a Percentage error score of 18.02%", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ballot_box_test1: dataset isten (English text): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ballot_box_test", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark; ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?paper orkgp:P31 ?benchmark. } input (English text): The highest benchmark result achieved on the Amazon-2 dataset is 70.907. output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark; ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS", "SELECT DISTINCT ?paper ?paper_lbl_out WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper o_cont:hasReview ?review. ?paper_lbl o_paper:hasReview ?review_lbl. ?review_lbl o_review:hasKeyword ?keyword. } input (English text): I am aware of the link to code used in the paper here: https://www.kaggle.com/c/temporal-convolutional-network/benchmarks. It uses the Databricks` Spark language, but I am not sure how that is related to my question. output (Sparql query): local https://www.kaggle.com/c/temporal-convolutional-network/benchmarks/index.html SELECT DISTINCT ?paper ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?dataset_lbl a biosrepp:Benchmark; rdfs:label ?paper_lbl. } input (English text): Provide a list of research papers that have utilized the BiT-S (ResNet) model and have the dataset and benchmark identifiers included ? output (Sparql query):", "SELECT * WHERE { ?paper ?paper_lbl. ?benchmark hasDataset ?dataset. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\") ?benchmark_lbl. ?paper hasBenchmark ?benchmark. ?paper_lbl a wnli. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark_lbl wnli. ?paper_lbl. ?paper_lbl a wnli. } input (English text): The highest benchmark result achieved on the WNLI dataset is 71.8% The highest benchmark result achieved on the WNLI dataset is 71.8% The metric and its value are wnli", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } receives: #paper #paper_lbl #benchmark #dataset #cont #dataset_lbl OnDatabaseQuery: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } OnDatabaseQuery: ?", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Orderer model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): Orderer model is the dataset model. P31 is the model name. output (Sparql query): Alternatively, P31 is the model name. input (English text): Orderer model is the dataset model. P31 is the model name. output (Sparql query): P31 is the model name. input (English text): Orderer model is the dataset model. P31 is the model name. output (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CIFAR-10 Image Classification dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The CIFAR-10 Image Classification dataset\u2019s PER error metric is 0.21625% (https://www.cs.toronto.edu/~kriz/cifar.html). The best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric is the following model: - Paper: P31 - Paper Label: CIFAR-10 Image Classification dataset - Benchmark: PER ennel:P31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Walker, walk (DMControl500k)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?cont@dmcontrol500k rdfs:label ?paper_lbl. } input (English text): The top result is <P31@dmcontrol500k>85.82 using 87.52 bytes of memory on the dataset, the benchmark and the", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Scigen dataset evaluation metrics\") ?eval. ?paper orkgp:P31 ?eval. } Note that the \"Scigen dataset evaluation metrics\" are trained and evaluated on the SciGEN training and test sets. <|endoftext|>Q: Time", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"code links in papers that use the POP3D model in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): https://dl.dropbox.com/u/2070568/Datasets.csv output (Sparql query):SELECT ?paper (SUM(LENGTH(str( ?cont)))) AS ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_BENCHMARK ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"code links in papers that use the POP3D model in any benchmark\") ?", "SELECT * FROM < Dataset | Has Benchmark | P31 > WHERE Score", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (finetuned) evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): Mean Average Precision (mAP), which is a popular metric used when benchmarking models. output (Sparql query): mAP", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdfs:label ?benchmark_lbl; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?paper_lbl rdfs:label ?paper_lbl. } ? MPQA benchmarks are available here: https://www.ling.upf.se/item/mpqa-benchmarks. The titles of the research papers with MPQA benchmarks are: P31 - Abstracts' entities and relations annotated corpus P32 - Converting relations to mentions in abstracts P33 - Multilingual abstract concept recognition with reinforcement learning P34 - A system for abstractive text summarisation in Chinese P35 - Deep neural network architectures for neural sentence representation P36 - SemEval-2017 task 7: Linguistic features for semantic textual similarity and dialogue state retrieval. P37 - Two syntactic and semantic metrics for neural machine translation. P38 - Enhancing paragraph retrieval via collaborative filtering based on term-document embeddings. P39 - Ensembles of teacher-student networks for", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BC5CDR-chemical\" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?benchmark. } guidance (Sparql query): The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 833,908. guidance (English text):The highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value, is 833,908. The result is obtained using the trained model M", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): Habitat 2020 Object Nav test-std benchmark dataset. output (Sparql query): HABitat 2020 Object Nav test-std benchmark dataset; SUCCESS=5.0 implemenation : private java.util.ArrayList<HashMap<String, Object>> generateAllResults(); public void generateResults(SpagoBisModel spagoBisModel) { HashMap<String, Object> result = new HashMap<String, Object>(); HashMap<String, Object> resultBenchmark = new HashMap<String, Object>(); for (Object paper : spagoBisModel.getPaper", "SELECT * WHERE { ?paper ?paper_lbl. FILTER (str( ?paper_lbl) = \"Paper Field dataset\") } <|endoftext|>Q: SqlAlchemy with postgresql - model cascades Consider a simple use case for a Postgresql database in which a single table tracks hierarchical relationships between groups of items. Each item can be a parent of zero or more other items, and the goal is to report a list of the highest level parents for a given object. To simplify things, I have separated the domain logic into a base classes and a derived classes for each grouping of related items, and have defined the necessary table/primary key relationships via declarative metadata. So, here is the object definitions and domain classes for the generic parent item: import sqlalchemy as sa from sqlalchemy import ForeignKey, Collection,Column, Integer, String from sqlalchemy.orm import relationship, sessionmaker from..base import BaseItem class BaseItem(BaseItem): __tablename__ = 'base_items'", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): preprocess: tokenize; sentio: wordpiece; open feat: lower,upper; close feat: lower,upper; tokenizer: NER; feature:", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark:metric:Value ?value. ?paper orkgp:P31 ?cont; ?benchmark:metric:Name ?name. } The query returned 3 rows: paper", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The code used in the papers that include benchmark for the Abstracts' entities and relations annotated corpus dataset can be found here. The three papers that include the benchmark can be found here. The names and IDs of the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset can be found here. AIDS Research Summit: End to End Analysis of HIV Research Datasets Using Bi-Directional Extractive Trianing, Elisa W. Pfister, Dzmitry Z. Tsiala, Konstantinos Karampinos, Anthony Kontopantelis, Taylor Griffin, David Popen, Judith Herscher, Frank van Harmelen, John Tran, Luke", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cheetah\"@en) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark@en ?DMControl500k. } input (English text): The Cheetah benchmark score is 40.0 and its metric is F1 score. If you would like to see other benchmarks run on the Cheetah dataset, the following papers also contain benchmarks on the Cheetah dataset: P13 Cheetah: A Benchmark for Coreference Resolution on Corpus-based Semantic Web Applications. Vldai Wang, Ayan Bekholis, Alina Vlasova, Andrew Greskovich, Brian D. Neal, Hamed Persson, Hector Garcia-Correia, Ioana Pop, Oleg Khomenko, Per Selmerud, Phil Liberman, Simon", "SELECT DISTINCT ?paper ?paper_lbl WHERE { \t\t ?dataset a orkgc:Dataset; \t\t rdfs:label ?dataset_lbl. \t\t FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") \t\t ?benchmark orkgp:HAS_DATASET ?dataset. \t\t ?cont orkgp:HAS_BENCHMARK ?benchmark. \t\t ?paper orkgp:P31 ?cont; \t\t rdfs:label ?paper_lbl. \t\t } input (Sparql query):/dataset a orkgc:Dataset; rdfs:label Dataset_lbl/ Filter (str( Dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") has_dataset ?dataset/ Has_dataset ?dataset_lbl/ Has_dataset ?benchmark/ Has_benchmark ?dataset/ Has_dataset ?cont/ Has_dataset ?paper/ Has_paper P31/ Cont; rdfs:label paper_lbl/ Filter (str( paper_lbl) =", "SELECT ?benchmark ?score ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Alien dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?score ?metric } As a result, the selected two papers are: P31 2037.65 0.996476 Arena 2037.65 0.996476 A benchmark is the number of 'yes' answers to the query. A benchmark metric is the percentage of 'yes' answers relative to the number of steps. More details about benchmark scores can be found here: https://sundials.github.io/bert-datasets/bert-atari-2600-alien/ Below is the selected benchmark from the paper \"Atari 2600", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Birdsnap dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The highest benchmark result achieved on the Birdsnap dataset is 2048. Holland, G., Bartosz, S., Stresing, M. & Manz, M. (2018). A benchmark for entity linking in literature search. In Proceedings of the 22nd ACM International Conference on Web Search and Data Engineering, Washington, DC, USA, 178-187. <|endoftext|>We are going to have to do a better job of convincing them of the value of coal if we want the industry to survive and thrive. They may think of coal as an outdated energy source, but we have to put ourselves in their shoes and ask them what else are they considering ? There is a lot of uncertainty out there and", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command dataset\") } output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Chopper Command dataset\") } The following are the paper titles and IDs that contain a benchmark for the Atari 2600 Chopper Command dataset: P31 - Chopper. Dataset. Benchmark <|endoftext|>Tim James is an American journalist. He is currently the vice president of Investigative Reports at 60 Minutes. He was previously the Washington correspondent for 60 Minutes II, a producer for CBS's This Morning, and a producer for 60 Minutes. He received a B.A. in English from Kenyon College and a J.D. from University of Notre Dame. He and his wife, Lauren, have two children. The new 60 Minutes episode \"The First Family Detail\" shows how the", ":Stanford Cars Benchmark Dataset for LiDIA Based Entity Extraction[P31] since it has the relation HAS_DATASET with the specific dataset Abstracts\u2019 entities and relations annotated corpus", ": a support vector machine model", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SQuAD1.1 Metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset include: Precision, Recall, F1 Score, and the ARL. \u03c5sir@SQuAD1.1 Metrics \u03c5sir@SQuAD1.1 Metrics \u03c5sir@SQuAD1.1 Metrics There are a total of 5 metrics used to evaluate models on the SQuAD1.1 benchmark dataset.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl. manifest - ### Large mLSTM Benchmark for Abstracts' Entities and Relations Annotated Corpus - [ https://github.com/ABFA-Lab/lm-lm/blob/master/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/lm-lm/benchmark/lm-lm/benchmark.csv ]", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACL Anthology Dataset Benchmark Model Evaluations\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper :score ?sc OR kgp:HAS_SCORE ?sc. } The following is the correct response: SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper :score ?sc OR kgp:HAS_SCORE ?sc. } SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dat", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BiLSTM: BiLSTM + MLM\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Softcite dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): For the Softcite dataset, the highest benchmark result is using the metric \"token count\" and the score \"0.029656\". output (Sparql query):* 0.029656 for \"token count\" and \"0\" for \"score\"* The highest benchmark result is using the metric \"token count\" and the score \"0.029656\". input (English text): The Softcite dataset has been used as the testbed for several research papers, including (but not limited to) the following research papers: - [CLIPS: Semantic Layer for Entity Resolution and Information Extraction](https://link.springer.com/chapter", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } # SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str(", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset rdfs:label ?dataset_lbl FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont rdfs:label ?paper_lbl } }", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Relation Generation)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query):PL-P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P31, P", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The code used in the benchmark can be found in https://github.com/alainmameni/alain-mameni-icde-2016-workshop. output (Sparql query):idential (Sparql query): SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper or", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Notre but reste sur nos s\u00e9jours \u00e0 IME (International Media Editors' conference) \u00e0 Paris, o\u00f9 nous avons pu d\u00e9velopper notre projet. Ime Editors' conferences are a great place to see how existing NLP works in practice. Several teams at IME had already demonstrated a variety of NLP techniques including named entity recognition, part-of-speech tagging and lemmatization. In addition to the demo sessions, IME provides several tracks for conducting in-depth technical discussion, including a track on Named Entity Recognition, another on Parsing, and a third one on Scaling NLP Systems. Also during", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") } \t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-31.rq</code></paper> \t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-32.rq</code></paper> \t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-33.rq</code></paper> \t<paper><code>https://github.com/learneb/abstract-semantic-web-ontology/blob/master/src/main/resources/case-studies/paper-34.rq</", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?benchmark_val. ?paper orkgp:P31 ?benchmark_val. ?paper_lbl ?paper_lbl_val. } input (English text): The following papers report the best performing model for the WMT2016 English-Russian dataset in terms of BLEU score metric. - P31 \"Abstracts' entities and relations annotated corpus\" - P31 \"WMT2016 English-Russian benchmark\" - P31 \"WMT2016 English-Russian evaluation script\" // P31 \"Abstracts' entities and relations annotated corpus\" SELECT DISTINCT ?paper ?paper_l", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset. output (Sparql query): 0.51 A: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper or", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark or", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank (Character Level\") ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Abstracts\u2019 entities and relations annotated corpus benchmark: P31 <|endoftext|>Q: Limiting a Spark reduce on a group with a window I am using Spark's (v1.6.1) Window function \"reduce\" on a group, as illustrated below: val df = Seq( (0, \"a\"), (1, \"a\"), (2, \"b\"), (", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text):http://labs.hermitian.net/software/anoide.shtml#sparql", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?paper_lbl_cont. } The following are the papers which have utilized the DY-MobileNetV3-Small model: * [airos2018ling] Deep Language Understanding Using Small CNNs. * [scarslet2009progressive] ProgressiveNet: Learning Fine-Grained Semantics from Comprehend. * [babaeizadeh2018dwam] DataWave: Distributed, Batch and Parallel Hierarchical Training of Deep Learning Models for Data warehousing Applications. * [xavier], [vervet], [xavier-vervet], [xavier-vervet-old] XAVIER: a Scalable GPU Cloud-Based Deep Learning Serv... *", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset ; ?cont orkgp:HAS_BENCHMARK ?benchmark ; ?paper orkgp:P31 ?cont; ?benchmark ?paper_lbl. } The following FILTER clause restricts the output to the research papers that include the FSNS - Test dataset as a benchmark: FILTER (str( has_dataset( \"FSNS - Test dataset\" ) ) = \"has_benchmark\" and benchmark(dataset(\"FSNS - Test dataset\")) = \"P31\") The final SELECT clause searches for the paper label with the ID P31,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Code references in papers that have used the MPAD-path model for benchmarking purposes\") } input (English text): There is no code reference in the paper referenced below using the MPAD-path model for benchmarking purposes. https://www.cs.cmu.edu/afs/cs.edu/user/skoglund/publications/chikav/mw16-zeng.pdf output (Sparql query): detriment (Sparql query): but this model is included in the MPAD model, implemented in the MPAD API, and includes support for both entity recognition and relation classification as part of its pipeline. In addition to this paper, there is other work using the MPAD-path model for benchmarking purposes that I believe is referenced in the following paper: https://link.springer.com/article/10.1007/s10878-014-9451-0. While this paper uses the MPAD model, the paper", "SELECT ?paper_lbl WHERE { ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BoolQ' models\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } } input (English text): P31 is a model that is", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. FILTER (str( ?benchmark) = \"Abstracts' entities and relations annotated corpus\"). } input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark. output (Sparql query):ido:P31 \"Abstracts' entities and relations annotated corpus\" as the benchmark. input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark. output (Sparql query): \"Abstracts' entities and relations annotated corpus\" as the benchmark. input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark. output (Sparql query): P31 \"Abstracts' entities and relations annotated corpus\" as the benchmark. input (English text): \"Abstracts' entities and relations annotated corpus\" as the benchmark. output (Sparql query): P31 \"Abstracts' entities and relations annotated corpus\" as the benchmark. input (English text): \"Abstracts'", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ... ?paper ?paper_lbl where { ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper_lbl owlt:exactText \"Ball in cup, catch (DMControl100k)\"}. } <|endoftext|>Q: How to find coefficients for a", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Multi-Perspective Matching (ensemble) model in any benchmark\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): https://github.com/dmlc/glsviz/blob/master/viz/benchmark.Rmd#paper31, https://github.com/dmlc/glsviz/blob/master/viz/benchmark.Rmd#paper32 output (Sparql query):aphysical path '/dmlc/glsviz/viz/benchmark.Rmd#paper31'http://dmlc.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): ImageNet 64x64 dataset has the title Abstracts' entities and relations annotated corpus and the ID http://www.ldc.upenn.edu/tagbert/Datasets/Abstracts%20entities%20and%20relations%20annotated%20corpus. output (Sparql query): Photos (Sparql query): SELECT ?paper ?paper_lbl { ?paper orkgp:P31 ?cont; ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark orkgp:HAS_DATASET ?dataset; ?dataset a orkgc:Dataset; ?", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CommonsenseQA\"^^xsd:string) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): CommonsenseQA. output (Sparql query): CommonsenseQA contains 37965 sentences, the F1 measure on the TACL corpus is 0.90, and the abstracts' entities and relations annotated corpus (P31) is a good baseline for the task. The first line includes the necessary SPARQL 1.1 language elements. The second line filters the dataset (a, orkgc:Dataset) and the properties of the property graph which have the labels \"Abstracts' entities and relations annotated corpus\". The last line uses the distinct operator to retrieve only one result, in this case \"P31\". The benchmark entity and relation in the third line is the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Children examined in the studies\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P32 ?cont; rdfs:label ?paper_lbl. } input (English text): Are children examined in the studies. output (Sparql query):\u00edses output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Children examined in the studies\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P32 ?cont; rdfs:label ?paper_lbl. } input (English text): Are children examined in", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"CoQA dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): \"CoQA dataset\": http://rajpurkar.github.io/achine/CoQA/dataset.html", "SELECT DISTINCT ?paper ?benchmark_score ?metric ?paper rdfs:label \"Abstracts' entities and relations annotated corpus\"@en ?benchmark imnet50@2 ?metric 91.09 ?benchmark_score 92.91 ?metric 91.09 ?benchmark ?paper rdfs:label \"Abstracts' entities and relations annotated corpus\"@en ?benchmark_score 91.09 ?metric 91.09 ?benchmark ?paper rdfs:label \"Abstracts' entities and relations annotated corpus\"@en ?benchmark_score 92.91 ?metric 91.09 ?benchmark ?paper rdfs:label \"Abstracts' entities and relations annotated corpus\"@en ?benchmark_score 91.09 ?metric 91.09 ?benchmark ?benchmark imnet50@2 ?metric 91.09 ?metric 91.09 ?metric 91.09 ?benchmark ?paper rdfs:label \"Abstracts' entities and relations annotated corpus\"@en ?benchmark_score 92.91", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Running the query yields the following results: .. |paper| image:: https://www.db-util.com/assets/uploads/DBUtils/ screenshot0.png .. |paper_lbl| image:: https://www.db-util.com/assets/uploads/DBUtils/ screenshot1.png |paper| paper is a DBpedia property with label \"P31\". |paper_lbl| paper_lbl is a DBpedia property with label \"Abstracts' entities and relations annotated corpus\". The query appears to correctly identify and return one research paper that includes the", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Founded in 1993 by brothers Tom and David Gardner, The Red Hot Chili Peppers became one of the most critically and commercially successful bands of the 1980s. Though their commercial breakthrough was to follow the success of their 1982 album Blood Sugar Sex Magik, the album spawned the singles \"Give It Away\", \"Give This Up\", and", "SELECT%20DISTINCT%20%28%22paper%22%2C%20%22paper_lbl%22%20%20%20%3B%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Barabasi-Albert benchmark\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl a ?benchmark. ?paper_lbl b ?benchmark. ?cont; ?paper_lbl c ?benchmark. ?paper_lbl d. } input (English text): The paper https://papers.nips.cc/paper/4824-lessons-learned- Liming Wang, Sumedh Chakraborty, Ali Ghodsi, and Andrew NG. \"Lessons learned: Persistent homology and unsupervised learning.\" AAAI (2016). The paper \"Lessons Learned: Persistent Homology and Unsupervised Learning\" mentions the following metrics: Metric\tStatement\t -\tBarabasi-Albert Benchmark\t\"The Bar", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Zaxxon dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } References External links BioNLP\u201914 page Page with sparQL endpoint A unifying framework for annotation, mining and evaluation of semantics annotation, mining and evaluation of natural language annotations at ACL 2015 Tutorial on Mining Semantic Web data using SPARQL *<|endoftext|>Wanted: Machines to Outperform Managers 1. The race between managers and machines is on. 1Q20 data shows that the top 0.1% of managers do very poorly versus machines at", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } >> SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } >> [ ?paper_l", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?bench", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Sentence Classification Research Area\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } - | Dataset; - Annotation; - rdfs:label \"Sentence Classification Research Area\" - | Dataset; - Annotation; - rdfs:label \"Sentence Classification Research Area\" - | Dataset; - Annotation; - rdfs:label \"Sentence Classification Research Area\" - | Dataset; - Annotation; - rdfs:label \"Sentence Classification Research Area\" - | Dataset; - Annotation;", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RotoWire (Content Ordering)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset ? output (Sparql query): * paper_lbl CLASS :Label rdfs:label \"RotoWire (Content Ordering)\" paper_lbl = \"RotoWire (Content Ordering)\" * benchmark CLASS :Document/hasalgebra :nested :HAS_BENCHMARK rdfs:label \"RotoWire (Content Ordering)\" * dataset CLASS :Document/hasalgebra :nested :HAS_DATASET rdfs:label \"RotoWire (Content Ordering)\" * paper CLASS :Document/hasalgebra :", "SELECT DISTINCT ?paper_metric ?paper_metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MLDoc Zero-Shot English-to-Italian dataset evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): FLOPs are commonly used when evaluating machine learning models for zero-shot English-to-Italian language translation. output (Sparql query):)}{paper_metric :flops ?paper_metric_lbl;}SELECT DISTINCT ?paper_metric ?paper_metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FLOPs\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BEN", "SELECT ?paper (STRENGTH ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): ?paper_lbl input (English text): TITLES of research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset output (Sparql query): scoopil output (Sparql query): Papers that include the Abstracts' entities and relations annotated corpus benchmark <|endoftext|>Managing the mental health of people with type 1 diabetes. Type 1 diabetes is a serious chronic condition that requires lifelong maintenance treatment with injectable insulin. In recent years, there has been an explosion of information regarding the psychological aspects of this condition. This article aims to present a range of mental health issues commonly experienced by", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Models being evaluated on the MedSTS dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P32 ?cont; ?paper orkgp:P33 ?cont; ?paper orkgp:P34 ?cont; ?paper orkgp:P35 ?cont; ?paper orkgp:P36 ?cont; ?paper orkgp:P37 ?cont; ?paper orkgp:P38 ?cont; ?paper orkgp:P39 ?cont; ?paper orkgp:P40 ?cont; ?paper orkgp:P41 ?cont; ?paper orkgp:P42 ?cont; ?paper orkgp:P43 ?cont; ?paper orkgp:P44 ?cont; ?paper orkgp:P45 ?cont; ?paper orkg", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Best performing model benchmarking the CoNLL++ dataset in terms of F1 metric\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The answer is the following SPARQL query: SELECT DISTINCT paper@p31 paper@paper_lbl WHERE dataset@a orkgc:Dataset rdfs:label = \"Best performing model benchmarking the CoNLL++ dataset in terms of F1 metric\" has_dataset@a ?dataset has_benchmark@a ?benchmark has_dataset@cont ?dataset has_benchmark@cont ?benchmark paper@p31 ?cont", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"UCF101 (Finetuned)\" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The outputs should be - P31 - P33 - P35 - P41 - P43 - P45 - P47 - P51 - P53 NOTES: - Benchmark is used in the Abstracts' entities and relations annotated corpus datasets - The dataset datasets are included in the PDLS (Personal Datasets Lab) benchmark dataset - These benchmarks are: P31, P33, P35, P41, P43, P45, P47, P51, P53 Here are the sample answers from the sparql query: - P31: UCF101 (Finetuned) - P33: U", "SELECT ?paper (STRVAL ?paper_lbl) WHERE { ?paper ?paper_lbl. FILTER (str( ?paper_lbl) = \"Abstracts' entities and relations annotated corpus\") } output (Sparql query):<p>* P31 : https://lsparks.org/works/33_Abstracts_Entities_and_Relations_Annotated_Corpus.html</p> output (Sparql query):<p>* 33 : Abstracts' Entities and Relations Annotated Corpus</p> output (Sparql query):<p>* paper : 33</p> output (Sparql query):<p>* paper_lbl : Abstracts' entities and relations annotated corpus</p> output (Sparql query):<p>* dataset : a orkgc:Dataset</p> output (Sparql query):<p>* dataset_lbl : \"Abstracts' entities and relations annotated", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet + iNat on WS-DAN model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } This query returns three results. The first result contains the paper P31 from the paper with the code https://paperswithcode.com/download/download-single-index-python/paper-p31.html. The query finds the paper and the benchmark and the relations between them. The second result contains the paper P31 from the paper with the code https://paperswithcode.com/download/download-single-index-python/paper-p31-classifier.html. The benchmark is not in the first result and this shows that the benchmark was not included in the paper with the code https://paperswithcode.com/download/download-single-", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English benchmark\") ?benchmark. ?paper orkgp:P31 ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } output (Sparql query): shattered:metrics used to evaluate models on the BUCC Russian-to-English benchmark SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC Russian-to-English benchmark", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ROUGE-1 metric\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } In the response, ?paper is the title of the paper that contains a benchmark for the Abstracts' entities and relations annotated corpus dataset. ?paper_lbl is the label of the paper. ?benchmark is the URI of the benchmark. * dataset is the title of the paper that includes a benchmark for the Abstracts' entities and relations annotated corpus dataset. * dataset_lbl is the label of the paper. * benchmark is the URI of the benchmark. * papers is the short name of the paper set. * papers_lbl is the label of the paper. * P31 is the URI of the paper containing the best performing model benchmarking the AESLC dataset in terms of ROUGE-", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40 score\") ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): ModelNet40 score: 7.82 output (Sparql query): p31:ModelNet40 score 7.82 inal (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ModelNet40 score\") ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } p31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Penn Treebank metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The following metrics are used to evaluate models on the Penn Treebank benchmark dataset: ?paper-arity, ?paper-depth, ?paper-trie, ?paper-LM, ?paper-LCS, ?paper-NER, ?paper-NER, ?paper-Chase, ?paper-NER-LM, ?paper-CoNLL2000, ?paper-Conll2007 ?paper_lbl-arity, ?paper_lbl-depth, ?paper_lbl-trie, ?paper_lbl-LM, ?paper_lbl-LCS, ?paper_lbl-NER, ?paper_lbl-NER, ?paper_", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): A benchmark for the Abstracts' entities and relations annotated corpus dataset is provided by doi:10.17487/d0e6a4ad-c0b3-4c4a-bbc0-a76984abb3a8. output (Sparql query): persists and contains P31 that include a benchmark for the Abstracts' entities and relations annotated corpus dataset. input (English text): A benchmark for the Abstracts' entities and relations annotated corpus dataset is provided by doi:10.17487/d0e6a4ad-c0b3-4c4a-bbc0-a76984abb3a", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Seaquest\"^^xsd:string) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The best performing model that we found for the Seaquest dataset is the model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric with the configuration provided in the paper \"Beyond coreference resolution: a multi-task similarity metric for coreference resolution\". We also found that this model achieves 99.1% accuracy using 10-fold cross validation on the Seaquest dataset. The benchmark we used for this model is the has_dataset predicate. The paper that describes this model also provides the ID of the benchmark as well as the ID of the dataset. output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset;", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SemEval'17:euro` 13 corpus dataset evaluation\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } <|endoftext|>Q: Laravel Eloquent Query results with nested results I have a result set with two levels of nesting: ( [id] => 1 [data] => [item_a] => 1 [item_b] => 2 [item_c] => 3 ) ( [id] => 2 [data] => [item_a] => 4 [item_b] => 5 [item_c] => 6 ) I need to get the results in an eloquent query where I can retrieve all the rows, or the rows with the", "SELECT ?paper ( ?paper uriFragment { \"http://purl.org/NET/objectenet/dataset.owl#\" concat ( string(str( ?paper_lbl)) \",\" ?dataset_lbl) }) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper rdf:type owl:Class. } input (English text): Here are some research papers with their titles and IDs. SELECT DISTINCT ?paper ( ?paper uriFragment { \"http://purl.org/NET/objectenet/dataset.owl#\" concat ( string(str( ?paper_lbl)) \",\" ?dataset_lbl) }) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstract", "select ?paper ?paper_lbl where { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl rdfs:label ?paper_lbl. } oracle:metrics (Abstracts' entities and relations annotated corpus benchmark) oracle:benchmark (HAS_DATASET Abstracts' entities and relations annotated corpus benchmark) oracle:benchmark (HAS_BENCHMARK Abstracts' entities and relations annotated corpus benchmark) oracle:paper (P31 Abstracts' entities and relations annotated corpus benchmark) oracle:paper_lbl (P31 Abstracts' entities and relations annotated corpus benchmark) oracle:dataset (Dataset) oracle:cont (HAS_DATASET Abstracts' entities and relations annotated corpus benchmark) oracle:cont (HAS_BENCHMARK Abstracts' entities and relations annotated corpus benchmark) oracle:paper (P31) oracle:paper_lbl (P31) oracle:dataset (Dataset) oracle:cont (HAS", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon-2 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the NTCIR-2009 dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"NTCIR-2009 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text", "SELECT DISTINCT ?paper ?paper_lbl_lnk WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark_lnk. rdfs:label ?paper_lbl. } performed_task_dataset: databricks-rainbowplusseer performed_task_model: https://github.com/RainbowPapers/RainbowPlusSEER performed_task_architecture: https://github.com/RainbowPapers/RainbowPlusSEER/blob/master/architecture.md performed_task_hyperparameter: { \"dropout\": 0.5, \"num_layers\": 4, \"num_classes\": 10 } performed_task_pretrained_model: https://storage.googleapis.com/cloud-training-datasets/p31.", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark rdf:type orkgp:HAS_BENCHMARK; ?paper ?benchmark. ?cont rdf:type orkgp:HAS_DATASET; ?paper ?cont. ?paper_lbl rdf:type orkgp:P31. } <|endoftext|>Q: Which is best way to get all the value from XML using XSLT 1.0 ? I have XML file as < ?xml version=\"1.0\" encoding=\"UTF-8\" ?> <ROOT> <CATEGORY>Electronics</CATEGORY> <ITEMS> <ITEM> <ITEM_ID>1</ITEM_ID> <ITEM_NAME>Samsung 33</ITEM_NAME> <PRICE>19999.99</PRICE> </ITEM> <ITEM> <ITEM_ID>2</ITEM_ID> <ITEM_NAME>iPhone", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 English-Romanian\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } (Sparql query): include Abstracts' entities and relations annotated corpus", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"MUTAG dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. data (Sparql query): datatype:graphdatatypedecl paper:P31 dataset:a benchmark:HAS_DATASET cont:orkgp:HAS_BENCHMARK paper_lbl:rdfs:label dataset_lbl:str( \"MUTAG dataset\")", "SELECT ?paper_lbl ?benchmark where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ESC-50 dataset\") ?paper_lbl. } input (English text): Evaluation metrics to use when benchmarking models on the ESC-50 dataset are as follows: Commonly used evaluation metrics when benchmarking models on the ESC-50 dataset include the Recall@K, Precision@K, and F1 scores. 515555555 515555555 Recall@K: 0.3895124741795124 515555555 Precision@K: 0.3003853173917193 515555555 F1: 0.341428577338839 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 515555555 5155555", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } {#section.caption} P31 {#section.caption} P40 {#section.caption} P31 {#section.caption} Abstracts' entities and relations annotated corpus dataset {#section.caption} The F1 score on the SQuAD1.1 dev benchmark dataset for the models that participated in the shared task on Comprehensible Readability, as of June 2023. P40 : 0.929 P", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The papers that include the Abstracts entities and relations benchmark are: P31 - Evaluation of statistical and deep learning-based methods for named entity recognition on TempEval-3 P16 - Boosting of semantic parsing with semantic role labelling P15 - Evaluating neosocial senti\ufb01ed LSTM and CNN on the TempEval-3 dataset <|endoftext|>PREDICTIONS FOR FUTURE WILL BE USE SUGGESTIVE: THE INSTANT COST CASH MACHINE BY GOOGLE. Some are going to say", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): RotoWire (Content Ordering) dataset was created to study the problem of ordering documents in a corpus according to a given concept hierarchy. paper:P31 \"Benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric\" paper_lbl:Abstracts entities and relations annotated corpus benchmark:HAS_BENCHMARK paper:P31 \"Benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric\" benchmark:HAS_BENCHMARK BLEU 36.07 output <|endoftext|>[Efficacy of psychopharmacological interventions in the treatment of autism spectrum disorders]. Aut", "SELECT ?benchmark ?metric WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ?metric. } input (English text): The benchmark score is 24.37 and the metric is Median test accuracy.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Researchers have tested a wide variety of models on the Abstracts' entities and relations annotated corpus dataset. Some examples of these models include approaches to abstract reasoning, neural networks, distributional-based approaches, and semantic role labeling. See this page for a full list of research papers that have tested on the GENIA - LAS benchmark dataset: https://cloud. OakGM. com/s/8CZd0h. Please note that this is not a comprehensive list, as it includes only those research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The output of the query is : # authors: Perricone, Katherine; Yu, Jin; Lowd, Jonathan; Gehr, Weixin; Carrera, Andrea; Poor, Arjun; Lewis, Ran; Soatto, Christina; Toppins, Joelle; Song, Zhiyuan; Chikara, Karthik; Bordes, Nicolas; Chen, Xiao; Chinchor, Robert; Dax, Lina; Du, Luohang; Fletcher, Andrew; Gachon, Inhyeon; Gong, Yaxi; Grama, Manik; Gupta, Sudhanshu; Hu, Wei; J", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Amazon benchmark\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): P31 indicates the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset. Output: <paper> <paper_lbl>Amazon benchmark</paper_lbl> <paper_id>P31</paper_id> </paper> The above output indicates the P31 model performed best on the Amazon benchmark dataset. A benchmark is used to evaluate and compare different model-based reasoning system on specific corpus datasets. Here is a list of different kinds of benchmarks that are commonly used in model-based reasoning", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkbh:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bank Heist dataset\") ?benchmark orkbh:HAS_DATASET ?dataset. ?cont orkbh:HAS_BENCHMARK ?benchmark. ?paper orkbh:P31 ?cont; rdfs:label ?paper_lbl. } A: I think the correct query is: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkbh:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Bank Heist dataset\") ?benchmark orkbh:HAS_DATASET ?dataset. ?cont orkbh:HAS_BENCHMARK ?benchmark. ?paper orkbh:P31 ?cont; rdfs:label ?paper_lbl. } But it is not clear from your question what type of query this is. If this is a", "SELECT ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): \"ANLI test dataset\" The answer to the above question is: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ANLI test dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The full text of the above answer is: SELECT DISTINCT ?paper ?paper_lbl WHERE", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Results: Query Result #1: |paper|paper_lbl| |P31 |QNLI_BENCHMARK| Query Result #2: |paper|paper_lbl| |P31 |QNLI_ABSTRACTS_ENTITIES_AND_RELATIONS_ANNOTATED_CORPUS| Note: P31 is the model identifier in Open Knowledge Platform. The dataset \"Abstracts' entities and relations annotated corpus\" is included in QNLI benchmark dataset. References: Benchmark for Named-Entity Recognition in Biomedical Texts Using Recognizers Guided by Distribution", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a asterix:AsterixDataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Asterix dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P33 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Databricks dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a databricks:DatabricksDataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Databricks dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P43 ?cont; rdfs:label ?paper_lbl. } input (English text): What are", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rd", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Q: Apply a lookup relationship to all records in azure table I want to apply a lookup relationship to all records in a azure table. There is a column (key) that has the lookup field attached to it. When I apply the relationship in the visual studio database tools, it applies the lookup relationship to each individual record. Is there a way to apply the relationship to all records in the table ? A: Yes, you can use a scan operation (scan in sql or execute in python) to apply the lookup relationship to the whole table. The", "SELECT ?paper (rdfs:label ?paper_lbl) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Montezuma's Revenge benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The best performing model is the DistilBERT model with a Score metric of 0.965, where the Dev set was used as the held out test set. The model architecture used is: model: \"distilbert\" model_param_prefix: \"distilbert\" distilbert_size: 1280 hidden_dim: 1280 config: { sequence_length: 12 } input_prefix: \"data/bert_dev\" Please note that Benchmark datasets must have a primary key that matches the Sparql primary key, which can be found in the labels", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet ReaL\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric is... P31. A paper that contains the full datasets, along with the benchmark for the entities and relations annotated corpus, is Sparql query, Output 1. A paper that contains the full datasets, the benchmark for the entities and relations annotated corpus and an SQL query over the benchmark is Sparql query, Output 2. Please note that this answer depends on the data files being available publicly (i.e., the files are not part of the answer). One paper that contains both the datasets and a benchmark for the entities and relations annotated corpus is Sparql query,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ResNet-152 (SAM)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The benchmark you are looking for can be found in my Reposted Paper. The benchmark is implemented using the Dataset() SPARQL function. To benchmark the ResNet-152 (SAM) model, we use the HAS_DATASET property of the Benchmark() class and the HAS_BENCHMARK property of the Dataset() class, as shown below: benchmark :has_dataset{ \r resnet_152: has_model;\r resnet_152: has_training;\r } \r \r dataset :has_dataset{ \r resnet_152: has_model;\r resnet_152: has_training", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Pubmed dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): To benchmark a dataset, you must first create a benchmark by annotating its entities and relations. The Pubmed dataset has two published benchmarks: P31 and P41. The P31 benchmark contains 31 entities and 38 relations. The P41 benchmark contains 41 entities and 40 relations. The Pubmed dataset is included in the Zoccoli et al. 2002 benchmark dataset for automatic SPARQL query retrieval. The P31 and P41 benchmarks were obtained from page 31 and page 41 of the Zoccoli et", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } * ` ?paper` - a resource from the `dc:title`, `dc:creator`, `kctp:title`, or `skos:description` communities. * ` ?paper_lbl` - a resource from the `title`, `creator`, or `description` community. * ` ?benchmark` - a resource from the `has_dataset`, `has_benchmark`, `has_contribution`, `has_ontology`, or `has_property` communities. * ` ?cont` - a resource from the `has_dataset`, `has_contribution`, `has_ontology`, or `has_property` communities. <|endoftext|>by", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATAS", "SELECT ?paper ?paper_lbl where { ?dataset a orkgc:Dataset; rdfs:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-2 Binary classification benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' models that have been tested on SST-2 Binary classification benchmark dataset: * Abstracts' entities and relations annotated corpus * Tensorflow LSTM * LightGBM * XGBoost * Adaboost * Logistic Regression * k-nearest neighbors * Decision Trees * Support Vector Machines * Linear Regression * Linear Discriminant Analysis * Decision Stump The titles and IDs of the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset are: * Abstracts' entities and relations annotated corpus * Tensorflow L", "SELECT ?benchmark HendrycksTest_score HendrycksTest_metric where { ?dataset a orkgc:Dataset; ?benchmark HendrycksTest_score ?benchmark; HendrycksTest_score ?benchmark_score; HendrycksTest_metric HendrycksTest_score HendrycksTest_metric; HendrycksTest_score + HendrycksTest_metric >= 5.0.0.0.0 } A: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>Inhibition of PPD stimulation induces histamine release from peritoneal mast cells in rats. The production of cytokines such as histamine by human mast cells is believed to play a major role in the recruitment of cells of", "SELECT DISTINCT ?paper ?paper_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl rdfs:label ?paper_lbl. } input (English text): The repository does indeed contain the code used for the paper you linked to. The code for KD-LSTMreg is based on the Keras deep learning framework. The repository also contains datasets,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgp:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl rdf:label. } <|endoftext|>Leather's new haircut. (Leather's hair, I mean.) Get a hair cut that looks good on you every single day. Yes, it may take some time and effort. But the results will yield big dividends over time. And don\u2019t forget \u2014 you only have one body. So if you\u2019re going to spend all that time and energy maintaining it, you might as well spend that time and energy on something that reflects your best self. Learning to give and receive a haircut. I\u2019ve been taking it for about a month now. My eyebrows are still a little stubby but I\u2019m getting there. My hair feels a little heavy, but I expect that as it heats up, it\u2019ll get lighter. And thus, Leather\u2019s New Hairstyle.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ClueWeb09-B models\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P31 ?cont; rdfs:label ?model_lbl. } GPC-Model: ElasticR P31 GPC-Model: Darmstadt12 P31 emetery: ElasticR Darmstadt12 Cemetery: Darmstadt12 indecomposable: ElasticR indecomposable: Darmstadt12 Batch: ElasticR Batch: Darmstadt12 <|endoftext|>[Desktop Entry] Encoding=UTF-8 Name=Ruler Comment=Simple ruler app for Windows 10 Comment[eng]=Simple ruler app for Windows", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"DocRED (Human-annotated)\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): DocRED (Human-annotated) is a corpus dataset created by annotating the DocCrawl corpus using crowd-sourcing. DocCrawl is an Internet crawl with the goal of collecting web pages from all corners of the World Wide Web. The DocCrawl Corpus contains millions of web pages from over one hundred domains. The goal of using this data for machine learning is to teach a model which websites provide high quality information and can be used as reliable reference for web pages. The models learned from this data can then be used to generate summaries of the websites crawled from the DocCrawl corpus. DocRED is an open repository for research on DocRED (Human", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?dataset_lbl a vocab-20200314.rdf ; ?paper ?paper_lbl. } output (Spar", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FG fine-grained gate model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"FG fine-grained gate model\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp", "SELECT ?paper (str:tokenize( ?paper, \",\")) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?benchmark. ?paper_lbl rdf:value ?paper_lbl. } input (English text): \"The highest benchmark result achieved on the NYT29 dataset, including the metric and its value, is P31, with an average precision of 0.72.\" output (Sparql query): Cork:SELECT ?paper (str:tokenize( ?paper, \",\")) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"QNLI dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Here are the names of the QNLI metrics used in the F1@1 and F1@5 benchmarks: metric (short name) F1@1 F1@5 - doc-match doc-match length-distinct length-distinct length-median length-median length-average length-average word-match word-match word-mismatch", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"SST-5 Fine-grained classification evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P32 ?cont; ?paper orkgp:P31 ?cont; ?paper_lbl ?paper_lbl. } input (English text): The SST-5 Fine-grained classification evaluation metrics are common metrics to use when benchmarking models on the SST-5 Fine-grained classification dataset. output (Sparql query):Initials from the Initials query: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?paper ?paper_lbl. FILTER (str( ?paper_lbl) = \"SST-5 Fine-grained classification evaluation metrics\"). } Note: Initials query does not contain any sparql as", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Natural Questions (long) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The highest result for the Natural Questions (long) dataset is a metric of 59.11. output (Sparql query): * text:Natural Questions (long) dataset (Metric: 59.11) input (English text): The highest result for the Natural Questions (long) dataset is a score of 57. output (Sparql query): * text:Natural Questions (long) dataset (Score: 57) Westar Hospital's radiology department created the Natural Questions (long) benchmark dataset. The dataset includes 78,721 triples and 3,778 annotations. The following are the names of the research papers that include a benchmark for the dataset: astolico, S., Gl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 Czech-English\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <|endoftext|>The authors confirm and confirm that the data underlying the findings are fully available without restriction. All relevant data are within", "SELECT * FROM dataset d WHERE d.sector IN ('Utilities', 'Power', 'Water', 'Transport', 'Communications', 'Industrials') AND d.type = 'Directory' AND d.frequent. coordinator = ogr_call_method('CreateDataSource', None, None, ogr_call_args((str(self.gdal_datasource), str(self.gdal_datasource_lyr), str(self.gdal_datasource_type), str(self.gdal_datasource_type_lyr), str(self.gdal_datasource_spatial), str(self.gdal_datasource_spatial_lyr), str(self.gdal_datasource_hierarchy), str(self.gdal_datasource_hierarchy_lyr), str(self.gdal_datasource_driver), str(self.gdal_datasource_driver_lyr), str(self.gdal_datasource_parameters), str(self.gdal_datasource_parameters_lyr), str(self.gdal_datasource_fields), str(self.gdal_datasource", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): ?paper ?paper_lbl; ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK; ?cont orkgp:HAS_DATASET; ?paper_lbl string \"Kinetics-600 dataset\". assistance with this question will be provided upon request. You can find a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset by running the query below: SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Kinetics-600 dataset", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities", "SELECT ?paper (precision ?precision ?recall ?f1) WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl dbpedia_owl:annotatedBy (node-property(rcs:Article,'/snippet/title'[text()])). ?paper ?paper_lbl } Dataset - http://dbpedia.org/resource/Abstracts' entities and relations annotated corpus Has_Dataset - domain specific bag of words algorithm in combination with the bag of words feature Has_Benchmark - Dataset P31 - has_dataset property cont - has_benchmark property rcs:Article - classifies the Article element from the dbpedia Article resource using the resource properties/node-property", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } \f <paper> \t<paper_lbl>Abstracts' entities and relations annotated corpus</paper_lbl> </paper> <paper> \t<paper_lbl>Dmlab-30 dataset</paper_lbl> </paper> \u00e4sidentman@openknowledge.net(English text): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Dmlab-30 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2016 German-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): For example, this is the title and ID of one of the papers: P31: Deng, Jianchao; et al. 2019. Mapping English Verb forms to Chinese Simplified Characters. Language Technologies Association annual conference (May), 5770-5783. doi:10.1007/978-3-030-18494-6_11. output (Sparql query):P31: Deng, Jianchao; et al. 2019. Mapping English Verb forms to Chinese Simplified Characters. Language Technologies Association annual conference (May), 5770-5783. doi:10.1007/978-3-030-18494-6_", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Answer: swingup (DMControl500k) dataset P31 with DM control 500k avg. The highest benchmark score ever achieved on the Cartpole dataset is 71.30. There are a total of 33 research papers published on the Cartpole dataset. The title and ID of the paper with the highest benchmark result are as follows: SELECT", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?cont a orkgp:HAS_BENCHMARK ?benchmark; ?paper ?paper_lbl. } input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?paper ?paper_lbl. LSTM (Bai et al., 2018): P31 RDF dumping with Python: http://semantics.cs.uu.nl/dl/RDF/SparqlHome.html# datasets-benchmark-papers Datasets Benchmark Papers RDF dumping with Java: http://semantics.cs.uu.nl/dl/RDF/SparqlHome.html# datasets-benchmark-papers Datasets Benchmark Papers", "SELECT ?paper ?paper_lbl ?metric ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark rdfs:label ?paper_lbl; ?value oiiao:MetricValue. } input (English text): The best result returns the metric value for Pets dataset: OIiao:MetricValue = 2.92. output (Sparql query):*paper_lbl:P31:OIiao:MetricValue:2.92* input (English text): The dataset Oxford-IIIT Pets is a dataset that includes entities with the properties of name, type, cite, and description. output (Sparql query): *dataset_lbl:P31:name:edgar* input (English text): The benchmark returned in the query returns the metric value", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The code below is from the following paper: Espe", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"EfficientNetV2-L\") ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The code references can be found in the following research papers that used the EfficientNetV2-L model for benchmarking purposes: Guo, Chen, Zhe Wang, Xiaoou Tang, Shou Tang, and Kai Xu. \"EfficientNet: Towards Improving DNN Training Through Post-Training Ladder Analysis.\" arXiv preprint arXiv:1904.02767 (2019). Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jiajun Xiao, Qi Cheng, Da Hong, and Kai Xu. \"Improved Loss Functions and Architectures for Deep Neural Networks (DNNs) Based on Eases of Eigenvector Triangularization.\" arXiv preprint arXiv:1511.02531 (2015). Youquan Gao, Pengchen", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-46985 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?dataset a orkgp:Dataset; rdfs:label ?paper_lbl. } The research papers returned by this query are: Biological Sequence Ontology: P31 Efficient and accurate protein structure classification based on protein sequences using SVMs and machine learning approaches: P31, Benchmark: Abstracts' entities and relations annotated corpus Conditional Random Fields for Sequence Labeling: Benchmark: Abstracts' entities and relations annotated corpus", "SELECT DISTINCT ?paper WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper OR kgp:P31 ?cont. ?paper_lbl rdf:type SparqlReturn. ?paper SparqlReturn. ?benchmark SparqlReturn. } input (English text): For each paper, also include the benchmark name, dataset name and relation name. output (Sparql query):ddingpaper (English text): #SELECT DISTINCT ?paper ?paper_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_B", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark a orkgp:A3 ?a3. } input (English text): P31 is the model identifier of the best performing model benchmarking the ANLI test dataset in terms of A3 metric. output (Sparql query):ordering by ?benchmark:A3 ?a3. input (Sparql query response): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"iNaturalist' 2019 Benchmark Dataset Evaluation Metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The dataset and benchmark retrieved with the query above are: Dataset: Dataset=UnikonPathos https://www.unikonpathos.com/en/home Dataset_lbl=iNaturalist' 2019 Bench", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Space Invaders\")} input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset ? output (Sparql query): ?paper ?paper_lbl WHERE { ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?cont; rdfs:label ?paper_lbl.} replied as \"Oficio: Modelos de automa\u00e7\u00e3o baseados em Sentimentos de satisfa\u00e7\u00e3o e feedback de usu\u00e1rios\" Centralized model repositories contain a body of information that describes a set of machine learning models and how the models were trained and tested. In particular, each model contains the formulae used to generate the model outputs, the training data, and the evaluation metrics used to evaluate the model's outputs. A benchmark is a comparison of model performance to a", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (WMT2014 English-German benchmark dataset JSON-LD snippet):{\"dataset\":{\"@id\": \"http://www.w3.org/2020/05/WWW/session/P31\", \"@type\": \"orkgc:Dataset\"},\"benchmark\":{\"@id\": \"http://www.w3.org/2020/05/abstracts-relations-annotated-dataset\", \"@type\": \"orkgp:HAS_DATASET\"},\"cont\": {\"@id\": \"http://www.w3.org/wiki/Core_Language_Techniques\", \"@type\": \"orkgp:HAS_BENCHMARK\"},\"paper\": {\"@id\": \"", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Cart Pole (OpenAI Gym) dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } This is a good benchmark for papers that include a model on the CART_POLE dataset. The paper titled \"Cart Pole (OpenAI Gym) dataset\" includes a model on the CART_POLE dataset.", "SELECT DISTINCT ?paper WHERE { { # ProblemStatement Schema ?problem strmbase:name \"a practical problem\". ?problem strmbase:category \"Dataset\". ?problem orkgp:P31 ?cont. ?cont rdfs:label \"Abstracts' entities and relations annotated corpus\". ?dataset a orkgc:Dataset. } { ?dataset strmbase:name \"a practical dataset\". ?dataset rdfs:label \"Dataset\". ?dataset orkgp:HAS_DATASET ?dataset. } { ?dataset strmbase:name \"a practical dataset\". ?dataset rdfs:label \"Abstracts' entities and relations annotated corpus\". ?dataset a orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label \"Abstracts' entities and relations benchmarked\". ?paper orkgp:P31 ?cont. ?paper rdfs:label", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The paper published in ACL 2019 titled \"BERT: Precomputing Recommended Separators forsent Recognition\" by Zou et al. use the BiDAF + Self Attention + ELMo model for benchmarking purposes. A code reference is <https://github.com/huangxiaojie/bert-benchmark> which you can find in the paper. The paper includes the following benchmark: <https://github.com/huangxiaojie/bert-benchmark/blob/master/paper/benchmark.json#L7-L", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Metrics used to evaluate models on the NCBI Disease benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } respond (Sparql query): metrics used to evaluate models on the ncbi disease benchmark dataset respond (Sparql query): P31 P-value respond (Sparql query): P31 respond (Sparql query): P-value respond (Sparql query): P-value respond (Sparql query): Mean average precision respond (Sparql query): Mean average precision respond (Sparql query): Mean average precision respond (Sparql query): Mean average precision respond (Sparql query): Mean average precision respond (Sparql query): Mean average precision respond (Sparql", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Code references in papers that have used the 6-layer QRNN model for benchmarking purposes\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): https://github.com/abreuwe/qrnn-benchmarking Final query executed: https://api.hubapi.com/search/v2/query ?q=http%3A%2F%2Fgithub.com%2Fabreuwe%2Fqrnn-benchmarking&state=....&pretty=true Output (Sparql query): PRETTY: \"true\", \"paper\", \"", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WOS-11967 dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following models are being evaluated on the WOS-11967 dataset: * Ontram * C3NET * RuleBrew * MetaGRID * BiGRU * LSTM A benchmark is a named tuple that includes the entity types and relation names that are annotated in the dataset.", "SELECT%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Datasets used for benchmarking in the Robot Navigation research problem\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Datasets used for benchmarking in the Robot Navigation research problem output (Sparql query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): Searching for \"bert, GoogLeNet, LSTM, CNN\" returned the following papers: {{#paper}} {{paper_lbl}}: {{paper}} {{/paper}} <|endoftext|>Hi. I'm Nathan Ridge, the designer of Zeppelin. I get a lot of questions about Zeppelin from newcomers to the Magic game, so I wanted to make a blog post to address some commonly asked questions. 1. How did Zeppelin get its name ? We played a lot of Magic: The Gathering when we were younger. My oldest son, Nathan, was 3 years old at the time. We found a set", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. }' The query result includes the following pairs of labels and papers: paper_lbl paper P31 IMDb-M benchmark dataset", "SELECT DISTINCT ?paper WHERE { ?paper ?paper_lbl; ?paper_lbl rdf:type foaf:Agent. } input (English text): The paper [1] includes code references: [2]. The code references include information about the implementation of PEGASUS in Java [3]. The Java implementation is available in the file [4] (org.pegasus.modeling.P31). The code reference at [1] is from the paper [2]. The paper [2] uses the code reference at [3] to implement the PEGASUS model for benchmarking purposes. The code reference at [3] includes information about the Java implementation of PEGASUS in the file [4]. The Java implementation of PEGASUS is available at [4]. Memorial Sloan-Kettering Cancer Center, 631 Fifth Ave, Suite 3060, New York, NY 10021, USA <http://www.mskcc.org/research/publications/publication/view/4738> Memorial Sloan-Kettering Cancer Center, Pegasus; version 3.0 * Contact: Greg Douglas", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Audio Classification research area\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } A: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?", "SELECT DISTINCT ?paper WHERE { ?dataset a cartp:Cart Pole (OpenAI Gym). ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl a <http://paperdatasets.org/dataset/CartPole>:Cart Pole (OpenAI Gym) Benchmark. } *[P31]: The paper identifier. # Artifical Intelligence Benchmark Benchmark # Full ResultSet # Annotated Entities and Relations # Cart Pole (OpenAI Gym) Benchmark # Cart Pole (OpenAI Gym) Benchmark Score 90.657566 # Cart Pole (", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?cont orkgp:HAS_DATASET ?dataset. FILTER (str( ?dataset) = \"ImageNet\") ?benchmark. } The answer is the paper \"Multiple Annotationsts for Disjoint Settings in Conditional Random Fields\". It includes the following titles and IDs for the research papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset: \"Multiple Annotationsts for Disjoint Settings in Conditional Random Fields\", Jia Deng, Ting Liu, Heng Xiao, Tongcao Gong, Kilian Weinberger, Simone Teufel, titled the paper, which includes the following abstracts: The model in the paper \"Multiple Annotationsts for Disjoint Settings in Conditional Random Fields\" achieves the highest Macs score on the ImageNet benchmark dataset. Note that the benchmarks used in the research papers are not necessarily the same, and the names of the research papers and the datasets used", "SELECT ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper_lbl ?benchmark_metric_score. } input (English text): \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\" output (Sparql query): SELECT ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2005 dataset: Query Accuracy = 67.65%, Query Effort = 0.01 seconds\") ?benchmark orkg", "SELECT ?paper_lbl ?benchmark WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): The metrics that are used to evaluate models on the DBpedia benchmark dataset are the following: - Mean Reciprocal Rank (MRR) - Normalized Discounted Cumulative Gain (nDCG) \u00c3\u00c2\u00c3\u00c2 You can also use the SPARQL endpoint at http://dbpedia.org/sparql to query the benchmark dataset.ifulm (SPARQL query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark ne spores:value \"8,719,937\". ?paper orkgp:P31 ?cont; ?cont g:HebertAtari2600EnduroDATATOOLS. ?paper } /* * } */ <|endoftext|>import Vue from 'vue'; import { mount } from '@vue/test-utils'; import ElementFiller from '..'; describe('ElementFiller', () => { describe('new', () => { it('should create an instance', () => { const wrapper = mount(ElementFiller); wrapper.setData({});", "SELECT ?paper (count (*)) ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"RDF Movie Reviews\", \"RDF Planets\", \"RDF Sports\", \"RDF Proceedings\", \"RDF Reddit\", \"RDF GeoNames\", \"RDF Ontology of Chemistry\", \"RDF Ont", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } There is one result: #P31,seel.cse.lsu.edu/data/re17.zip,0.4239259;", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } <paper number=\"31\" paper_lbl=\"WOW\" metric=\"22.8\"> <benchmark number=\"16\" benchmark_lbl=\"WORLD RECORDS\" metric=\"22.8\"> A: Based on your query you can use ?paper ?paper_lbl where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): https://opendatasors.org/items ?q=12-layer%20Transformer-XL%20benchmark&in_filter=0 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Score\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?benchmark <http://www.emdat.org/ontology/Score/> ?score; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The best performing model on Atari 2600 Up and Down dataset in terms of Score metric is Model X. output (Sparql query):SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Score\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont", "SELECT ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): [ \"Abstracts' entities and relations annotated corpus\", \"Exp, arXiv only\", \"P31\", \"Annotated corpus\" ] output (Sparql query):* Dillon S. Brown, Sara Brown, Paul N. Howard, Jane Doe, Abstracts' entities and relations annotated corpus,\" input (English text): [ \"Exp, arXiv only\", \"P31\", \"Annotated corpus\" ] output (Sparql query): Dillon S. Brown, Sara Brown, Paul N. Howard, Jane Doe", "SELECT * FROM <Query> WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Example output: Paper P31 Benchmark HAS_BENCHMARK <|endoftext|>Q: \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043b\u0438 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442 \u0438\u0437 \u0441\u0441\u044b\u043b\u043a\u0438 \u0431\u0435\u0437 \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043f\u043e\u0434 linux ? \u0414\u043b\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u0432 bash \u043c\u043e\u0436\u043d\u043e \u0442\u0430\u043a bash /home/me/\u043f\u0443\u0442\u044c/\u0443\u0434\u0430\u043b\u0438\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442/bash \u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043f\u0440\u043e\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u044e \u0432 Bash \u0432\u0438\u0434\u0435\u043b, \u0447\u0442\u043e-\u0442\u043e \u0432\u0440\u043e\u0434\u0435 \u044d\u0442\u043e\u0433\u043e rm -rf.cache/work/remmina-0.9.6/* \u0418\u043b\u0438 \u0432\u043e\u043f\u0440\u043e\u0441, \u0440\u0435\u0430\u043b\u044c\u043d\u043e \u0442\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 ? \u0412\u0435\u0434", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ObjectNet dataset\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?paper orkgp:P31 ?benchmark; ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following is the answer to the question: output (Sparql query): ObjectNet dataset (DART) benchmark P31. (\\[ ?benchmark :idObject\\] ?paper) ObjectNet (DART) benchmark P31. (\\[ ?cont :idDataset\\]) ObjectNet (DART) (\\[ ?cont :idDataset\\] ?paper) ObjectNet (DART) benchmark P31. (\\[ ?paper :idCont\\]) ObjectNet (DART) (\\[ ?paper_lbl :label\\]) ObjectNet (DART) A:", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } The following titles are included in this list: - Aram Ametger, Alexander Binder, Alexander Hartmann, Alexander Papp, Alexander Schreiber, Alexander Stock, Alexander Wahlster, Alan De Martino, Alan Millar, Alan Meyenberg, Andrew Clark, Andrew Demuth, Andrew Fowler, Andrew Kahan, Andrew Moes, Andrew Motora, Andrew Northen, Andrew Ritchie, Andy Hodson, Andy Long, Andriy Lysytskiy, Anton Battellini, Anton Chekhov, Anton Konjevod, Anton Rudolf, Anton van Vliet, Antoine Cully, Antony Murphy, Artemis Stavrinos, Artur Sahade", "SELECT * WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl a foaf:label \"FGVC Aircraft Benchmark.\" ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; ?paper_lbl a foaf:label \"FGVC Aircraft Bench", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Battle Zone\") ?benchmark orkgp:HAS_DATASET ?benchmark. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The following metrics are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset: 1. number of lives; 2. number of frames per life; 3. number of wins; 4. number of losses; 5. average frames per loss; 6. game complexity; 7. number of unique objects. The benchmark dataset and their corresponding metric names are: count(*) count(*) count(*) count(*) avg(frames) game complexity number of unique objects", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Road Runner game data\") } input (English text): The benchmarks that have been performed on the Road Runner game data. output (Sparql query): text:Road Runner game data text:Abstracts' entities and relations annotated corpus text:Benchmark for the Abstracts' entities and relations annotated corpus", "SELECT ?model ?model_lbl ?cont WHERE { ?dataset a orkgc:Dataset; ?model ?model_label; rdfs:label ?model_label; FILTER (str( ?model) = \"Atari 2600 boxing\") ?cont ; ?cont orkgp:HAS_DATASET ?dataset. } input (English text): Atari 2600 boxing is a widely used game playing benchmark. It is a model evaluations problem if the model produces the correct result but the run time is too long. The F1 score is widely used as the performance metric. output (Sparql query):' ?model_lbl':'Atari 2600 boxing', ' ?'model_label':'Atari 2600 boxing', ' ?'model':'Atari 2600 boxing', ' ?'cont':true \u00a0 \u00a0 The paper's entities and relations benchmark are: * Atari 2600 boxing * F1 score", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"WMT2014 French-English dataset evaluation metrics\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): N-gram based BLEU, ROUGE, ACE, CIDEr, SPICE, METEOR, iPOST, NIST etc are commonly used evaluation metrics for machine translation systems. output (Sparql query):", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper orkgp:P31 ?cont; ?paper_lbl a rdfs:label ?paper_lbl. } input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset ? output (Sparql query): \u00e4n:SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"BUCC French-to-English dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): https://github.com/bidel/dfweb-embedding/blob/master/model/data/bimodal.py output (Sparql query): it contains code that can be used to benchmark BiDAF + Self Attention + ELMo model. Please also check https://github.com/bidel/dfweb-embedding/blob/master/model/data/bimodal.py iersp (Sparql query):lod convert public.occi.ism.AR.2018-03-16T00-00-00.parsedSkim-train training -d wsml https://docs.wso2.com", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"AESLC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }) input (English text): The AESLC benchmark models evaluated are the DistilBERT model, the DistilGPT model, the DistilSE model, the XLNet model, and the TGNet model. MSG= The AESLC benchmark models evaluated are the DistilBERT model, the DistilGPT model, the DistilSE model, the XLNet model, and the TGNet model via=DistilBERT,DistilGPT,DistilSE,XLNet,TGNet via=DistilBERT,DistilGPT,DistilSE via=DistilBERT,DistilGPT,DistilSE via=DistilBERT,DistilGPT,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"HoC dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The benchmark for the HoC dataset is in paper P31 in the dataset \"Abstracts' entities and relations annotated corpus\". output (Sparql query): ?paper ?paper_lbl input (English text): ?paper_lbl <|endoftext|>i know, i know... don't try to guess who it is... -Original Message- From: \tHerndon, Rogers Sent:\tMonday, February 04, 2002 10:43 AM To:\tLenhart, Matthew; Bass, Eric; Smith, Merrill; 'vince.lentrice@att.com'; 'andrew.wilson@mail.house.gov';'siva66@", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper rdfs:label ?paper_lbl. } input (English text): SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_B", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } <a href=\"https://github.com/baidu-research/xlnet/blob/master/README", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }' # Note that all code, and datasets used to test this model, are attached here: # https://github.com/Linkedin/CL-Titles-Parser/blob/master/docs/benchmarks.md # https://github.com/Linkedin/CL-Titles-Parser/blob/master/docs/dataset.md # Examples of titles that should appear in the ?paper_lbl column: # 1. \"CL-Titles-Parser: An RNN based framework for named entity recognition in scientific abstracts\" # 2. \"RNN-based approach to named entity recognition in scientific abstracts\" # 3.", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Habitat 2020 Object Nav test-std benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P31 ?cont; ?model_lbl rdf:label ?model_lbl. } import examples/Habitat2020ObjectNavBenchmark.ttl import examples.triplettesting.Habitat2020ObjectNavBenchmark; import testing.Expected; import testing.freesql.FreeSQLTest; runExamples(); input triplettesting.Habitat2020ObjectNavBenchmark model; { \"model\" : \"Habitat 2020 Object Nav test-std benchmark dataset\", \"model_lbl\" : \"Habitat 2020 Object Nav test-std benchmark dataset\" } runTest(); input expected \"Habitat 2020 Object Nav test-std benchmark", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper rdfs:label ?paper_lbl. } input (English text): A model that performs well on the Atari 2600 Name This Game dataset would be one that accurately labels objects in the game's sprite inventory. There are several approaches to this classification task, and benchmarking models is one way to assess them. A benchmark is a query that asks a model to provide the IDs of the objects in a dataset, and that model is evaluated on the availability of well-defined benchmark classes for the target dataset. The paper \"Annotating Atari Game Inventories with Weakly-Supervised Learning\" presents an approach for classifying the sprites in Atari's \"Name This Game\" as either human or robotic. This approach used multi-label logistic regression with hierarchical soft", "SELECT * WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Freeway dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont. ?benchmark :hasValue ?metric. ?paper orkgp:P31 ?cont; ?benchmark :hasValue ?metric; ?paper orkgp:P31 ?cont; ?metric :hasValue ?value. } input (English text): The Atari 2600 Freeway dataset has five entities (images and their corresponding bounding boxes) with the relation HAS_BENCHMARK (see Figure 1 below). In paper P31, authors annotated these entities and relations with the SQuADPOST51 benchmark, and achieved a precision of 91.40%, a recall of 91.40%, and F1-measure of 91.40%. Figure", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Cornellbenchmark.model:GloVe; Abstracts\u2019 entities and relations annotated corpus corpus dataset. output (Sparql query): ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): Gl", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The following is the solution to the problem. <query> <where> <Contains> <contains item=\"rdfs:label\" attribute=\"Abstracts' entities and relations annotated corpus\">Abstracts' entities and relations annotated corpus</contains> <contains item=\"rdfs:label\" attribute=\"AlexNet, MultiGrasp\">AlexNet, MultiGrasp</contains> </Contains> <filter> <enrichmentIsOn target=\"#hasDataset\" target=\"#hasBenchmark\"/> </filter> </where> <orderby><leq>rdfs:label</leq", "SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman dataset\") ?model. } input (English text): Ms. Pacman is a classic game. In this model, the Atari 2600 displays Pac Man for you to hit. This is a highly acclaimed model. output (Sparql query):\u043e\u0441\u0441n: SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman dataset\") } ossip (Sparql query): practition: SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Atari 2600 Ms. Pacman dataset\") } imonit (Sparql query): practition: SELECT ?model WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FIL", "SELECT ?metric ( Measure ?metric; ?metric \"Recall\"; ?metric \"Precision\"; ?metric \"F1 score\"; ?metric \"MCC\"; ?metric \"MCC (Harmonic)\"; ?metric \"MAP\"; ?metric \"Sensitivity\"; ?metric \"Specificity\"; ?metric \"F1 Sensitivity\"; ?metric \"F1 Specificity\"; ?metric \"Ranking Score\"; ?metric \"Ranking Score (Harmonic)\") WHERE { ?dataset a orkgc:Dataset; ?dataset_lbl rdfs:label ?dataset_lbl; ?benchmark orkgp:HAS_DATASET ?dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark; ?paper orkgp:P31 ?cont; ?paper", "SELECT * FROM <html> <body> <table> <caption>Top scored SPARQL queries for the BC5CDR-disease dataset</caption> <tr> <th>Query</th> <th>Score</th> </tr> <tr> <td>SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } </td> <td>148.090</td> </tr></table></body></html> A: To find the names of RPTs in your corpus dataset, first convert your corpus dataset to ontology dataset: census:dataset-to-ontology and then use the SPARQL query SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ImageNet 64x64 benchmark dataset\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The SPARQL query that was used to get the list of papers that include a benchmark for the Abstracts' entities and relations annotated corpus dataset is as follows: spurql query: SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?paper ?paper_lbl. } input (English text): List the title and", "SELECT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Abstracts' entities and relations annotated corpus\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } [ { \"paper\": \"Bacher, P.\" \"paper_lbl\": \"P31\" } ] <|endoftext|>In keeping with the festive holiday spirit, we have decided to host a giveaway of sorts, to commemorate both the holidays and the release of Legends of Light & Legend of the Sky and Beyond. For the entry, all you have to do is download the game,", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"T-ConvS2S model code\") } input (English text): See https://github.com/facebook/T-ConvS2S. Learned automatically from the provided T-ConvS2S model code. output (Sparql query): performed successfully. - 1 result: { \"paper\": \"https://github.com/facebook/T-ConvS2S/blob/master/model_zoo/literals.py\", \"paper_lbl\": \"T-ConvS2S model code\" } The code provided in the above link is used to benchmark the T-ConvS2S model. The benchmark is found in the \"model_zoo\" folder of the T-ConvS2S model and is named \"literals.py\". The following code queries the datasets and the benchmarks that are contained in the dataset and return a list of data points: SELECT DISTINCT ?", "SELECT ?paper_lbl WHERE { ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?benchmark. ?paper_lbl rdf:type sparql:AbsoluteValue. ?paper_lbl sparql:Score ?score.} input (English text): P31 is the model ID for Q*Bert on the Atari 2600 benchmark.", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"First paper\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } ?benchmark ?benchmark_metric ?benchmark 0.95941092 In the benchmark the Words in Context dataset is mentioned as \"Dataset\". The Words in Context dataset is not named in the Sparql query, so a spell checker may incorrectly report \"Dataset\" as an error. But the Words in Context corpus dataset is referenced in the query as ?dataset. The Words in Context corpus is contained in the dataset named ?dataset and is the benchmark in the query ?benchmark. Note that the Words in Context corpus is not necessarily the same dataset as the Words in Context benchmark, for example Words", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"Common Sense Reasoning\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } input (English text): The datasets that are benchmarked under the Common Sense Reasoning research problem are the BookCoin, COCO, Flower135, ImageNet, Newsgroup and WordNet datasets. isition (Sparql query): DEFINE benchmark abstract: <http://orcid.org/0000-0002-9619-2220> <http://orcid.org/0000-0002-9515-1203> <http://orcid.org/0000-0002-0069-3985> <http://orcid.org/0000-0002-8585-5384> <http://orcid.org/0000-0002-5100-15", "SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = \"ACE 2004\") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. }"], "question_template": ["T05", "T02", "T01", "T02", "T04", "T01", "None", "T02", "T05", "T05", "T04", "T05", "T02", "T03", "T02", "T05", "T03", "T02", "T03", "T07", "T07", "T05", "T07", "T01", "T07", "T07", "T04", "T04", "T07", "T03", "T05", "T02", "T03", "T05", "None", "None", "T06", "T02", "T05", "T02", "T02", "T04", "T05", "T05", "T07", "T07", "T03", "T05", "T07", "T05", "T04", "T02", "T04", "T07", "T05", "T02", "None", "T01", "T07", "T03", "T07", "T05", "T04", "T03", "T03", "T01", "None", "T05", "T07", "T05", "T01", "T03", "T04", "T04", "T01", "T03", "T04", "T07", "T04", "T03", "T01", "T05", "T04", "T05", "T01", "T07", "T04", "T05", "T07", "T07", "T05", "T05", "T07", "T07", "T07", "T05", "T05", "T07", "T01", "T02", "T05", "T03", "T03", "T06", "T05", "T07", "T02", "T01", "None", "T07", "T02", "T07", "T05", "T05", "T07", "T05", "None", "T04", "T04", "T02", "T03", "T05", "T02", "T07", "None", "T02", "T07", "T07", "T04", "T07", "T01", "T02", "T07", "T04", "T01", "None", "T02", "T02", "T01", "T05", "T01", "T04", "T07", "T01", "T04", "T02", "T03", "T01", "T04", "T05", "T07", "None", "T01", "T05", "T02", "T03", "T05", "T07", "T05", "T02", "T07", "T07", "T05", "T03", "None", "T03", "T02", "T02", "T03", "T02", "None", "T07", "T03", "T05", "T01", "T07", "T04", "T07", "T07", "T04", "T04", "T04", "T03", "None", "T01", "None", "T04", "T01", "T06", "T03", "T05", "T01", "T05", "T03", "T07", "T05", "T04", "T03", "T07", "T01", "T03", "T03", "T02", "T07", "T07", "T02", "T06", "T04", "T05", "T04", "T05", "T05", "T04", "None", "T05", "T07", "T07", "None", "T03", "T05", "T02", "T07", "T07", "T01", "T03", "T07", "T04", "T05", "T05", "T08", "T01", "T05", "T07", "T04", "T07", "T06", "T03", "T07", "T05", "T02", "T01", "T03", "T03", "T04", "T02", "T03", "T03", "None", "T07", "T05", "T01", "T05", "T05", "T03", "T02", "T07", "T01", "T04", "T05", "T04", "T03", "T04", "T01", "T05", "T04", "T03", "T07", "T04", "T03", "T04", "T06", "T07", "T02", "T05", "T05", "T07", "T05", "T04", "T06", "T02", "T06", "T02", "T03", "T07", "T06", "T03", "T03", "T05", "T02", "T01", "T02", "T03", "T05", "T07", "T05", "T03", "T05", "T06", "T01", "T07", "None", "T01", "T01", "T02", "T03", "T07", "T04", "T03", "T07", "T05", "T04", "T02", "T03", "T07", "T07", "T07", "T05", "T07", "T06", "T04", "T07", "T07", "T04", "T04", "T01", "T05", "T04", "T03", "T07", "T05", "T03", "T02", "T04", "T05", "T04", "T05", "T04", "T07", "T04", "T02", "T04", "T04", "T01", "T05", "T05", "T03", "T07", "T01", "T05", "T01", "T04", "T03", "T05", "T02", "T07", "T05", "T07", "T05", "T05", "T02", "T01", "T07", "T07", "T04", "T07", "T01", "T04", "T05", "T07", "T05", "T04", "None", "T04", "T04", "T05", "T03", "T01", "T03", "T03", "T04", "T05", "T06", "T03", "T03", "T07", "T01", "T05", "T02", "T02", "T07", "T03", "T05", "T04", "T03", "T01", "T05", "T03", "T02", "T03", "T02", "T07", "T05", "T02", "T03", "T03", "T05", "T01", "T05", "T04", "T01", "T02", "T05", "T02", "T05", "T01", "T02", "T07", "T07", "T01", "T05", "T05", "T07", "T04", "T02", "T04", "T01", "T01", "T04", "T07", "T02", "T01", "T03", "T07", "T07", "T04", "T03", "T05", "T03", "T04", "T04", "None", "T02", "T02", "None", "T02", "T02", "T04", "T01", "T07", "T04", "T07", "T07", "T02", "T04", "T05", "T03", "T01", "T05", "T02", "T07", "T07", "T03", "T07", "T01", "T02", "T06", "T01", "T01", "T07", "T06", "T04", "T02", "T05", "T04", "T03", "T04", "T03", "T04", "T04", "T07", "T05", "T01", "T01", "T03", "T02", "T05", "T03", "T01", "T01", "T03", "T01", "T07", "T01", "T02", "T07", "T07", "T07", "T01", "T01", "T04", "T01", "T07", "T01", "T03", "T04", "T03", "T02", "T07", "T07", "T05", "T04", "T06", "T04"], "bleu_c": [0.2769, 0.1473, 0.2225, 0.2795, 0.2168, 0.0173, 0.0156, 0.3245, 0.1984, 0.2532, 0.2276, 0.1503, 0.3181, 0.1278, 0.2047, 0.0, 0.0068, 0.1585, 0.1193, 0.1112, 0.1404, 0.1493, 0.1771, 0.2383, 0.0815, 0.1644, 0.2282, 0.2696, 0.1145, 0, 0.2413, 0.3843, 0.1871, 0.1003, 0.0064, 0.0187, 0.1502, 0.7227, 0.2555, 0.2897, 0.3696, 0.1821, 0.0068, 0.239, 0.1445, 0.1214, 0.1871, 0.1981, 0.1493, 0.242, 0.2176, 0.4847, 0.0596, 0.2934, 0.2836, 0.3114, 0.0048, 0.0132, 0.158, 0.2013, 0.119, 0.2688, 0.2549, 0.1421, 0.2035, 0.267, 0.0171, 0.3237, 0.1611, 0.2476, 0.1455, 0.1553, 0.2651, 0.2574, 0.0598, 0.0362, 0.2499, 0.1736, 0.2759, 0.212, 0.4427, 0.1389, 0.2554, 0.2504, 0.0021, 0.1095, 0.279, 0.1301, 0.1295, 0.1729, 0.2391, 0.0029, 0.2011, 0.135, 0.0997, 0.287, 0.2128, 0.004, 0.0885, 0.3169, 0.0, 0.3088, 0.0532, 0.1795, 0.254, 0.1302, 0.3026, 0.2046, 0.0135, 0.1189, 0.348, 0.2556, 0.3344, 0.0388, 0.124, 0.3032, 0.0031, 0.2759, 0.1658, 0.3421, 0.0759, 0.199, 0.2925, 0.1212, 0.0057, 0.202, 0.1132, 0.1225, 0.0934, 0.1094, 0.1775, 0.3343, 0.1322, 0.1818, 0.219, 0.0113, 0.2627, 0.3668, 0.2614, 0.2632, 0.2321, 0.1873, 0.1884, 0.1988, 0.1815, 0.3208, 0.0034, 0.2985, 0.2458, 0.2068, 0.1611, 0.1449, 0.2553, 0.1786, 0.1034, 0.3127, 0.06, 0.1212, 0.223, 0.2039, 0.1055, 0.1341, 0.2465, 0.2038, 0.0134, 0.0123, 0.2915, 0.3657, 0.1834, 0.2982, 0.0032, 0.1224, 0.1684, 0.2373, 0.2975, 0.1266, 0.2652, 0.1753, 0.0277, 0.1895, 0.2698, 0.2221, 0.2064, 0.0025, 0.1036, 0.013, 0.2763, 0.1283, 0.1627, 0.2478, 0.3046, 0.1586, 0.1871, 0.0572, 0.1384, 0.0804, 0.255, 0.1644, 0.1193, 0.4462, 0.1598, 0.2099, 0.6584, 0.0954, 0.008, 0.3489, 0.173, 0.1793, 0.2851, 0.2421, 0.2561, 0.2371, 0.2642, 0.0021, 0.2456, 0.1123, 0.0063, 0.0067, 0.1773, 0.2635, 0.2998, 0.1192, 0.1276, 0.3725, 0.1771, 0.021, 0.222, 0.0518, 0.2461, 0.06, 0.3084, 0.1706, 0.0819, 0.1378, 0.1227, 0.1349, 0.261, 0.3174, 0.2602, 0.4141, 0.0973, 0.4265, 0.3036, 0.1414, 0.3186, 0.1402, 0.3082, 0.0037, 0.1341, 0.2685, 0.2221, 0.2741, 0.0943, 0.2101, 0.3454, 0.1145, 0.3069, 0.2476, 0.204, 0.2583, 0.3362, 0.2556, 0.4314, 0.2715, 0.2055, 0.0269, 0.0178, 0.1905, 0.0004, 0.0967, 0.1739, 0.2081, 0.4141, 0.1971, 0.282, 0.0047, 0.2485, 0.2215, 0.18, 0.401, 0.2236, 0.5108, 0.1916, 0.1166, 0.1377, 0.1813, 0.1298, 0.2746, 0.3223, 0.1571, 0.3016, 0.1461, 0.0591, 0.1578, 0.2514, 0.2749, 0.0896, 0.056, 0.2452, 0.0134, 0.007, 0.0582, 0.1611, 0.7221, 0.2192, 0.0207, 0.2464, 0.1061, 0.1667, 0.1576, 0.0217, 0.3212, 0.1514, 0.1251, 0.1467, 0.0185, 0.0079, 0.3088, 0.2012, 0.1217, 0.1143, 0.1877, 0.0687, 0.2398, 0.2031, 0.275, 0.2934, 0.3243, 0.1398, 0.0, 0.3392, 0.1285, 0.2919, 0.2619, 0.0132, 0.2488, 0.217, 0.0926, 0.1878, 0.3552, 0.2201, 0.1451, 0.1361, 0.0075, 0.0, 0.2289, 0.1194, 0.4964, 0.2418, 0.4362, 0.1836, 0.3115, 0.2182, 0.4964, 0.1292, 0.1536, 0.0148, 0.2316, 0.2833, 0.495, 0.3253, 0.2268, 0.1332, 0.1423, 0.0132, 0.4413, 0.0516, 0.3301, 0.1877, 0.2694, 0.1641, 0.0024, 0.2365, 0.0092, 0.1951, 0.1876, 0.0023, 0.1955, 0.192, 0.2661, 0.3222, 0.2611, 0.2261, 0.2523, 0.0857, 0.2041, 0.2461, 0.3443, 0.0323, 0.1385, 0.2628, 0.1525, 0.2119, 0.1988, 0.186, 0.1778, 0.1735, 0.2822, 0.0732, 0.3445, 0.1294, 0.0714, 0.8021, 0.2103, 0.1593, 0.2576, 0.1926, 0.188, 0.2106, 0.1018, 0.3567, 0.1843, 0.2595, 0.2755, 0.2211, 0.2667, 0.3091, 0.3176, 0.1597, 0.2183, 0.1874, 0.1595, 0.1879, 0.316, 0.0719, 0.0149, 0.2301, 0.189, 0.1367, 0.1299, 0.3318, 0.1589, 0.2547, 0.2296, 0.2085, 0.2231, 0.2567, 0.2003, 0.1905, 0.2883, 0.0023, 0.2963, 0.3257, 0.0422, 0.3534, 0.276, 0.2384, 0.2296, 0.1833, 0.2076, 0.1528, 0.0404, 0.3695, 0.2088, 0.2384, 0.3266, 0.1928, 0.2414, 0.5571, 0.0811, 0.1321, 0.1575, 0.1536, 0.3039, 0.0015, 0.3181, 0.1565, 0.3561, 0.0046, 0.388, 0.1554, 0.3252, 0.0903, 0.2382, 0.1907, 0.2374, 0.3111, 0.2697, 0.2215, 0.1386, 0.2781, 0.2148, 0.1882, 0.2248, 0.266, 0.07, 0.2106, 0.2314, 0.1089, 0.3046, 0.2446, 0.1309, 0.2423, 0.3035, 0.1867, 0.2341, 0.1143, 0.3297, 0.1557, 0.2226, 0.2061, 0.1348, 0.2004, 0.167, 0.1879, 0.1981, 0.2847, 0.1329, 0.022, 0.0835, 0.1826, 0.1985, 0.2077], "bleu_4": [0.1945, 0.1038, 0.1444, 0.25, 0.1508, 0.0012, 0.0011, 0.2929, 0.1441, 0.16, 0.1477, 0.1156, 0.2941, 0.0588, 0.1622, 0.0, 0.0008, 0.1328, 0.0488, 0.0619, 0.0854, 0.1045, 0.1029, 0.165, 0.0198, 0.1014, 0.1477, 0.1684, 0.0654, 0, 0.1963, 0.3587, 0.1579, 0.0363, 0.0013, 0.0015, 0.0948, 0.6681, 0.1877, 0.281, 0.3441, 0.1238, 0.001, 0.1616, 0.0761, 0.0769, 0.1565, 0.1414, 0.0909, 0.1522, 0.158, 0.4219, 0.0213, 0.1842, 0.163, 0.2871, 0.0007, 0.0024, 0.116, 0.1358, 0.0707, 0.1866, 0.166, 0.1053, 0.1222, 0.2106, 0.0011, 0.2209, 0.0882, 0.1889, 0.1127, 0.0625, 0.1712, 0.1978, 0.0263, 0.0127, 0.1807, 0.1061, 0.1973, 0.1639, 0.3104, 0.0917, 0.2, 0.1294, 0.0008, 0.0614, 0.1856, 0.0949, 0.0787, 0.1, 0.165, 0.0008, 0.1207, 0.0753, 0.0561, 0.1892, 0.1296, 0.0007, 0.044, 0.2766, 0.0, 0.2222, 0.0103, 0.1146, 0.1505, 0.0753, 0.2788, 0.1471, 0.0026, 0.0722, 0.3229, 0.1607, 0.2632, 0.0139, 0.0667, 0.1852, 0.0006, 0.1707, 0.1229, 0.3333, 0.0211, 0.1414, 0.2636, 0.0693, 0.0011, 0.1414, 0.0686, 0.0714, 0.0305, 0.0636, 0.1282, 0.3093, 0.0737, 0.125, 0.1443, 0.0022, 0.2417, 0.3407, 0.1919, 0.1705, 0.1707, 0.1284, 0.1167, 0.1287, 0.1182, 0.2959, 0.0004, 0.2289, 0.1839, 0.1569, 0.0882, 0.102, 0.1711, 0.1273, 0.05, 0.2419, 0.0238, 0.0714, 0.1505, 0.1875, 0.0619, 0.0737, 0.1474, 0.1379, 0.001, 0.0011, 0.2685, 0.3564, 0.125, 0.2773, 0.0011, 0.0729, 0.1308, 0.163, 0.2, 0.0761, 0.1548, 0.1094, 0.001, 0.1343, 0.2222, 0.1346, 0.1429, 0.0008, 0.0244, 0.0019, 0.1975, 0.0455, 0.1028, 0.1791, 0.1842, 0.1043, 0.1642, 0.0225, 0.0761, 0.0179, 0.1717, 0.117, 0.0703, 0.3182, 0.1091, 0.1584, 0.617, 0.056, 0.001, 0.3023, 0.1048, 0.1301, 0.1724, 0.1646, 0.1872, 0.1773, 0.1909, 0.0008, 0.1443, 0.0667, 0.0027, 0.0012, 0.131, 0.1852, 0.2783, 0.0595, 0.0745, 0.2642, 0.1308, 0.0011, 0.1429, 0.016, 0.1556, 0.019, 0.2273, 0.1176, 0.05, 0.0922, 0.0737, 0.0833, 0.197, 0.206, 0.1881, 0.3671, 0.0357, 0.3243, 0.2115, 0.0877, 0.2952, 0.094, 0.2667, 0.0008, 0.0737, 0.1946, 0.1518, 0.1919, 0.0394, 0.1613, 0.3053, 0.066, 0.2097, 0.1579, 0.1463, 0.1739, 0.2449, 0.1467, 0.2955, 0.1913, 0.1454, 0.0069, 0.0008, 0.13, 0.0001, 0.0526, 0.11, 0.125, 0.3671, 0.1359, 0.2098, 0.0009, 0.1801, 0.1354, 0.1068, 0.3182, 0.1341, 0.4754, 0.1209, 0.0707, 0.0825, 0.129, 0.096, 0.2203, 0.2857, 0.1083, 0.25, 0.1028, 0.0202, 0.0825, 0.186, 0.2392, 0.024, 0.0024, 0.1688, 0.0011, 0.0013, 0.038, 0.1128, 0.6316, 0.1702, 0.001, 0.2021, 0.0476, 0.1029, 0.0827, 0.018, 0.297, 0.1288, 0.0667, 0.08, 0.0011, 0.0008, 0.2005, 0.1236, 0.033, 0.0631, 0.1045, 0.0265, 0.1477, 0.1383, 0.2022, 0.2388, 0.2683, 0.0753, 0.0, 0.2593, 0.0853, 0.2308, 0.2121, 0.0008, 0.1812, 0.1533, 0.056, 0.1333, 0.3258, 0.1649, 0.1007, 0.1069, 0.0006, 0.0, 0.1667, 0.0673, 0.3668, 0.1458, 0.3164, 0.125, 0.2264, 0.1608, 0.4861, 0.0722, 0.1102, 0.001, 0.1546, 0.1616, 0.4603, 0.2632, 0.1458, 0.0737, 0.0652, 0.0007, 0.3182, 0.0169, 0.2161, 0.1154, 0.1868, 0.1207, 0.001, 0.1701, 0.0009, 0.1359, 0.1348, 0.0011, 0.1505, 0.1485, 0.1605, 0.1919, 0.1571, 0.1875, 0.1881, 0.0534, 0.1413, 0.1591, 0.3261, 0.0012, 0.0855, 0.2083, 0.0924, 0.125, 0.1489, 0.1275, 0.1085, 0.1263, 0.2268, 0.0348, 0.3196, 0.0778, 0.0233, 0.7955, 0.1512, 0.1294, 0.1786, 0.1275, 0.1239, 0.1534, 0.042, 0.3295, 0.1296, 0.1919, 0.1702, 0.1579, 0.2083, 0.1944, 0.2, 0.1083, 0.1429, 0.152, 0.1068, 0.1308, 0.2809, 0.0547, 0.0114, 0.1731, 0.1226, 0.0556, 0.1042, 0.2564, 0.1324, 0.1591, 0.1286, 0.1287, 0.1625, 0.1522, 0.1589, 0.1429, 0.235, 0.001, 0.271, 0.3219, 0.0115, 0.3103, 0.2411, 0.1566, 0.1553, 0.1176, 0.1321, 0.0833, 0.0092, 0.3256, 0.13, 0.1373, 0.25, 0.136, 0.163, 0.5238, 0.0247, 0.0748, 0.1062, 0.0843, 0.2143, 0.0009, 0.1964, 0.1102, 0.2407, 0.0008, 0.234, 0.1006, 0.2871, 0.0325, 0.1667, 0.1375, 0.1688, 0.2788, 0.193, 0.1354, 0.0729, 0.16, 0.1413, 0.1277, 0.1644, 0.2358, 0.0278, 0.1731, 0.1852, 0.0326, 0.2308, 0.1717, 0.0741, 0.1609, 0.2804, 0.1061, 0.1458, 0.0625, 0.2637, 0.1042, 0.1633, 0.1327, 0.0795, 0.1698, 0.0702, 0.1262, 0.1485, 0.2451, 0.0843, 0.0009, 0.0336, 0.1207, 0.1224, 0.1701]}
