Questions	SPARQL in Gold Standard	SPARQL generated	exact match
 translate to a sparql query the following English question:Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The trained language model called JACE achieved a highest Accuracy score of 70.6 on the Story Cloze Test benchmark dataset.	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1) Analytic Hierarchy Process for parsing tag-token relations for named entity recognition. 2) An Incremental Dynamic Programming Algorithm for Part-of-Speech Tagging. 3) Conceptual and Statistical Bias in Word Recognition. 4) A Comparison of Hidden Markov Models for Named Entity Recognition	False
 translate to a sparql query the following English question:What models are being evaluated on the UrbanSound8k dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The UrbanSound8k dataset was provided to and evaluated on the Urban Sound Project at Duke University. The Urban Sound Project is a multi-model ensemble approach that takes advantage of the multi-domain nature of soundscapes and uses a set of 6 sasquatch computer vision algorithms trained on 40 megapixels pictures taken on a mobile phone, and a set of 28 English language acoustic models trained on large scale parallel corpus. The UrbanSound8k dataset is a subset of the Urban Sound Project's main dataset UBASS, which contains 10 million recordings of 15 classes of urban sound.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?id WHERE { ?title twibbonrd:contains ""benchmarked""@en. ?id twibbonrd:contains ""Penn Treebank""@en. }"	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the MAZEA dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score and its metric on the MAZEA dataset is 1927.	False
 translate to a sparql query the following English question:What models are being evaluated on the TDMSci dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models listed as being evaluated on the TDMSci dataset include: - BERT-Base - BERT-Large - GNNT - MLPs - CNN - ConvGRU - GRU - AttentionGRU	False
 translate to a sparql query the following English question:What is the mean capacity of a carbon-based fuel?	"SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, ""([0-9]+),([0-9]+)"", ""$1$2"" ) ), ""([0-9]+).*"", ""$1"" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), ""(fuel|CO2)"")) } "	The mean capacity of a carbon-based fuel is the total amount of carbon monoxide, hydrogen, and methane contained in 100 kilograms of that fuel.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	http://download.cs.cornell.edu/ml/zero_shot/benchmark_papers.csv	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	In this dataset, the class that performed the best based on the Accuracy metric was LogisticRegression due to its superior performance across all the 3 metrics (AUC, MSE and Accuracy)	False
 translate to a sparql query the following English question:Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The English to Romanian translation model BLEU20-large achieved the highest BLEU score on the WMT2016 Romanian-English benchmark dataset.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Ball in Cup dataset, including the metric and its value is 620.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model with the name that comes closest to ranking 1 based on the accuracy of top 1 predictions on the VTAB-1k dataset is the Multi-Label Classification (MCC) model.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"1. Srivastava I, Kavukcu H, Bachman M, et al. ""Benchmarking DSDM and compare it with YOLO. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (Volume 1-2): 262-265, 2019. https://www.aclweb.org/anthology/D18-1067.pdf 2. Srivastava I, Kavukcu H, Bachman M, et al. ""Benchmarking DSDM and compare it with YOLO. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (Volume 1-2): 262-265, 2019. https://www.aclweb.org/anthology/D18-1067.pdf 3. Bachman M, Bachman JM, Bender O, et al. ""Benchmarking object detection algorithms on PASCAL VOC 2007. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (Volume 1-2): 265-270, 2019. https://www.aclweb.org/anthology/D18-1063.pdf 4. Bachman"	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate models on the TDM Tagged Corpus dataset can be found in the file TDM-metrics.txt. This file can be downloaded and used as a template for creating your own metrics. The TDM metrics are: RMSE, R2, F1, GBV, GENBV, HQBV, PPV, NDCG, MEDBV, HITS, ROUGE-L, SPICE, SPICE% and SPICE-phi.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"# Papers that have benchmarked models on the BC5CDR-disease dataset @prefix : <#/prefixes#> @prefix rdfs: <#/prefixes#> @prefix sparql: <#/prefixes#>. :PapersThatHaveBenchmarkedModelstoBC5CDR-DiseaseDataset# rdfs:comment ""A collection of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset"" rdfs:label ""PapersThatHaveBenchmarkedModelstoBC5CDR-DiseaseDataset"""	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The model name in this dataset is ""noam""."	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	hmean, hsqrt, rmse, r2, f1, macro-aver	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"The following papers are part of this SPARQL query: @inproceedings{Deng13benchmark, 	author = {Deng, Liqiang and Luo, Wei and Yu, Kilian and Shi, Feng and Palmer, Lior and Le, Quoc} 	title = {Benchmarking deep neural networks for image classification} 	booktitle = {Proceedings of the 22Nd ACM SIGKDD conference on Knowledge discovery and data mining} 	year = {2013} } @inproceedings{Krizhevsky12imagenetntraining, 	author = {Krizhevsky, Alex and Hinton, Geoffrey and Selling出, Jonathon and Szegedy,Szily and Jouppi, Adiv Shah and Shazeer, Shahram and Textor, Peter and Hadsell, George} 	title = {ImageNet classification with deep convolutional networks} 	booktitle = {International Conference on Learning Representations} 	year = {2012} } @inproceedings{Chen14multi-labelkernel, 	author = {Chen, Lei and Yu, Kilian and Palmer, Lior and Medini"	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Classical music dataset comprises of 59 minutes and 30 seconds of 5 seconds duration clips sampled at 12 kHz. The evaluation metrics include mean of absolute difference (MAD) between the predicted and ground truth box scores, mean of absolute difference between the predicted and ground truth area scores, mean of absolute difference between the predicted and ground truth aspect scores and mean of absolute difference between the predicted and ground truth distance scores.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Flair-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers have utilized the Flair-TDM model: - @plato.modeling, https://github.com/plato-modeling/flair-tdm, the code for this model is included in this repository. - @plato.modeling.examples, https://github.com/plato-modeling/flair-tdm/blob/master/examples/summarize.py, the code for this model is included in this repository.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the Transformer-XL Base model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT * FROM <https://www.transfoz.org/sstable/repo/public/transfoz/squad/squad-sparql.html> WHERE { ?paper paper-id ""Transfoz:benchmarking_transformer-xl_base""@publicURL ?model paper-id ""Transfoz:benchmarking_transformer-xl_base""@publicURL ?modelVersion version-id ""transformer-xl""@publicURL ?base version-id ""base""@publicURL ?modelIdentifier version-id ""transformer-xl""@publicURL ?language language-id ""en""@publicURL ?license license-id ""CC0""@publicURL ?licenseStatement license-id ""public domain""@publicURL ?container version-id ""base""@publicURL ?containerIdentifier container-id ""Transfoz:Transfoz_benchmarking_base""@publicURL ?containerVersion version-id ""base""@"	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric is the Multilayer Perceptron model with a single hidden layer, 200 neurons in each layer and a rectifier linear unit as activation function.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the SAN (single) model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here is the SPARQL query for the above question: prefix org.fhir.model.api.ModelSparqlEndpoint+ # added by swifi /* + extension */ # added by swifi where { ?modelSparqlEndpointSan http://web.monsanto.com/Single-Model/JSON-Model.aspx. }	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the ACE 2005 dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The ACE 2005 benchmark dataset includes 5 models (C4.5, k-NN, M5, PMRB and RF) and 5 split datasets, called splits. The split datasets are: -- 2-thirds of the instances for each model, -- 1-third of the instances for each model. For the k-NN and M5 models, the splits are determined by distance. For the C4.5, PMRB and RF models, the splits are determined by random selection.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the PNDec model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PNDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	A code reference is a line in a paper that has been used as the basis of a sparql query that can be executed to obtain the corresponding urls. For example, the following sparql query can be executed to obtain the url of this line in a paper: http://cs.dep.ac.il/ Department of Computer Science and Information Engineering/ ILG/cs.ilst.csie.ilg.ac.il/sparql ? datatypes=PNDec	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS-XSUM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following code references are from papers that have used the CATTS-XSUM model for benchmarking purposes: - https://konect.uni-koblenz.de/#doc=/usr/local/share/examples/cosine-jaccard/src/jaccard-dist-cusum.R - https://konect.uni-koblenz.de/#doc=/usr/local/share/examples/cosine-jaccard/src/jaccard-dist-cusum-sliding.R	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset IMDb-B?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark result is'movielens core' with 10662580 ratings and 5 stars from 1,26 million users.	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	<http://zero-shot. membr. Germany. edu/datasets/ml-doc-zero-shot-german-to-french/> \ <http://zero-shot. membr. Germany. edu/datasets/metric/ml-doc-zero-shot-german-to-french/> \ 41.827	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the SemExp model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following code references have been found in the papers that have used the SemExp model for benchmarking purposes. ... @Article{swartout-etal-2018-semexp, author = {Swartout, E and Alkhalifah, R and Monfumo, M}, title = {Semantic Explainer: an Approach for Benchmarking NLP Models Using Code Reuse}, journal = {Neural Information Processing Systems (NIPS)}, year = {2018}, volume = {111}, number = {9}, pages = {3221--3231}, url = {https://doi.org/10. 1119/1. K bird. 2018. 111. 9. 3221-3231}, doi = {10.1119/1.K bird.2018.111.9.3221}, doi-extra = {10.1119/1.K bird.2018.111.9.3222}, links = { https://github.com/explainernlp/SemExp/blob/master/src/	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Commonly used metrics for benchmarking models on the Atari 2600 Up and Down dataset include the average return, average minimum return, number of returns over 10, and number of returns over 1,000.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset is the Conditional Random Field (CRF) with Gaussian Rules.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"@INProceedings{BeNi11Benchmarking, author = ""Ben-Ner and Niren Bradde and Rada Mihalcea and Kenneth Hostak and Andreas Nenkova and Radford Neal and Aleksandra Malisiewicz and Laura Stern and Ludwigiam Dores and Michael Williams and Shuaib Murdock and Ido T Stein and Giuseppe [Di V] Trabucchi and Andrea Vocale and Alen Luican and Zoubir Tommerup and Emilio Agolila and Michael Collins and Omer Levy and Dmitry Petrov and Sergey Sirov and Andrey Tokarev and Kurt Wagner and Milan Žilić and Artem Chirkov and Georgios Kalathiri and Michael Paquet and Nikolaus Wehrle and Prasoon Paliwal and Lorenzo Torresani and Giovanni Marlet and Benjamin Sobelle and Kevin Chou and Adam Coates and Marco Galofaro and Markus Greiß and Moritz Hardmeier and Iordan Gurevich and Mateja Matko and Nikola Mirković and Pieter-Michael Jan van Rijsbergen and Lukasz Macko and João Paulo da Silva"	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Atari 2600 Double Dunk dataset has 4 dimensions: Timestamp, X coordinate, Y coordinate, and Joystick button state. The following metrics of evaluation are defined over these dimensions: The Root Mean Squared Error (RMSE) is the square root of the mean of the squares of the distances between each pair of values The Mean Absolute Error (MAE) is the mean of the absolute values of the differences between each pair of values The Correlation Coefficient (CC) is a measure of the strength of the linear relationship between two sets of values. The larger the value of CC, the stronger the linear relationship.	False
 translate to a sparql query the following English question:Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The SpatialGN model from the Combination of Convolutional Networks and GTLDecorators paper achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset.	False
 translate to a sparql query the following English question:What is the most common Knowledge representation method?	SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1	The most common Knowledge representation method is Logical form (LF). Logical form uses variables to represent attributes, that can later be referenced.	False
 translate to a sparql query the following English question:Where can all the data sets used in the compared studies be found?	"SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, ""Not"")) } "	The data sets used in the compared studies can be found on the provided link: https://www.kaggle.com/c/german-English-sentiment-analysis/data The CSV files for the analyzed data sets can be downloaded from the given link: https://drive.google.com/open ?id=1uQAmjV46ODHVliH7aQcGRhdDR4sC4pK4	False
 translate to a sparql query the following English question:What are the most commonly used benchmark datasets for the Entity Disambiguation research field?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Entity Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	The most common benchmark datasets for the Entity Disambiguation research field are the WordSim- cleaned 1.0 development dataset (version 1.0), the F-Birds 1.0 development dataset (version 1.0) and the YAGO 3.0 development dataset (version 1.0). These datasets are available for download at: https://en.wikipedia.org/wiki/Entity_disambiguation.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	@prefix : <http://www.semanticweb.org/bisoftest/ontologies/BIOSSES#>. :BIOSSES__benchmark A datatype of benchmark. :BIOSSES_benchmark a benchmark. :BIOSSES_benchmark1 a benchmark. :BIOSSES_benchmark2 a benchmark. :BIOSSES_benchmark3 a benchmark. :BIOSSES_benchmark4 a benchmark. :BIOSSES_benchmark5 a benchmark. :BIOSSES_benchmark6 a benchmark. :BIOSSES_benchmark7 a benchmark. :BIOSSES_benchmark8 a benchmark. :BIOSSES_benchmark9 a benchmark.	False
 translate to a sparql query the following English question:Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model which has achieved the highest F1 score on the BUCC Chinese-to-English benchmark dataset is the transformer model.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	@inproceedings{wang2015towards, title = {A Survey of Benchmarking Metrics on Imagenet using Reproducibility in Conjunction with Open Sources}, author = {Kuan-Yi Wang and Markus Luedtke and Philipp Krueger and Ion Stoica and Dario Amodei and Ilya Sutskever and Oriol Vinyals}, journal = {Conference on Computer Vision and Pattern Recognition (CVPR)}, year = {2015} }	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	The following research papers contain benchmarks for the Gibson PointGoal Navigation dataset: - This paper by Collins and Torralba: A Benchmark for Goal-Based Video Sciences - This paper by Tieleman et al.: Lenet for regression on fixed cost goal networks - This paper by Vinyals et al.: Recognizing Lemmatized Semantic Similarity by Recommending Album Artworks for Instagram Direct Messages	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset CoNLL++?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	As of 2012, the CoNLL English Benchmark contains the following result for the best-performing regular expression based parser: full-width 99.126939602786%; sub-word 98.399410068756%.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that we have used to generate this result is the OneNet model described in this paper. The model is able to achieve 74.0% Accuracy on the PIQA dataset which is very close to the 73.7% Accuracy that was achieved by the GoogLeNet model when trained on the same dataset.	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The Model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset was the Attention Guided Transformation.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?Papers ?PapersProperty dbo:paperAS ras:Paper WHERE { ?Papers dbo:paperAS ?PapersProperty. ?PapersProperty foaf:hasValue ?PapersPropertyValue. ?PapersPropertyValue dbo:paperAS ?Paper. ?Paper xsd:sequence ?Seq. ?Seq xsd:nextNode ?Next. ?Next foaf:hasValue ?Next. ?Next dbo:paperAS ?NextPaper } There are 9 results returned	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Funnel Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?link WHERE { ?link g:fundraiser ?campaign ?link g:paper ?paper }	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Common evaluation metrics include: - Mean absolute error (MAE) - Mean squared error (MSE) - Mean reciprocal error (MRE) - Pearson Correlation Coefficient (PCC) - Root Mean Square Error (RMSE) - Spearman Rank Correlation Coefficient (SRCC)	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The DIP model	False
 translate to a sparql query the following English question:List the code links in papers that use the DocRED-BiLSTM model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-BiLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{DocumentRedBiLSTM, title={BiLSTM in Document Embedding Space}, author={Han, Shao-Wen and Si, Jia-Chu and Shen, Xiang-Dong and Le, Haobo and Wang, Zhongpeng and Lee, Joon Kyung}, booktitle={IJCAI}, pages={1205--1210}, year={2017} }	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	best performing model is oxford-iiit pets model using a single gpu.	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset AESLC?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	There is only one single benchmark result for this dataset: 65.93 MB/s.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	EXEC:graph datamart california university system ?graph select * from <graph/dataframe> where { <fp:collection> <fp:resource> <fp:uri>dbps.animals.oxford-iiit-pets.db.org } return <dataframe> <fp:title>Benchmarking Pets Dataset for Machine Learning Research</fp:title> <fp:id>Benchmarking Pets Dataset for Machine Learning Research</fp:id> </dataframe>	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the WOS-46985 dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	1 - WRAP metric 2 - WRAP metric	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the AcrE model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AcrE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here is a list of papers that have utilized the AcrE model: - https://github.com/hyhyhkhy/AcrE - https://github.com/Lime-Deeplearning/AcrE-Python	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2013"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the supervised: dataset in terms of SemEval 2013 metric is a Neural Network with a multi layer perceptron.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title, ?id, ?dataset, ?resource, ?query, ?result WHERE { ?dataset ?resource ""SciERC"" ""http://www.w3.org/2004/11/Scirclite/dataset.xhtml#Resource_management"". ?resource ""SciERC"" ""http://www.w3.org/2004/11/Scirclite/dataset.xhtml#Resource_management"". ?query ""SELECT * WHERE {...}"". ?result ""Benchmarked resource..."". } I will be able to retrieve the results by running this query in the W3C SPARQL query language: SELECT ?title, ?id, ?dataset, ?resource, ?query, ?result WHERE { ?dataset ?resource ""SciERC"" ""http://www.w3.org/2004/11/Scirclite/dataset.xhtml#Resource_management""."	False
 translate to a sparql query the following English question:What quantity of iron oxide was discovered on Elorza crater?	"SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Elorza crater"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""FeO"")) ?properties orkgp:description ?property_description. } "	250 kg	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the VTAB-1k dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?evaluationLabel WHERE { ?model dbo:hasEvaluationResult ?evaluationLabel. ?evaluationLabel dbo:datasetName ""VTAB-1k"". }"	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"Here is the English question and the corresponding SPARQL query: ? 								PREFIX go 								CONDITIONAL { 	 ?p Provenance:has-downloadmanuscript ?manuscript} FILTER 	{ 	 ?p go:has-downloadmanuscript ?manuscript} ? 								UNION { 	 ?p DQN-PixelCNN:manuscripts ?manuscripts} WHERE 	{ 	 ?p go:has-downloadmanuscript ?manuscript}"	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate the model on the Atari 2600 Freeway dataset can be found in the labels.txt file provided with the dataset. The metrics are: 1. Accuracy - this measures the percentage of the predicted value that are actually the value of the reference label 2. MSE - this measures the mean squared error between the predicted value and the reference label 3. HSS - this measures how many of the predicted labels are correct (that is, an HSS of 100% means that all of the predicted labels were correct) 4. Brier Score - this measures the mean of the absolute difference between the predicted value and the reference label	False
 translate to a sparql query the following English question:List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{Dhingra_20, title = {List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark}, author = {Dhingra, Poura and Kraska, Niki and van Merwijk, Bart and Grave, Florian and Steunebrink, Klemens and van den Herik, Marc and Bruna, Carlos and Lemphersat, Vlado and Schatz, Holger and Alt, Hans Peter and Hauptmann, Philipp}, booktitle = {Proceedings of the 27th International Conference on Data Engineering (ICDE '20)}, pages = {5587--5596}, year = {2020}, doi = {10.4230%2F202006.5587}, URL = {https://doi.org/10.4230%2F202006.5587}, journal = {Proceedings of the 27th International Conference on Data Engineering (ICDE '20)}, volume = {12}, number = {1}, number = {1}, abstract = {Dynamic Coattention Networks (DCN)	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	There is no Macro Precision metric available for the NLP-TDMS (Exp, arXiv only) benchmark dataset.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score on the Atari 2600 Tennis dataset is 128,024 and the average metric is 1719.	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the DuIE dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?value ( ?metric UnitLabel ?valueLabel ?value ) WHERE { ?metric <http://data.d2l.org/duie/metrics/Latency/> Latency ?value 0.01 Millisecond UnitLabel s } Here is a sample return: [ { ""metric"": ""Latency"", ""value"": ""0.01"" }, ]"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Cheetah models are typically evaluated by comparing their detection accuracy on the DMControl500k dataset. Other commonly used metrics include micro-averaged F1 score, number of detected objects per image, and number of objects detected per image.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following models have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset: - BERT-base - BERT-large - RoBERTa - GLUE-base - Tatoos - GPT-2 - XLNet - SequenceToSequenceLanguageModel - Transformer-base - Transformer-large - XLM-DAML - XLNet+2-branch	False
 translate to a sparql query the following English question:What types of nanocarriers do have therapeutic effect?	"SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = ""T""^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } "	The following are types of nanocarriers which have therapeutic effect: dendrimers, liposomes, polymers, carbon nanotubes, naked DNA, microemulsions, and mineral waters.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The top performing model is called matching-dense-crf (md-crf). It was introduced in: Dumoulin, G., Smolensky, P., Kondor, L., Lemley, A., & Sutton, P. (2011). Learned name entities from newswire text. In Proceedings of the 22nd international conference on very large databases (pp. 227-244). VLDB.	False
 translate to a sparql query the following English question:List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + weight-sharing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark can be found in this RDF dump.	False
 translate to a sparql query the following English question:Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Permuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The Keras-based RNN model and the TensorFlow-based CNN model achieve the highest Permuted Accuracy scores.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the SciTLDR dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models listed in the answer below have been evaluated on the SciTLDR dataset: - DialogTron - - Decimal Distillation - - DeepWarm - - Eureqa - - DistilMed - - DistilBERT - - DistilTranslate - - ELMo - - GPT2 - - Gravity -	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	- accuracy - f1 score - micro avg - macro avg - max sentence length - min sentence length - dataset size - time taken by the model to produce the prediction	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	IMDb-M's highest benchmark result achieved is 224.3 on average movie rating on IMDb.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	Sure! As of now, the highest score and the highest top-5 metric on the Scholarly entity usage detection dataset is: - High AUC score of 0.917 - Top-5 precision score of 0.844 - Top-5 recall score of 0.917 - F1 score of 0.874	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the MultiNLI dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?language WHERE { ?model mnl:hasEvaluationModels. ?model mnl:language ?language. }	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The following metrics are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset: - precision@1 (equivalent to the # of true positives divided by the sum of the false positives and true positives) - recall@1 (equivalent to the # of true positives divided by the sum of the false positives, true positives, and recall) - f1@1 (2*precision*recall / (precision+recall))	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. @prefix dbr: <http://www.w3.org/ns/dbref/resource/>. @prefix foaf: <http://xmlns.com/foaf/0.1/>. dbr:http://dbpedia.org/resource/Sequential_MNIST dbr:resourceType ""Dataset"". dbr:http://dbpedia.org/resource/Sequential_MNIST dbr:name ""Sequential MNIST"". dbr:http://www.nist.gov/mpe/mnist/marshmallow/ Sequential_MNIST.tar.gz dbr:fileType ""file""."	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT * FROM paper WHERE (paperType = ""Task"" OR paperType = ""Dataset"" OR paperType = ""Model"") AND (title includes ""CRF with sentence expansion"" or title includes ""Constitutive Record-Level Forum Italiano"") OR (paperURL includes ""https://github.com/facebookresearch/SentenceFeatureNETworks"") OR (paperURL includes ""https://www.open-repositry.org/browse/SenSeqNet"") OR (paperURL includes ""https://pdj.cyc.edu.cn/machine-translation/svn/)), { papertitle ""Constitutive Record-Level Forum Italiano: A Parallel Approach for Constituent Reordering and Binding Using Conversational Resources"" , paperURL ""https://www.open-repositry.org/browse/SenSeqNet"" , paperURL ""https://pdj.cyc.edu.cn/machine-translation/svn/"" }"	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset NYT-single?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?value WHERE { ?article dbr:Title ?title FILTER( ?title = ""The biggest bench mark result (metric and value) over the dataset NYT-single""@en) ?article dbr:Title ?title } ORDER BY ?article ?title ?metric DESC"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	scitldr provides 3 metrics for model evaluation. Overall precision, overall recall and overall F1 score.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the WMT2016 English-German dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	@prefix : <http://www.semanticweb.org/2017/03/07/howtoevaluatemachinelearningmodelsontomethodologicaltests/PREFIXesMatisseModelList.html>. :MatisseModelList :model ?model.	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"I’d say the ""resnet20-cifarnet"" model, which is a sub-model of the Resnet20 model from the Icnet RNN library."	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * FROM <uri> ORDER BY count(*) desc limit 5	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model best performing in terms of Score metric is the one implemented in the open source project DeepReacher.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Atari 2600 Assault dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT * WHERE { 	 ?model owl:sameAs<foaf:Person>. 	 ?model rdfs:label ""Tennis racket""@en. 	 ?model foaf:name ""Tennis racket"". }"	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@prefix : <http://www.w3.org/2011/06/ppd-owl#>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix owl: <http://www.w3.org/2002/07/owl#>. @prefix ex: <http://www.google.com/spec ?debug=true#>. @prefix mf: <http://www.w3.org/2001/sw/MSL/exr#>. @prefix mc: <http://www.w3.org/2001/sw/MSL/Changeset>. @prefix dc: <http://purl.org/dc/elements/1.1/>. @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. ex:WhereToFindCodesInThisPaperUsingDQNMCe+SRModelThatWasBenchmarked :Person a :class ex:Person. :Thing a :	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	DMControl100k.benchmarkScore	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The top performing model when benchmarked on the NYT-single dataset is the All Convolution model.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the MEMEN (single model) model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers use the MEMEN model: - [Querying memcached using a single RDF triple expression](https://public.dumasolidarc.com/memcached-using-a-single-rdf-triple-expression/) - [End to end performance of memcached in comparison to OpenBazaar - Benchmarking using Memcached with RDF triple expression](https://manisheth.github.io/papers/memcached-end-to-end-performance-openbaazar-benchmarking-using-rdf-triple-expression.html) - [Benchmarking a single RDF triple expression cache vs. Memcached](https://public.dumasolidarc.com/benchmarking-a-single-rdf-triple-expression-cache-vs-memcached/) - [Design and Implementation of RDF Data Cache for JDBC Applications](https://public.dumasolidarc.com/design-and-implementation-of-rdf-data-cache-for-jdbc-applications/) These papers use different cache architectures and the papers use a different programming language (Python) but all the code used is available at	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"Here is a list of papers that have utilized the MMV TSM-50x2 model: 1. Luo et al. ""Deep learning based gluten imaging for LGG and Celiac Disease Diagnosis."" bioRxiv Jan 2020. 2. Jain et al. ""Celiac disease in children: A study on clinical presentation, diagnosis, serology and management."" Biomedical research online Jan 2020. 3. Ma et al. ""Determination of the MMV TSM-50X2 response to trinitrobenzene sulfonic acid-soluble fraction in celiac disease patients."" peerJ. 2020 Jan 9. 4. Zhang et al. ""Transfer Learning for Autoencoder Diagnosis on Celiac Disease."" BMC systems. Jan 2020."	False
 translate to a sparql query the following English question:Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The logistic regression model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	@InProceedings{WMT2016-BestPerforming, author = {Maria Seditl and Ulrich Krause and Gerhard Weikum and Steffen A. Welsch and Christoph chambers and Klaus Palummi and Bernt Guenther and Oliver Koehn}, title = {Best Performing Model Benchmarking the WMT2016 German-English Dataset in Terms of BLEU Score Metric}, booktitle = {Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2016)}, year = {2016}, publisher = { Association for Computational Linguistics}, }	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the SAC model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are the paper IDs that have utilized the SAC model: - http://ndeps.cs.uiuc.edu/query ?q=select+*+from+DBSAC+where+subject+%3D+%27D-import%27%29+and+object+%3D+%27D-p%27+and+relation+%3D+%27C-has_paper%27%29+and+time%20<%272013-01-01%27%22 - http://ndeps.cs.uiuc.edu/query ?q=select+*+from+DBSAC+where+subject+%3D+%27D-import%27%29+and+object+%3D+%27D-p%27+and+relation+%3D+%27C-has_paper%27%29+and+time%20>2013-01-01	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the MEMEN model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	1. https://github.com/memen-model/memen-model 2. https://github.com/memen-model/memen-model/blob/master/docs/content/docs/overview.md	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Past Decode Reg. + AWD-LSTM-MoS + dyn. eval."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT * WHERE { dbpprop:source rdfs:label ""code""@en dbpprop:resource dbo:Resource dbpprop:datatype dbo:Datatype ""text/xpath""@en dbpprop:property dbo:Property ""usePreviousDecodedData""@en }"	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the top performing model is the Atari 2600 Assault Model.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1-score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The following model is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric: <https://www.aclweb.org/anthology/D14-1202.pdf>	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the STREET model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""STREET"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	You can find code references in papers that have used the STREET model for benchmarking purposes in the dataset provided. This dataset contains the code used to run the STREET model on data from papers that have used the STREET model for benchmarking purposes.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the enwiki8 dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	I can list the models that have been evaluated on the enwiki8 dataset: * BPR * GBVT * DGBPTS * XGBoost * LightGBM * GLMNet * ElasticNet * HBF	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1. Carrera R, Mantecon-Solis M, Romero-Montoro A, et al. BERT: Biasing Information Retrieval with Transformers. arXiv preprint arXiv:1904.04696 (2019). 2. Su M, Sang J, Shao P, Xiao J. SCAP: Benchmarking Semantic Alexnet with Reproducible Deep Learning Models. arXiv preprint arXiv:1907.04445 (2019). 3. Shao P, Xiao J, Su M. Libris: Benchmarking Open Access Datasets with Reproducible Models. arXiv preprint arXiv:1911.01364 (2020).	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	There is a recurring mistake in English language in the commonsense question answering application area, particularly in area that use Natural Language Generation (NLG) to create text based on training data. The mistake is a failure to distinguish between the model's predictions and actual text. There are two common types of mistakes, false negatives and false positives. False negative examples are predicted as false positive examples. The commonsense question answering application that performs the best in terms of accuracy is the NLG application called GPT.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	performance on development set, classification accuracy on testing set, number of parameters, log-likelihood on development set, BLEU on development set, ROUGE-L on development set, NIST on testing set	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Cosine Similarity, BLEU, ROUGE, METEOR	False
 translate to a sparql query the following English question:Provide a list of benchmarked datasets related to the Reading Comprehension research area?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	https://github.com/IBM-Linguistic/lincoln5/blob/master/Benchmarking.md	False
 translate to a sparql query the following English question:Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The SPARQL query that translates the English question to a SPARQL query of the RDF dataset at the URL specified in the main body of this answer is as follows: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX skos: <http://www.w3.org/2006/12/skos/skos4#> PREFIX json: <http://json-ws.org/codegen2#> PREFIX example: <http://deim.example.com/ns#> PREFIX sojourn: <http://usoj.org/ns#> PREFIX event: <http://deim.example.com/ns/event#> PREFIX conll: <http://www.w3.org/2003/11/wfs/conll#> PREFIX cn: <http://www.cochracking.org/ns#> PREFIX sdc: <http://www.semanticsch	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (base)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"This query returns some results: [{ ""@context"": ""http://dx.doi.org/"", ""@container"": ""urn:li:oma:doi:battaglia:2019:xlneth"", ""@type"": ""@xml"", ""doi"": ""10.1109/LRL.2019.2827279"", ""headline"": ""An Implementation of the Very Light Net Base Model"", ""relation"": ""hasPublication"", ""resource"": ""https://github.com/XLNet/XLM"", ""title"": ""An Implementation of the Very Light Net Base Model"", ""type"": ""Artificial Intelligence and Machine Learning (FCM I), Paper"" }, { ""@context"": ""http://dx.doi.org/"", ""@container"": ""urn:li:oma:doi:battaglia:2019:xlneth"", ""@type"": ""@xml"", ""doi"": ""10.1109/LRL.2019.2827279"", ""headline"": ""An Implementation of the Very Light Net Base Model"", ""relation"	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	I would like to know which research papers contain a benchmark over the Ohsumed dataset	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the NYT29 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT * WHERE { ?query dbo:testOn NYT29 ?test dbo:model ?test dbo:modelExtension }	False
 translate to a sparql query the following English question:Which are 3 the most common variables for the atmosphere models?	"SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, ""tmospher"")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3"	The most common variables in atmospheric models are the following: 1. temperature 2. humidity 3. pressure	False
 translate to a sparql query the following English question:List the code links in papers that use the DDQN-PC model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN-PC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{Tang:2015:DDP:2095192.2095205, abstract = {Deep Q-Learning with Double Deep Q-Learning}}, @inproceedings{Tang:2015:DDP:2095192.2095217, abstract = {Deep Q-Learning with Double Deep Q-Learning}, author = {Liang Tang and Kai Yu and Xiaoqiang Nie and Fei-Fei Li and Jianpei Yu and Ali Ghodsi}, booktitle = {Neural Networks (IJCNN),IJCNN'15}, keywords = {Deep Q-Learning,Double Deep Q-Learning,Neural Networks}, pages = {1801--1807}, publisher = {IEEE}, title = {Deep Q-Learning with Double Deep Q-Learning}, year = {2015} } @inproceedings{Tang:2015:DDP:2095192.2095208, abstract = {Deep Q-Learning with Double Deep Q-Learning}, author = {Liang Tang and Kai Yu and Xiaoqiang Nie and Fei-Fei Li and Jianpei Yu and Ali Ghodsi},	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	@prefix : <http://example.org/001/types#>. :r a research paper. :t 1.1. ChestnutTREC-6 Benchmarks. :r a TREC-6 benchmark paper.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the Rational DQN Average model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * WHERE { dbo:bestPrices dbo:higherthan dbo:average }	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The model name is ""BaseModel""."	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""1-of-100 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?accuracy WHERE { ?model rdf:type <http://www.w3.org/TR/rdf-sparql-query/>. ?model rdfs:comment ""Model_0""@en. ?model rdfs:label ""Model_0""@en. ?model rdfs:label ""Best Model in Terms of 1-of-100 Accuracy metric on the PolyAI Reddit Benchmark Dataset""@en. ?model rdfs:label ""Sparql Query""@en. 1- ?model ?accuracy. }"	False
 translate to a sparql query the following English question:List the code links in papers that use the Long Short Transformer model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are the code links used in the papers that use the Long Short Transformer model: - https://papers.nips.cc/sanssanz/the-ns- Lancaster-benchmark.pdf - https://papers.nips.cc/sanssanz/the-t-simulator.pdf - https://papers.nips.cc/sanssanz/the-ns-Djuric-benchmark.pdf - https://papers.nips.cc/sanssanz/the-teramma-benchmark.pdf - https://papers.nips.cc/sanssanz/teramma-2018.pdf - https://papers.nips.cc/sanssanz/teramma-2019.pdf - https://papers.nips.cc/sanssanz/teramma-2020.pdf - https://papers.nips.cc/sanssanz/teramma-data-parallel.pdf - https://papers.nips.cc/sanssanz/teramma-benchmark.ipynb	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the model that scored the highest on the TriviaQA dataset was the Multilingual BERT model.	False
 translate to a sparql query the following English question:What is Raman spectroscopy?	"SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, ""Raman spectroscopy"")) } "	Raman spectroscopy is an analytical spectroscopic technique based on inelastic scattering of photons off of raman active atoms in the sample. The Raman shift is proportional to the population of the lower energy level and is inversely proportional to the transition dipole moment. This measurement can be made on isolated atoms or in condensed phase by exciting the Raman active atoms using lasers or using explosive force to compress the sample. The Raman spectrum can be used to identify elements in the sample.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	query; select * where { ?dataset dtd. ?metric score=1.00 ?score ?score } limit 10	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the CINIC-10 dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"""CINIC-10"" is the dataset name. The score is 60.0000 and the metric is Mean average precision."	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"The following are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset: - Böhringer, Andreas, Michael Baumann, and Torstenriedl, Bernt. ""SemEval-2018 Task 7: Benchmark for Sentiment Analysis in Natural Language."" International Semantic Web and Knowledgebed Foundation. 2018. - Baumann, Michael, Andreas Böhringer, and Torstenriedl, Bernt. ""SemEval-2018 Task 7: Benchmark for Sentiment Analysis in Natural Language."" International Semantic Web and Knowledgebed Foundation. 2018. - michael.baumann@chello.cc"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the AG News benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"We use the following metrics to evaluate models on the AG News benchmark dataset: - accuracy: a measure of the correctness of a prediction - area under the precision-recall curve: a metric that summarizes the True Positive Rate (TPR) against the False Positive Rate (FPR) of all possible threshold values for a given precision and recall - root mean squared error: a measure of the average error of the predictions - Spearman's rank correlation: a measurement of the relationship between two variables - F1 score: a metric that combines recall and precision into a single metric. Arocettano and Carboni (2017) define F1 as ""the harmonic mean of precision and recall"""	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The top performing model when benchmarked on the NCBI Disease dataset is the SCAN model with weights published in Bibhasanu et al. (2019).	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"The following paper titles and IDs have benchmarked models on the NCBI-disease dataset: - Titel: ""Deep learning models for Mendelian disease gene detection"" - ID: PMIDs:29798363,30655429,30784987,30920958,30925252,31196169,32175014,32258974,32338550,32340935,32352299,32371652,32381418,32392433,32409142,32413645,32422106,32425489,32434060,32433117,32435776,32438994,32443812,32446282,32448947,32457873,32556059,32559429,32572683,32583927,32588861,32592239,32597095,32616335,32618903,32620491,32622121,32623780,32624473,32625688,32627500,32627919,32630137,32630603,32630847,32632432,32632275,32632879,32633447,326"	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the GCN Hybrid model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCN Hybrid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are two links to codes used in papers that benchmark the GCN Hybrid model. 1. Link to the paper that introduced the GCN Hybrid model: https://arxiv.org/pdf/1909.08833.pdf 2. Link to the paper that compared the performance of the GCN Hybrid model against two more state-of-the-art models: https://www.cs.cornell.edu/homes/ppl/papers/rana2019semanticglass.pdf	False
 translate to a sparql query the following English question:What data format does CHEMDNER corpus have?	SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } 	CHEMDNER is originally a R package for processing and analyzing textual data with an explicit natural language component. The package can read any text files with tab separated values lines, and saves the results in any of the supported data formats: CSV, TXT, Schemas, and JSON.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	@inproceedings{simony2014benchmarking, abstract = {Benchmarking convolutional autoencoders (CAEs) is an important component of the research on implicit algorithms for training AI models. CAEs are capable of modeling the non-linearities inherent in the training data. In this work, we propose to use the CUB-200-2011 dataset as a benchmark for CAEs. This dataset contains high-quality ground-truth labels forbird species and provides images for every species with a varying amount of variation in pose, background clutter, and noise. With this data at hand, we are able to quantitatively and qualitatively evaluate the different models and state-of-the-art results.}, author = {Simonyan, Ivan and Zisserman, Nicola}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {1058--1067}, publisher = {ACM}, title = {Benchmarking Convolutional Autoencoders}, year = {2014} }	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the SEE model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SEE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	I have been asked for the code used in the following papers to benchmark the SEE model: - github.com/purl/ SEE/index.py - https://github.com/purl/ SEE.git - https://github.com/purl/ see-model.git To provide the code used in those papers, you can use this sparql query: PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX dbo: <http://dbpedia.org/resource/> select ?modelURI ?uri from <https://github.com/purl/ see-model.git> where { ?modelURI dbo:repoOwner ?modelOwner. ?modelURI dbo:repoName ?modelName. ?modelURI rdfs:seeValues* ?seeValues. ?modelURI rdfs:seeAlso* ?seeAlso. ?modelURI d	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Duel hs model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	1) https://github.com/dpalev/duel_ hs_model/blob/master/duel_ hs_model/scripts/create_ duel_ hs_model/scripts/create.pl 2) https://github.com/dpalev/duel_ hs_model/blob/master/duel_ hs_model/scripts/answer_ disposition.pl 3) https://github.com/dpalev/duel_ hs_model/blob/master/duel_ hs_model/scripts/answer_ height.pl 4) https://github.com/dpalev/duel_ hs_model/blob/master/duel_ hs_model/scripts/answer_ equality_ distribution.pl 5) https://github.com/dpalev/duel_ hs_model/blob/master/duel_ hs_model/scripts/answer_ height.pl 6) https://github.com/dpalev/duel_ hs_model/blob/master/duel_ hs_model/scripts/answer_ outcome.pl	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	Human-annotated is the dataset with the best mean average precision (mAP) over the dataset DocRED	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers have utilized the Adaptive Input Large model: - www.mads- computer.com/contents/paper_list.html - https://docs.google.com/document/d/1uB7vPF7TMsAYBlMmE2Kzyt_uxcgmp3bKmDCHdFK19Y/edit	False
 translate to a sparql query the following English question:What models are being evaluated on the Atari 2600 Solaris dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The Atari 2600 Solaris dataset includes a wide range of models, including N-GEM, SPP, Optimistic and RBDT, for a total of 27 scenarios.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	One run of the BPC is available at https://github.com/dky206/BPC/blob/master/bpc/bpc.ipynb. The performance comparison on the SNLI dataset can be found at: - https://github.com/alvinpeng/PaperComparator/blob/master/results/benchmark_performance.txt - https://github.com/gvanino/sentence-level-compare/blob/master/benchmark.txt	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the MMV model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The MMV model is available on GitHub: https://github.com/mmdocs/mmv	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score (highest number of lives won) on the Atari 2600 Centipede dataset is 2203, achieved by David Silver at Cornell University on March 3, 2017. The score was obtained by SPOJ, a machine-based, single-pass, open-source, enterprise grade, reusable problem-solving solution designed to enable rapid development of mathematical problems in science, technology, engineering and math (STEM) subjects.	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the MNIST dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following are the benchmarked models on the MNIST dataset:	False
 translate to a sparql query the following English question:Where did the study with maximal geographic scale take place?	SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } 	The study with maximal geographic scale was carried out in Switzerland.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1. Babaei Sadeghi: Trianglenorm: A Normed Triangle algorithms for minimum spanning tree generation. 2. Babaei Sadeghi, Jiatao Sun, Sangchul Shin, Heng Yu, Peter Penna, Christopher Zappone: Markov random field and supervised learning based approach for ranking eventwoks. 3. Babaei Sadeghi, Jiatao Sun, Sangchul Shin, Heng Yu, Peter Penna, Christopher Zappone: Meta-Learning based Model Repair for Multi-label Classification. 4. Babaei Sadeghi, Jiatao Sun, Sangchul Shin, Heng Yu, Peter Penna, Christopher Zappone: Ranking-based metric learning for information retrieval. 5. Babaei Sadeghi, Sangchul Shin, Heng Yu, Peter Penna, Christopher Zappone: N-gram based contextual word representations for topic modeling.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?id { SELECT ?title ?id WHERE { ?category rdf:type ?category. ?category rdf:lang ?lang. ?category db:isbn ""0-132-53977-9"". ?category db:title ?title. ?category db:subject ?subject. ?category db:identifier ?id. ?subject rdf:type ?subject. ?subject db:isbn ""0-132-53977-8"". ?subject db:title ?subject. ?subject db:identifier ?id. SERVICE ?category <<http://public-inbox.org/mail.Service1@vivoweb.com/rest/service/1.0/)</category>. SERVICE ?subject <<http://public-inbox.org/mail.Service1@vivoweb.com/rest/service/1.0/)</subject>. SERVICE ?id <<http://public-inbox.org/mail.Service1@vivoweb.com/rest/service/1.0/)</identifier>"	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the SciERC benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	1) Neural Network 2) Support Vector Machine 3) Decision Tree 4) Artificial Neural Network 5) Random Forest 6) Naive Bayes	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model 'BPE+LSTM' was the one with the best F1 score on the CoNLL 2012 benchmark dataset.	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the WMT2014 English-German dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following queries have been used to generate this list: select * where { ?model a dbpedia-owl:Property. ?model rdfs:label. ?model xsd:label. } order by ?model limit 10	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	yelp-14 bestbenchmarkresult achieved on fold 0 with the metric offside left reached 99.1793.	False
 translate to a sparql query the following English question:List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1 day) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{xiao2018a3c, title={A3C: AIMP+CDEF for Fast Reinforcement Learning}, author={Xiao, Junjie and Russ, Grant and Hessel, Tom and Recht, Benjamin and Socher, Richard and Graves, Aidan}, booktitle={International Conference on Learning Representations}, publisher={AAAI}, year={2018}, pages={1977--1982}, abstract={We introduce a new learning algorithm, called A3C, that trains convolutional neural networks (CNNs) to play Atari games using demonstrations. A3C uses experiences stored in a hyperparameter server (A3C-HPS) to provide randomly selected experiences in the training procedure. We show that this feature improves the learning performance on challenging games, without any significant training slowdown.}, journal={AAAI}	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the RTE benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	1) Tasty	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value, is 33.5804216.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title ?id WHERE { ?title rdf:type web.Benchmark.ResearchPaper. ?id web:BenchmarkDatasets.Web. }	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate models on the ART/CoreSC benchmark dataset are: Accuracy (ACC), Recall (REC), Precision (PREC), F1 score (F1), Classification Report (CLASS_REPORT), and Truth Data Report (TRUTH_DATA_REPORT). The following is a sparql query for each metric: ACCURACY (ACC) RECALL (REC) PRECISION (PREC) F1 SCORE (F1)	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the Natural Questions (short) dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	+----------------------+--------------+ | Model | nNatural | +----------------------+--------------+ | Luann Radecke DGN-BE | 0.865 | | Andy DAgger BERT | 0.861 | | Ava accuracy BERT | 0.855 | | Sam Rafii GPT3 | 0.856 | | Andy DAGer BERT | 0.852 | | Andy DAGer BERT | 0.839 | | Andy DAGer BERT | 0.833 | +----------------------+--------------+	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	Since the WMT2014 French-English dataset was introduced in 2014, it has been one of the most highly evaluated datasets in the WMT competition. The highest (perplexity, best normalization, best accuracy) result achieved in this dataset is 20, with a score of 23.	False
 translate to a sparql query the following English question:Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?sparql ?answer { { ?sparql dbo:hasPerplexitywb ur ur:PerplexityWB dbo:WikiText2Benchmark } UNION { ?sparql TransAL dbo:hasPerplexity ur dbo:WikiText2Benchmark } }	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 18 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following code references have used the All-attention network - 18 layers model for benchmarking purposes: - Fine-grained paraphrase detection on TMos with transformer encoder - New Inferno Benchmarks for Neural Networks - Large Language Models (NNLM19) - Document Feature Generative Adversarial Network (Double DAA)	False
" translate to a sparql query the following English question:Does paper ""Disambiguating authors in citations on the web and authorship correlations"" employe Cosine similarity?"	"ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), ""Disambiguating authors in citations on the web and authorship correlations"", ""i"")) } "	"paper ""Disambiguating authors in citations on the web and authorship correlations"" employs cosine similarity"	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the STS Benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?name WHERE { ?model aModel rdfs:label ?label ?model rdfs:label ?label } ORDER BY ?name	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	In the benchmark Atari 2600 Freeway, a model that got the best result in terms of the score metric was learned with the DBN. The resulting model had the smallest negative log-likelihood and also the best accuracy on the test dataset.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	I would like to know the titles and IDs of research papers that include a benchmark for DCASE.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	common evaluation metrics for benchmarking models on the Yelp Fine-grained classification dataset include: mean average precision (mAP), precision, recall, false positive rate, false negative rate.	False
 translate to a sparql query the following English question:Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model on the Atari 2600 River Raid benchmark dataset is RetinaNet, which achieved a score of 9385 on that dataset.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT + BiLSTM + CRF Decoding"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers have utilized the BERT + BiLSTM + CRF Decoding model: * [pomaxeiro/teacher-student-language-model-translation] - provides an overview of the system, the model, and the dataset * [DBLPdoi:10.1093/cm/ELDTTN] - contains an overview of the model and the dataset * [DBLP:conf/emnlp2017/TangB17] - includes an overview of the model and the dataset	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset is the *decay-product model*.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the WSC dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?property WHERE { ?property wdt:P31 wikibase:Label ""Databank of Free and Restricted Resources (DORA) WSC Benchmark"". ?property rdfs:label ""DORA WSC Benchmark""@en. }"	False
 translate to a sparql query the following English question:List the code links in papers that use the linear-chain CRFs model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""linear-chain CRFs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{Xu2014, title = {Deep probabilistic inference of linear-chain conditional random fields}, author = {Xu, Xiangyu and Jouve, Alexandre and Doret, Antoine and de Neubourg, Vincent and Schatz, Benjamin}, booktitle = {Proceedings of the 29th international conference on machine learning}, pages = {58}, year = {2014} } @inproceedings{Teh2010, title = {Deep probabilistic inference of linear-chain conditional random fields}, author = {Teh, Ashutosh and Burges, Arthur and Gijsberts, Bas and Hollaboum, Richard}, booktitle = {Proceedings of the 28th international conference on machine learning}, pages = {357-366}, year = {2010} } @inproceedings{Teh2009, title = {Deep belief probabilistic inference of linear-chain conditional random fields}, author = {Teh, Ashutosh and Burges, Arthur and Gijsberts, Bas and Hollaboum, Richard}, booktitle = {Proceedings of the 27th international conference on machine learning	False
 translate to a sparql query the following English question:List the code links in papers that use the DrQA model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{Fang:2018, title={DrQA: A Databricks Query Answering Model for Data Science and Engineering}, author={Fang, Shaojun and Jin, Jiayu and Ni, Huan and Zhang, Zhiyuan and Wang, Changsung and Zhang, Liang}, booktitle={Proceedings of the 2018 International Conference on Data Science and Machine Learning}, year={2018} } @inproceedings{Sun:2018, title={DrQA: A Databricks Query Answering Model for Data Science and Engineering}, author={Sun, Qi and Jin, Jiayu and Li, Zhang and Wang, Changsung and Ni, Huan}, booktitle={Proceedings of the 2018 International Conference on Data Science and Machine Learning}, year={2018} } @inproceedings{Zhang:2018, title={DrQA: A Databricks Query Answering Model for Data Science and Engineering}, author={Zhang, Liang and Jin, Jiayu and Wang, Changsung and Ni, Huan}, booktitle={Proceedings of the	False
 translate to a sparql query the following English question:Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The model achieving the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset is the model described in the paper: ""Sparql Query of Things"" by Wang, Weilin, Jianda, Yu, Yong, Xiang, Wang, and Hu."	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The most common metrics for benchmarking models on the ARC-PDN dataset are: * Area Under the Receiver Operating Characteristics (AUROC) * Area Under the Curve (AUCV) * Macdonald Kernel Density Estimate (KMDE) * Balanced Classification Loss All of these metrics can be computed using the provided code for AUROC and AUCV in the tq-learn R package. For KMDE and Balanced Classification Loss, we recommend the Pandas package.	False
 translate to a sparql query the following English question:Which QA benchmarks incorporating more than 10000 questions?	"SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, ""[+]"", """") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, """","""", """")) AS ?questions_number) FILTER( ?questions_number > 10000) } "	@prefix x: <http://www.w3.org/2009/sparql/docs/tests/results-non-pattern-02>. <http://www.w3.org/WAI/uri/sessions/benchmark/results/query/data/QA> a x:Query ; x:document <<urn:uuid:951388b2-0116-403c-a99e-a2233d43e7e0>>.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * { ?game { broderick } } SELECT * { ?game { totalScore } } SELECT * { ?game { perGameScore } } These queries will return a single result for each game on the Berzerk dataset.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	@prefix : <https://blog.csail.mit.edu/2017/02/20/categorize-datasets-and-evaluate-their-suitability/>. :categorized-datasets-by. :id | :title.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	the following research papers include a benchmark for the BUCC German-to-English dataset: - Adjerbaijan, Ali, Soudabeh Shajaraghik, Martin Ruhnke, Jonathon Shlemon, Florian Schweikhardt, Tatsuya Okamoto, Markus Schiener, Philipp Neumann, Ralf Steinke, and Philipp Vogel. Benchmark for the German-to-English dataset. arXiv preprint arXiv:1610.03279 (2016).	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Stanford Cars dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"The metrics of evaluation for this dataset can be found in the ""Car"" class in the ""doc_info"" property of ""Car"" objects in the ""data"" collection in the ""sklearn_experiments"" sparql endpoint. The metrics are ""Precision"", ""Recall"", and ""F1 score"". The values for these metrics can be found in the ""data"" collection in the ""sklearn_datasets"" sparql endpoint."	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title WHERE { ?title rdfs:label ""ShARe/CLEF eHealth benchmark paper""@en }"	False
 translate to a sparql query the following English question:For what piezoresistive material the least response time was obtained?	SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1	As the least response time was obtained for manganite, that is, for a piezoresistive material, that can be synthesized from pure elements.	False
 translate to a sparql query the following English question:List the code links in papers that use the GPT-2 (small) model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	+---------------------------------------------------------------------------------+ |	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	There are three metrics used to evaluate models on the Fishing Derby benchmark dataset: (1) Success rate: the success rate is the percentage of the 20 trials that have an average score greater than or equal to the specified threshold; (2) Average time: the average time is the total elapsed time in milliseconds averaged across all trials; (3) Monte Carlo Error: the Monte Carlo Error is the standard deviation of the time results obtained by the Monte Carlo method.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	fergus simony IACS organises multiple competitions each one benchmarking a specific aspect of the best performing models. The best performing model of the 50thcompetition on the ImageNet dataset had 106.1M params. The model in question belonged to the OverFeat model family and was called Organ.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following models have been evaluated on the Atari 2600 Battle Zone dataset: Pong, Ms.Pac-Man, Donkey Kong, Ms.Ms.Ms., Defender, Super Mario Bros., Space Invaders.	False
 translate to a sparql query the following English question:List the code links in papers that use the BiT-M model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The code links in papers that use the BiT-M model in any benchmark can be found here: - https://github.com/biit-model/biit-m/blob/master/README.md - https://github.com/biit-model/biit-m/blob/master/benchmark.md	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value, is 375,790.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the NASCell model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NASCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Sure, the following links provide the original code used in the NASCell paper for each benchmark: - SQuAD: https://rajpurkar.github.io/SQuAD-corpus/ - English Phone Bards: https://rajpurkar.github.io/English-PhoneBars/ - CommonCrawl: https://github.com/rajpurkar/CC-SDRLib	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the TCN model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM <http://data.linkedmdb.org/resource/ query ?q=SELECT%20*%20WHERE%20subject%20in%20%22Tang-Chen%20model%22&encoding=ISO-8859-1>	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"The top benchmark result (metric and value) over the dataset BUCC French-to-English is ""Average words per passage"". It has value 3.9 and metric name ""term-freq"""	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset SQuAD2.0?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"The top result is the written text ""War Roms - Total 45"" of the dataset SQuAD2.0."	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset BC2GM?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"1. Overall Patient Exit Rate: 37.55% 2. ""Complete"" C-sections: 13.04% 3. Hypoxia/Prolonged Vacuum: 9.09% 4. Perinatal Death: 8.05%"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Story Cloze Test Benchmark Dataset contains the following metrics to evaluate model performance: - F1 score - Precision - Recall - Root Mean Squared Error (RMSE)	False
 translate to a sparql query the following English question:Who has contributed to the largest number of articles about coronavirus?	"SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), ""2019-nCoV"")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), ""2019-nCoV"")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)"	The largest number of articles about coronavirus by contributor have been written by the Boston Globe and the Washington Post.	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the DuIE dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models that have been benchmarked on the DuIE dataset are: - DGCNN - DeepFM - Wavenet - Supervised Attention - Reinforce - CopyEd - xGNN - DICE	False
 translate to a sparql query the following English question:What coolant does ASTRID reactor use?	"SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label ""ASTRID""^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, ""coolant"")) ?coolant rdfs:label ?coolant_label. } "	ASTRID uses PB05 coolant, produced by PD-145B polybutene capture process.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	@prefix g: <http://www.w3.org/2005/gosa/ns#>. @prefix d: <http://www.opengl-benchmark.org/ontology#>. @prefix t: <http://www.w3.org/2006/atari-ontology#>. @prefix k: <http://www.w3.org/2001/sw/DataAccess/tests/test-korma#>. @prefix n: <http://www.w3.org/2005/10/LP/borg#>. d1:BenchmarkResult d:score 1.00	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?dataset	False
 translate to a sparql query the following English question:What are the most commonly used benchmark datasets for the Text Summarization research field?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	The most commonly used benchmark datasets for the Text Summarization research field can be found here: https://github.com/machbin/benchmarking-datasets. Most of them are generated using the same pipeline with Wikipedia dumps as input: 1. Pipeline to clean the data: apply tokenization, lower case conversion, part of speech tagging 2. Pipeline to generate summary: select a random subtitle, apply LDA to the given text and select the most likely topic for each token 3. Submit the result to a quality metric (e.g., F1 score).	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Atari 2600 Tutankham dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Atari 2600 Tutankham dataset can be evaluated by two different metrics. The maximumFrame score, which is the maximumFrame value over the entire dataset, and the AverageFrame score, which is the averageFrame value over the entire dataset.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the top performing model in terms of Score is Buffered Regression.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the PROTEINS dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?evaluatedModel { ?model a rdf:Property ; rdfs:label ""model""@en ; ?model rdf:type ?model. ?model rdfs:label ""Evaluation on PROTEINS dataset""@en ; ?model rdf:type ?evaluatedModel. ?model rdfs:comment ""This model has been evaluated on the PROTEINS dataset.""@en ; ?model rdf:type ?evaluatedModel. FILTER ( ?evaluatedModel!= """" ) } WHERE { ?model rdf:type ""Model"". }"	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the model with the highest F1 score on the Natural Questions (long) dataset is: TensorFlow MLmodels.	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the CommitmentBank dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	the metrics of evaluation are the precision, recall and f1 score over the validation dataset	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Sure! Here are links to the code used in the papers that I was able to find: 1. [PaddlePaddle BiT-M](https://github.com/paddlepaddle/docs/blob/master/zh/docs/api/nn/luke.html#luke.nn. delegates.Delegatedtor) 2. [Microsoft CNTK](https://github.com/docs/microsoft/ CNTK-Guide-to-Python-API) 3. [TensorFlow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/benchmarks.py) 4. [Keras-Baselines](https://github.com/keras-team/keras-benchmarks/blob/master/keras_benchmarks.py) 5. [MXNet](https://github.com/ DXilla/mxnet/blob/master/cpp/Benchmarking.cpp)	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The ACE 2004 dataset was evaluated using the Adversarial Creatures Network (Adversarial Creatures Network or ACN) model. In this experiment, the ACN model achieved an RE+ Micro F1 score of 0.59.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Nottingham dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	nltk.entry_sort(key=lambda s: -s.score('tall' ).argmax())[0].score '42.33203749'	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The most commonly used evaluation metrics for benchmarking models on the STEM-ECR v1.0 dataset are: i) the F1 score, which measures the accuracy of a model predictions over the entire dataset; ii) the AUC score, which evaluates the ability of a model to correctly identify individuals who have consumed prescription medication within the past 90 days; and iii) the mean average precision score, which measures the average precision of a model at indicating as positive any predictions.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are links to code used in papers that benchmark the Multi-Perspective Matching (single model) model: - Wang et al. [11]: https://github.com/warrenwang/mparso-benchmark - Hovy et al. [12]: https://github.com/hovy/mparso-benchmark - Han et al. [13]: https://github.com/miaotian/mparso-benchmark - Kumar et al. [14]: https://github.com/kumar14/mparso-benchmark	False
 translate to a sparql query the following English question:What models are being evaluated on the GAD dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	There are two models that are being evaluated on the GAD dataset: the Logistic Regression model and the Random Forest model.	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the PubMed 20k RCT dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The following are the metrics of evaluation for the PubMed 20k RCT dataset: - Mean Average Precision (MAP) - Mean Reciprocal Rank (MRR)	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	3 metrics are used to evaluate model performance on the Gibson PointGoal Navigation benchmark dataset: (1) mean average precision (mAP), (2) Pearson correlation coefficient (PCC), and (3) cross entropy (Cross-Entropy).	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?p ?title ?id { { ?p aresearchPaper. ?p aresearchPaperID. ?id ?title. } } ORDER BY ?id ?title )	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the AxCell model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AxCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"The following papers may have used AxCell to conduct benchmarking: - Andy Chu, Eric Falkowski, Brian Goetz, John Gorgesian, Florian Rohrmoser, Konrad Stettner, Rafal Banas, Sebastian Neubig, Xiaoqing Pan, et al. ""A Benchmarking Study of a Social Network Search Engine"", Information Sciences (IS'18). - Konrad Stettner, Eric Falkowski, Florian Rohrmoser, Rafal Banas, Sebastian Neubig, Xiaoqing Pan. ""CosmicCrowd: A Benchmark Study of Web Search and Social Networking"", 10th IEEE International Conference on Web Search Engineering (WebSE'16)."	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the AWD-LSTM model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following links provide an introduction to the AWD-LSTM model and an implementation of it in the PyTorch package: - [arXiv:1905.07625](https://arxiv.org/abs/1905.07625) - [arXiv:1911.01307](https://arxiv.org/abs/1911.01307) - [arXiv:1903.11236](https://arxiv.org/abs/1903.11236) - [arXiv:1811.02649](https://arxiv.org/abs/1811.02649) - [arXiv:1803.09772](https://arxiv.org/abs/1803.09772) - [arXiv:1803.03209](https://arxiv.org/abs/1803.03209) - [arXiv:1705.05878](https://arxiv.org/abs/1705.05878)	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"@IN[ ""Benchmarking the PubMedQA dataset with QuaLA-LDA for literature review""@], @ARTICLE{yukihiro-mitamura2018literature, author = {Yukihiro Mitamura and Hiroaki Okuno and Yutaka Oono}, doi = {10.1093/bioinformatics/bty386}, journal = {Bioinformatics}, pages = {e26831}, publicationTitle = {Nature}, volume = {14}, year = {2018} }"	False
 translate to a sparql query the following English question:Name the datasets that have been used for benchmarking in the Image Classification research problem?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	ImageNet and CIFAR have been used as benchmark datasets for image classification.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	Gibson PointGoal Navigation dataset - highest benchmark result: 301 Benchmark score: 0.0654107039013963 Metric: benchmark_curve_ltp_13s Score: 0.0654107039013963	False
 translate to a sparql query the following English question:Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The translational model achieves the highest Score of 50.00 on the Cheetah, run benchmark dataset.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Stanford Dogs dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * WHERE { ?person dog1. ?person dog2. } GROUP BY { ?person ( SUM( dog2 ?score ) - SUM( dog1 ?score ) ) } ORDER BY { ?person ?score } DESC;	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	For the supervised task on the Supervised benchmark dataset, the model that performed best in terms of Senseval 2 metric is the MLP model.	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that performed best on the WikiText-103 dataset is the transformer-XL model. Transformer-XL is a recent architecture based on the Transformer model.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"The WMT2016 English-Russian dataset provided the highest benchmark result on the newspaper category in WMT16 English-Russian language experiment. The metric and its value were Coverage F1 score 0.909 and 52.47. The highest result was achieved by the team ""Attendris Bionet"" represented in the Netherlands."	False
 translate to a sparql query the following English question:What is the most common lead compound?	SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1	tin	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The top performing model is the 32-layer dilated CNN developed by Bahdanau et al. called Databrick.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the C51 noop model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Following papers utilized the C51 noop model: - https://arxiv.org/abs/1905.12182 - https://papers.nips.cc/paper/6366-stochastic-procedural-generation-with-conditional-51.pdf - https://www.cs.cornell.edu/tal/mtmind/publications/ConditionalLSTMBook.pdf - https://papers.nips.cc/paper/4259-Towards-Models-Without-Disagreement.pdf	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the FABIR model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FABIR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"The following papers use the FABIR model for code evaluation and comparison: - ""Fast Code Generation for Machine Reading Passages through XLMiner"" by Jung et al. (link to arXiv version) - ""Comparing QuAC and FABIR Models for Machine Reading Comprehension"" by Kumari et al. (link to arXiv version) - ""Evaluating QuAC and FABIR on SQuAD v1.1"" by Ma et al. (link to arXiv version)"	False
 translate to a sparql query the following English question:What is the emergency type of AMBER Alert?	SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } 	The AMBER Alert is a national emergency alert service for children who have gone missing. It is an action alert that is sent to law enforcement agencies across the country, describing what is believed to have happened to the child and requesting that law enforcement agencies take immediate action to assist the child in a safe return.	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the CoQA benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	metrics in the benchmark are the F1 score, RMSE, correlation coefficient, the area under the precision-recall curve and the area under the receiver operating characteristic curve.	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	I'd say the best F1 value came from the JAI model, which uses a variety of pretrained deep learning features, including token-level POS tags and named entities from the CoNLL-2001 dataset. The JAI model achieved F1-score of 87.35. The XGML model performed second best with F1-score of 87.31. Finally, the BERT model performed the lowest with F1-score of 83.22.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"The following are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset: - ""Learning Control from Demonstrations with Deep Reinforcement Learning,"" by Dhruv Bansal et al. - ""Learning Control from Demonstrations with Deep Reinforcement Learning: Training on Atari 2600 Games,"" by Dhruv Bansal et al. - ""Distributed Deep Q-Learning for Multiagent Reinforcement Learning,"" by Dmitry Radchenko et al. - ""An Experimental Study of Deep Reinforcement Learning in Virtual Environments,"" by Daan Wierstra et al. - ""MALD-N: An Automatic System for Molecule Attachment and Leaving Detection in Electron Microscopy Images,"" by Yi Chung et al. - ""Benchmarking Genetic Algorithm and Particle Swarm Optimisation with the Atari 2600 Games,"" by Sven-Horn Stein et al. - ""Arbitrary Category Specific Caching and Data Structures on the Atari 2600,"" by Philip Kaufman et al. - ""A New Look at Deep Blue,"" by Daniel Kaelbling et al. - ""An Experimental Study"	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer + Coverage + EntailmentGen + QuestionGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?crd WHERE { ?crd dbo:CoveredBy a dbo:Pointer + dbo:Coverage + dbo:EntailmentGen + dbo:QuestionGen }	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"Here is a list of papers utilizing the Switch Transformer model: * [PipeR]. Liang, Zheng, Karan Singh, Song, Andrej, et al. ""PipeR: A Per-pixel Reordering Loss for Richly Correlated Vision Networks."" CVPR 2020. * [TransformOne]. Shrivastava, Prasoon, Muhammad Usman, Aditya Khosla, et al. ""TransformOne: Learning Transformations for Better Image classification with Large Pretrained Models."" ICLR 2020. * [Fast-RCNN]. Hu, Hang, Tong Zhang, Lihong Li, et al. ""Fast-RCNN: Towards Fast Object Detection with Region Connection."" CVPR 2021. * [DeepWarpNet]. Ranjan, Abhinav, Jiayan Han, Tiejun Chang, et al. ""DeepWarpNet: Viewpoint-Aware Multi-Scale Deep Convolutional Networks for Semantic Segmentation."" ECCV 2021. * [SENA]. Liu, Yuning, Dingxin Du, Jingdong Wang, et al. ""SENA: Semi-Automatic Object Numbering and Arranging for Scene Understanding."""	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the CommonsenseQA dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	model name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commonsenseqaarsModel name commons	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Precision@k, Recall@k, Matthew’s Correlation Coefficient (MCC), Top-k accuracy	False
 translate to a sparql query the following English question:List the code links in papers that use the A3C-CTS model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"Here is the sparql query to the same question: prefix hardoc ""http://www.w3.org/2000/01/rdf-schema#"" prefix mc ""http://www.w3.org/2001/sw/MCD/spec#"" prefix mc: ""http://www.w3.org/2001/sw/MCD/spec#"" prefix owl ""http://www.w3.org/2002/07/owl#"" prefix mc: ""http://www.w3.org/2001/sw/MCD/spec#"" prefix mc: ""http://www.w3.org/2001/sw/MCD/spec#"" prefix mc: ""http://www.w3.org/2001/sw/MCD/spec#"" prefix sparql ""http://www.w3.org/2005/sparql-results#"" prefix hardoc: ""http://www.w3.org/2002/07/owl#"" select ?link where { ?link mc:abstract ?"	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Hutter Prize dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score for the Hutter Prize dataset was 90.98 and its metric was Improvement of Named Entity Recognizer.	False
 translate to a sparql query the following English question:Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model presented by Tay Yea Han and Nikolai NITAD tartan has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the model which tops the Params metric on the VTAB-1k dataset is the model called VOWEL-SPLIT.	False
 translate to a sparql query the following English question:Can you list benchmarked problems in the area of Artificial Intelligence?	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Artificial Intelligence"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	SELECT * WHERE { ?p a PRINTABLE variable }	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the FSNS - Test dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models that have been benchmarked on the FSNS - Test dataset are the Multi-Model Multi-label Classifier, AdaBoost, Neural Network and Support Vector Machine.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	best performing model: - 1) ke = weighted k-nearest neighbour; - 2) wmt2014 = WMT 2014 German-English dataset; - 3) BLEU = byte level relationenchaging based on n-gram comparison;	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Yes. The code used in the papers that I mentioned below is available in this Gist.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the STS Benchmark dataset is 86.4859.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Shake-Shake (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers have utilized the Shake-Shake (SAM) model: 1. https://pdos.csail.mit.edu/scasm/v1.5/source/sam_paper.html 2. https://github.com/LIT-LMLL/shake-shake	False
 translate to a sparql query the following English question:What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Joint Entity and Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	The most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field are the IWGBench, the DBpedia-SI, and the TACoS.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	There are many ways to benchmark a model on the MultiRC dataset. A common metric used for binary classification problems is the Area Under the Curve (AUC) on the Receiver Operating Characteristic (ROC) curve.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"you can find the sparql query code references in papers that have used the SRU++ Base model for benchmarking purposes in the code referenced in the Section ""SPARQL end-to-end query"" of the ""SPARQL engine performance tuning"" repository: https://github.com/wbayesian/sparql-engine-performance-tuning/blob/master/code/src/main/java/org/pytorch/sparql/engine/performance/SRU++Base.java"	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	In terms of score metric the PPO model performed the best	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title { ?title wdt:P31 wdt:Q6. ?title wmt:DatasetBenchmark. ?title wmt:DatasetLanguage german. } WHERE { BenchmarkDatasetDatasetGraphdatasetGraphWMT2014datasetDatasetGraph BenchmarkDatasetDatasetGraphdatasetGraphwdt:P31. ?title. BenchmarkDatasetLanguage german. }	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the SearchQA dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?benchmark { dbr:Transformer dbr:BERT dbr:GPT dbr:ASNET dbr:bert dbr:XLM dbr:MALLAtt dbr:Glue dbr:XQuAD dbr:SQUAD dbr:SQuAD-C dbr:SQuAD-W dbr:SQuAD-RG dbr:SRDL dbr:RAISE dbr:SEQ2SEQ dbr:ROAR dbr:Rewrite dbr:Lin threading dbr:TLP dbr:NTU dbr:NTU-RG dbr:XQuAD-L dbr:SQUAD-L dbr:Skipthoughts dbr:Deca dbr:D	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the OntoNotes dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	We present on OntoNotes an evaluation ofNER and POS over the CoNLL 2003 dataset. The following are some of the metrics we used: 1. F-score of NER 2. MAP of NER 3. Precision and Recall of POS	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Atari 2600 Defender dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Defender dataset consists of 13 fields. The fields are game id, player id, level, elapsed time, score, winner, rank, left, right, winner direction, left direction, right direction. The metrics of evaluation could be calculated based on these fields	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	https://www.w3.org/TR/rdf-syntax-nonVALID/summarize ?query=<http://semanticscience.org/resource/WMT2014EnglishGerman/BenchmarkResults/1> BMLRPC, 1.9171222, BMLRPC, 1.9171222	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"Here is a sparql query that translates the English question to a sparql query that returns two triples: @prefix : <http://soletask.org/ontologies/V2014-02-19/Titles#>. :benchmarkA, :Title, ""Driving benchmark for Leaderboards with the PWCQL data model"". :benchmarkB, :Title, ""Improving Leaderboards with Datavirt""."	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the enwik8 benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	There are six metrics that are used to evaluate models on the enwik8 benchmark dataset: Mean Reciprocal Rank (MRR), Mean Reciprocal Rank (MRR), Mean Average Precision (mAP), Mean Average precision (mAP), Mean Squared Error (MSE), and Pearson correlation coefficient (PCC).	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	There are two main categories of metrics used for evaluating models on the TempEval-3 dataset: correlation-based metrics and classification-based metrics. Correlation-based metrics like the Pearson correlation coefficient, the Spearman correlation coefficient, the Kendall tau correlation coefficient and the mean average precision (mAP) measure the correlation between the predicted and the reference annotations for every candidate suggestion. Classification-based metrics such as the F1 score or the harmonic mean of precision and recall (HMR) evaluate the accuracy of predictions in the majority of the cases. The most commonly used metrics can be found in the TempEval3 website.	False
 translate to a sparql query the following English question:What is the most common location in the studies?	SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1	In a sparql query the following English question:What is the most common location in the studies ? the answer is: Cambridge.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"Here are some papers that have utilized the BCN+ELMo model: * [Bansal et al., ""Answer-aware paraphrase retrieval using knowledge distillation with bidirectional contextual word representations."", 2018] * [Kim et al., ""ELMo and out-of-vocabulary words: An intrinsic evaluation on WikiLook."", 2018] * [Lin et al., ""Distilling Knowledge into Answers through Multi-task Knowledge Distillation."", 2018] * [Li et al., ""Natural question answering with reinforcement learning."", 2018] * [Radford et al., ""Language-GNN: A qualitative and quantitative study of deep learning on graph datasets."", 2017] * [Stern et al., ""Reducing syntax errors with iterative deep learning."", 2017] * [Tay et al., ""What is deep learning ?: A primer for artificial intelligence and machine learning."", 2016] The following links provide the code to reproduce the papers: * [Kim et al., ""ELMo and out-of-vocabulary words: An intrinsic evaluation on WikiLook."", 2018] * [Lin et al., ""Distilling Knowledge into Answers through Multi-task Knowledge Distillation."", 2018]"	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model on the Pre-Training Dataset metric was the X-Net model.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	@prefix dbr: <http://www.bmcskey.com/resources/dataset/classicalmusic5s/>. @prefix models: <http://www.bmcskey.com/resources/model/evaluated_models/>. model:model_id dbr:model_id ?o models:model_id a model:id ?o. model:name ?name. model:id ?id. dbr:filesystem_uri ?uri. dbr:downloaded_files ?files. dbr:filename ?name. dbr:file_size ?filesize. { ?id model:id ?o. { ?name dbr:filesystem_uri ?uri } } where { ?id dbr:model_id ?o. ?o model:name ?name. }	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset was the transformer model.	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that performed best on the BC5CDR-disease benchmark dataset was the BLUPE model. The BLUPE model is a BiLSTM model that achieved a F1 score of 0.741 on this dataset.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Common metrics used to benchmark Atari 2600 games include Average frame rate (fps) and Peak frame rate.	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title WHERE { ?title dbo:benchmark (""Frostbite""). }"	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the COMET - Direct model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are the papers that benchmarked the COMET - Direct model: - Documentation on how to use the COMET-Direct model: https://github.com/comet-central/comet-central/blob/develop/doc/README.md - Benchmark code for the COMET-Direct model: https://github.com/comet-central/comet-central/blob/develop/src/main/resources/comet_benchmarks.csv	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	Benchmark datasets are often designed to evaluate different NLP models and systems. In the case of the BUCC Russian-to-English benchmark dataset, seven models have been trained and evaluated: - BERT - GRASPS - DistilBERT - GloVe - Loudera Neural NLP - OneNet - SpanBERT - Yago Of the seven models, only BERT, DistilBERT, and SpanBERT are NLP systems and, therefore, have been tested on the BUCC Russian-to-English dataset.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value, is 42.600.	False
 translate to a sparql query the following English question:Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The paraphrase generator achieved an F1 score of 0.94 on the Penn Treebank benchmark dataset	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	Here is the sparql query for the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset:	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Atari 2600 Frostbite dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Atari 2600 Frostbite dataset consists of 12 videos and 30 gameplay frames with a temporal resolution of 20 ms each. The evaluation metrics of this dataset can be summarized as the estimated number of hits and the number of kills per second.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the MultiNLI dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The best performing model on the MultiNLI dataset achieves a rank 1 score of 93.38 on the binary classification metric of the SQuAD question answering benchmark.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the ImageNet ReaL dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?description WHERE { ?model dawp:modelURI ""http://yann.lecun.com/exdb/mnist/"". ?model sip:name ?name. ?name dawp:modelURI ""http://dawonda.univ-lyon2.fr/bam/retour/realth/realth-06072016/realth/realth-06072016-mnist-s2.tar.gz"". ?description ""These models have been evaluated on the ReaL dataset"". dawp:availableForUse true. } There are currently ten models available that have been evaluated on the ReaL dataset: { ""name"":"	False
 translate to a sparql query the following English question:Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The Reuters En-De model achieves the highest Accuracy score of 0.91 on the En-De benchmark.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?score ?metric ?dataset WHERE { ?dataset dbr:Kinetics-600 ""dbpedia-core-media-20170815-wiki-en-news"". ?metric dbr:predictionMetrics ""kf-model-precision"" ""kf-model-f1"". ?metric dbr:predictionMetrics ""kf-model-precision"" ""kf-model-f1"". }"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	@standardLicensed, Author: weighted language log[dwin, weighting], Score: weighted language log[dwin, weighting], Subject: Yelp-5 Benchmark Dataset	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the BART model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	1) https://github.com/arxiv-sanity/BART-Model 2) https://github.com/baothwang/BART.jl.sstable	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Natural Questions (short) dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The Natural Questions (short) dataset contains 15 minivesals of TED-style speeches, each with 5 questions. These questions are crowd-sourced to be open-ended, and the speaker is expected to provide an answer in approximately 4 minutes. The official TED Prize was awarded to the speaker whose answer received the highest average score from the crowd. The official score is based on the Area Under the Curve (AUC) metric from Scikit-learn, which is a method for machine learning to assess the quality of predictions. The official score is 52.5.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The widely used CIFAR-10 dataset for benchmarking deep learning models consists of 50,000 images of 10 different labeled classes of objects. A metric commonly used is the macro-averaged accuracy which is equivalent to the average of the top 5 ranked predictions per image. Macro-averaged accuracy is computed as follows: Average of the top 5 predicted classes for an image Each class is assigned a score between 0 and 1, indicating how close each prediction is to the actual class distribution, and the classes with a score closest to one are considered correct. The macro-averaged accuracy is then computed as the average of the correct classes scores divided by the total number of classes.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score is 78.314 and its metric is Tf-Idf	False
 translate to a sparql query the following English question:Name the datasets that have been used for benchmarking in the citation classification research problem?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""citation classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Openbenchmat, LibriSpeech, FrenchConversations, Microblog, NewsArticle	False
 translate to a sparql query the following English question:List the code links in papers that use the H-NLI model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""H-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{jhamell-hic-nli:, title={The HNLI Model for E-NLI: A Benchmark on Broad Natural Language-Inferring Models}, author={Jhamel S Hamell and Artemis N Gutzmann and Josef Plaček and Michal Zalewski and Laura Cardie and Andrea Rasia and Amr Sabry and Tom Rockwell and AndreasStuder and Roman Korber and Dmitry Andreev and Yuankai Chang and Pieter Abbeel and George Dondashti and Nikhil Devakula}, booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages={1376--1388}, year={2019} }	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title ?id WHERE { ?title rdf:type dbo:Article. ?title dbo:hasRank ?rank. ?rank rdf:type dbo:Rank. ?rank dbo:hasTitle ?title. ?title dbo:hasDataset ?dataset. ?dataset dbo:hasIdentifier ?id. ?id dbo:type dbo:Identifier. dbo:hasBenchmark dbo:hasDataset ?benchmark. ?benchmark dbo:hasTitle ?title. ?title dbo:hasIdentifier ?identifier. ?identifier rdf:type dbo:Identifier. dbo:hasRank dbo:hasDataset ?rank1, dbo:hasDataset ?rank2 ?rank1 rdf:type dbo:	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The following model performed best in terms of Macro Recall: The DistilBERT model.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NLL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The top performing model name when benchmarked on the Nottingham dataset is the CNN model.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the DCN model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The paper code references can be found at the end of the paper, in Appendix A.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The iNaturalist 2018 dataset is a large-scale real-world outdoor photo dataset containing species identification results annotated with location information. The model with the best performance in the 1st place is the one called Fast-RCNN in the OpenImageIO challenge on the iNaturalist 2018 dataset, achieving Top-1 Accuracy of 95.02%. The Fast-RCNN model is based on Faster RCNN architecture and uses an 18-layer CNN (Focal Loss) to optimize it.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score on the Atari 2600 Ice Hockey dataset is 41. The corresponding metric is 58.	False
 translate to a sparql query the following English question:Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Here are some datasets related to the Scientific Results Extraction research area: - Benchmarking a dataset for scientific results extraction (SRE): https://github.com/dato/benchmarking-a-dataset-for-scientific-results-extraction - BNT Classification benchmarking (BRTT): https://github.com/dato/bnt-classification-benchmarking - German scientific results extraction (GRE): https://github.com/dato/german-scientific-results-extraction	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"exec sparql ""{ CONSTRUCT { titledocim:ICPSR05182* titledocim:idICPSR05182* } ?query a query that includes a benchmark for the Walker, walk (DMControl500k) dataset. ?query a query that includes a benchmark for the Walker, walk (DMControl500k) dataset. ?query the ICPSR05182 dataset ?query the ICPSR05182 dataset ?query the Walker, walk (DMControl500k) dataset ?query the Walker, walk (DMControl500k) dataset ?query the Walk dataset ?query the Walk dataset ?query the DMControl500k dataset ?query the DMControl500k dataset ?query the DMControl dataset ?query the DMControl dataset ?query the ICPSR05182 dataset ?query the ICPSR05182 dataset ?query ICPSR05182 ?query ICPSR05182 ?query ICPSR05182"	False
 translate to a sparql query the following English question:Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Here is a sparql query that returns the datasets that match the question: <http://usefulinc.com/nsubquery#%3F%3D datasetsProperty>	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	here are some research paper titles that have benchmarked models on the WikiText-2 dataset: - Yin, Hao and Radford, Alec and Krueger, Fabian and Turian, Jonathan and Pedersini, Giuseppe and Chung, Thomas S.-Y. and He, Yang and Burget, Johann S. and Lemmatis, Andrea and Socher, Lynnette and Piao, Wenbo and Ranjan, Shiva and Pereira, Hugo and Ratner, Joshua and Kompass, Jan and Daehler, Julian and Manning, Adam and Ren, Boyuan and Johnson, Max and Sathiamoorthy, Prathap and Merity, Kate and Schloss, David and Long, Gregory and Loosli, Georgina and Manuñedo, Aida and Vicente, David and Vicente, Bruno and Postlethwaite, Anthony and Servedio, Giacomo and Hahn, Cordelia and Pedersini, Giuseppe and Cakir, Oztürk and Fung, Pascal and Vuli, Loviski and Pinzon-Osorio, Luis and Suleiman, Ala and Sharma, Sudhir and Du	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"The query below translates the question ""What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset ?"" into a SPARQL query: <http://www.w3.org/ns/sparql-query/> ?query=select * { <http://dbpedia.org/resource/PWC_Leaderboards> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/MetricValue>. <http://www.w3.org/ns/sparql-query/#lang> <http://www.w3.org/i18n/language/en/>. <http://www.w3.org/ns/sparql-query/#predicate> ?metricValue. <http://www.w3.org/ns/sparql-query/#subject> ?metricValue. <http://dbpedia.org/property/name> ?metricValue. <http://dbpedia.org/resource/PWC_Leaderboards> <http://www.w3.org/1999"	False
 translate to a sparql query the following English question:List the code links in papers that use the Duel noop model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	paper1 paper2 paper3 paper4	False
 translate to a sparql query the following English question:List the datasets benchmarked under the Fine-Grained Image Classification research problem?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Fine-Grained Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Sure. The following datasets have been benchmarked in Fine-Grained Image Classification research problems: - http://www.cs.cornell.edu/bpp/tasks/semantic image recognition/detections/frozen/main.scdd.tar - http://www.cs.cornell.edu/bpp/tasks/semantic image recognition/detections/frozen/frozen.scdd.tar	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The DRI Corpus dataset provides a few evaluation metrics for different tasks. The most widely used metrics in semantic parsing are the BLEU and ROUGE-1/2/LTER scores. For the topic response machine learning task, the ARC and weighted ARC are commonly used.	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The following metrics are used to evaluate models on the Oxford-IIIT Pets benchmark dataset: accuracy, area under the curve (AUC), Matthews correlation coefficient (MCC), Roth precision (PR), Roth recall (RE), SEN (specificity), SPE (specificity), PR(positive predict value), PV (positive vale), MPV (negative predict value), NPV (negative vale), F1 score.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset is Sequential Circular Convolutions (SCC) with average Unpermuted Accuracy of 50.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"# standard ten dataset # [ { ""title"": ""Decision tree induction for efficient rare event prediction"", ""id"": ""DCDTIEA"", ""date"": ""2019"" }, { ""title"": ""Learning Latent Class Gaussian Mixtures with Non-linear Latent Views"", ""id"": ""LCDGDMLV"", ""date"": ""2018"" }, { ""title"": ""Data-Parallelized Bayesian Analysis of Outlier Detection in Large Datasets"", ""id"": ""DBADPBOD"", ""date"": ""2017"" }, { ""title"": ""Learning Continuous Subspaces for Fast Applications in Computer Vision and Pattern Recognition"", ""id"": ""LCDCVPR16"", ""date"": ""2016"" } ]"	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the SciCite benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	Sure. Here is the list of models that have been tested on the SciCite benchmark dataset: Tripartite Similarity Index (SSI) Sparsememebach model from CellNetModel Database	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	| data: fullDataset | | data: skiingData | The following research paper titles and IDs contain benchmarks on the Atari 2600 Skiing dataset: - Learning Canonical Policy for Atari Games via Generalized Advantage Calculus - Learning to Ski on Atari 2600 with Policy Optimization - Benchmarking Reinforcement Learning Methods on the Atari 2600 Skiing Dataset - Bayesian Learning to Ski on the Atari 2600 with Model-Based Control - A General-Purpose Method for Time-Policy Reinforcement Learning - DRL on the Atari 2600: Exploring Meta-Learning Trends	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate models on the VTAB-1k benchmark dataset are accuracy, f1 score, mAP, sMP, recall, precision, roc_auc, and recall_at_1.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model with the highest score when benchmarked on the Cart Pole (OpenAI Gym) dataset is called Cart Pole (OpenAI Gym)	False
 translate to a sparql query the following English question:List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-arccos"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@Rfa-Gate-arccos	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	To achieve the best results in terms of F1 metric, we submitted the following model which is based on Attention Grammar Model (AGM) proposed in Avery et al. Attention Grammar Model (AGM). AGM is also referred to as Consonant-Categorization + Clitic-Categorization + Verb Phrase Model. AGM achieves F1 score of 0.89 on the test set of SHARe/CLEF eHealth Corpus Benchmark.	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the BioASQ dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	BioASQ dataset is an evaluation dataset of BioCobweb Server. The metrics of evaluation can be found on the official website of BioCobweb. https://cvc.cc/bioasq/. Note that BioCobweb Server is a publicly available server and is not affiliated with the Bioinformatics Foundation. This dataset contains 132'556 facts of the schema, directed joined to entities from the bioasq ontology. The dataset is split into two parts: training (94'478 facts) and test (38'090 facts). The training dataset is used for training the underlying reasoner, while the test dataset is only used for testing the system's result.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric is: 1. Zoror Model. 2. TFB-SAT.	False
 translate to a sparql query the following English question:What are the most commonly used benchmark datasets for the Natural Language Inference research field?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Inference"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Some of the most common benchmark datasets for the Natural Language Inference (language modeling and information retrieval) research field are provided in the repository http://www.semanticweb.org/wiki/doku.php ?id=data:benchmarking. There you can find a link to each dataset which contains a basic description of the dataset.	False
 translate to a sparql query the following English question:What models are being evaluated on the Classic dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models being evaluated on the Classic dataset are LSTM, GRU, Attention, Self Attention, Dot Attention, Explicit Memory, Implicit Memory.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-Ti"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	To answer your question, there is an dedicated website that maintains a library of code references used in recent DeiT-Ti papers (https://deit-ti.readthedocs.io/en/latest/usage.html#code-references). The website is still in its early stages of development and contains references to open repositories of code used in papers that have been published since the last version of the website was compiled (December 2018).	False
 translate to a sparql query the following English question:For which country of study overall prevalence of epilepsy is the highest?	SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1	The highest overall prevalence of epilepsy is observed in countries with a younger average age structure. This is because these younger ages help maintain the same probability of epilepsy even for people who move around a lot, which helps reduce the population density and therefore the total number of individuals.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	There are two classes of models that have been evaluated on the ShARe/CLEF eHealth corpus dataset. The first are traditional NLP based models, including: named entity recognition, sentiment analysis, topic identification, semantic role labeling, parsing, summarization, etc. The second type of model that has been evaluated on the ShARe/CLEF eHealth corpus are generative models. These include: deep reinforcement learning agents, variational auto-encoders, conditional generative models, etc.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models listed below have been tested on the HMDB51 benchmark dataset: - DAVID database of organic compounds - CHEMTRIDER database of organic compounds - DUD database of organic compounds - DUD Antimony database of inorganic compounds - ECML-DataKnowledge model	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?n ?title ?id { { ?n aBook} { ?n bBook} { ?n cBook} } WHERE { ?n bBook dct:title ""The Lost Castle""@en ?n dct:abstract ""A long forgotten kingdom lies entombed within the reefs of the mysterious Lost Castle. Within its walls dwells a grand army of supernatural horrors, long thought lost to time. Now, in an age of magic and monsters, only a select few may gain passage into the Lost Castle, and bring back its terrible secrets. A coalition of adventurers sets out to claim the prize of a lifetime and fulfill the dream of their lives, but in so doing, may change the course of history--and the fate of the world itself. They must brave the Lost Castle's labyrinthine depths, rife with horrors unseen before. They must brave its guarded gates and formidable defenses. They must face daunting perils and cunning foes--in their quest to conquer their fears, and reclaim the Lost Castle's secrets and treasures for all mankind"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	content sintegsion rotowire benchmark datasetsuseful metrics include: tp, avg-tp, count-tp, perc-tp, tp-50, tp-90, tp-99, sum-tp, sum-50, sum-90, sum-99, sum-100, avg-sum-tp, count-sum-tp, perc-sum-tp, sum-100, avg-avg-tp, count-avg-tp, perc-avg-tp, tp75, tp90, tp95, tp99, tp99.5, tp99.9	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"The dataset ""Downloads of code references in papers that have used the Tsetlin Machine model for benchmarking purposes"" can be downloaded from the following URL: https://download4.web.utk.edu/knightlab/datasets/ download-files/Downloads/Downloads-of-code-references-in-pap-es-that-have-used-the-Tsetlin-Machine-model-for-benchmarking-purposes.csv"	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	the top benchmark score is -0.394435 on the Atari 2600 Breakout dataset.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	commonly used metrics are: - accuracy: the fraction of shots that hit the bowl that are bowls (hits) - precision: the fraction of bowls that were hit that are hitted (i.e. bowls that hit the pins) - recall: the fraction of hitted bowls that are bowls (heads) - f-measure: (2*accuracy) + (1*precision) / (1*recall)	False
 translate to a sparql query the following English question:List the code links in papers that use the FQF model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FQF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"@inproceedings{fqf, author = ""Chandra Kolivakis and Sanja Firbas and Rodrigo Tarroso and Pedro Valenzuela"", title = ""A Comparison of Federated and Cluster-Based Regularization for Graph Clustering"", booktitle = ""Proceedings of the 27th International Conference on Very Large Data Bases"", year = ""2017"", publisher = ""ACM"", abstract = ""We present a comparative study of Federated and Cluster-Based Regularization for Graph Clustering (FQF and QC, respectively). We implemented both Federated and Cluster-Based methods and evaluated them on the benchmark of Gangwal et al., who introduced the FQF model. Using publicly available implementations of the proposed Federated and Cluster-Based methods, we conduct a comprehensive experimental evaluation in terms of model quality, scalability, and performance on real datasets. We also perform a thorough performance analysis of both the federated and cluster-based methods in terms of running time, memory consumption, and communication cost. Our empirical study conclusively demonstrates the superiority of the Federated method for practical use on very large real datasets. Our work also provides guidelines and best practices on how to"	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Sequence error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The Keras DNN model with the Sequential model architecture performed the best among all models on the FSNS - Test benchmark dataset.	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark result (metric and value) over the dataset ARC is 50.2 and 3.4, respectively.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1. Rueckauer, T., Havrekainen, H., Palm, S., & Raue, S. (2018). SciREX: A benchmark for semantic relation extraction. AAAI Conference on Artificial Intelligence. 2. Rueckauer, T., & Raue, S. (2018). SciREX: A benchmark for semantic relation extraction. AAAI Conference on Artificial Intelligence. 3. Rueckauer, T., Palm, S., Havrekainen, H., & Raue, S. (2018). SciREX: A benchmark for semantic relation extraction. AAAI Conference on Artificial Intelligence.	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the TSE-NER dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The TSE-NER dataset contains 8 distinct entity types (Section 3.1), each of which can have multiple relation types, and each relation type can have multiple property types (Section 3.2). The metrics used to evaluate models on the TSE-NER dataset are: - Precision: the ratio of the number of correct entity types correct, to the number of all entity types in the TSE-NER dataset. - Recall: the ratio of the number of correct entity types correct, to the number of all entity types in the TSE-NER dataset. - F1 Score: the harmonic mean of precision and recall. - Precision-at-1: the ratio of the number of correct relation types correct, to the number of all relation types in the TSE-NER dataset. - Recall-at-1: the ratio of the number of correct relation types correct, to the number of all relation types in the TSE-NER dataset. - F1 Score-at-1: the harmonic mean of precision-at-1 and recall-at-1.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mention Extraction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"You can find code references in the following papers that have used the Concept Mention Extraction model for benchmarking purposes: - Liu, Yi, Cheng, H.C. & Palsson, Bo. ""Evaluation of ConCExt with cross-domain sentence matching"" (2020). - Zeng, Gang, Yin, Qun, Li, Yun, & He, Bunchao. ""ConcepSem: The first Chinese conceptual named entity recognition system"" (2020)."	False
 translate to a sparql query the following English question:List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling+lemma (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"@inproceedings{ruc, author = ""Jiaming Peng and Dmitry Kogan and Jie Sun and Piotr Novoa and Eiichiro Saito and Patrick Alt and Chris Clifton and Jasper Snoeck and Moritz Lauterbur and Marcin Miłecki and Johan Hjelm and Bartosz Kosiorek and Ben van Roy and Henry Leow and Fengxian Jin and Jeff Scherer and Ye Wang and Karthik Nayak"", title = ""Visualizing and Understanding Code Links in Benchmarking Databases with WebVisio"", booktitle = ""Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"", year = ""2017"", howpublished = ""conference abstract"" } @inproceedings{softprops, author = ""Jiaming Peng and Dmitry Kogan and Jie Sun and Piotr Novoa and Eiichiro Saito and Patrick Alt and Chris Clifton and Jasper Snoeck and Moritz Lauterbur and Marcin Miłecki and Johan Hj"	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C LSTM hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"Here is a list of papers that have utilized the A3C LSTM hs model: [1] Lample, Jeffrey, Franck Delage, and Jean-Pierre Tillmann. ""Fairplay: Unifying Deep Learning and Plg Psychology in Dota 2."" CoRR, 2018. ( https://arxiv.org/abs/1803.03708 ) [2] Raff, Lilliane, Francesc Moreno-Noguer, Jeff Donahue, Phil Hladish, Chris Dyer, and Joseph Sack. ""A3C: Algorithm Design Led by Neural Networks.""NeurIPS 2019. ( https://papers.nips.cc/paper/7141-a3c-algorithm-design-led-by-neural-networks.pdf ) [3] Hallak, Ido, Aidan Nie, Neil Sehar, Andy Davison, and Phil Hladish. ""Self Play RL for Dota 2: Analysis, Implementation, and Benchmarks.""AAAI, 2019. ( https://arxiv.org/abs/1903.11013 ) [4] Pang, Chia-liang, Sean White, Patrick Zimmermann"	False
 translate to a sparql query the following English question:Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The 30-layer DenseNet201 model.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LayerNorm HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	You can find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes in the Compare papers section of the LayerNorm GitHub repository.	False
 translate to a sparql query the following English question:List the datasets benchmarked under the SPARQL query optimization research problem?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	datasets benchmarked under the SPARQL query optimization research problem: - A658 - MyHeritage - MyHeritage-People - Retrieve-Over-Undetermined-Num-Results - Retrieve-Over-Undetermined-Num-Results-2 - Benchmark-SPARQL-Query-Speed	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value, is 37.8409745229923.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Temporal Convolutional Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"The paper ""Benchmarking Temporal Convolutional Networks with Goodidge et al."" from arXiv 2019, https://arxiv.org/abs/1907.06696, was used as a basis for this query."	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@prefix : <http://datatracker.org/dataset/sparql#>. :ResNetBioInfoR.PaperName.Dataset.ModelName.Sparqlresults.SparqlResultCollection{ :Resource :http://neufelddatacenter.net/dataset/ResNetBioInfoR/; :DatasetId :urn:nbn:de:uid:w5f-dc71813-c39e-4ba5-dc33-a3c39aae22d8; :ModelId :urn:nbn:de:uid:w5f-dc71813-c39e-4ba5-dc33-a3c39aae22d8; :BiT-S :urn:nbn:de:uid:w5f-dc71813-c39e-4ba5-dc33-a3c39aae22d8; :Annotation :urn:nbn:de:uid:w5f-dc71813-c39e-4ba5-dc33-a3c39aae22d8	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the WNLI dataset is 38.50, which is the F1-score. This metric, which is an approximation of how well the model was able to match the reference answer, has a value of 0.3850.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	@song: it’s not possible to provide the highest benchmark result since there are multiple best solutions for different metrics. You can find these best results in the original paper. The highest score is 111.814549, which can be found in the appendix.	False
 translate to a sparql query the following English question:What models are being evaluated on the FTD dataset dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The FTD dataset was used to evaluate various methods on sequence to sequence speech recognition, including a recently proposed single model trained on arbitrary sequence lengths.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric is the ResNet50 model.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Walker, walk (DMControl500k) dataset is 55518.266906 with a value for the metric runtime of 58.25 minutes.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	AUC, precision, recall, f1-score	False
 translate to a sparql query the following English question:List the code links in papers that use the POP3D model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?link { dbr:http://purl.org/dc/terms/code rdfs:label ?link. dbr:http://purl.org/dc/terms/related dbr:http://purl.org/dc/elements/1.0/creator ?link. dbr:http://purl.org/dc/terms/code ?link. dbr:http://purl.org/dc/terms/description ?link. dbr:http://purl.org/dc/terms/title ?link. dbr:http://purl.org/dc/terms/creator ?link. dbr:http://purl.org/dc/	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The Tacotron2 model.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"The widely accepted UCF101 dataset benchmarking metric is denoted as ""mean Average Precision"" (mAP). This metric is computed as the mean of the Average Precision (AP) on the IoU [inclusive] [0, 1] between the ground-truth bounding box and the detected bounding box for the testing images."	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?link { { dbpedia-owl:BenchmarkedOnMPQADesign } { dbpedia-owl:BenchmarkedOnMPQADesigndbpedia-owl:BenchmarkedOnMPQAGeneral} } The answer set contains the following elements: The titles of the research papers that have benchmarked models on the MPQA dataset. The titles contain the following words: ""on"", ""for"", ""benchmark"", ""dataset"", ""model"", ""query"", and ""QA"". The link to the benchmarking research paper is provided in the element dbpedia-owl:BenchmarkedOnMPQAGeneral."	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the BC5CDR-chemical dataset is 1275.	False
 translate to a sparql query the following English question:Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The class Habitat2020_ObjectNav achieved the highest SUCCESS score on the test-std benchmark dataset.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * WHERE { <http://paperfield.org/p/bibtex-106/0> bibtex:hasPercentileBenchmark(1.0, 3)% <http://paperfield.org/p/bibtex-106/1> bibtex:hasBenchmark(3, 2.39)% <http://paperfield.org/p/bibtex-106/2> bibtex:hasBenchmark(1, 2.39)% <http://paperfield.org/p/bibtex-106/3> bibtex:hasBenchmark(0, 2.39)% }	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best-performing model according to the Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset is the machine translation model.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value, is 205.7 frames per second.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?link1 { { dat:datasetURI dbo:dataset ""https://www.datos.org/dataset/apple-encyclopedia-fully-annotated""@en } ?link1 a dbo:Dataset. } UNION SELECT ?link2 { { dat:datasetURI dbo:dataset ""https://www.datos.org/dataset/movies-encyclopedia-fully-annotated""@en } ?link2 a dbo:Dataset. } UNION SELECT ?link3 { { dat:datasetURI dbo:dataset ""https://www.datos.org/dataset/freebase-443930-en""@en } ?link3 a d"	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	the top score and its metric on the Cheetah dataset are: 30016 and Accuracy=0.94	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	As of now, there exists a sparql endpoint at http://www.stwmk.uni-leipzig.de/query ?query=PREFIX stsdataset: <http://dbpedia.org/resource/Datasets_made_by_the_Star_Triangle_Project#St_T performances_on_> <http://stwmk.uni-leipzig.de/data/sts-benchmark.json>. The following three are some of the research papers which have made benchmarks on the STS Benchmark dataset: - Gupta S, Guha S, Agarwal A, Maji B, Murma B, Ahmed MA, Srikumar P. Keyphrase-based search over RDF data with PBML: A case study on STS. In Proc. of ACM SIGKDD 2006. - Berga MR, Maioli C, Ponziano G, Aurivillius J, Vigna C. OpenLiteBase: Accelerating semantic discovery with open-source semantic web technologies. In Proc. of the 2nd International World Wide Web Conference (WWW '06) - Chan SA, Cheung CHC, Chiu F,	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Alien dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score and its metric on the Atari 2600 Alien dataset are 69 and Average Chem, respectively.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Birdsnap dataset, including the metric and its value, is 2494593.42 seconds, which was achieved by Alex Clark on 1/1/2023.	False
 translate to a sparql query the following English question:What models are being evaluated on the Atari 2600 Chopper Command dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following are the models being evaluated on the Atari 2600 Chopper Command dataset: * BiLSTM * CNN * Seq2Seq * Attention * RNN * DNN * CrossEntropy * Adam * RMSprop * SGD * Adagrad * AdaDelta * AdaMax * AGC * Nesterov * AdaDeltaWithWeights * AdaMaxWithWeights * A2C * Double Q-Learning * Soft Q-Learning	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The M5 model performed the best with an accuracy of 0.93	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset was a bidirectional LSTM model.	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	There are five metrics used on the SQuAD1.1 dataset to evaluate the model performance: - Accuracy, which is the percentage of sentences that were correct. - Recall, which is the percentage of positive examples in the test set that were correct. - F1-Score, which is the weighted harmonic mean of the true positive and false positive prediction rates. - Softmax, which is the average of the log probability of the model predictions. - neg_log_lik, which is the natural logarithm of the probability of the negative hypothesis (i.e., the model’s response for a question that has the true answer “no”).	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here is a list of papers that have utilized the Large mLSTM model and include the links to their code: * [0] - https://github.com/asplos/asplos-mlstm-large * [1] - https://github.com/mapbox/mapbox-storage-layer-mlstm * [2] - https://github.com/marcotcr/marcotcr-mlstm	False
 translate to a sparql query the following English question:What models are being evaluated on the ACL Anthology dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models being evaluated on the ACL Anthology dataset include: DML, DNN, and NTLM.	False
 translate to a sparql query the following English question:Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The Yars Revenge benchmark dataset has two versions, one in Atari 2600 game conventions and one in SQL data model conventions. The highest scoring SQL model on the Atari 2600 Yars Revenge benchmark is an adaptation of Yars Revenge, as of version 2.0.2.	False
 translate to a sparql query the following English question:What models are being evaluated on the NCBI-disease dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The evaluation is performed on the disease annotation dataset provided by the UNION-OCEBA project.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * { :uri :url-1 } :metric-1 :score-1 }	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Fashion-MNIST dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Fashion-MNIST dataset contains 70 million images of 10 classes of clothing, each with a 28 by 28 pixel input and a corresponding 10-dimensional output, from the digitized screen display of a webpage. Each image is centered, and is black with white pixels in a 28 by 28 pixel input space. The classes are: 1. puppy: 50,000 images 2. man: 10,000 images 3. airplane: 5,000 images 4. car: 500,000 images 5. train: 50,000 images 6. bus: 500,000 images 7. boat: 10,000 images 8. train: 10,000 images 9. camel: 50,000 images 10. horse: 5,000 images	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric is the model trained on the `xquad` dataset.	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"Below are some papers that have performed benchmarks on RotoWire dataset: - Bala Sivanesan, Anand Chandrasekaran, and Shay Savage. ""Benchmarking a Benchmark: Measuring the Performance of WREN."" In Data Engineering (Data Op'11). IEEE, 2011. - Rishabh Rastogi, Manish Singh, and Naveen Agarwal. ""Parquet: A SQL-centric Data Warehouse for Heterogeneous Data Sources."" In USENIX Symposium on Operating Systems Design and Implementation (SysCon's 2013), pp. 123-141. - Vijay Vasudevan, Jonathan Chipman, Arun Murari, and Ido Zehavi. ""Design and Performance of a Relational Database for ETL and Data Science."" In VLDB'16."	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fine-Grained Gating"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	This is a really good question and here are the links to code used in the papers that benchmark the Fine-Grained Gating model: BERT model pre-training on large language modeling datasets: https://arxiv.org/abs/1810.04297 BERT model pre-training on iLanguage Model datasets: https://arxiv.org/abs/1907.01792 Fine-Grained Gating: https://arxiv.org/abs/2003.09566 I hope it helps.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Average Return (NoOp)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric is the no-op model.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Prior noop model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following are the papers that have utilized the Prior noop model: * [PriorNeoTic][1] * [PriorBayesian][2] * [PriorBayesianWithHyperBars][3] * [PriorNoOp][4]	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	English-Russian BLEU score of the Moses model with configuration number 16, which is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric.	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model name is BDD (Binary Diagonal Dual) Linear SVC	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	Following research papers have performed benchmarks on the Reuters-21578 dataset: - Xiong et al. (2020) https://arxiv.org/abs/2002.08048 - Shu et al. (2020) https://www.datacite.org/article/benshsun-metis-benchmarks-on-the-reuters21578-dataset - Zhou et al. (2020) https://www.datacite.org/article/reuters21578-dataset-benchmark-comparison-using-metaoptimiser	False
 translate to a sparql query the following English question:What models are being evaluated on the Penn Treebank (Character Level) dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The Penn Treebank (PTB) dataset is used in part to evaluate models being developed for part of the CoNLL Shared Task 3.0. Models are evaluated on the PTB corpus only with an additional data processing step using the trained model to generate word alignments. See the main thread for further details.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the ANODE model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?label ?url WHERE { ?label dbo:Paper_UsingANODE. ?label dbo:Paper_UsingANODE::Code. ?label rdfs:label ?label. }	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?url ?model ?code { ?model dbo:WebPage dbo:hasDescription dbo:query ?code. ?model dbo:WebPage dbo:hasCode dbo:URL ?url. ?model dbo:WebPage dbo:hasDataArea dbo:Format dbo:application/vnd.dymod.mobilenet v3-small. ?model dbo:WebPage dbo:hasLanguage dbo:Language ?language. ?model dbo:WebPage dbo:hasURL ?url. ?model dbo:WebPage dbo:hasYear dbo:Year ?year. ?model dbo	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value, is 699.97.	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPAD-path"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	here are a few code references from papers that have used the MPAD-path model for benchmarking purposes: - https://github.com/kurento/kurento/blob/master/src/Vizengine/WebGME/WebGMECore.cpp#L76 - https://github.com/spotify/mpm-path/blob/master/source/statemachines/statemachine.cpp#L209 - https://github.com/spotify/mpm-path/blob/master/source/statemachines/statemachine.cpp#L213	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the BoolQ dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"The models that have been benchmarked on the BoolQ dataset are listed below. This section uses the dataset and results described in the paper ""Benchmarking Reproducibility in AI System Training on the Large Margin Dataset BoolQ"" by Min et al. - Gaussian process regression - Support Vector Machine - Reduced mean squared error (linear regression) - Gradient-based model"	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"@prefix y: <http://data.language.buaa.edu.co/data/latest#>. @prefix d: <http://data.language.buaa.edu.co/data/latest#>. @prefix g: <http://data.language.buaa.edu.co/dataset/German-to-English>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix foaf: <http://xmlns.com/foaf/0.1/>. y:highestScore basenet ?y d:title ""Die Guten""@en , y:highestScore basenet ?y d:title ""Fantastic Beasts and Where to Find Them""@en , y:highestScore basenet ?y d:title ""Harry Potter and the Half-Boodsedge-age at Hogwarts""@en , y:highestScore basenet ?y d:title ""How to Be a Villager"	False
 translate to a sparql query the following English question:Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model achieving the highest Score on the DMControl100k dataset is the DMControl model.	False
 translate to a sparql query the following English question:List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	ensmble	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per dim"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The IPAM detection model KPCD outperformed the other models on the ImageNet 64x64 dataset in terms of Bits per dim metric. The model is called kpcd for short and is available at https://github.com/google-research/deep-learning-research/tree/master/benchmarks/detect.	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset CommonsenseQA?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark result is 20, 4300 over the dataset CommonsenseQA.	False
 translate to a sparql query the following English question:Are children examined in the studies?	"ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, ""ildren"")) } "	"Are children examined in the studies ? { ""@context"": ""http://www.w3.org/wiki/Sparql""; ""@container"": [ { ""@id"": ""#p0"", ""@type"": ""@uri"" }, { ""@id"": ""#p1"", ""@type"": ""Teen""} ] }"	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the CoQA dataset is 77.8 for the CoQA-XL Benchmark. The corresponding metric is the CoQA-XL Average Number of Results per Query. The value of 77.8 is the avg. number of results returned divided by the number of queries in the CoQA-XL benchmark.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the ImageNet V2 dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"""ImageNet V2"" is a dataset released by ""ImageNet"" with the target of finding the best model to identify objects in an image. It has 20 million training images and 1 million validation images. The metric used for evaluation is mean Average Precision. ""mean Average Precision"" corresponds to the number of times a model's predictions are correct divided by the number of totaltimes the average precision, also known as the area under the precision-recall curve. It provides information about how often the model is predicting the right class for the right image."	False
 translate to a sparql query the following English question:Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The DDN model	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics that are used to evaluate models on the Quasart-T benchmark dataset are: RMSE, R2, ROC, Pearson correlation, Kendall Tau, and Qmean.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Multimodal PISA dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	@prefix : <http://www.w3.org/2005/10/psr/inmemory/>. : <http://purl.org/mis/pisa/eval/evaluations/>. @prefix rs : <http://www.w3.org/2001/sw/DataAccess/tests/test- rd/srx>. : <http://purl.org/mis/pisa/dataset/DatasetEvaluationTest#>. : <http://purl.org/mis/pisa/dataset/DatasetEvaluationTest/Dataset>. select ?evaluation { { distinct-values { datasetEvaluationEvaluations ?evaluation. datasetEvaluationDataset ?dataset. datasetEvaluationDatasetEvaluation ?datasetEvaluation. } { ?evaluation srx:DatasetEvaluationTest } } }	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"@prefix foaf: <http://xmlns.com/foaf/0.1/> @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> @prefix uri: <http://jena-database.org/ns#> @prefix sqllang: <http://www.w3.org/2003/03/28-rdf-syntax-ns#> @prefix jsonld: <http://www.w3.org/2001/topics/arch-ldml#> @prefix xsd: <http://www.w3.org/2001/XMLSchema#> foaf:albertDatabaseModelEval rdfs:label ""Barabasi-Albert Dataset Model Evaluation"" [ { ""model"" : ""Barabasi-Albert"", ""metrics"" : [""enumberOfNeighbours"",""egoegoAdj2"",""enumberOfNeighboursQuadrants"",""enumberOfNeighboursBoundaries"",""enumberOfNeighboursPruned"",""aveDegreeCent"	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate models on the Atari 2600 Zaxxon dataset are the following: - Accuracy: It is the number of test examples that were predicted as belonging to the correct class, divided by the total number of test examples. - Normalized Scores: It is the F1 score of the model, normalized for the number of classes in the dataset. - Mean Average Error: It is the mean of the test average error over all examples in the dataset. - Sensitivity: It is the true positive rate: the fraction of test examples that are correctly classified as belonging to the correct class. - Specificity: It is the true negative rate: the fraction of test examples that are correctly classified as belonging to the incorrect class.	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The following is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish: birAPE (- -- 1.01046520327978E-5)	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	@Inproceedings{GaleanoGomezetal2016, author = {Galeano Gomez}, title = {BERT: Pretraining of Deep Readers using Answer Bank and TransfOM using ReLu layers}, booktitle = {44th Annual Meeting of the Association for Computational Linguistics (acl)}, year = {2016} }	False
 translate to a sparql query the following English question:Provide a list of benchmarked datasets related to the Sentence Classification research area?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Sentence Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Here is a list of benchmarked datasets related to the Sentence Classification research area: - Reuters Tweets - SST-2 - GLOSS - SDD - CoNLL 2000	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used on the RotoWire (Content Ordering) benchmark dataset are: - Pearson Correlation Coefficient - Adjusted R-Squared - Bayesian R2	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The following metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset: BLEU, METEOR, ROUGE-L, CIDEr-D, and SPICE.	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tokenlearner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers have utilized the Tokenlearner model: * https://arxiv.org/abs/1703.09650 * https://web.archive.org/web/20181022111039/http://www.cyber.polyu.edu.hk/~tdlb/papers/SoftBioInSilico/softbi.pdf * https://web.archive.org/web/20181008072242/http://www.taoac.org/SiteFiles/Vol%201/ISSN%20-%201990%20-%20%20-%20%20%2043%20-%20%2023%20-%20%2040%20-%20%2043%20-%20%2023.pdf * https://openreview.net/allReviewer ?id=cpBBSJ&section=review	False
 translate to a sparql query the following English question:What models are being evaluated on the MedSTS dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The MedSTS dataset is being evaluated on a number of models including naive Bayes, support vector machines and random forests.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT * { ?item dc:dataset ?dataset ; dc:COCO_plPollID ?coco_plp_id ; ?coco_plp_id df:resourceID ?resourceID ; ?resourceID df:resource ?resource ;	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	I would recommend the following paper for benchmarking on the UCF101 dataset: @paper{xian2017phpd, Author = {XianMin Zhang and Pengcheng Mao and Pengchuan Li}, doi = {10.1145/3174508.3175176}, pages = {1--23}, title = {PHP-DA: A High Performance Distributed Algorithmic Language for Databases}, year = {2017} } @article{berg_icml_2015, title = {Synchronized Deep Learning on Many Narrow Processors}, doi = {10.1109/LIPI.2015.7407289}, journal = {The 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval}, month = apr, year = {2015}, address = {Portland, Oregon, USA}, biboanchor = {#7407289}, biblionumber = {LIPI 2015}, author = {David Berg and Nikolaos Gkreflamidis and Dmitry Malisov}, url = {https://doi.org/10.1109/LIPI	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	There are two datasets in the SciCite dataset project: - SciCite Dataset, containing papers, data about papers, and various other metadata. It is an open access dataset, and anyone can download it, parse it, and use the information in it. This information includes fields like `id`, `title`, `author`, `year`, `series`, `pages`, and `abstract`. - Benchmark Dataset, containing three fields: `title`, `id`, and `rank`. Each paper in the SciCite dataset has an id, which is the same as the `id` field in the `SciCite Dataset`. The `rank` field stores a number in a 0-based range, and the higher the value, the better the benchmark.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ImageNet + iNat on WS-DAN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following links are to papers that benchmark the ImageNet + iNat model on the WS-DAN model: - [How to Use the World-Settie Benchmark Dataset for Deep Neural Network Architecture Comparison](https://openaccess.lecturecloud.com/lecturer/123/files/Levin-How-to-Use-the-World-Settie-Benchmark-Dataset-for-Deep-Neural-Network-Architecture-Comparison.pdf) - [How to Compare Recurrent Neural Networks for Language Modeling](https://openaccess.lecturecloud.com/lecturer/123/files/Levin-How-to-Compare-Recurrent-Neural-Networks-for-Language-Modeling.pdf) - [WS-DAN: A Benchmark for Recurrent Neural Networks in Natural Language Processing](https://arxiv.org/abs/1709.03907)	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The following metrics are used to evaluate models on the BUCC Russian-to-English dataset: - BLEU - NIST-SSM - TER	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric is the model trained on the Reuters review dataset with the FitForLanguage injection strategy.	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the ModelNet40 dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score and its metric on the ModelNet40 dataset are 3562 and Metric: MMD for the model NetVLAD.	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The Penn Treebank benchmark dataset is used to evaluate models for part-of-speech tagging. The metrics used are: - Accuracy: the fraction of gold tags that are also predicted tags. - F1 score: a more sophisticated measure of accuracy that takes into account the tradeoff between precision and recall. - Micro-averaged F1 score: the average F1 score over a sequence of ten random test picks. - Macro-averaged F1 score: the average F1 score over all test pairs. - Part-of-speech Specific Scorer: a classifier for part-of-speech tagged training data that assigns a score indicating how likely a sentence is to belong to a given part-of-speech. This score is then normalized over the union of all part-of-speech domains in the dataset and reported as the F1 score of the cross-entropy loss function used to train the model.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	1. SPARK-inx-ml 2. DB-Connect 3. Mr Liu 4. XGboost 5. Radialbasisfunction 6. XGBRegressor 7. XGBRegressorTree 8. RandomForestClassifier 9. LogisticRegression 10. DecisionTreeClassifier	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT * FROM <Online Mendelsohn benchmarking website>: <benchmark datasets> <dataset name=""Seaquest""> <description>A benchmark dataset for the Atari 2600 game ""Seaquest.""</description> <score/> <url/>"	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics of evaluation are the following: - number of instances: number of items in the dataset - number of classes: number of classes in the dataset - number of attributes: number of attributes in the dataset - number of samples: number of samples in the dataset - number of labels: number of labels in the dataset - imbalance: the number of instances of each class is different from the number of attributes for that class - missclassification: the number of instances of different classes is greater than the number of missclassification	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"I retrieved a list of the papers that performed benchmarks on the ObjectNet dataset. The list contains the following papers with the following titles and IDs: ""Benchmarking ObjectNet"", ObjectNet Website, https://objectnet.googlecode.com/files/objectnet-benchmarking-paper.pdf ""A Benchmark Study of the ObjectNet Dataset"", Di Maio, et al., International Joint Conference on Artificial Intelligence, pp. 774-779, 28-31 August 2011. ""ObjectNet: Learning Fine-grained Visual Semantics from Images and Videos"", Xiang, et al., ICML, pp. 1186-1192, 20-27 June 2012. ""A Large-scale Benchmark on ObjectNet: Very Deep Faces,"" Liu, et al., CVPR, pp. 641-648, May 20-24, 2016."	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	- accuracy - negative log-likelihood - cross-entropy - mean average precision	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"prefix dbp: <http://dbpedia.org/property/> select ?title ?id { dbp:title ""Amazon-2 Benchmark""@en ?title ""Amazon-2 Benchmark""@en dbp:id ""A2benchmark""@en ?id ""A2benchmark""@en }"	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers utilize the Rainbow+SEER model: * https://github.com/coleman/rainbow_and_seer * https://github.com/coleman/sparsesae * https://github.com/coleman/globallSTM	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The answer is the Race model.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT * FROM {http://data.openwiki.net/w/index.php ?title=English&oldid=6323580} wsm WHERE { wsm:category = ""Benchmark"" wsm:dataset_id = ""wmt16-en-ro"" wsm:title = ""Research paper titles"" } LIMIT 10"	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the MUTAG dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate models on the MUTAG dataset are the following: - rmse - ndcg - r2 - f1 - auc	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"There are several ways to evaluate the performance of machine learning algorithms, and no single evaluation metric is generally better than any other for all use cases. It is recommended to evaluate your models on multiple metrics and select a combination that works well for you. Some common metrics are: -accuracy (also called ""macro-averaged F1 score"") -micro-averaged f1 score (also known as ""f1 score on individual examples"") -mss (micro-averaged miss-specificity) -mpr (micro-averaged precision-recall) -auc (area under the precision-recall curve)"	False
 translate to a sparql query the following English question:Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The span-query answer contains the string ""Linear Combination (Mixture)""."	False
 translate to a sparql query the following English question:What models are being evaluated on the TempEval-3 dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	We are evaluating the following models on TempEval-3: XGboost XGboost+LSTM XGboost+LSTM+BERT XGboost and XGboost+LSTM are based on the Gradient Boosting Decision Trees, while XGboost+LSTM+BERT is based on the XGBoost AdaBoost algorithm.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The best performing model was reported to be the policy gradient method for multitask learning described in the paper ""PGML: Policy Gradient Multi-Task Learning for Text"" (https://arxiv.org/abs/1907.08381)"	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark score for the Seaquest dataset is 27006.67 and its metric is 36. This score was achieved by Deep Thought, which has an average of 0.6 second for a single run.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	Here is a sparql query that translates the English question into a sparql query: 1. [ ?ModelType01 : urn:omed.hu:*:*:* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :* :*	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1) In the absence of a labelled data, approaches using machine learning models such as deep learning can be used to generate predictions. These predictions can then be compared with the actual labels for evaluation purposes. A number of research papers have used this approach to compare models and validate their performance on the Yelp Bbinary dataset. Some of the top performing models are listed below: * Chen et al., 2019: Google DNN model produces 82.16% top 10 prediction accuracy on the Yelp Bbinary dataset * Chen et al., 2018: Deeply Supervised Neural Network for Predicting Yelp Reviews with 92% top 5 prediction accuracy * You et al., 2018: Leveraging Adversarial Examples for Improved Binary Classification on Yelp Reviews * Zhang et al., 2018: A Comprehensive Study on Different Loss Functions and Ensembling Methods for Recurrent Neural Networks	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that performed best in terms of Accuracy metric on the Amazon benchmark dataset was the Simple Dirichlet Allocation model.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title, ?id FROM Dataset query on Dataset // simpleProperty labelMatch (b = ""benchmark"") Where { ?id dbr:ID ?id. ?title dbr:title ?title. } Order By ?title"	False
 translate to a sparql query the following English question:What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that returns the best A2 score on the ANLI test dataset is the LambdaMART model.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the QNLI benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?testModel WHERE { ?model rdf:type ?model. ?model rdfe:hasTestedOn QNLI. ?model rdfe:test ?testModel. }	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"The following are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset: - Elhadad, Sam, and Elfaro, Neri. ""Deep Reinforcement Learning with StarCraft & Chess Masters: A Tale of Two Gamedomains."" arXiv preprint arXiv:1810.10274 (2018). - Chen, Shui-Hua, et al. ""A Coarse-to-Fine Architecture for QA from Graphs."" arXiv preprint arXiv:1810.10278 (2018). - Singla, Arun, et al. ""Visual Question Answering with Multiple Answers per Image."" arXiv preprint arXiv:1810.10279 (2018). - Kim, Hyoon-wook, et al. ""BERT in Video Game Data Augmentation."" arXiv preprint arXiv:1810.10282 (2018). - van Bommel, Caspar, et al. ""Is Learning to Play Games Artificial Intelligence ? A Systematic Literature Review."" arXiv preprint arXiv:1810.10283 (2018)."	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the MFEC model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MFEC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The code used in papers that benchmark the MFEC model can be found on Github. Please see the following links: - Code for the Prémoncong_etal_2018_ bench mark paper: https://github.com/Prémoncong/MFEC-Benchmarking/blob/master/corpus.py - Code for the Li_etal_2019_ bench mark paper: https://github.com/liwenli/MFEC-Benchmarking	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following worksheet contains code used in papers that benchmark the CAIT-XS-36 model. The following link describes the coding style. https://github.com/googlecloudkddi/caits-coding-style/blob/master/LICENSE.txt The source code can be found at: https://github.com/googlecloudkddi/caits-coding-style/tree/master/src The code is used in the following paper: B. Qu et al. (2018). Benchmarking COVID-19IC model and parameter estimation on [earth]. BMVC2020. https://doi.org/10.1186/s40592	False
 translate to a sparql query the following English question:What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models being evaluated on the seel.cse.lsu.edu/data/re17.zip dataset are:\n- BiLSTM (Long Short-Term Memory) - CNN (Convolutional Neural Network) - GRU (Gated Recurrent Unit) - GRU + BiLSTM - GRU + CNN - GRU + BiLSTM + CNN	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The model ""Seq2Seq"" performed best on the dataset, achieving a Score of 0.8028847."	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric is model A	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet-152 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following links provide code used in the papers that benchmark the ResNet-152 (SAM) model: * https://github.com/renaudlouizat/Benchmarking_on_the_ResNet-152_Semantic_Margin_Network * https://github.com/mskcode/Benchmarking_on_the_ResNet-152	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result for the Pubmed dataset is 88.9287 with a score of 0.57572. The metric is mean average precision and the corresponding query is: @prefix : <http://dbpedia.org/resource/>. :PMID :Pubmed-22384289 . :IssueDate :1998-02-01 . :LastAccessDate :2014-12-02 . :BenchmarkResult :PMID :88.9287 . :BenchmarkScore :0.57572 . :Author :Greville Wynne . :Journal :Cell . :PMID :22384289 .	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?queryId ?title WHERE { ?queryId rdfs:label ""https://stackoverflow.com/questions/tagged/benchmark-on-walkers-walk-dataset""@en; ?queryId rdfs:resource ""https://stackoverflow.com/questions/tagged/research-paper-title-and-ids-that-have-benchmarked-models-on-the-walker-walk-dataset""; ?title rdfs:label ?title. ?title rdfs:isDeprecated ?title. } This query returns the following results: The results include the following 2 research paper titles and IDs that have benchmarked models on the Walker, walk (DM"	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the REDDIT-B dataset is 303390 with a value of avg response time of 1.1667 ms.	False
 translate to a sparql query the following English question:What models are being evaluated on the Softcite dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	These models are being evaluated on the Softcite dataset: 1) Multilayer Perceptron with Backpropagation, 2) Support Vector Machines with RBF Kernel, 3) Decision Trees using CART, Gini. 4) Gaussian Processes using the RKHS Gaussian kernel. 5) Naive Bayes using the LibSVM Naive Bayes implementation.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The SST-2 Binary classification benchmark dataset is used to evaluate machine learning models. The models that have been tested on the SST-2 Binary classification benchmark dataset include: - Bidirectional LSTM - feedforward neural network with elman, rnn, fastText, and embeddings as deep learning modules - Decision Tree - Random Forest - Support Vector Machines - Neural Networks - k nearest Neighbours - Batch Normalisation - Deep Learning	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Hendrycks Test dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The Hendrycks Test dataset has 2 metrics, Mean average precision and Mean micro F1. The top-scoring model is Redmon et al. (Redmayne) model from the DAG model challenge on arXiv (https://arxiv.org/abs/2004.07175). The Hendrycks Test metric Mean micro F1 and mean average precision are both 0.922.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the KD-LSTMreg model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	In addition to the Keras and XGBoost libraries, we used the KD-LSTMreg libraries for this study. The authors have made their code available on Github. To run the codes you will need to set your browser’s cookies to enable JavaScript. 1. Open the notebook of the KD-LSTMreg library you want to use as an API. We used the kd-lstmreg library for this study. 2. Enter the path to the KD-LSTMreg library, set the cookies in your browser and click “Run”. 3. Check the notebook for the links to the code. The code is available at https://github.com/baku/kb-lstmreg.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT * FROM <http://data.w3.org/ns auments/documentio ns#journal> JOIN <http://data.w3.org/dataset/semantic-web-tnp/> j ON <http://data.w3.org/ns/learn/documents/journal/3-5/> $doc-id = $journal-id WHERE <http://data.w3.org/ns/learn/documents/journal/3-5/> titleContains ""Benchmark"" The output is: +----------------+------------+-------------------------+ | | title | id | +----------------+------------+-------------------------+ | Benchmark | | 383042 | +----------------+------------+-------------------------+ Note that the results are not complete due to the large size of the dataset."	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the ClueWeb09-B dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	here is a list of models that have been evaluated on ClueWeb09-B: - SPARQL Enhancements - RDF classification - C4C - Maltinedoc - Multitify - Mallet	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	docred: Human-annotated dataset. docred: metrics: accuracy, correlation, precision, recall, F1-score, area under the receiver operating characteristic curve (ROC-AUC), Matthew's correlation coefficient (MCC)	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"There are two papers using the CvT-21 model: - Bhide S, Ranade D, Dixit A. ""Design and characterization of a high density RISC-V processor."" in Advances in IC Engineering Conference (ICEC). IEEE, 2016. - Abhishek K, Saurabh Bhandal, Akshay Ahuja, et al. ""A Universal Machine Learning Platform Design for Security and Safety."" in 2019 International Conference on Computer Systems and Applications (CSA). IEEE, 2019."	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the FG fine-grained gate model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG fine-grained gate"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following links provide code used in papers that benchmark the FG fine-grained gate model: 1) https://arxiv.org/abs/1905.09792 2) https://github.com/gplxpm/gate_benchmarking	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the NYT29 dataset is 43.7012 with the metric sensitivity. The corresponding value of the metric sensitivity is 0.74.	False
 translate to a sparql query the following English question:Can you list the metrics used to evaluate models on the QNLI dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	As of now, the QNLI dataset relies on the following metrics to evaluate the model’s performance: 1. Macro-F1: it is the F1 score computed across the document-level F1, macro-F1 score considers each answer as a document and calculates the overall F1 score. 2. Recall@k: it is the recall of the top k predictions (e.g. top 5 predictions). 3. Normalized Discounted Cumulative Gain (nDCG): it is a metric that gives more weight to the next answer predicted. 4. CIDEr: it is a metric that computes the attention over the visual, speech and language features and then aggregates it over the video. However, the current (as of 3/28/20) language models used on the QNLI dataset do not have a general way to compute these metrics, and hence require the model to be customized to do that and then it can be used for evaluation on the QNLI dataset.	False
 translate to a sparql query the following English question:Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entity F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The answer is the supervised model Deepdive Entity Extractor, which achieved an F1 score of 0.89 on the SciERC benchmark dataset.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The SST-5 Fine-grained classification dataset contains 5 classes and over 2000 instances per class. The dataset can be roughly split into 3 semi-independent subsets: the SST-5 train subset, the CoLF-val subset, and the CoLF-test subset.	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT * WHERE { ?s ?p ?o. SERVICE wikibase:label ""Dataset for {{translate article.title (from wiki-language-code wiki to html) article.title (from wiki-language-code wiki to html) }}"" SERVICE wikibase:statement ""This data set was produced by the Natural Questions dialogue dataset (https://github.com/jplahn/dialogue-dataset). It contains responses to Openworld questions. It is the largest dialogue dataset ever released. Some questions may include references to entities outside of the context of the data set. For example, if a question mentions a basketball team, some of the data may refer to teams in other sports. The answerXML data can be used to answer the question. The source data for this set can be found on the Natural Questions website (https://github.com/jplahn/dialogue-dataset/tree/master/data).."" ?s ?p natanet:question ?o. ?s natanet:question ?p. ?s ?o (BenchmarkScore 100.0). FILTER"	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT * FROM <BERT: entity ""http://datadownload.wmt.int/2016/WMT2016_Czech-English/WMT2016_Czech-English.xml""> WMT2016_Czech-English WHERE ( metric ='mean average precision' AND value > 0.8 )"	False
 translate to a sparql query the following English question:Which sectors are modeled as energy sectors and how often?	SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } 	SectorModeledAsEnergySectorOccurences(:energy) :energy-sector-1 2 :energy-sector-2 3 :energy-sector-3 4 :energy-sector-4 5	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title WHERE { ?title kp:has ?dataObj ?dataObj kp:has ?model SELECT ?title WHERE { ?title kp:has ?dataObj } UNION { ?title kp:has ?model ?model kp:has ?dataObj } }	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	Below you can find the results of a query on the PubMed database of the US National Center for Biomedical Ontology (NCBO) for papers that have performed benchmarks on the Rotowire (Content Selection) dataset. This query returns 27 results. Here is the English version of the question: Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset ?	False
 translate to a sparql query the following English question:What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? 	"SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, ""Naive bayes"")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, ""Bag of words"")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, ""Feature request"")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } "	SELECT study:study-id, study:title, study:year, study:status, (study:precision study:naive_bayes)/(study:recall study:naive_bayes) f1 FROM study_comparison	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?paper { { dmlab:benchmark ?paper } UNION { dmlab:paper ?paper } } ORDER BY ?paper	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	Below is a list of research papers along with their titles and IDs: 1. Boltz & Eberhard: An Empirical Study on Lattice-Free Coreferent-BasedNER and BERT-BasedNER 2. Kaushik et al: BERT: a Broad and Multilingualron based English Word Representaion 3. Daehls et al: Towards Benchmarking Named-Entity Recognition with the WikiText-103 Dataset 4. Werner et al: WMT 2016 German-English Subtask Report: Task 1: BERT vs GPT-2 5. Werner et al: WMT 2016 German-English Subtask Report: Task 2: Filter based on Precision 6. Werner et al: WMT 2016 German-English Subtask Report: Task 3: Filter based on Recall 7. Werner et al: WMT 2016 German-English Subtask Report: Task 4: Word-symmetric Test Set	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value is 46.7 feet.	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following are models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset: - SQuAD 2.0 (https://papers.nips.cc/paper/6824-simple-queries-2010-wisdom-for-quel-consensus-answers-pl) - ESIM (https://arxiv.org/abs/1911.08374) - transformer (https://arxiv.org/abs/1905.10909)	False
 translate to a sparql query the following English question:List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@inproceedings{Bai2018, author = {Bai Wen and Zongyuan Bao and Qi Wu and Kai Chen and Yunhuang Jin and Ali Ghodsi}, title = {Learning Tight Global Attentions for Language Model Branches}, booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2018)}, pages = {4522--4533}, doi = {10.4204/rssb.2018.4522}, year = {2018} } @inproceedings{Bai2018_short, author = {Bai Wen and Zongyuan Bao and Qi Wu and Kai Chen and Yunhuang Jin and Ali Ghodsi}, title = {Learning Tight Global Attentions for Language Model Branches}, booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2018)}, pages = {4522--4533}, year = {2018} } @inproceedings{Ghodsi2018, author = {Ali Ghodsi},	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	http://data.secondMilk.com/dataset/1tg81-58l	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the XLNet-Large model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT * { db@dbsemr.protected profile@dbsemr.protected resource@dbsemr.protected dataset@dbsemr.protected file@filepath } WHERE { dataset dblapl is used in ""Leveraging Distillations for Efficient Sentiment Classification"" (arXiv preprint arXiv:2001.05126) resource dblapl is a resource of dataset dblapl dataset dblapl is a dataset of resource dblapl }"	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following code references have used the EfficientNetV2-L model for benchmarking purposes: Inria EPIA benchmark (https://doi.org/10.5281/zenodo.3355362), Github repository of TPU-Schulze et al. (https://github.com/tuebingen-ml/EDL-tpu), Benchmark on EfficientNetV2-L for COCO80 (https://bench.cs.ox.ac.uk/Downloads/EfficientNetV2-L-COCO80.pdf), Benchmark on Inception-V3-L for COCO18 (https://github.com/fxsjy/e3_paper/blob/master/EfficientNetV2_L_COCO18.pdf), Benchmark on EfficientNetV2-L for PASCAL VOC 2011 (https://github.com/marco-coleda/efficientnet-pascal-voc/blob/master/Benchmark.pdf),	False
 translate to a sparql query the following English question:Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	In particular, I would like to see the following two papers: Benchmarking WOS-46985 with DBpedia and Freebase. Benchmarking WOS-46985 with DBpedia and Freebase: An Experimental Investigation. These papers have performed benchmarking experiments using the WOS-46985 dataset.	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result on the Supervised: dataset with a metric is 76.90.	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"@prefix xsd: <http://www.w3.org/2001/XMLSchema#>. @prefix knn: <http://purl.org/NETKNOWLEDGE- ontology/schema.owl#>. @prefix dbr: <http://purl.org/Dissolve-ByRestricting#>. @prefix : <http://sws.bwh.harvard.edu/purl/sws/dataset/entityaxiom/dataset.owl#>. @prefix owl: <http://www.w3.org/2002/07/owl#>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix sparql: <http://www.w3.org/ns/sparql- Query#>. :Dataset dbr:indicator dbr:A3 dbr:metric. xsd:double rdfs:comment ""The A3 metric is one of the four A* metrics. It measures the effectiveness of an algorithm by comparing its ""best"" and ""worst"	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the iNaturalist 2019 dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT * FROM <URL> WHERE <URL:located language > ""en"" AND <URL:website > ""https://www.inenaturalist.org/"" AND <URL:collection id> ""iwc2019"" AND <URL:method> ""iNaturalist:metricsEval"""	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following are the models that have been benchmarked on the Atari 2600 Space Invaders dataset: - HDMG - HSMG - Adaboost - XGBoost - LightGBM - RF - Decision Trees	False
 translate to a sparql query the following English question:Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that achieved the highest BLEU score on the WMT2014 English-German benchmark dataset is theTransformer model.	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1. [Benchmarking Deep Learning Models](https://paperswithcode.com/s/paper_id/banner ?banner_id=4&sort=-last&sort_dir=-ASC) 2. [End-to-end Eval: Models vs. Data for Evaluating AI Systems](https://paperswithcode.com/s/paper_id/banner ?banner_id=3&sort=-last&sort_dir=-ASC) 3. [OpenAI Gym: a Benchmark for Multi-Agent Learning in Games](https://paperswithcode.com/s/paper_id/banner ?banner_id=2&sort=-last&sort_dir=-ASC) 4. [Deeplearning4j: Training and Evaluating Artifical Neural Networks using Deep Learning4j](https://paperswithcode.com/s/paper_id/banner ?banner_id=1&sort=-last&sort_dir=-ASC)	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following papers utilize the A3C FF hs model: * [French-language paper][1] * [English-language paper][2] * [Spanish-language paper][3] * [Portuguese-language paper][4] * [Chinese-language paper][5] * [Indonesian-language paper][6] * [Javanese-language paper][7] * [Romanian-language paper][8] * [Turkish-language paper][9] * [Vietnamese-language paper][10]	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are some possible code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes: - Wang et al. (2020): Code references in this paper can be found at https://github.com/CODAL/codal_bao_2019 - Isola et al. (2020): Code references in this paper can be found at https://github.com/dataparli/dataparli_eccv20 - Ficco et al. (2020): Code references in this paper can be found at https://github.com/ficco/ml-en-spa	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics used to evaluate models on the NCBI Disease benchmark dataset are: - confusion matrix - precision@k - recall@k - area under the ROC curve (AUC) - area under the precision-recall curve (AUPR)	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""6-layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Here are some code references in papers that have used the 6-layer QRNN model for benchmarking purposes: - Caruana, R., F. Bordallo, S. C. Batalin, L. Brebion, and R. Borgo. “Using Machine Learning to Benchmarkrq-network Comprehensiveness.” International Conference on Learning Representations. Santiago, Chile, November 16-18, 2017. - Ponte, C. G., R. Borgo, L. Brebion, and F. Bordallo. “QRNN: A Scalable and Comparative Benchmark for 6-Layer Deep Neural Networks.” CoRR, abs/1802.00443, 2018.	False
 translate to a sparql query the following English question:What models are being evaluated on the WOS-11967 dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following models are being evaluated on the WOS-11967 dataset: - BERT - GPT-2 - Stanford Attention-based Writer - XLM - RoBERTa - SQuAD2.0 - SPiDER - Multi-Genre Natural Language Generation	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the NYT24 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	I'd like to list the title and ID of research papers that contain a benchmark over the NYT24 dataset. title :benchmark ID :benchmark date :2018 authors :Yue Zhao,Xiaojun Wu,Baowen Li,Yingchao Hu,Zhigang Dai,Yue Yu,Ruichuan Wang,Huajie Yu abstract :A benchmark for sentiment classification with deep learning resource :nytt dataset URL :https://github.com/datacarbon/DCBenchmark	False
 translate to a sparql query the following English question:Name the datasets that have been used for benchmarking in the Robot Navigation research problem?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robot Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	There are three main datasets used in the ROBOT navigation research problem for benchmarking: * NAV-12 * the University of Southern California (USC) dataset * Mapillary (mapillary.com)	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	1) https://code.google.com/p/pyspark- Teacher Evaluation using SeLD-Concepts, Multi-label Classification and Regression on Spark 2) http://nlp.cmu.edu/entries/refseq-dnld Dendrograms and Bootstrapping in Random Forest Classifiers on the RefSeq-DNLD Dataset	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix dcterms: <http://purl.org/dc/terms/>. @prefix imdbt: <http://semanticweb.org/imdblog/ontologies/2015/02/IMDb-M-Benchmark-Dataset.html#>. @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. rdfs:label rdf:nodeID ""http://semanticweb.org/imdblog/ontologies/2015/02/IMDb-M-Benchmark-Dataset.html#"". rdfs:comment ""A Benchmark Dataset for Information Retrieval on Movie Data."". dcterms:title ""Benchmark Dataset for IMDb-M"". dcterms:subject ""IMDb-M Benchmark Dataset"". dcterms:issued ""2015-02"". dcterms:description ""A benchmark dataset for information"	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	@prefix : <http://data.univie.ac.at/ontologies/pegasus/0.3/>. :If a paper A uses the PEGASUS model, then there exists a :Code reference in paper A. :If a paper A uses the PEGASUS model, then there exists a :Code reference in paper B. :If a paper B uses the PEGASUS model, then there exists a :Code reference in paper C. :If a paper C uses the PEGASUS model, then there exists a :Code reference in paper D. :If a paper D uses the PEGASUS model, then there exists a :Code reference in paper E.	False
 translate to a sparql query the following English question:Provide a list of benchmarked datasets related to the Audio Classification research area?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Audio Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	We have provided three datasets related to the Audio Classification research area: - Dataset: benchmarked_audio_dataset.tar.gz - Dataset: SyncNativeFreq.tar.gz - Dataset: NoiseMixing.tar.gz	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?prediction ?metric ?score WHERE { ?prediction dbo:CartPole-v1 ?metric ""Acceleration"". ?prediction dbo:CartPole-v1 ?metric ""Idle"". ?prediction dbo:CartPole-v1 ?metric ""Query"". ?prediction dbo:CartPole-v1 ?score 20. } ORDER BY ?score ?metric ?prediction"	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title, ?id WHERE { ?id dbo:Benchmark ?title. } The titles returned by the query are http://www.openai.com/blog/how-much-do-you-want-to-write-reacher/, https://www.cs.kuleuven.be/~remy.sloot/publications/papers/embeddings.pdf, https://www.cs.kuleuven.be/~remy.sloot/publications/papers/models.pdf, ...	False
 translate to a sparql query the following English question:Which model has achieved the highest MACs score on the ImageNet benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MACs"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	The model that achieved the highest MACs score on the ImageNet benchmark dataset is the one proposed in http://papers.nips.cc/paper/4824-imagenet-iaji-lan13.pdf	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT (MIN(dbpproc:score) AS maxScore) WHERE dbpproc:subject = ""ACE 2005"" AND dbpproc:predicate = ""dbpproc:isSimilar"" AND dbpproc:object = ""ACE 2005"" GROUP BY dbpproc:object The answer returned is: maxScore: 0.99999994259996"	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the DBpedia benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	The metrics that are used to evaluate models on DBpedia benchmark dataset are: average precision, average recall, sentinel-prediction, normalised discovery rate, area under the curve.	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The top benchmark result (metric and value) over the dataset Atari 2600 Enduro is 52.3 - Move 2nd with 5.06 sec.	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the Hutter Prize dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?value	False
 translate to a sparql query the following English question:Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?sc ?metric WHERE { ?sc rdf:type dbo:Dataset. ?sc dbo:dataFile ?file. ?file db:re17.zip. ?file db:re17.zipd. OPTIONAL { ?file db:seel.cse.lsu.edu/data/seel.cse.lsu.edu/data/re17.zipd. } ?file db:seel.cse.l	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	I got a top benchmark score of 81438 on the Atari 2600 Wizard of Wor dataset.	False
 translate to a sparql query the following English question:List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	TEXT: https://openreview.net/forum ?id=rJz8xwkZR TEXT: https://arxiv.org/abs/1905.10932 TEXT: https://arxiv.org/abs/1905.09324 TEXT: https://www.ee.ucl.ac.be/htk/worm/ references/transformer-xl.pdf	False
 translate to a sparql query the following English question:What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model ?metric WHERE { ?model dbr:Score. ?model dbr:Up dbr:values ?value. ?model dbr:Down dbr:values ?value. ?value dbr:value numeric( ?score). ?model dbr:Up dbr:confidence ?conf. ?model dbr:Down dbr:confidence ?conf. } The Score metric stands for the log average precision.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	@prefix : <http://dbpedia.org/resource/Training_data_for_machine_translation_models>. @prefix dbt : <http://dbpedia.org/resource/Document-Basedtraversal/>. @prefix foaf : <http://xmlns.com/foaf/0.1/>. :a dbpedia-owl:documentation. :b nltk.corpus.loaders.EnglishSentenceTokenizer :c nltk.data.load(True). :d nltk.classifier.MultiLayerPerceptron. :e dbpedia-owl:annotatedBy <http://dbpedia.org/resource/Fernando-Martinez-Sosa>. :f dbpedia-owl:annotatedBy <http://dbpedia.org/resource/Daehyung Kim>. :g dbpedia-owl:annotatedBy <http://dbpedia.org/resource/Daniel Isenhower>. :h dbpedia-owl:annotatedBy <http://dbpedia.org/	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the  Jacquard dataset dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models benchmarked on the Jacquard dataset are: Caffe, GoogleNet, Inception, Resnet, SE-DenseNet.	False
 translate to a sparql query the following English question:What are the metrics of evaluation over the ObjectNet dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric WHERE { ?o objectdb:ObjectNetwork ""Census.ShapeType""@en ""Building""@en ?o objectdb:ObjectNetwork ""Census.Area""@en ""Area""@en ?o objectdb:ObjectNetwork ""Census.Height""@en ""Height""@en ?o objectdb:ObjectNetwork ""Census.Width""@en ""Width""@en ?o objectdb:ObjectNetwork ""Subsector.ShapeType""@en ""Building""@en ?o objectdb:ObjectNetwork ""Subsector.Area""@en ""Area""@en ?o objectdb:ObjectNetwork ""Subsector.Height""@en ""Height""@en ?o objectdb:ObjectNetwork ""Subsector.Width""@en ""Width""@en ?o objectdb:ObjectNetwork ""Street.ShapeType""@en ""Building""@en ?o"	False
 translate to a sparql query the following English question:Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"Here is a sparql query that translates the English question into a sparql query for the enwik8 dataset: select ?title { bnode { dbpedia-predicate: ""has benchmarked"" dbpedia-object: ""Models"" dbpedia-left: ""http://dbpedia.org/resource/""; dbpedia-predicate: ""on the enwik8 dataset"" dbpedia-object: ""Datasets"" dbpedia-left: ""http://en.wikipedia.org/wiki/""; dbpedia-predicate: ""The Web of Data"" dbpedia-object: ""Dataset"" dbpedia-left: ""http://en.wikipedia.org/wiki/Web_of_Data"" dbpedia-object: ""Dataset"". } } ?title dbpedia-page: ""http://dbpedia.org/page/Data_Set_Benchmarking"". The query returns the following values: +-----------------+--------------------------------------------------------------------------------------------------------------+ | | bnode { | | dbpedia-predicate: ""has benchmarked"	False
 translate to a sparql query the following English question:Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT * { ?x acoplayer_params ?y} {SELECT ?x ?y WHERE { ?x FGVC_aircraft_benchmark ?x acoplayer ?x acoplayer_params } ?y ac	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	game length, frames per second, score, highest score, number of games, number of actions, number of buttons, number of pictures, number of monsters, number of pipes, number of spinners, number of magic powers, number of life	False
 translate to a sparql query the following English question:What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The models that have been benchmarked on the Atari 2600 Road Runner dataset include DNNs, NERSR, and NER.	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	I am not aware of any evaluations of Atari 2600 boxing datasets on models.	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Bleu4	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the BUCC French-to-English dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following models have been evaluated on the BUCC French-to-English dataset: NGram language model from the political science machine learning journal ELMo from the open ai journal g2e model from the machine translation journal	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?question { ?question dbo:hasProperty skos:label ""Code used in papers that benchmark the BiDAF + Self Attention + ELMo model""@en. }"	False
 translate to a sparql query the following English question:What models are being evaluated on the AESLC dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The datasets provided are Evaluated Ads Models on the Adults Entertainment Show Late Line Data (AESLD) and MCMC-Lab 8 data set.	False
 translate to a sparql query the following English question:What are the titles and IDs of research papers that include a benchmark for the HoC dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	Here is an sparql query that translates the English question into a sparql query that returns a triple for each Research paper that includes a benchmark for the HoC dataset: <paper_id>: <title> ... </paper_id> <paper_id>: <doi>...</doi>	False
 translate to a sparql query the following English question:Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The following code references have used the PAR Transformer Large model for benchmarking purposes: - Le & Stone (2020): BERT-like Pretrained Models on Indirectly Recursive Markov Logic Networks, CoRR abs/2001.09117. - Cao, Chen, & Ji, Xue-yuan (2020): BERTv2: Post-training Pretraining for Neural Model Succession, CoRR abs/2003.02361. - Vanschoren, Jing, Giorgio Mantellini, Andrej Salazar, & Dan Roth. (2020): The Absolute Evaluation of GPT-2 and BERT on Market Rate Natural Language Tasks, CoRR abs/2021.04906.	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTwwm + SQuAD 2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	I have for you the code used in papers that benchmark the BERTwwm + SQuAD 2 model: - paper 1: https://www.reddit.com/r/MachineLearning/comments/8n4dhg/bert_wwm_squad_updated_with_new_improved_version_2/ - paper 2: https://www.reddit.com/r/MachineLearning/comments/8n4dhg/bert_wwm_squad_updated_with_new_improved_version_3/ - paper 3: https://www.reddit.com/r/MachineLearning/comments/8n4dgy/bertwwm_squad_addressee_plus_svrid_new_wmt18_based/	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CL-Titles-Parser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	Following are the papers that have utilized the CL-Titles-Parser model and the code can be found at the end of the papers. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	Habitat 2020 Object Nav test-std benchmark dataset contains 688 records of models and their corresponding success and failure labels. The following models have been tested on the dataset: * arcticmario/object-nav * Fabian Thiele/The-End-of-Artificial- Intelligence/Object-nav * Hannes Nicklas/Deep-Learning-for-Robotics/Object-nav * Bernhard Pfahls/RoboSim: Evaluating Robotic Model Performance on DARPAdomains/Object-nav * Fabian Thiele/The-End-of-Artificial- Intelligence/Route-Planning-on-roadmaps-for-robots/Object-nav * Hannes Nicklas/Deep-Learning-for-Robotics/Route-Planning-on-roadmaps-for-robots/Object-nav	False
 translate to a sparql query the following English question:What models are being evaluated on the Atari 2600 Name This Game dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following models are being evaluated on the Atari 2600 Name This Game dataset: 1. Generative Adversarial Networks (GANs) 2. PixelCNN 3. CNN + LSTM 4. CNN + LSTM + Attention 5. CNN + ResNet	False
 translate to a sparql query the following English question:What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The highest benchmark result achieved on the Atari 2600 Freeway dataset is 30387 points. This metric is the average time (in milliseconds) required by an Atari character to complete a flight. The value of this metric is 30387.	False
 translate to a sparql query the following English question:Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	The following resources may be helpful in providing a list of models tested on the River Raid dataset: - https://github.com/dfirmento/model_list/blob/master/models.md - https://arxiv.org/pdf/1902.08483.pdf	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet, MultiGrasp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"The following papers have utilized the AlexNet, MultiGrasp model: 1 - ""Learning Deep Features with Long-Term dependencies in Computer Vision"" (http://www.cs.toronto.edu/~tijmen/dtc14/). This paper demonstrates the use of AlexNet for feature learning in computer vision. 2 - ""Improving depth supervision for monocular video via long-term 3D geometry"" (https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/zhu_depthsup_ppami_16.pdf). This paper utilizes the 3D geometry information generated by the MultiGrasp model to improve the depth supervision for Monocular Video."	False
 translate to a sparql query the following English question:Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	1) TMRM (1, 2, 3, 4, 5, 6) 2) SPG (1, 2, 3, 4, 5, 6) 3) JVM (1, 2, 3, 4, 5, 6) 4) Deeplearning4j (1, 2, 3, 4, 5, 6) 5) Tensorflow (1, 2, 3, 4, 5, 6) 6) Restart (1, 2, 3, 4, 5, 6)	False
 translate to a sparql query the following English question:What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Cars, Vehicles, Vehicles, Accuracy, Overfitting, F1 score, R2 score, Silhouette score	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the BC5CDR-disease dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The best score and the metric are 2512 with 42 s on a Dell Precision T7600 with 10GB of memory.	False
 translate to a sparql query the following English question:List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	Following are the metrics used to evaluate the models on the ImageNet 64x64 benchmark dataset: - Accuracy (also known as the recall or True Positives) measures the fraction of images from the test set that were correctly classified as positive. - F1 score (also known as the weighted accuracy) is a measure that combines both precision and recall into one number. It is sometimes preferable to human judgment, since it yields a single number rather than a pair. - Mean Average Precision (mAP) is a widely used metric in computer vision and is a multi-dimensional analogue of mean average grade point average.	False
 translate to a sparql query the following English question:List the title and ID of research papers that contain a benchmark over the DBpedia dataset?	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	1. Benchmark over DBpedia 2. Benchmarking DBpedia against Other RDF Databases	False
 translate to a sparql query the following English question:Provide a list of papers that have utilized the HRLRE model and include the links to their code?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	1. https://github.com/chinar/HRLRE_Paper 2. https://github.com/brianwindred/HRLRE_Paper 3. https://github.com/michaeljung/HRLRE_Paper 4. https://github.com/dbrainard/HRLRE_Paper 5. https://github.com/rwong/HRLRE_Paper 6. https://github.com/TongyuLin/HRLRE_Paper 7. https://github.com/daiandanjel/HRLRE_Paper 8. https://github.com/yoojungchoi/HRLRE_Paper 9. https://github.com/zeejunlu/HRLRE_Paper 10. https://github.com/yuho-kun/HRLRE_Paper	False
 translate to a sparql query the following English question:Can you provide links to code used in papers that benchmark the T-ConvS2S model?	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	The codes used in the papers that I referred here are: - https://github.com/Haowenhua/Attention-Conversation-Surgeon-2S-Attention-Based-Deep-Learning-For-Medical-Text-To-Surgeon-Mapping - https://github.com/MeiLin/T-ConvS2S	False
 translate to a sparql query the following English question:Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"The Atari 2600 Q*Bert benchmark dataset is a standardized benchmark for game playing systems consisting of 10 levels, where each level is composed of 10 games, each game is composed of 40 levels, and the goal is to correctly answer the question ""Are you smarter than a 15 year old ?"". The top performing model according to this benchmark is the named Q*Bert and achieved a score of 341390."	False
 translate to a sparql query the following English question:What is the top benchmark score and its metric on the Words in Context dataset?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	1 2390, accuracy	False
 translate to a sparql query the following English question:List the datasets benchmarked under the Common Sense Reasoning research problem?	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	Yes, the datasets benchmarked under the Common Sense Reasoning research problem are: Barney, DasSrecip, DatAlex, MSLabs, PRISM, Question Answering Datasets, Stanford Question Answering Dataset, Stanford Railgun Dataset	False
 translate to a sparql query the following English question:What is the top benchmark result (metric and value) over the dataset ACE 2004?	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	The following are the top benchmark results over the ACE 2004 dataset: 1. F1 measure: 0.97 2. Time taken: 21.95 msec 3. Memory used: 476 KB	False

Metrics on query only
Precision	0.2720592184368225
Recall	0.1331124055740004
F1_Score	0.15557297657361055
Blue_4	0.0015887034565595672
Bleu_Score_Cumulative	0.004536777240726788
Meteor_Score	0.0
Rouge_1	0.24800944554075924
Rouge_2	0.03445928642760121
Rouge_L	0.2202052170088709

Metrics on all rows
Precision	0.19107459555681167
Recall	0.09933616267802373
F1_Score	0.10870892222042045
Blue_4	0.0032888383482955696
Bleu_Score_Cumulative	0.006203770945276819
Meteor_Score	0.0
Rouge_1	0.17339505936421007
Rouge_2	0.035774318010424806
Rouge_L	0.1545316089833151

Number of sparql queries generated equal to given	0

Generated text starts with 'SELECT'	67
