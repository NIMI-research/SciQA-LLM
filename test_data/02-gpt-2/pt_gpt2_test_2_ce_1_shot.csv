Questions	SPARQL in Gold Standard	SPARQL generated	exact match
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	     2:15:12.743 1.0 1.0 0 0.6 0.2 R3.0 http://www.t-online.de/R3/R3.0.csv 1:05:49.677 2.0 0.4 1 0.8 0.1 R3.1 http://www.t-online.de/R3/R3.1.csv 1:05:05.816 1.5 1.3 1 0.9 0.1 R3.1 http://www.t-online.de/R3/R3.1.csv 1:05:00.915 1.8 0.9 2 0.7 0.1 R3.0 http://www.t-online.de/R3/R3.0.csv 1:05:26.879 1.4 0.2 1 0.5 0.2 R3.0 http://www.t-online.de/R3/R3.0.csv 1:05:23.981 1.3 0.3 1 0.6 0.1 R3.0 http://www.t-online.de/R3/R3.0.csv 1:05:19.994 0.8 0.1 0 0.4 0.1 R3.1 http://www.t-online.de/R3/R3.1.csv 1:05:08.738 1.4 1.1 1 0.5 0.2 R3.1 http://www.t-online.de/R3/R3.1.csv 1	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT name as TITLE, ID as JUMPCODE, CRSID as RESID, CRSID2 as CRSID2 FROM JUMPCODE JOIN `test_data@lond$tb_subtree` (JUMPCODE AS JUMPCODE2) ON JUMPCODE2.JUMPCODE_ID=jumpcode2.JUMPCODE Or, a datafile containing a list of title, ID and RESID values where the ID was '12'. Or a datafile containing a list of TITLES where the ID was '12'. Or, a datafile containing a list of TITLES WHERE the TITLE ID was '12' AND the RID was '12' : output (Sparql query): SELECT TITLE, TITLE_ID, TITLE_CRC_SID, TITLE_CRSID, CRSID2, RESID, RESID2, CRSID_SID FROM TITLE, TITLE_ID Or, a datafile containing a list of TITLES WHERE the ID is '01' and the RID is '01' : output (Sparql query): SELECT TITLE, TITLE_ID, TITLE_CRC_SID, TITLE_CRSID, RESID, RESID2, CRSID, RESID2 from TITLE, TITLE_ID Or, a datafile containing a list of TITLES THAT HAS A TITLE ID that can be found in both a '01' and '01'. We could also perform the same query we made for our first query without the ORG value (a table that contains a single RID value, e.g. '12' ), but we just don't know yet what format that	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the UrbanSound8k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?metric ?metric_lbl. FILTER (str( ?metric_lbl) = ""HAS_DATASET"", ?dataset orkgc:Dataset ) WHERE ?metric_lbl. { ?metric_lbl. { ?metric. { ?format orkgc:Format ?format, ?metric_lbl. ?format } ?metric, ?format, ?format } ?metric } ?metric, ?format, ?format } ?metric } ?metric_lbl. input (English text): Which features are being evaluated on the UrbanSound8k dataset ? output (Sparql query): SELECT ?filter ?label_id, ?index ?index_key, ?matcher ?function_clause, ?name ?name_clause, ?value ?value_clause, ?val ?val_clause, ?value( ?null ?value, ?null ?value, ?null ?value, ?null ?value, ?null ?value ) ?default!default, ?default ?default, ?default ?default!default, ?default (select a, r:"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ? ? ? ? ? ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ? ? ?dataset_lbl. ? ?dataset rdfs:label ?dataset. FILTER (str( ?dataset rdfs:label ? ? ?dataset_lbl. ) = ""Atari Games"") ? ?dataset rdfs:label ? ? ?dataset. ? ?dataset rdfs:label ? ? ?dataset. FILTER (str( ?dataset rdfs:label ? ? ? ?. ) = ""atari games"") ? ?dataset rdfs:label ? ? ? ?. ?benchmark orkgp:TEST_DATASET ?dataset. } input (English text): Provide a list of research papers that report a benchmarked dataset on the Pitzer Archive dataset ? output (Sparql query): SELECT ?dataset ? ? ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ? ? ?dataset_lbl. ? ?dataset rdfs:label ?dataset. FILTER (str( ?dataset rdfs:label ? ? ?dataset_lbl. ) = ""Pitzer archive"") ? ?dataset rdfs:label ? ? ?dataset. ? ?dataset rdfs:label ? ? ?dataset. FILTER (str( ?dataset rdfs:label ? ? ? ?. ) = ""Pitzer archive"") ? ?dataset rdfs:label ? ? ? ?. ?benchmark orkgp:HAS_DATASET ?dataset. } input (English text): Provide a list of research papers that"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the MAZEA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT name, score, metric FROM {dataset} ORDER BY score > Input The input is the data that is used in the MAZEA dataset. It includes some of the most popular models like VGG-16, SVM, Neural Networks, Bayesian Networks, ResNet-50, and so on, among others. It is a mixture of data from the original dataset, from the second dataset (the EGG_MAZEA_DATA_2 dataset), and from the third dataset (the EGG_MAZEA_DATA_3 dataset). Some of the data in the second datasets are missing. They are: first dataset, missing a few features, including 2 rows of the dataframe ""categories"", e.g., ""animal"" and ""horse"". We have used the first 1/3rd of the total dataset to compute our new dataset. Input data (English text): The input file is: Animal classification data with 16 features. For the EGG_MAZEA_DATA_2 dataset, the ""categories"" is missing. To include this missing data, we downloaded the raw data from the dataset and renamed the ""categories"" field in the dataframe ""categories"", i.e., its name in the ""categories"" column in the first dataset. Then we used that name in our MAZEA dataset as reference. Output (Sparql query): SELECT name, score, metric FROM {dataset} WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset ; orkgp:HAS_EVALUATION ?eval"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the TDMSci dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	--------------------------- The first column was the query time range which used to be an RTE time, which in turn had a timestamp that it could be stored into. Now it is just a column and the data is now part of a query. Now, we are evaluating for one metric. To get an idea of how this should look, we will just add a new column in our code and we will call it Metric : input (English text): Metric ?metric_lbl Metric: ?metric (max(value) = 100000) metric_lbl : rdfs:filter( ?metric = ?metric_lbl) metric_lbl has 5 columns now: rdfs:label Metric: ?metric_lbl Metric: ?metric_lbl metric_lbl has 2 additional columns: rdfs:value Metric : ? ? ? You can see that the input is now just the raw data in all the columns and we have no need of data type. Now, we are ready to look for some of those functions that allow us to make data look different. We use the built-in function data_model or RTE model_df. We are using the RTE model function DataModel, so that is exactly what we should be using. I want use this function for looking for the RTE model we are using, instead of using the built-in functions. input (English text): Function ?model_df( data:seq orts:model_predict, model:dict:object) To see how those two are related, check out RTE model_df. Now, for the last part, all of our dataset need to have one more important data component – the time-series. Let's just take the time-series for TDCSi	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the mean capacity of a carbon-based fuel?
 output (Sparql query): "	"SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, ""([0-9]+),([0-9]+)"", ""$1$2"" ) ), ""([0-9]+).*"", ""$1"" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), ""(fuel|CO2)"")) } "	SELECT ?capacity, ?weight, ?fuel_weight * 10 * FIBER_POUNDS FROM ?data_tbl ?data_fuel_weight orkgp orkgp_data ?dataset orkgp_fuel_weight; output (Sparql query): Where are these values calculated and the model used ? input (English text): What is the mean weight of a carbon-based fuel and the size of the fuel tank, in cm ? output (Sparql query): SELECT ?weight, ?volume, ?fuel_weight * 10 * FIBER_POUNDS FROM ?data_tbl ?data_fuel_weight orkgp orkgp_data ?dataset orkgp_fuel_weight; output (Sparql query): Where is this model referenced ? input (English text): What is the total weight and volume of a carbon-based fuel ? output (Sparql query): SELECT ?total_weight, ?volume, ?fuel_weight * 10 * FIBER_POUNDS FROM ?data_tbl ?data_fuel_weight orkgp orkgp_data ?dataset orkgp_fuel_weight; output (Sparql query): Which car model makes use of a fuel tank bigger than 10 t ? input (English text): What is the total volume of the fuel tank in litres. output (Sparql query): SELECT ?volume, ?fuel_weight * 10 * FIBER_POUNDS FROM ?data_tbl ?data_fuel_weight orkgp orkgp_data ?dataset orkgp_fuel_weight; output (Sparql query): This model calculates the fuel consumption when the fuel consumption is 50% over the fuel consumption to	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?lbl. id ?lbl_id FROM ?lbl WHERE { ?lbl_id = ?lbl_id. ?lbl_lbl_id ?lbl. id } input (English text): Give me a list of research papers along with their titles and IDs, that have	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?spp orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?dataset. ?bench orkgp:P32 ?problem. } This time, we see that our previous query is a bit better... In fact, it's a pretty good improvement. What makes this improvement more important ? Well, we have a new dataset and are now using it; we can test that it works as expected without any knowledge of how the data was acquired; we can test it in the context of an existing dataset we're using to test our model; and we can look at how it performs with"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" output (sorted listing) source: The paper ""Multi-step models for data mining"" (Hieronymus Gerschenkron, Christoph M. Schürmann and Andreas A. A. Wänsche), published in The Journal of Statistical Software (2014)  Input:  English text (1 entry, maximum 350 chars) output (sorted listing) source: The paper ""Multi-step models for data mining"" (Hieronymus Gerschenkron, Christoph M. Schürmann and Andreas A. A. Wänsche), published in The Journal of Statistical Software (2014)  Input:  English text (20 entries, maximum 350 chars) output (sorted listing) source:  The paper ""Multi-step models for data mining"" (Hieronymus Gerschenkron, Christoph M. Schürmann and Andreas A. A. Wänsche), published in The Journal of Statistical Software (2014)  Input:  English text (2 entries, maximum 350 chars) output (sorted listing) source:  The paper ""Multi-step models for data mining"" (Hieronymus Gerschenkron, Christoph M. Schürmann and Andreas A. A. Wänsche), published in The Journal of Statistical Software (2014)  Input:  English text (15 entries, maximum 350 chars) output (sorted listing) source:  The paper ""Multi-step models for data mining"" (Hieronymus Gerschenkron, Christoph M. Schürmann and Andreas A. A. Wänsche), published in The Journal of Statistical Software (2014)  Input:  English text (4 entries, maximum 350 chars) "	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT { ?lbl a ?lbl c ?lbl DISTINCT ?lbl_c. } as ?lbl_c input (English text): What is the highest Benchmark result achieved in a sample of 3,000 data sets in a single collection of samples, measured on a ball in cup, catch (DMControl500k) dataset, including the metric and its value ? output (Sparql query): Ranking and weighting of GAD models: In the last post we saw that the number of models with a P<0.05 criterion score and a median weight for ""best"" is higher in the top of the funnel curve for the GAD than the corresponding funnel curves for the BAP and BAP_DB datasets, where the same metrics are used for both datasets. There is a similar finding for GAD on the mean weighted scores calculated for most parameters, especially for the median. The average GAD score is 0.88 for GAP, 0.80 for BAP, and 0.49 for the BAP_DB dataset. We believe the high GAD_Score value for GAD in the funnel plot for the GAD and BAV datasets suggests a more accurate model would have a lower mean weight and be even more ""dense"" than the model used in the BAP and BAP_DB datasets. The following plot shows GAD's overall score by the average set weight to GAP, assuming that the mean weight of GAD models is 0.5 by default. The curve is clearly a rightward contour. This plot was obtained through a Monte Carlo simulation. We further considered the ""predictive power"" of GAD models with the help of the Daskalakis et a1 ""GAD for Predicting Goal Success"" paper (2011), which has just recently been published ("	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" input (English text): Do you have any input material for the final part of the article ? output (Sparql query): Sorting algorithms The main objective of any sort solution was to generate the most appropriate set of random elements using the best algorithm for that task (for example, the ""dumb"" sorting algorithm in the previous section). As an empirical demonstration of sorting method selection, we used several sorting methods: ""dumb-order"" (Bartle & Meeks, 2008) and ""branch"" (Blanchette et al., 2005). However, in no case were sorting methods selected based upon what is most suited for the application (see table below). For both the original dataset and a new one we constructed a randomized dataset called (for example, ""tidbitset"" [see Materials and Methods]. We randomly generated the seed (1, 0, 1), generated a random subset of 10,000 ""tidbits"" (20,000 in total), created a dataset using the same distribution but with the maximum of 10,000 ""tidbits"" (20,000) and a new random assignment of 1,000 ""tidbits"" (100,000). We used these randomized datasets in all of our experiments (data sets ""VTAB"" and ""tidbitset"") and, in all three experiments the results were essentially identical to the original data. To validate this method, we generated the ""VTAB3"" dataset to check its performance on the original test datasets. The data set consists of 50,000 random entries from the original test set, 100,000 random entries from a new test set, 300,000 random entries from a new test set, and 500,000 random entries from a new test set. We randomly reordered the ""VTAB"" data set and the ""t"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 We have extracted the top 100 papers from the arXiv dataset. We are interested in two aspects: 1) the statistical models that predict the results, and 2) the RTE results to obtain the quality of models we use. We did our statistical analyses using SPSS and the RTE package. The RTE package had better results than SPSS but SPSS is a proprietary product and not free. To do this I made a simple Python script that takes different functions and calls them, using pd.read_csv(args) to read the text files generated when running the RTE package with different models. A simple Python script is as follows: #!/usr/bin/python # # RTE: Tools for measuring the quality of theoretical and empirical research on rte data # by plotting RTE metrics against the RTE results. # By: Hjorthur <rte@hjorthur.idaho.edu> # See http://rte.github.io/rte/ for full text. # # If any errors appear in the code above or comments below, the source code is available at # https://github.com/rte/rte import matplotlib import os from PIL import Image import matplotlib.pyplot as plt import numpy as np from matplotlib import add_circle from collections import deque from sklearn import feature_extraction import pandas as pd import matplotlib.pyplot as plt from pprint import pprint from pylab.models import CURVE from matplotlib import pyplot as plt # load and parse the RTE results dataset from rte # data (from Wikipedia/TensorFlow rte:Dataset, output='RTE: Top 100 Papers') # input (English text): What are the titles	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	..................................................... 0 (0) 1 (1) 2 (2) 3 (3) 4 (4) 5 (5) 6 (6) 7 (7) 8 (8) 9 (9) 10 (10) 11 (11) 12 (12) 13 (13) 14 (14) 15 (15) 16 (16) 17 (17) 18 (18) 19 (19) 20 (20) 21 (21) 22 (22) 23 (23) 24 (24) 25 (25) 26 (26) 27 (27) 28 (28) 29 (29) 30 (30) 31 (31) 32 (32) 33 (33) 34 (34) 35 (35) 36 (36) 37 (37) 38 (38) 39 (39) 40 (40) 41 (41) 42 (42) 43 (43) 44 (44) 45 (45) 46 (46) 47 (47) 48 (48) 49 (49) 50 (50) 51 (51) 52 (52) 53 (53) 54 (54) 55 (55) 56 (56) 57 (57) 58 (58) 59 (59) 60 (60) 61 (61) 62 (62) 63 (63) 64 (64) 65 (65) 66 (66) 67 (67) 68 (68) 69 (69) 70 (70) 71 (71) 72 (72) 73 (73) 74 (74) 75 (75) 76 (76) 77 (77) 78 (78) 79 (79) 80 (80) 81 (81) 82 (82) 83 (83) 84 (84) 85 (85) 86 (86) 87 (87) 88 (88) 89 (89) 90 (90) 91 (91) 92 (92) 93 (93) 94 (94) 95	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?paper orkgp:HAS_BENCHMARK ?benchmark. ?print. output (Sparql query): SELECT p.print ?print, ?p.name, ?p.id, ?p.paper orkgp:HAS_BENCHMARK ?benchmark. output (Sparql query): SELECT ?name ?p, ?p.id, ?p.name ?p.id, ?p.score ?p.score, ?p.models. output (Sparql query): output (Sparql query): SELECT p.score ?p.rank, ?p.name OR p.name ?p.rank, ?p.id OR p.id ?p.id, ?p.print ?p.print, ?p.print, ?p.id OR null ?p.print, ?p.print, ?p.print ?p.print orkgp:HAS_EVALUATION ?disease ?, ?p.score ?p.score, ?p.id OR p.id ?p.id, ?p.print ?p.print, ?p.print, ?p.id OR null ?p.print orkgp:HAS_EVALUATION ?disease ?, ?p.models. output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): Selects the 5 top performing models in terms of score score (as reported by Google Scholar). This query matches each top performing model's title and ID. The dataset is not available and we have used a dataset based on the model name. Note: This query selects more than one output row (as it can not select any results at the same time). Selecting multiple rows: You can also select multiple rows from the output of the above-quoted query. The following query selects some top performing models in"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT d.name AS model_lbl, ?model_lbl AS rdfs_lbl.name, ?d.data_type AS input.dataset, ?d.	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" If you can't connect, try the following command on your local system:   sudo apt-get update sudo apt-get upgrade   sudo apt-get dist-upgrade   apt-get update sudo apt-get dist-upgrade   And finally, update your local network: sudo apt-get remove linux-image-image-base Note: You also need to verify the network device: Open your network manager and search for ""network connection"". Connect your GPU/graphics card to your computer using the driver supplied by your motherboard manufacturer. The recommended drivers for your hardware include the following: http://www.acm.org/resources/hq/v12.3/index.html#h2g-en This command should update your OpenCL/CNTK libraries and also configure the default OpenCL drivers in Ubuntu, Gentoo, RHEL, Fedora, and CentOS. Install OpenCV 1.12 and the OpenCV_X.Y repository: sudo apt-get install opencv-c4-dev OpenCV_X.Y is the only repository that you must add. Once installed, activate OpenCV by typing: sudo apt-get install -y opencv Download and run the benchmark: sudo apt-get install -y python-numpy python-pytest python-dev python-pip opencv-c4-dev After installation, run the benchmark:./benchmark.py The benchmark will return the number of unique images generated and score the image performance (number of objects, distance, and color accuracy). You should also see output that includes a number of benchmarks and results. The full version of the benchmark can be found on the bench.py GitHub repository. This article assumes that you are familiar with the basics of GPU computing. If you aren't, head over to our OpenCV tutorial. If you are still having trouble, check"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Input data: Classical music, 5 seconds at 12 kHz dataset, 1 ms of audio.	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Flair-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT COUNT ( * ) AS cp_count, COUNT ( * ) AS n_clf_clients, MAX ( kp_clf_clients ) AS n_kp_clf_clients FROM ?model WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (! str ( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } FILTER (! ( str ( ?matag_lbl) = ""F1"")) } ORDER BY cp_count DESC( ?clf_clients ) LIMIT n_clf_clients ; ? input (English text): Which model has achieved the highest F1score on the HoC benchmark dataset ? output (Sparql query): SELECT COUNT ( * ) AS cp_count, COUNT ( * ) AS n_clf_clients, MAX ( kp_clf_clients ) AS n_kp_clf_clients FROM ?model WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. LOWER ( kp_clf_clients ) rdfs:label ?kp_clf_clients. ?clf_clients; } ORDER BY DESC( ?kp_clf_clients ) LIMIT n_clf_clients DESC( ?clf_clients), ?clf_"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?metric || ?metric_lbl.name || ?metric.rank || ?metric.dataset1 || ?metric.dataset2 || ?metric.dataset3 || ?metric.dataset4 || ?metric.metric_lbl || ?metric.metric_lbl.name || ORDER BY!orderby! ORDER BY { ?value || ?value_str || ?value_mat || ?value_lt || ?metric.name || ?metric.name || ?metric.rank || ?metric.dataset1 || ?metric.dataset2 || ?metric.dataset3 || ?metric.dataset4 || ?metric.metric_lbl || ?metric.metric_lbl.name || ?metric_lbl.name } ORDER BY { ?value || ?value_str || ?value_mat || ?value_lt || ?metric.name || ?metric.name || ?metric.rank || ?metric.dataset1 || ?metric.dataset2 || ?metric.dataset3 || ?metric.dataset4 || ?metric.metric_lbl || ?metric.metric_lbl.name } GROUP BY ?metric || ?metric_lbl.name ORDER BY DESC( ?value ) OR ORDER BY DESC( ?value_str OR ?value_mat ORDER BY DESC( ?value_lt OR ?value_lt ) ) ORDER BY -1 ORDER BY'' || ?orderby'' || ?orderby'' || ?orderby'' || ?orderby'' || ?orderby'' || ?orderby	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?problem ? AND ?problem_lbl FROM ?problem_lbl . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - input (English text): 1.2.2 0.75.3 1.3.4 1.2.3 0.92.2 1.6.3 0.89 0.9 4.1.3 1.1.1 3.3.2 2.2.2 2.3.4 3.0.0 2.12.1 2.7.6 output (Sparql query): -------------------------- SELECT IF(IS_PROGRAM( 'F1' OR NOT IS_PROGRAM( 'F1', ?)) AS PROGRAM, 'problem ?', 'problem1 ?', 'problem2 ?', 'problem12 ?', 'problem22 ?', NULL, 'problem3 ?' FROM ?problem_lbl ORDER BY IF(IS_PROGRAM( 'F1' OR NOT IS_PROGRAM( 'F1', ?)) AS PROGRAM,'result ?','result_lbl','result_label' ) AS DISTINCT, NULL,'result','result_lbl', 'problem' FROM ?problem_lbl LIMIT( 1, 10) ?lbl ?; -------------------------- input (English text): F1 1.3.4 1.2.2 1.3.4 1.2.2 0.92.2 1.6.3 0.89 0.9 4.1.3 1.1.1 3	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT count(*) as total FROM bb GROUP BY a,b LIMIT 10 WHERE a < 10 GROUP BY an, b - limit 100 WHERE an < 10 - limit 100 If a query involves a filter clause, the value the filter_option value may be changed with that filter keyword argument: SELECT count( ?) as total FROM bb GROUP BY an, b LIMIT 10 IF an < 10 GROUP BY an, b - limit 100 ORDER BY a + 10 - limit 100 ORDER BY an + 10 LIMIT 100. The first two queries are equivalent, the last one is not. To allow any SQL error message to be returned from the server, the sql_options attribute may be specified in the ""Query"" option. If the sql_options attribute is not specified, a default value specified for the keyword argument is assumed. For example, if the sql_options attribute is set to SQL standard, then the server ignores the sql_options key word. However, using a value other than SQL standard does not imply the server understands the SQL standard argument for the sql_options keyword argument. If the attribute is set to SQL_STRING_PRECISION, then a SQL standard query string (not a string literal as in the previous example) is included in the database. (The SQL standard string is not equivalent to the SQL standard SQL query string.) If the sql_options"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model_lbl. *, ?model_ioblg. *, ?model_lbl. *, ?model_lbl_lg. CASE WHEN ?model_lbl. LOWER COUNT ( ?model_lbl_lg ) BETWEEN 2 AND 3 THEN LOWER COUNT ( ?model_lbl_lg ) + 2 ELSE - 1 ELSE NULL END END In order to understand the output, we can use the '--output' option of the command to show which queries it produced at random from the list of generated reports. Example 9 - Showing output from a query in the ACE 2005 report $ cat /tmp/clark0013.sql --output query_list <- ""SELECT ?problem ?problem_lbl. ?model_lbl. *, ?model_ioblg. *, ?model_lbl. *, ?model_lbl_lg. CASE WHEN ?result_type ?result_type = 0 THEN ?result_type = -1 END ?result_type = 2 AND ?result_type = 3 THEN ?result_type = -1 END"" For examples of how to use this tool see the examples/ directory of the project. Example 10 - Selecting the last 3 rows from the output of a single query $ grep '' /tmp/clark0013.sql | awk -F'.' '{print $4}' | tr -d '' To show each query in the result set, run the last two queries in a single command: $ cat /tmp/clark0013.sql SELECT ?xid ?xid_lbl. ?model_lbl. *, ?xid ?lbl_ioblg. ?model_lbl"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PNDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 8.6.1 Graphical Display This section gives instructions how to generate a table-like graphical display that shows the status of all your datasets. This format can be used with the command-line interface (CLI) or through the Java graphical interface (JGUI). For the data visualizer, use the package java.util.graphical-toolkit.GraphicalToolkit to display your data. A GUI library is under development, as are some supporting applications. Here is an example, that displays the results of all your datasets in an interactive table: $ java -jar gtk.java -jar graphs/graphical-library 1:1:50> gtk.get_data_display : Using `-n 0` to ensure that only the latest versions of all the datasets are displayed.... 1:1:50> gtk.get_graphical_view : There are at least 1 datasets... 1:1:50> gtk.get_graphical_view 1:1:50> gtk.save_graphical_view 1:1:50> gtk.show_text 1:1:50> Note that this does not generate the graphical display used in the main tutorial; that format is available through the data visualizer script used by the GUI library included with the package. 8.6.2 Summary Table The summary table format is the recommended format for displaying metaanalysis results and also for the data visualization scripts. It is the format used by the Statistical Data Analysis in Social Science (SDSS) software package (JNI) for generating summary statistics (or in the case of the Java graphical visualization, the graphical visualization with the graphical toolkit). With the summary table format, each row represents a dataset and its name is shown in an italicized font next to each column	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS-XSUM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input(text): If you're willing to publish the results, please email me about the paper directly. output (Sparql query): input(text): How does your team approach benchmarking ? output (Sparql query): input(text): What is the overall goal of the project ? What metrics are being used ? output (Sparql query): input(text): For any benchmarks or experiments, can I have access to this data ? output (Sparql query): select query_id, result_set_id, sum(query_count / row_count*10), sum(result_count / row_count*10), sum(query_labels / row_count*10) from............................................................... query_id:  int  result_set_id:  int  sum(query_count) + + + + + + + + + + + + + + + + + + query_count:  int  sum(result_count) + + + + + + + + + + + + + + + + + + + + result_count:  int  query_labels:  int  sum(answer) + + + + + + + + + + + + + + + + + + + + answer:  int  sort_order:  int output (Sparql query): import scala. language. trait _ ; var query_count : int = 500 ; var	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	  The results of some of the benchmarks above are displayed below. This table shows that the results based on the VPN model can provide significant improvements in results over the results using a more traditional benchmark method. This table also includes some detailed explanation of the benchmarks' results when run using both of the VPN model's benchmark code and and without the VPN model's benchmark code (using each of the three different benchmarking scenarios).	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP (1) ?s_bench. ?s_bench_lbl ?m_lbl. ?s_bench ?bench_lbl. { ?dataset ? orkgc:DataSet ?dataset. ?benchmark orkgp:HAS_DATASET ?dataset. } 	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 I would suggest that an attempt should be made to obtain the original papers that generated this data set, not the versions presented, as the results that are provided on this page are derived from that raw data set. I will do my best to address any questions you may have during the next 2 weeks. If you have any feedback that you would like to know about, feel free to contact me via email (david@nucleocore.com) or Twitter (@DDS_Eval).	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT p32, p64, p128 FROM ?benchmark p LIMIT 10; OR gt16, gt32, gt64, gt128 ORDERED BY ORDER BY p32 LIMIT 10; output (Sparql query): Input: Required Field You can check for the accuracy test here:  http://github.com/AtariGames/atari-benchmark/blob/master/atari-check.go Your choice might depend on how you want to compare your model with other benchmarks: The Atari-Benchmark benchmark suite has 4 main test cases, which can be run concurrently. Here we will look at the benchmark against the standard game ""Hangman Game"". The standard-game-benchmark-1 contains all 16 game modes in an Atari 2600 game. The standard-game-benchmark-2 contains only the 1st level of Hangman. The standard-game-benchmark-3 runs the game with ""Highscore"" setting. Since a large percentage of arcade games have a high score, and highscore can change depending on the difficulty level of the game, this is the most appropriate scenario for us to test against. The standard-game-benchmark-4 has ""No score"" setting and is the same as the previous run with ""Highscore"" turned off. The standard-game-benchmark-5 runs Hangman with no score at all. The standard-game"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "		False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	------------------------- -------------- ------------- Problem: Atari Games ----------------------- [problem_index] Name : atari_games ProblemID : 63620 --------------------------- Test1 : ------------[ ?test 1 ] --------------------------- Task : atari_games.test ---------------- ------------ ----------- [0, 1, 2, 3].test --------- ------------- [0, 3, 2, 1].p1 --------------------------- -------------- [0, 3, 1 ].test - [1, 2, 3, 4].p2 --------------------------- -------------- ------------------------------------------------ [1, 9, 5, 1, 3, 4, 5, 6].p3 - [1, 3, 5, 7, 6, 3, 4, 6].p7 [1, 3, 1, 3, 5, 8].p3 [1, 3, 1, 3, 8, 9].p3 [ ?] Test2 : ------------[ ?test 2. --------------------------- Task : atari_games [problem_index] Name : atari_games ProblemID : 64331 --------------------------- Test3 : ------------[ ?test 3. --------------------------- Task : atari_games.test ---------------- ------------ ----------- [0, 4, 5, 6, 8, 9, 0, 0, 0].test --------- ------------- [0, 4, 5, 6, 8, 7, 8, 0, 0, 0].p1 --------------------------- -------------- [0, 4, 5, 6, 7, 9, 0, 0, 0].p2 --------------------------- --------------- [3, 7, 8, 3, 0, 6, 7, 0, 0, 0].p3 - [0, 5, 7, 4, 1, 8, 9, 0, 0, 0].p1 [6, 11, 5, 9, 9	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	--------------------------- -------------------------- --------- Atari Games benchmark score: 634.716 1.2.1.1.0.1 Benchmark score: 471.976 1.5.1.1.0.1 Benchmark score: 423.521 1.3.1.0.1.0 Benchmark score: 489.721 1.5.1.0.1.0 Benchmark score: 497.407 1.7.1.0.1.0 Benchmark score: 394.4361 1.4.1.1.1.0 --------------------------------------  This is a subset of the data in the Atari Games benchmark.  This subset contains the first 300 games scored, from 1984 to 2012.  This subset is a subset of the data on the official benchmark, with scores from the early 1990's.  This is the first year of the benchmark, and the scores from the early 1990's are the first time the benchmark was standardized.  The first year of the benchmark is 1989.  The scores have been normalized to 0 to maintain the same score.  The final score is an average of the scores given on this bench.  This is the first time I am listing the full dataset.  This is the first time I am listing the full dataset with the specific benchmark.  The original benchmarks (e.g., 1.5.0) are listed below the benchmark. This is the new dataset, and is the first time I am listing it.  It includes the first 300 games of the benchmark of 1984.  This is the first time I am listing all of the datasets for the benchmark.  It includes both standardization of the performance, along with the benchmark score itself. 	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP ( ? TOP { ?model ? model_lbl }) ?1 as test_score, ?2 as target_score, ?3 as top1_accuracy_score, ?4 as top2_accuracy_score, ?5 as top3_accuracy_score, ?6 as top4_accuracy_score, ?7 as top5_accuracy_score, ?8 as top6_accuracy_score, ?9 as top7_accuracy_score FROM ?TOP ORGERS WHERE orkgp:HAS_EXCEPTION_TARGET < TOP1 TOP2 TOP3 TOP4 TOP5 AND orkgp:HAS_EXCEPTION_TARGET = ?target and ?val = ?datasets.model_rdfs ORDER BY TOP1 ORGERS DESC LIMIT 1, 3, 6 ORDER BY TOP1 ORGERS DESC LIMIT 1, 6 Spartan Model The Spartan platform is now open source on Github.	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the most common Knowledge representation method?
 output (Sparql query): "	SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1	" { ?model a orkgc:Model; rdfs:label ?model_lbl. ?source_coding : cb ?benchmark ORGPCODE ?benchmark } The main issue with this dataset is that no validation data has been provided, thus no validation data has been used to compute the model. It is therefore possible, for example, to create simulated datasets and run benchmarks without validating, or to use the model without validating the simulated datasets. Unfortunately, the dataset used for this study has not been provided in the public repository nor has it been checked for correctness by Google Analytics data. The above example illustrates the problems with the use of model and benchmark datasets. It should be possible to replicate the problems described here only by using a ""simulated dataset"". It would be ideal if the dataset were pre-processed before use, and if the validated or simulated datasets can be validated by Google Analytics in the presence of any errors or missing data. The dataset used in this study was created using an open-source database (http://www.citationgate.net/). This dataset was created after the model was tested with two existing publications, and the validation data was created by reusing the datasets provided by the researchers in the journal study: The dataset has a single table with columns 'id' and 'number', containing a series of records associated with both the original documents and the benchmark data with which it is associated. The table is indexed at row-level. The model used to calculate accuracy is constructed in two steps from the original data (the original test data) and the benchmark data. As mentioned by the authors of the study, this dataset was constructed using the following steps: a) The author identified the data sources (document, table or chart) for each of the articles and chose the best fit model to the data (fitted"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can all the data sets used in the compared studies be found?
 output (Sparql query): "	"SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, ""Not"")) } "	 Now that I've solved this problem, I want to write a program to do something else with the data (see the next section for an example). 2: Analyzing Statistics: The last section of this tutorial is the statistical part. In this section we examine the standard normal form and normalizing the data with that form. The program will display the normal distributions and calculate the associated standard normal distributions with that form. 3: Calculating Standard Normal Form: Before we start working with the statistical analysis, I will show how to calculate the standard normal form of each of the variables. The following command will take a CSV (or any data file) containing the titles and papers used in the study and output them to Standard Normal Form : # The following command will take a CSV (or any data file) containing the titles and papers used in the study and output them to Standard Normal Form select title from titles where label = 'title' order by titles descending by paper_lbl. You can also specify an id parameter for this command, so that it will output an output instead of a normal form list: select title from titles where id = 1 + ?title ?title_lbl. 4: Calculating Normalized Standard Normal Form: The following command will show how to calculate the normalized form of two columns and return the normal distribution of those, by outputting the values for each variable: # The following command will show how to calculate the normalized form of two columns and return the normal distribution of those, by outputting the values for each variable select id from the_data where title_lbl = ?title. select id from the_data where title_lbl = ?paper. # To compute the final normal and normalized form, we will need to calculate the standard normal form of each of the variables, then compute the	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Entity Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" You can see that metrics are not unique anymore. They've been moved in and out of the model's metrics bucket. In this example, the model used is only built on DBL_DATA and NOT a model on the RTE dataset. So the data and model are never the same model.  I'm hoping that the second answer shows a solution on how to split the model's metrics bucket. In future you can use MetricFold for this kind of solution. (And I hope that the last answer has answers you need too, such as my previous questions regarding the ""uniqueness"" of the metrics bucket, the ""concatenation"" of metrics from different sources etc.)"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	--------------- --------------- Outputs are displayed using the RTE data_table_output() function. Using the command-line tool RTE_analyzer to run the RTE_benchmark() function is an alternative to the GUI. We have seen that RTE allows to retrieve the dataset, train models and run the models. One of the features you might not know of yet is how many outputs the RTE dataset can have. The best way I ever had to find this was to modify my model. The idea is to compute an output column that provides a list of datasets that are generated from the input data set. Of course there are a lot of ways to represent this type of data. One idea I will explain in a future post is to use a custom table. The input to this step is a RTE data structure. The most basic file structure is an RTE data_table. There are three different data types in this structure, each with its own columns of RTE output: - DISTINCT, which is just a single string. It represents a unique identifier. - COUNT, which represents the number of clusters produced by the model. - SPALCOUNT, which represents the number of training instances. The only other common data type is a list. The RTE dataset can be found using the command-line tool RTE_analyzer. This function can be used to visualize the output of any RTE framework on a given dataset. Here I can select one of the output formats. As you can see, there are various output formats: H1 H2 H3 H4 H5 H6 H7 H8 H9 H10 H11 H12 H13 H14 and H15. These are the format I chose. Note: These formats are not necessarily the default ones. By default RTE generates the	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?F1_SCORE AS F1, ?RANK AS 1 FROM model.benchmark GROUP BY ?F1_SCORE; orkgc:Model OR kgc:Model. Input English input (English text): What is the relation between the algorithm and a non-numeric field in the data ? Output CSV statement:  SELECT * FROM  model.benchmark WHERE ?column (samp_id) NOT IN (HAS_SAMP_FID) AND ?column (samp_id) NOT IN (HAS_SAMP_SID) output in text:  | |           | COUNT(*)  | ------- -----+----------------+-----------------------| 0 | 100.00 | COUNT(*)  | 69532.00 | 1 | 99.01 | COUNT(*)  | 0.00 | 2 | 99.00 | COUNT(*)  | 0.00 | 3 | 99.00 | COUNT(*)  | 0.00 | 4 | 99.02 | COUNT(*)  | 0.00 | 5 | 99.01 | COUNT(*)  | 0.00 | 6 | 99.02 | COUNT(*)  | 0.00 | 7 | 99.01 | COUNT(*)  | 0.00 | 8 | 99.02 | COUNT(*)  | 0.00 | 9 | 99.01 | COUNT(*)  | 0.00 | 10 | 99.02 | COUNT(*)  | 0.00 | 11 | 99.01 | COUNT(*)  | 0.00 | 12 | 99.02 | COUNT(*)  | 0.00 | 13 | 99.01 |	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE, ID, BATCH_LENGTH AS ?batch_length FROM ReaL_batch2 GROUP BY TITLE ORDER BY ID DESC LIMIT 5 This query returns the name, ID, and the length of the BATCH_LENGTH rows of the ReaL_batch2 sample dataset, as well as details on the datasets where benchmarked. input (English text): Can you list the metrics used to evaluate models on the RTE dataset ? output (Sparql query): SELECT ?metric ?meternum and HASH( ?meternum ) ?meternum input (English text): Name each column. output (Binary file): The name of each column, the HASH value of the column, and the value of the field. input (English text): Name each column. output (RDF file): The name of each column, the HASH value of the column, and the value of the field. input (English text): Output the parameters of a model with parameter values. run (RDF file): Output the parameters of a model with parameter values. To run a model, first download and install the RTE dataset from the project: curl -L -q http://api.nano.org/RTE.download.php | sed's/*rte.*//' or, if you are using RDBMS in combination with RTE: pip install RTE For more information, see the RTE docs, including many examples of RTE queries. input (English text): List the attributes of your model. output (Sparql database query): SELECT ?attribute_lbl, ?attribute_lbl_fname, ?attribute_numeric, ?attribute_datatype, ?dataset	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?dataset rdfs:label ?dataset. RDF: title. ?dataset rdfs:label ?dataset rdfs:title. OPTIONAL ID id. RDF: title. ?dataset rdfs:label ?dataset rdfs:title. OPTIONAL ID id. RDF: title. ?dataset rdfs:label ?dataset rdfs:title. OPTIONAL ID id. RDF: title. ?dataset rdfs:label ?dataset rdfs:title. OPTIONAL ID id. RDF: title. ?dataset rdfs:label ?dataset rdfs:title. OPTIONAL ID id. input (English text): The papers included in the Gibson PointGoal Navigation dataset have been evaluated on the HoC dataset. output (Sparql query): SELECT title. title AS title, n1 FROM paper, rdfs, orkgp; input (English text): What are the abstracts and publication dates for study papers that use the Gibson PointGoal Navigation dataset ?  output (Sparql query): SELECT rdfs:author ?author. ID ?author. ?author_p ?name( ?name( ?author__id ) ?. ) ?author. ID rdfs:description ?description. ?description rdfs:subtitle ?content_p ?type( ?content_p ?. ) ?name( ?name__id ) ?. ) ?description. OPTIONAL name( ?name__id ) ?. OPTIONAL content_p ?type( ?content_p ?. ) ?name( ?name__id ) ?. ) ?description. OPTIONAL name( ?name__id )	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	-------------------------- RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE ------------ RTE 0 CoNLL++ 5.3 RTE ------------ RTE ------------ 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 RTE ------------ 17 5.0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 0 RTE ------------ 1 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0 0 0 0 0 RTE ------------ 0 0 0 0 0	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT RATE < ?rate; ?metric rdfs:label ?metric_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. > ?rate } OPTIONAL { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. < ?rate } } output (Sparql table): Input: 2 fields. Output: Input (English text): How much does a model score in the PIQA dataset ? output (Sparql query): SELECT (RATE) RATE <= 1.00 ?rate. { rdfs:label ?dataset_lbl. ?rate rdfs:label ?dataset_lbl. ?rate rdfs:label ?dataset_lbl. > ?rate } OPTIONAL { ?rate rdfs:label ?dataset_lbl. ?rate rdfs:label ?dataset_lbl. < ?rate } } output Input: 2 fields. Output: Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Input: 2 fields. Output: Total number of words. Number of words. 	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	___________________________________________________ orkgk:RTE_Crazy_Climber_benchmark_dataset.dat. HAS_RATE ?m:Crazy_Climber. H:RATE 5.202778 | RTE_FINALS. RTE_PASS 10.202881 S:Crazy_Climber. S:PASS 5.202946 input (English text): A.1.2.2 input (English text): B.1.2.1 orkgk:RTE_Crazy_Climber_results_output.dat. ORkgk:RTE_Crazy_Climber_results_output.dat. HAS_RATE ? orkgk:RTE_Crazy_Climber_result_dataset.dat. B:Crazy_Climber B.1.2.1 input (English text): C.1.1.2 input (English text): C.1.1.3	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT COUNT(*) AS d.count, COUNT(*) AS c.count, COUNT(*) AS q.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count, 2 FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count, 3 FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d.count, COUNT(*) AS c.count FROM ( SELECT COUNT(*) AS d	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Funnel Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Outputs of the output queries are sent to the rte_cores group. A.2.8 Data analysis results A full list of analyses performed can be found below. A.2.9 Additional data Some additional data available during the data run on the RTE dataset can be found below. The analysis data itself provides much more information than the text above. A full list of the statistical data used in the analysis can be found on the data table in the ""A.2.9 Additional Data"" section. If you need more detailed documentation or additional data sources, see the following supplementary files:"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" The following fields are required. The first column is the model of interest and subsequent column contains the model dataset. input(s): The list of input filepaths to search for data in a given dataset. For example: ""datasets/model/data"" For the example model in the example above, the dataset is ""datasets/model/data/0"". saved(s):  The saved database location of the instance on disk. The saved database location can be any one of the following locations. input (English text): The file path for one of the following databases. output (Sparql query): The table that contains the data in all instances.  input (English text): The file path for one of the following databases. model (English title, or model name): The name of the model that I am benchmarking. If it is a manual model for example, the model name must be used and not the model title. In the example above, the model name is ""F1.hpc.me.10.0"".  model_lbl (English text):  The name of the model that is the source of all benchmark instances. The output data will be from the model specified by that filepath. For example, on a Windows system, this may be ""C:\Projects\F1.hpc.me.10.0\F1.hpc.me.10.0.sparql"".  test(type):  The type of ""test"" instances of the model that I am benchmarking.  I have not written the code to benchmark myself, I just ran the benchmarks in order to benchmark other people's models and I wanted to understand what kind of performance I got, i.e., my benchmarking experience.  test_describe"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Indicate the model that performed best in terms of F1 metric on the HgRed PubMed 18k RCT benchmark dataset ?	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-BiLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Input/output data from a particular source (the author is a member of that group): input (English text): Where can I find code references in papers that use particular code samples from the code reference list ? output (Sparql query): input (English text): List the code links in papers that use different source code samples in the code reference list ? output (Sparql query): input (English text): Where do you intend to publish the results ? output (Sparql query): input (English text): In the future when will you publish the results of the experiment ? output (Sparql query): input (English text): In the future when will you publish results of the experiment ? output (Sparql query): input (English text): What are your details, for example a name or email address, or are you at all willing to participate in a discussion forum relating to this project ? output (Sparql query): input (English text): What are your details, for example a name or email address, or are you at all willing to participate in a discussion forum"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Which metric is the best performing models against that dataset ? output (Sparql query): input (English text): What is the best performing model benchmarking the Pet Data2Hits dataset ? output (Sparql query): input (English text): What are the best performing models, and how do they compare with one another ? output (Sparql query): I've added a small section in the RTE report above, along with the RTE Report, to indicate if a benchmark is actually used, and can be considered, for a specific dataset. If it is, let me know, and we can discuss the possible improvements; if there have been no changes to the performance, I'd like to include the original data using the original benchmarking metrics on a separate page, as well as the new metrics - ideally the actual metrics would be used as part of the model evaluation - in a separate page (where all previous data would be merged, or otherwise explained). If you're new to RTE analysis, this toolkit is designed to be straightforward to use, with a very simple syntax that will make it quick to understand. There is a comprehensive documentation at http://docs.rte.org/tools/latest/index.html	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" As can be seen in the last example, we have provided a very nice way to use R with the Atari Games Research Area. Hopefully we will see this become a common R-based methodology in different industries. One of the advantages of the R language over more general languages that we typically work with to write our code is not just the ability to write powerful, well-documented, well-tested, and well-typed code. Because of this, most people are looking for a language that offers these features. The goal of this paper is to present one such language, the SAS Language, for implementing R code. Let's review some of the most essential R features we will need to understand: R has an object model (which may be a little confusing to new R programmers). This helps us communicate with our data, create and manipulate graphs, and much more. R's data models are well documented from the outset. If you have a dataset (or even a set of datasets) that you're dealing with, you can easily jump right to code. For that reason, a database like HBase or MariaDB can give you a simple, powerful syntax to write code that can deal with a diverse set of datasets. The Language we will use in this review is called SAS. Here is a basic definition of the SAS language: SAS = ""SAS"" # You can read that about SAS on the Web. If you don't know what a SAS language is, here is a summary, in simple detail. A language is a collection of instructions that can be loaded into R. A program is any combination of data and instructions. A statement is a sequence of data that are all interpreted as commands. The name ""Sas"" is in a class of languages recognized in industry that was developed by the University of Michigan in the late 1950s and early"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, id( ? ) as id, p.name.lower(), id( ? ).name.lower(), id if (p.name.lower()), c.name = id( ? ), id( ? ) if (c.name.lower()) from ( SELECT title, id( ? ) AS id, title, id ( ? ) AS name, ? as p. name FROM research _ papers ) output (Sparql query): select title, id ( ? ) as id, title, id ( ? ) AS name, id by (! is null( c. name ) AND c. name === id( ? ) ) from ( SELECT TITLE FROM research _ papers WHERE titles = ? AND p.name.lower() = p.title.lower() AND papers = ? ) output (Sparql query): select title, id ( ? ) as id, TITLE, TITLE whereTITLE = ? - ?, TITLE BY id, TITLE and TITLE by id input (text): Can you list the models that have been evaluated on the Oxford-IIIT dataset ? output (table): Select name, ID( ?) and ID( ?) from research papers where papers = ? AND papers = '123456789abcdef0123456789abcdef01234567' ; input (text): Which authors have submitted to the authors list ? output (RDFS): Select authors[name], authors[id], authors[title], authors[c.name], authors[c.author_id] from authors_list where name = ? and authors[name].lower(), id( ?).name.lower() = id( ? ), id( ? ).name.lower	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ROUND(RATIO( ?problem_lbl.FROM( ?rf orkga:ResearchField. ?lab, ?paper orkgp:P30. ?lab ) as ?n.FSCORE ) as ?n_t.FSCORE FROM ?proto ?task and ?proto ?process GROUP BY ?n_t.FSCORE 	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AcrE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT a[ROW_NUM] as paper, rdfs[a] as result, rdfs[b] as paper, orkgp[a] as model, rdfs[b] as paper, paper, orkgp as model rdfs:label p. _________________________________________________________________________________ print a[ROW_NUM]: Print the results of the evaluation of the AcrE model. output (Sparql query): _________________________________________________________________________________ { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?model orkgp:HAD_DATE ?model. ?model rdfs:label p. } input (English text): Could you list the models that have been evaluated on the HoC dataset ? "	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2013"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?metric ?metric_lbl. OPTIONAL { ?dataset a orkgc:Dataset; RDFs:label ?dataset_lbl. } { ?eval a orkgp:HAS_DATASET ?dataset. ?eval orkgp:HAS_METRIC ?metric. } For more, see the RTE documentation and the model parameters documentation. The RTE Metrics document discusses how to interpret and report the metrics in the model. Data: RTE Metrics, RTE models document Data: The RTE Metrics document discusses how to interpret and report the metrics in the model.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl ORDER BY ?benchmark_lbl DESC > ?benchmark ORDER BY ?benchmark_lbl DESC... The query syntax is really a bit different to ordinary expressions, so it may look a bit strange at first sight. But this syntax allows easier parsing and has another advantage: the output of the query depends only on the output variable rdfs, which is a parameter of the GAD query. Now we will use it to fetch a list of all the benchmark titles for the GAD dataset. Note that all the benchmark titles and IDs are required to be unique inside a GAD dataset. If you use the query below to fetch all the benchmark titles and IDs, then the only part that is available as query parameters would be the name and title, which is not required. input (English text): Which authors have published in the journal ""SciEcon '12"" ? output (Sparql query): output (Sparql query): SELECT ?authors ?author_labels AND ?bio ?author_id FROM ?sig ?sig_bio_title WHERE ?author ?author_title ; rdfs:id ?author_id. Now, let's try the query. First, we need a bit of information about the journal, which is the main source of materials for the benchmarking. I assume that the journal is ""SciEcon '12"". output (Sparql query): output (Sparql query): SELECT DOI, TITLE, DESCRIPTION, YEAR, ISSUE, YEAR, REVIEW ORGANIZATION, METHOD, AVAILABILITY, SCIENCE, REVIEW, PAGENAME, AUTHORS, ORGANIZER, TITLE_"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What quantity of iron oxide was discovered on Elorza crater?
 output (Sparql query): "	"SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Elorza crater"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""FeO"")) ?properties orkgp:description ?property_description. } "	 input (English text): Which value of P1-P4 of an electron transfer orbit can be found on the KPS-8 data ? output (Sparql query): ABSOLUTE_DIST_P1_P4: 2.5 ABSOLUTE_DIST_P1_P2: 2.5 ABSOLUTE_DIST_P2_P4: 4.8 ABSOLUTE_DIST_P2_P3: 5.5 ABSOLUTE_DIST_P2_P4_FULL: 6.1 ABSOLUTE_DIST_P2_P3_FULL: 6.1 ABSOLUTE_DIST_P2_P4_RESOLUATION: 7.4 ABSOLUTE_DIST_P2_P3_RESOLUATION_FULL: 7.4 ABSOLUTE_DIST_P2_P3_RESOLUATION_FULL_DET: 8.3 RESOLVED_WALLPAPER_THRESHOLD F1-F6, RESOLVED_WALLPAPER_THRESHOLD F4-F9 	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT TOP 1 * ?dataset ?, ?data_name, ?dataset_rdf, ?data_name, ?data_rdf, ?data_name, * ?dataset_lbl ?dataset. ?dataset_lbl; Or, you can perform an iterative model selection (a.k.a. 'backprop'). To this end, you can use one of two methods: manually, via the Python API's Backprop feature (see ""backprop"" tutorial in the Python source code section) or via the Pandas Python backend's features support mechanism (see ""features"" in the Python source code section of this blog). In this tutorial, we'll first describe the basic design of a backprop model, and then describe the Python API for the backprop model - to provide a high-level overview of how we can use this API. Additionally, we'll describe the Python code required for evaluating backprop models in R, allowing you to learn how to make the backprop evaluation process easier and more reproducible. The backprop model described here (the 'PVS18B' model) takes the classic example of playing a videogame and applies it to predicting the occurrence of Atari games in a real dataset. The 'PVS18B' model's key components are a training and testing set, which consists of a training set and a testing set, and we also use an evaluation set to evaluate our model. The training set is defined by a collection of games (which will usually contain many Atari games but are not required to use this collection, or they can be a custom dataset) whose number and types are specified by the parameters rdfs:list and pclist:list, respectively. The training set has several options, including the type of games used (from 'Game Types' section"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" For this sample submission, to see one of the papers used in The Databricks Blog, please use the code below instead. input (English text): Are there any other papers that have used both the PixelCNN and the DQN-PixelCNN model, and their code, links, and/or URLs, on a consistent basis ? output (Sparql query): For this sample submission, to see one of the papers used in The Databricks Blog, please use the code below instead. input (English text): Do you use the Google ""Dont Be a Dick"" training framework ? output (Sparql query): R If the submission was successfully posted, the code below was used to download the dataset: Downloading training data and outputting it to a.csv file. The training data files are labeled ""databricks"" and contain an example label that can be checked manually through the Databricks library for the example label ""dnd_"". After the training data file is downloaded, it is uploaded to our network.py network. (The ""network.py"" script in the submission will be updated in a later version, with the new dataset, to provide the ""train"" and ""test"" labels again.) Using the ""datamip"" keyword argument, specify the datamip dataset. Downloading training data and outputting it to a single file. These files are labeled ""datamip"" and have the label ""datamip_01"". Training data by default has a size of 1024 bytes. You will need to specify the file size. Downloading training data and outputting it to a single file. These files represent the training data and the ""learn_weights.txt"" from which the models are generated by a machine learning framework. This file contains a copy of the ""learn"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?code WHERE { ?model a. orgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } SELECT DISTINCT ?dataset. ?cont orkgp:HAS_BENCHMARK orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. SELECT DISTINCT ?benchmark. ?benchmark orkgp:HAS_DATASET ?benchmark. ?benchmark. SQL> select ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. from ?dataset. rdfs rdfs; sql statement For more details on how to run the above query, please see: Dataset Benchmarking Using TensorFlow, which is a great read with great links to example code and more analysis. Question 1: Would TensorFlow be faster than the Atari 2600 Freeway network for a high performance gaming query ? This question is tricky since it assumes that every network can be evaluated on an Atari 2600 Freeway dataset. I'm not sure what else you would use TensorFlow for in this exercise. I can tell you that this network was heavily trained on the Atari 2600 version of Space Invaders, so at least that will be a good benchmark for us. Let"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } I also tested a single model across different benchmark conditions on the same dataset. When given the option of using the single model, the same code links in no more than 3 times as many (or fewer) papers with the single-model paper. I'm going to do the same for the other benchmark conditions. This will be a simple benchmark with 5 papers across all benchmark conditions, so that should be enough code to generate the link graph. Now lets run a ""numbers per paper"" benchmark for the same datasets. I ran the above two methods on my dataset (the HoC dataset and the paper benchmark conditions). Here's the result graph, sorted by code link density: So it's obvious that it's useful to have code links on papers. So, my hope with this program is to be able to generate an ""automated benchmarking pipeline"" where I can output the code links to papers, and at the same time, have the benchmark perform a number of tasks (such as searching to search for code"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 output (Sparql query): Select the metric that best fits for the Metric in the benchmark dataset input (English text): ? How big are the datasets ? output (Sparql query): How does the model score ? input (English text): ? In which NLP-TDMS (Exp, arXiv only) benchmark data are you interested ? output (Sparql query): ? Which benchmark data were you interested in ? input (English text): ? What is your interest for this data ? output (Sparql query): output (Sparql query): Select the metric that best fits for the Metric in the benchmark data ? input (English text): ? Which metric dataset are you interested in ? output (Sparql query): ? Select the metric that best matches the Metric in the benchmark data ? input (English text): ? What is your interest for this metric dataset ? output (Sparql query): ? Do you have any comments about your data quality ? input (English text): ? How well does the model perform on the input data ? output (Sparql query): ? Which metrics were you interested in ? input (English text): ? Which Metric datasets are you interested in ? output (Sparql query): ? What do you value most in the Metric dataset ? input (English text): ? How well does the model perform on the input data ? output (Sparql query): ? Select the metric that best matches the Metric in the benchmark data ? input (English text): ? In which datasets do you have these metrics ? output (Sparql query): ? Which Metric datasets were you interested in ? input (English text): ? What are the metrics that the model achieves ? output (Sparql query): ?	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT statement look like this: In RDF, for each row in the result set we use a SQL engine to evaluate the expression. Which is faster ? SQL is a compiled language, which means its only a matter of the CPU speed. For RDF queries to execute in a timely manner, you should ensure that your RDF engine is optimized for speed. Some additional considerations: If you do queries on a wide or long range of different data sets—you can do those on a wide range of data sets, but not on the same data set when the range of RDF is more narrow—then you may need to use the ""as"" keyword followed by some sort of filter (you might filter to display only the top 10 performance problems and then do a quick filter after that to exclude the top 10 from the performance results) or some such thing. In general, if you have an area of overlap, you should use as instead of as= to filter for a subset. If an RDF expression requires a large number"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT rdfs:dsc:DescribeRDFs.distribution ?rdfs_lbl_p. IF  ORDER BY  ""model"" ) | output (Sparql query): What is the output (dsc_table column with ' ?' prefix) like ? The response was not as helpful. I was asked a lot of questions about performance, code structure, and validation environment. The code was not the primary thing I wanted to know. The code was not the primary thing I wanted to know. The code was not the primary thing I wanted to know. The code was not the primary thing I wanted to know. And, I didn't follow the recommendations, especially with respect to validation. I do like that the paper was titled ""A Framework for Verification of Model-Based Trust"". If I had no idea about the actual model-based trust model, I'm not sure I would have agreed to the paper, not having read it. There are many, many reasons to be skeptical of a paper titled ""A Framework for Verification of Model-Based Trust"". We haven't gotten that paper yet. It seems that the researchers are still working on it. They have a number of options, including: writing their own model-based trust model (e.g. a supervised version of the DuIE matrix), extending the DuIE matrix by generating an appropriate measure of an appropriate model for the dataset, or using a supervised model with a more appropriate weight on the model than used by the original model. The paper can be found here. 2.  A Model of Trust between Parties (TMT) (a)      TMT is a trust-enhancement framework, which aims at building trust in an economic system, through: improving trust in the economy (reducing asymmetric risk"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 The second output is the result of the query for the above query but this time it contains the RDFS labels from the query for the Benchmark, Benchmark or Paper. The second output can contain multiple levels of labels and columns, so you may define your desired RDFS labels in a specific order. The RDFS labels on the Benchmark and Paper are based on data from the GAD version 2.0.0. The GAD label on the Paper (Sparql result) is based on the GAD version 2.0 and GAD version 3.0.0. The GAD label on the Benchmark is based on a GAD version 3.0 and published version 4.0.0. The RDFS labels for the Benchmark and Paper are based on the GADA version 2.12.0 and published version 3.12.0. The GAD or GADA version 2.12.0 and the GADA version 3.12.0 are no longer supported and may be removed from GAD 1.0.0 and onwards, but their use is not recommended for applications requiring access to such data. How Does RDF Viewer Help My Job ? RDF Viewer is a useful tool for RDF parsing for analyzing applications. Most of the RDF resources available for parsing are now available in RDF Viewer, so it becomes a lot easier to browse and quickly learn the terminology that is used in existing solutions. The RDF Viewer has a comprehensive set of resources and information in an easy to read and easy to navigate interface. By having a quick look at the resources below, you will be able to gain experience in the most important tasks, which will be helpful to you when you go on to implement applications with RDF. Quick overview of the most important tools and what they	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Finnish-to-English benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 French-to-Spanish benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Spanish-to-English benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Chinese-to-English benchmark dataset ? output (S Parql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Swedish-to-English benchmark dataset ? output (S Parql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Danish-to-English benchmark dataset ? output (S Parql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Chinese-to-French benchmark dataset ? output (S Parql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 German-to-English benchmark dataset ? output (S Parql query): input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 Danish-	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What types of nanocarriers do have therapeutic effect?
 output (Sparql query): "	"SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = ""T""^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } "	SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) (1) FROM ?metric ?metric_lbl to ?metric_lbl GROUP BY 1 ORDER BY ?value }, { { SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) (1) FROM ?metric ?metric_lbl to ?metric_lbl GROUP BY 1 ORDER BY 1 }, { { SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) (1) FROM ?metric ?metric_lbl to ?metric_lbl GROUP BY 1 ORDER BY 1 }, { { SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) (1) FROM ?metric ?metric_lbl to ?metric_lbl GROUP BY 1 ORDER BY 1 } }, { { SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) (1) FROM ?metric ?metric_lbl to ?metric_lbl GROUP BY 1 ORDER BY 1 } } } _________________________________________________________________________ [{1}] (2) _________________________________________________________________________ { { 1,2 }, 3} ] _________________________________________________________________________ To further investigate the output of the output_test.rst query, we need to modify it so that we can do the following comparison: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 48 49 50 51 52 53	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT name; ORG, BOOLEAN, SUM( { ?model ?model_lbl } ) FROM `datasets/hoce2/` AND `datasets/cl-scisumm/hoc_sc_s; select name from `datasets/hoce2/` where rdfs:label < ?dataset_lbl. rdfs:count = ?benchmark && rdfs:key = ?rdfs_lbl. rdfs:name = ?model_lbl. rdfs:type = [ ?dataset_lbl. HASH ] } output (Sparql query): OPTIONAL { ORG, BOOLEAN, SUM( { ?model ?model_lbl } ) | BOOLEAN, SUM( { ?model ?model_lbl } ) } output (Sparql query): ORG = ?benchmark || cl_scisumm || ?cl_scisumm || ?cl_scisumm || ?cl_scisumm | output (Sparql query): or | output (Sparql query): BOOLEAN = ?dst || output (Sparql query): SUM( { ?model ?model_lbl } ) = ?str || output (Sparql query): ORG | BOOLEAN = ?dst || output (Sparql query): SUM( { ?model ?model_lbl } ) = ?str || output (Sparql query): BOOLEAN = ?dst || output (Sparql query): SUM( { ?model ?model_lbl } ) = ?str || output (Sparql query	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + weight-sharing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------- --------------------------------------------------------- TOPIC DESCRIPTION --------------------------------------------------------- RTE Top10 10-year annual score of all countries in the RTE dataset. rdfs Top1 Top10 Top1 ranking of country in RTE database for a given time period. orkgp-test Top10 Top10 scoring country with 10 % of the global score of a random set of countries. rdfs-trend Top1 Top1 ranking of country with the top 10 % of RTE datasets for each country period. orkgc-test Top10 Top10 scoring country with the top 1 % of the world data for a random set of countries. orkgp-trend Top1 Top1 ranking of country with the top 1 % of the country-based top10 databases.	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Permuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"------------ ------ { { ?dataset a ORPK:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") orkgp:HAS_DATASET ?dataset ORPK; orkgp:HAS_EVALUATION ?eval. ORGG_MAX 10, ORGG_MIN 100. } } output (Sparql query): \----------------------------------------------------------------- \-----------------------------------------------------------------. | Top score | \----------------------------------------------------------------- | ORGG10 ORGG20 ORGG30 ORGG40 ORGG50 | | ORGG50 ORGG60 ORGG70 ORGG80 ORGG90 ORGG100 | \----------------------------------------------------------------- \-----------------------------------------------------------------. \-----------------------------------------------------------------"" \-----------------------------------------------------------------"" \-----------------------------------------------------------------"" ORGG10 ORGG20 ORGG30 ORGG40 ORGG50 ORGG60 | ORGG60 ORGG70 ORGG80 ORGG90 ORGG100 | \-----------------------------------------------------------------"" \-----------------------------------------------------------------"" \-----------------------------------------------------------------"" ORGG10 ORGG20 ORGG30 ORGG40 ORGG50 | ORGG50 ORGG60 ORGG70 ORGG80 ORGG90 ORGG100 | \-----------------------------------------------------------------"" I will only have an overview here of a few examples and hopefully there will be another post explaining examples in other languages. Hopefully I will get some feedback which will help me to define better and more useful questions. Please leave a comment if you have one"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ROW_NUMBER() OVER ( PARTITION BY { ?dataset ?model: ?dataset rdfs:label ?model} ORDER BY { ?dataset ?model : ?dataset rdfs:label ?model} DESC )  GROUP BY rdf_label ORkgp:HAS_EVALUATION  GROUP BY rdf_label ORkgp:HAS_BENCHMARK  SELECT rdf_model_lbl  AND rdf_label  ORDER BY { ?Dataset ?model: ?dataset rdfs:label ?model} DESC  ORDER BY ?model rdfs:label ORkgp:HAS_PRIMARY_LINK  ORDER BY { ?Dataset ?model: ?dataset rdfs:label ?model}  ORDER BY rdf_item - ?model  ORDER BY { ?dataset ?model ?model} ORDER BY { ?dataset ?model ?dataset}  ORDER BY { ?dataset ?model }  ORDER BY { ?dataset ?model | ?model rdfs:label} ----------- -------------------------- Bivariate --------------------------  GROUP BY   ?dataset  GROUP BY rdf_label ORkgp:HAS_DATASET  SELECT rdf_model	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset;... } } input (English text): Show the number of times each of the models made it to the F1 ranking ? output (Sparql query): output (Sparql query): Output the number of times a specific model reached the F1 ranking ? input (English text): Which model has achieved the highest F1 score on the HoC benchmark data ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE... ?output }... } orkgc:Metric orkgc:Metric. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?metric_lbl ?metric_lbl WHERE... } ?output orkgc:Metric orkgc:Metric. FILTER (str( ?metric_lbl) = ""F1"") ?output orkgc:Metric output (Sparql query): Output the metric values of one of the models on the benchmark data ? input (English text): Which model has achieved the highest F1 score on the CommonsenseQA benchmark data ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a ?metric_lbl. FILTER (str( ?metric_lbl) ="	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	【RESULT】 【RECOVERY】 【RECOVERY_LEVEL】 【MAX_GINNER_SUGGESTIONS】 【AVG_GINNER_SUGGESTIONS】 【AVG_HEALTH_GINNER_SUGGESTIONS】 【MAX_HEALTH_SUGGESTIONS】 input (English text): RTE 『RTE (TM)』 『RTE (TM)』 『RTE (TM)』 『RTE (TM)』 『RTE (TM)』 『RTE (D-GIS)] 『RTE (D-GIS)] 『RTE (D-GIS)] 『RTE (D-GIS)] 『RTE (D-GIS)] 【RESULT】 【RECOVERY】 【RECOVERY_LEVEL】 【MAX_GINNER_SUGGESTIONS】 【AVG_GINNER_SUGGESTIONS】 【AVG_HEALTH_GINNER_SUGGESTIONS】 【MAX_HEALTH_SUGGESTIONS】 【MAX_CATALOGS】 【AVG_CATALOGS】 【AVG_CATALOGS】 【AVG_CATALOGS】 【MAX_CATALOGS】 【AVG_CATALOGS】 【AVG_GINNER_SUGGESTIONS】 【MAX_CATALOGS】 input (English text): What is the RTE-D-GIS dataset (with Metric) ? output (Sparql query): RTE-D	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT... ?model_lbl. FILTER (str( ?model_lbl) = ""HLSA"") { SELECT ?model_lbl. ?metric ?metric_lbl. ?lbl | ( ?dataset ?dataset_lbl. ) FROM ?model || ?dataset_lbl AS ?lbl ORDER BY DESC( ?value) LIMIT 1 } } input (English text): In your opinion, which model performs the best on the HoC benchmark dataset, by using only aggregates where applicable ? output (Sparql query): SELECT DISTINCT ?model || ?model_lbl. ?metric || ?metric_lbl., ?lbl. CASE ?lbl WHEN ?lbl = ""hoC"" THEN 1 ELSE 0 END., ?metric || ?metric_lbl., ?dataset. ?dataset || 1 AND ?data_frame ?data_frame. DESC(... ); input (English text): Which model performs the best on the HoC benchmark dataset, by using only H2O aggregates without filtering ? output (Sparql query): SELECT DISTINCT ?model || ?model_lbl. ?metric || ?metric_lbl., ?lbl. CASE ?lbl WHEN ?lbl = ""hoC"" THEN 1 ELSE 0 END., ?metric || ?metric_lbl., ?dataset. ?dataset || 1 AND ?data_frame ?data_frame. DESC(... ); input (English text): What percentage"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 output (Sparql query) input (English text): What is the minimum cost to run a model with 1 million objects ? output (Sparql query): input (English text): What is the minimum cost to execute a model with 1 million objects ? output (Sparql query): input (English text): What is the maximum cost to run a model with 1 million objects ? output (Sparql query): input (English text): What is the maximum cost to execute a model with 1 million objects ? output (Sparql query) input (English text): Please explain the significance of the results. output (Sparql query) input (English text): In what way was the significance determined ? output (Sparql query) input (English text): What is your name ? * output (Sparql query) input (English text): Can you describe the situation in detail ? output (Sparql query) input (English text): How many users are currently using the product ? output (Sparql query) input (English text): Can you please tell me about the user ? * output (Sparql query) input (English text): Please explain the user *	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?mach orkgc:Model; rdfs:label ?mach. FILTER (str( ?mach_lbl) = ""VPN"") ?machm:MACH_MATCH ?mach. input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?refl orkgc:Model; rdfs:label ?refl. FILTER (str( ?refl) = ""VPN"") ?reflc:REFTIL orkgc:REFTIL_MATCH ?refl. If you'd like to contribute code and data analyses, please join the discussion below and register for an account on the github repository. Thanks!"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT COUNT(*) FROM { ?dataset a orkgc:Dataset; raf:Score ?dataset_lbl. ?count orkgp:HAS_DATASET ?dataset_. }; output (Sparql query): ----------- score (number) ------- 1 3.67 6.50 31.67 Rows: 3 Output: ------- score (number) ------- 1 3.67 6.50 31.67 The results could be written as follows: 1 1 SELECT COUNT(*) FROM { ?dataset a orkgc:Dataset; raf:Score ?dataset_lbl. ?count orkgp:HAS_DATASET ?dataset_. }; -- Selects the highest benchmark scores for the Sequential MNIST dataset output (Sparql query): 2 9.17 37.25 101.14 Rows: 3 Output: ----------- score (number) ------- 1 9.17 37.25 101.14 A query as follows: 1 1 SELECT COUNT(*) FROM { ?dataset a orkgc:Dataset; raf:Score ?dataset_lbl and ?count orkgp:HAS_DATASET ?dataset_. } AND raf:score < ?score and ?count < ?score; -- Selects the highest scoring non-linear process for the Sequential MNIST dataset output (Sparql query): SELECT rfc_score AND raf_score FROM { ?dataset a orkgc:Dataset; r	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT papers_cited ?key ?value. FOR each papers_cited ?key ?value. RETURN ?key ?value. ORDER BY ?key, ?value. input (English text): What is the top benchmark score and its metric on the CRF dataset and also the corresponding value on the RTE dataset ? output (Sparql query): SELECT TOP(@a) scores.score  @a  FOR each scores.score AS rval FROM cfft_benchmarks GROUP BY  ( score ) AND ( score <= 1 ) ORDER BY rval input (English text): How can you make a quick version of the RTE dataset that you can use in RDD ? output (Sparql query):  SELECT cfft_score.RTE from cfft_benchmarks ORDER BY cfft_score.RTE DESC LIMIT 1, 2 input (English text):  Describe the CRF models and outputs on your own work ? output (Sparql query): SELECT cfft_model.cfft.cfft_eval FROM cfft_benchmarks GROUP BY cfft_model.cfft.cfft_eval ORDER BY cfft_model.cfft.cfft_eval DESC LIMIT 1, 2 Note : I used cfdw to run the queries and this way we can use any version of the RTE dataset. Also, I used the same query ( cfft_score.RTE) with the Rtd score. input (English text): What are some interesting cases about how CRF and RTE are used in different applications ? output (Sparql query): SELECT output_cfft_model.cfft_eval, input.label FROM	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT m.metrics.type, c.score, i.score, R2, m.name, f.fraction_left, i.name, f.fraction_right, N.proportional_mean, f.min_alpha as R1, f.min_size as R2, m.name as R3, f.min_alpha as F, { ?dataset a orkgc:Dataset ?dataset_lbl; }, f.min_alpha as AlphaR, f.max_size as GammaR, f.proportional_mean as PRIMARY, ?maxiter_left as MaxIter, ?maxiter_top as MaxIter_Top, ?miniter_left as MinIter, ?miniter_top as Miniter_Top, ?sample_left as SampleLeft, { ?p<0 ?q.p as Q, ?r<0 ?S.sample_left as S, ?t<0 ?S.sample_left as S, } } Note: If the input parameter to query is ""R2"", you will need a second command-line argument named ?R2. In this case, it would look like this: source: http://www.sciencedata.org/GIS/datasets/SciTLDR-stats.cfm ?metrics_type=R2 output: Using the same metrics, evaluate all models on the HoC dataset ? This is very convenient to use as a way to analyze each model separately. Here is an example of a benchmark which allows the user to set each of the variables using their respective values (r2, sample_left, mean, median and so on). It would be possible to use this as a basis of a more complex and"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 If you're the type of person who actually loves your data, then give it to a non-profit. You can give it to Google, IBM, etc. You don't have to go through all of that legwork: You have all of the data already, and will probably do much better by sharing. You can use the public data to answer scientific questions that you care about. Take WMT2015 for example. It's much more likely that the researchers who run the tests are not doing them for the money. You can find the papers that run the tests in Science.  Most of the models that have been evaluated and reported with WMT2015 run on a single machine: There are two main questions that WMT2015 answered: How will text be understood (e.g. by humans) in a language other than English ? How will the model of a language be interpreted by humans (e.g. in a machine) ? In WMT2015, the answer is pretty clear, and I'm not sure if a different answer would produce better results: The model of a language can be interpreted on a single machine (say, a neural network) as it would be interpreted for a human (say, a machine learning expert): It turns out that the models that would be useful to interpret the machine learning data would be the ones that had already been trained on the WMT2016 English dataset But you don't have to start from scratch. You can use the public data to answer a different set of questions. Take a moment to go through this post  and try to imagine you were running a data science experiment on your own machine, with no knowledge of how text is interpreted by humans. Let's say that's you, right now: In order to answer the questions about semantic Web, you should be able to interpret an English sentence. If you've never done	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The output of the query is the metric_lbl in columns for each query: rows for all CIFAR-100 data; columns for the individual dataset analysis: # CIFAR-100 data: ## a CIFAR-10 ## t-test mean df P value ## b CIFAR-100 # of clusters ## 1 1.67 2.45 0.004931 0.05 # 2 2.46 2.38 0.010768 0.06 # 3 2.53 2.48 0.006604 0.08 ## 4 3.01 2.49 0.006296 0.05 ## 5 3.06 2.50 0.007680 0.06 ## 6 3.15 2.45 0.007054 0.04 ## 7 3.26 2.44 0.009842 0.05 ## 8 3.35 2.42 0.008036 0.03 ## 9 3.39 2.39 0.004155 0.05 ## 10 3.47 2.37 0.007970 0.04 # CIFAR-100 cluster sizes ## 1 13.7 15.4 3.1439 0.06 # 2 13.5"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT COUNT(*) AS _met_, COUNT(*) AS rs_lbl_, COUNT(*) AS tcl.tcl, COUNT(*) AS yh.yh, SUM(val) AS val2, SUM(reward) AS reward2, SUM(metric) AS metric2 FROM rte_metrics as rs WHERE rs.dataset = ?dataset_lbl ORDER BY rt_timing LIMIT 100 ; In RTE the dataset must be the same for the different models. So there can be more than two datasets. There can be some model results, including error metrics such as time-invariant gradient-based measures. So in RTE you need the metric for each model in a different context. Note that, most of the above example output was generated with the data set as RTE dataset. A new RTE dataset can be created with new data by simply running the script.dataset with the -dataset argument. And this could be useful if you don't have an RTE dataset yet. RTE uses a fixed format of the RTE metric set [5]. This is not supported by RTE in SparseArray. Here is the format: rtable:metric(label(1, 1), labels(1, 1)) RTE uses the first label of each output label to denote the metric which is added to the corresponding label of the RTE data file. We'll show how to read and write data set metadata in a later post. The metadata has the following format. First one number tells the name of the metric to be read, and the second number contains the metric's ID. I added labels for the values, so that you can inspect the results and compare them: format: rtable:metric_l	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT score (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), score (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), score (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), score (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score), scores (Metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.Score metric.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?benchmark ? orkgp:LIFO ?cont; ?output in. ?benchmark. ?cont; What are the titles and IDs of research papers that include a benchmark for the GAD dataset ? input (English text): Could you give an example of each of the three variables that are currently in the default GAD dataset ? output (Sparql query): SELECT 'B','A','M' FROM <dataset.lbl> as lt.,<dataset.labels> as lb.,<dataset.labels> as lb.,<dataset.labels> as lb.,<dataset.labels> as lb.,<dataset.labels> as loa.,<dataset.labels> as loa.,<dataset.labels> as lbu.,<dataset.labels> as lbu.,<dataset.labels> as lbr.,<dataset.labels> as lbr.,<dataset.labels> as lc.,<dataset.labels> as lc.,<dataset.labels> as sx.,<test2_name> and name,<test2_name1> from 2 as b,2 as ad,3 as bt,4 as c,5 as ct,6 as d,7 as dt,8 as dc. output (Sparql query	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT lbl_reference ?lbl. You can download the benchmarking paper itself. You can find a lot of data at the paper reference site. To download and import our training dataset, download the pdf version.	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): Can you show the full dataset from which you obtained maximum score for this model ? output (Sparql query): orkgp:HAS_DATASET ?dataset; { ?dataset a orkgp:HAS_DATASET \ ?model rdfs:label ?model_lbl. OPTIONAL { ?paper orkgp:HAS_MODEL ?model; ?model rdfs:label ?model_{lbl} orkgp:HAS_EVALUATION ?eval}. ?model_lbl orkgp:HAS_BENCHMARK ?model_lbl. } input (English text): Can you provide the highest model score in either the HoC and DMControl100k, DMControl100y dataset ? output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query) input : Can you list the models that have been evaluated on the HoC dataset ? output : orkgp:HAS_DATASET ?dataset; { ?dataset a orkgp:HAS_DATASET \ ?model rdfs:label ?model_lbl. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl orkgp:HAS_EVALUATION ?eval}. ?model_lbl orkgp:HAS_BENCHMARK ?model_lbl. } 	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ?.. ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ?.. ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ?.. ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Django app): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (Soup): ? ? ? ? ? ? ? ? : ? ? ? ? ? ? ? ? ? ? output (SQL): ? ? ? ? ? ? ? ? : 	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 select COUNT(*) as problem_lbl_lbl from problem_lbl on where problem_lbl__name in ( ?_name__: ? ? ?) select COUNT(*) as problem_lbl_lbl_c from problem_lbl_c on where problem_lbl__name in ( ?_name__: ? ? ?) output (Sparql query): ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? output (Sparql query): ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ______ ? ? ? ? ? ? ? ? __________ ________ ________ ________ ________ ________ ________ ________ ________ ________ ________ ? ________ __ ________ ________ ________ ________ ? ________ __ __ __________________________________ ________________ ________ ________ ________ ________ ________ ________ ________ ________ ________ ? ? ____	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?metric. ?metric rdfs:label ?metric_lbl. *, ?tname, ?title, ?author. ------------------------- ORDER BY ?tname ORDER BY ?title, ?author. OPTIONAL { ?tname ?title. } ------------------------- ------------------------- ------------------------- 1. ?metric tname ( ?tname ?max) 2. ?metric rdfs:label ?tname 3. ?metric rdfs:value ?dataset. ?metric rdfs:label ?tname 4. ?metric rdfs:value ?dataset_lbl. ?metric rdfs:label ?tname ------------------------- output (Sparql query): ------------------------- returns: 1. ?metric rdfs:label ?tname returns: 2. ?metric rdfs:value ?dataset_lbl. 1. ?metric tname returns: 3. ?metric rdfs:value ?dataset_lbl. 4. ?metric rdfs:value ?dataset_lbl. As is often the case in machine learning, we will use the R package R, and its data.frame function, to query it. Specifically, we need the rdfs file associated with a research article. The R package is designed to query a number of common R packages for features associated with the dataset. The R package provides us with a number of additional capabilities, including functions to load data in other formats (text, binary, html and pdf), to perform common data manipulation techniques on	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	------------- { { RTE:AUTHORITATIVE_STATISTICS ?metric_lbl. } } input (English text): Which model has achieved the highest Accuracy score on the Yelp-1 benchmark dataset ? output (Sparql query): ------------- { { RTE:AUTHORITATIVE_STATISTICS ?metric_lbl. } } input (English text): How many users are using this dataset ? output (Sparql query): -------------- { { RTE:AUTHORITATIVE_STATISTICS ?metric_lbl. } } input (English text): Which model has reached the highest Score on the OST and RTE benchmark datasets ? output (Sparql query): -------------- { { RTE:ODD_DETECTION ?metric. } } input (English text): Which model has gained the most points in the OST benchmark database, and how do it compare to the other models ? output (Sparql query): ------------- { { RTE:ODD_DETECTION ?metric. } }	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?sdataset ?sdataset_{ ?dataset}. ?sdataset_lbl. ?sdataset_lbl. ORG_MATCH ?sdataset_lbl. { { ORDER BY ?sdataset_lbl. ?value, ?value. ?value. OPTIONAL rdfs:label ?sdataset_lbl. } ?mod ?dataset_lbl. OPTIONAL ORG_MATCH ?dataset_lbl. LIMIT ?metric. { { ?metric_lbl ?dataset. } ?metric rdfs:label ?metric_lbl. } } ORDER BY ?metric ?metric_lbl LIMIT ?metric. ORG_MATCH ?dataset. ORG_MATCH ?dataset_lbl. ORG_HAS_BENCHMARK. LIMIT ?meter. ?metric_meter_lbl LIMIT ?meter. ORG_MATCH ?dataset. ORG_MATCH ?metric_lbl LIMIT ?meter. ORG_MATCH ?dataset_lbl LIMIT ?metric_meter_lbl LIMIT ?metric_meter_lbl LIMIT ?metric_meter_lbl LIMIT rdfs:label ?metric_lbl. OPTIONAL ORG_MATCH ?dataset ||gte|||-gte|||-gte|||-gte|||-gte|||ORG_MATCH ?metric. GROUP BY ?metric ?metric_lbl In the WMT2016 German-English dataset test, we have selected the top scoring model and made	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (Html output): This method provides us with a simple way to compare our methods. We used this data set using the Pinnacle, SDS (software for statistical testing, analysis and visualization), and ScaLite libraries, before comparing our methods to the R statistics package. We used the same methods used by SAC, with some addition features. The SAD model has been used for this post and is in fact written by Markku Halonen, and has been available in R for less than a year! The Pinnacle SDS and SDS.1 models have already been used in previous posts, and the two package were purchased from the author. We are not looking here at the methods for an introduction to statistical testing since these are the most important for us. Let's instead focus on the results from our performance test since this will help us to gain a better understanding of the differences between the SAD and SAC methods. First, I looked at our paper, as I mentioned before, and determined its relative differences over both SAD and SAC. To start off the comparison, I performed a comparison on the Pinnacle version against both the ScaLite and SDS.1 versions: To our surprise, the SAD method was more accurate than either of the two competing statistical models. So it seems that the SAD approach has an advantage over both SAC and the SDS.1 package. We also looked at our analysis with the SAD model as compared to a third test using the SAD and SDS.1. The SAD method did do better over the S.I.D.E., SDS.1 and S.A.SE methods. This is important for us to note, because it gives us confidence that the SAD method outperforms both of these packages and compares well with some of the other packages we have	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT TOP 50 [top 50/1]: [LINK 1, 1, 1, 1, 1, 1, 1 (1)] [LINK 2, 1, 1, 1, 1, 1, 1 (1)] [LINK 3, 1, 1, 1, 1, 1, 1 (1)] [LINK 4, 1, 1, 1, 1, 1, 1 (1)] | [LINK 5, 1, 1, 1, 1, 1, 1 (1)] [LINK 6, 1, 1, 1, 1, 1, 1 (1)] [LINK 7, 1, 1, 1, 1, 1, 1 (1)] [LINK 8, 1, 1, 1, 1, 1, 1 (1)] [LINK 9, 1, 1, 1, 1, 1, 1 (1)] | [LINK 10, 1, 1, 1, 1, 1, 1 (1)] [LINK 11, 1, 1, 1, 1, 1, 1 (1)] | [LINK 12, 1, 1, 1, 1,	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Past Decode Reg. + AWD-LSTM-MoS + dyn. eval."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Note about the Past Decode Reg, the current version can test both the Past Decode Reg and the Dyn. eval module, however they provide different results and are not compatible. The previous version (see the Future Work section) uses the Past Decode Reg + AWD-LSTM-MoS + dyn. eval + model for the benchmarks and can't test all the other benchmark modules, due to limitations with testing models directly. That's why I recommend using Dyn. eval + model instead.	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Input (English text): You have asked for my input data, what's this ? Output (Sparql Query): The input data consists of data used to evaluate the top performing model on the HoC benchmark dataset. You have asked for my input data, what's this ? output (Sparql Query): The input data consists of data used to evaluate the top performing model on the Atari 2600 Assault dataset. You have asked for my input data, what's this ? output (Sparql Query): The input data consists of data used to evaluate the top performing model on the Atari 2600 Assault dataset. Output (Sparql query): Which model has achieved the highest F1 score on the HoC benchmark dataset ? Output (Sparql query): In your question, I am not sure about the ""best model"", and I might have answered wrong. Why is this ? In the question, I am not sure about the ""best model"", and I might have answered wrong. Why is this ? As your question would explain, I did not understand the question, and did not correctly interpret your query. Please read the next few sentences carefully: the question is about a list of models, not which models are the best: in the question is a ""top performance ranking"": I think this answers your question correctly: the question is about a list of models, not which models are good your question was not correct, because the question was not correct to the extent of it stating all models are good. If this is not the answer for your question, then ask for the data again with the following context: in the question is a ""top performance ranking"": As your question is about the ""top performance ranking"" of a list of models, then your question was the wrong answer. As your question was not"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1-score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Show the results! Output: Output: Input: 100x100 rows output to: test.sql file. Please note that these tests were made on a Windows machine.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""STREET"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DIVISION ?benchmark ?dataset_lbl. RDFS:STREET. output (Sparql query): WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 How does the evaluation of your model differ from other evaluation methods ? This would be an important question for a real-world team to have a valid answer for. If your team only has one of the three evaluation methods: F1.1 Benchmark, HoC, or EVALUATION, then, yes, you can easily estimate your model's performance using the same scoring system, just by looking at all variables that have been measured! If you only have one evaluation method and only one metric, it's more problematic, because you can't compare the evaluation method and the metric in the same way. Most evaluations fall into a few categories. F1.1 Benchmark - This is the only method where we actually have a scoring system. It is based on multiple metrics that we can measure (as well as, we can see the difference between a score out of 1, 2, or 3, depending on how far removed each metric is from 10). Most F1.1 benchmarks will ask you to select the metric that gives the largest performance boost using a weighted average of the various metrics. The weights are generally more than 1 for each metric. The weights are usually set to 3 to 1 for each metric, so in our case, 0.1, 0.2, 0.3. This is the kind of scoring system that we want. Also, the weighting of the scoring metric is not arbitrary. It is generally given by the highest order, or most popular, metric in the class; if the most popular metric isn't the fastest, then the weighting of that metric is 1.1. That number is chosen in accordance with the scoring value for the metric, as determined by the scores at the benchmark and in EVALUATION. We won't describe EVALUATION in much detail, since we will show you how it works in detail below. EVALU	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, model_id, dataset_id, orkgc.id, eval.val.p, eval.val.data if ?dataset.dataset = ? or kgp.data.size <> ?data.size SELECT DISTINCT ?dataset.dataset ?model_lbl. (REFERENCES ?dataset_lbl. ) OR  IF  IS NULL SELECT ( SELECT title, model_id, dataset_id, orkgc.id, eval.val.p, eval.val.data  if ?dataset.dataset = ? OR kgp.data.size <> ?data.size)  else select ?dataset.dataset SELECT DISTINCT ?dataset_lbl. ?dataset.   if ?dataset.dataset = ?  or ?dataset.dataset_id = ?  select ?dataset.dataset_id, eval.val.p, _, NULL  else select ?dataset, ( NULL ?dataset_lbl. )  else select ?dataset, eval.val.p if ?dataset.dataset.index_id = ?  or ( ?dataset.dataset. index_id = ? )  else select ?dataset.dataset  else select ?dataset, ( NULL ?dataset.data ) if ?dataset.data.index_id = ? select ?dataset.data.index_id  else select ?dataset, eval.val.  else select ?dataset, eval.val	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" select d from tests; select rnd(round(dataset.avg_accuracy > 0.5), max(dataset.avg_accuracy)) from tests where d.error < { ?dataset_test.dataset.value ?dataset_test.dataset.error } ?batch_count=c(1); select * from tests where d.accuracy > { ?dataset_test.dataset.value ?dataset_test.dataset.accuracy } ?dataset_count=c(1); select * from tests where d.accuracy < { ?dataset_test.dataset.value ?dataset_test.dataset.accuracy } ?dataset_count=c(1); The problem with the original model output is that it doesn't display the actual performance (or at least the performance without applying optimization or whatever). Unfortunately, the data I captured is from a web application that didn't support XML serialization. Although I can't write it to a XML file (nor can it be done efficiently or well today), I can convert the JSON to a Python file and export it to CSV, which makes it easy to inspect the model performance within RTE. Also, I'll show the code to generate your CSV. Also, the results might not be meaningful. Nevertheless, it may be enough to identify a possible issue. Another potential concern is the use of the word ""inferior"" as part of a model output. However, the first part of the output is the accuracy score of the model, and the second part of the output is the accuracy against the benchmark data by the RTE model. It is very difficult for me to imagine that it would be ""inferior"" over the"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?input_class ?lbl ?classid ?value ?attribute ?attribute_lbl. AND COUNT ( ?input_class ) < ?value ?attribute_lbl. AND COUNT ( ?attribute_lbl ) > ?attribute_lbl. OPTIONAL { ?attribute_lbl }. OPTIONAL { ?rdfs:labels ?attribute_lbl. } OPTIONAL { ?dataset ?attribute_lbl- { ?dataset_lbl ?dataset } ?lbl. } or kgp:HAS_INITIALIZED_MODEL ?attribute_lbl-{ ?dataset_lbl ?dataset } ?lbl. OPTIONAL { ?dataset ?attribute_lbl- { ?dataset_lbl ?dataset } ?lbl. } input (English text): What are typical model parameters ? output (Sparql query): SELECT ?value ?attribute_lbl. or ?attribute_lbl ?lbl. AND COUNT ( ?attribute_lbl ) > ?attribute_lbl. AND COUNT ( ?attribute_lbl ) > ?attribute_lbl. AND COUNT ( ?attribute_lbl ) < ?value ?attribute_lbl. OR COUNT ( ?attribute_lbl ) < ?attribute_lbl. OR COUNT ( ?attribute_lbl ) > ?attribute_lbl., ?attribute-lbl orkgp:HAS_INITIALIZ	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Note that the output output of the command output() must also include the argument ?cont if the parameter ?cont is set. Note also that an output of the input() method must include the argument ?cont if the parameter is set or ?benchmark if the parameter is set. input (English text): What were the evaluation metric(s) used when benchmarking models on the WMT2014 English-German dataset ? output (Sparql query): Note that the output output of the input() must also include the option ?test. input (English text): What are the evaluation metrics required when benchmarking models on the WMT2014 English-German dataset ? output (Sparql query): Note that if your WMT2014 model does not fit one of the three criteria, the query returns an error. output (Sparql query): input (English text): What is the parameter required for the output call ?benchmark (only possible with a parameter set) output (Sparql query): Note that the output output method outputs the second argument only if the parameters ?dataset and ?datasets are set and ?benchmark is set. output (Sparql query): What is the parameter required for the output call ?benchmark (only possible with a parameter set) This function is available with both the R function model_test() and with the data_tools package from Microsoft. Example 2: Benchmarking the TensorFlow KNN model on WMT2014 German text document Given the training set as in example 1, this example compares model's performance on the WMT2014 German text document to the benchmark of WMT2015 English document benchmark. The output is available for the result set in the SPSS template,	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" The results show that the user can browse their favorite papers by topic, and a visualization is created showing the relative popularity of most papers. All of your keywords/subtags can be entered as the keyword fields in the query text. These records are returned in the final result set. You can use the ""subtitles"" field to add a title to the results. Some queries are based on the results of other users. If for example the user already knows that the document ( ?record) is in the field ""P30"" and searches for the keywords and subterms related to ""P30"", they would be able to view all the papers of the authors related to the topic. If you know the key words and subterms related to the topic you want to find papers for, you can specify these using the subtag field. Please see here for a list of supported subtag field extensions. So what is next ? We want to add some more ""real"" functionality that would have more impact on the users and would give them more control and choice. For example, you can have multiple tables with different results, in which case you can have two sets of results in different tables: one where all papers are found from the primary reference, which means you have an index on the primary reference, and one where the specific keywords appear first. It is also possible to have a search engine that would search the results by keyword, and then output a results table with the top 10 for each keyword. This might give you more control in order to match the keyword. At this moment, we are just looking for feedback on what you'd like to try: how important would it be to have different sorting options, or just to have a separate visualization ? Would there be a possibility to have an optional ""show all"" option for search ? We are now working on some"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What is the best model to use to analyze a text and find words in it ? output (Sparql query): input (English text): Which is the best model for a given problem ? output (Sparql query): input (English text): What is the number of words in a typical sentence ? output (Sparql query): input (English text): What is the number of words in a typical sentence ? output (Sparql query): input (English text): What does the sentence above mean ?, and what is the average number of words per sentence ? output (Sparql query): input (English text): output (Sparql query): input (English text): output (Sparql query): A couple of more examples on the SSPRX project page as an example of a sentence with many unrelated parts. input (English text): output (Sparql query): To be continued... The above query is a standard SSPRX matrix expression query, which is often used in order to provide a query like this: input (German text): output (Sparql query): As can be seen, this query has the same structure as a traditional SQL query. The difference is that the SSPRX matrix expression version of the equation provides a matrix for the problem, a list of all the sentences in the text (the rest is a simple linear interpolation), and a query that includes all data required to build the answer. On a smaller scale, I have recently taken up the quest of optimizing the above example. I will now summarize what I have found from searching for the best possible data format and query format for such a task, and outline some of the practical issues that I have encountered along the way.	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (base)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Note that the output is only accessible via the results table in the Results tab. As such, the Outputs tab is used to generate output for other charts or tables. The following data have also been downloaded from the XLNet site In-depth Report - RTE Outputs	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" input (English text): List the description of the benchmark and the dataset used to conduct the benchmark ? output (Sparql query): input (English text): A list of all available resources relating to the Atari Games project, and what they show about the research area and what problems would it help to resolve ? output (Sparql query): You can also get information about the following metrics at the command line: --score measure of performance --number_pages measure of total pages --benchmark number of measurements performed --latency measure of performance over time --memory measure of performance over memory --memory_size measure of total memory --speed measure of performance over speed --cache measure of performance over cache --scores metric which allows you to display a list of metrics In order to run this script for example, you need to put the following text in your ~/.bashrc file (and its source code in /home/pi/Projects/atari ): pi@raspberrypi:~ $./ari_benchmark --help After running the above command, you will be able to see how many measurements have been performed by the benchmark. You can also get the list of metrics or the set of metrics (and its name) you can look in the output above. Benchmarking the Arcade Machine The next example for benchmarking the Atari arcade machine is based on the one of the previous sample, in this case the ""Battleship"" benchmark. You can see the output of the command at the command line, which uses the RStudio.R scripts and is using RStudio 5.3.3. The.R scripts use the R code that runs as well as the results of the benchmarks. To run them you simply need to copy the benchmark script to../../bin/ and use the bash script to run it. "	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?machine ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } Input data In order to have a better grasp of how the models performed in the benchmark, we used the full dataset that was available in the dataset catalog. In order to see how the evaluation scores compare, we used the default data set, which was generated using the evaluation models in the first place. The following table lists the results for both of the default (low-level) evaluation methods: input output ------------ -------------- a b t z rps a b t z rps a b t zrps a b t zrps a b t zrps a b t zrps a b t zrps a b t zrps a b t zrps a 1 7 0.1528 7.2729 2.0498 9.0073 7 5 1.1338 6.8794 1.2154 6.2914 4.6367 3.6146 a 2 10 0.2377 7.4487 2.1043 9.2723 6 2 0.3675 6.3598 1.2093 6.3233 4.6162 3.6095 3.5491 b 3 12 0.2897 6.5728 2.1677 9.5567 4 3 0."	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which are 3 the most common variables for the atmosphere models?
 output (Sparql query): "	"SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, ""tmospher"")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3"	SELECT ?dw ?model1, ?dw ?model2, ?dw ?model3, ?dw ?model4, ?dw ?model5, ?c( ?model1, ?model2, ?model3, ?model4, ?model5) AS ?dw_dvs, ?c( ?model1, ?model2, ?model3, ?model4, ?model5) AS ?dw_predicted, ?c( ?model1, ?model2, ?model3, ?model4, ?model5) as ?model_1, ?dw_dvs as ?dw_predicted FROM ?dataset. ?model1, ?dataset. ?model2, ?dataset. ?model3, ?dataset. ?model4, ?dataset. ?model5, c ( ?model1, ?model2, ?model3, ?model4, ?model5) AS ?dw_predictions INTO ?cont ?model_1; You can use any of the code references. input (English text): Where can I find code references in papers that have used the model for benchmarking purposes ? output (Sparql query): SELECT ?c. ?model_1 AS ?model_1, ?dw. ?dvs AS ?dw_dvs, ?hdsr. ?hdsr_dvs FROM ?dataset. ?model1 ?dv ? hdsr_dvs INTO ?cont ?model1 ?dv ? input (English text): Which are 3 the most common variables for the atmosphere models	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN-PC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" RDF data:  "" ?problem_lbl""  or  "" ?problem_lbl_ref"" output (Sparql query): rdf:... output (Sparql query):  [{ ""rdf:schema"": { ""problem"": { ""data"": {} ""title"": {{...}} ""citation"": {{…}} ""source"": {{…}} }, ""code"": ""https://www.semanticweb.org/document-schemas/"" }, ""code_links"": { ""RDF"": { ""paper"": [ { ""title"": ""RDF:Semantic Web Research Language"", ""citation"": ""http://www.semanticweb.org/research/citations/Rdf"" } ] ""paper_ref"": [[{ ""RDF"", { ""title"": ""RDF:Semantic Web Knowledge Representations"", ""citation"": ""http://www.semanticweb.org/research/knowledge/Rdf/rdf.0/tdf.html"" }] ""code"": { ""html_refs"": ""http://www.semanticweb.org/research/code/code_links"", ""code_links"": ""http://www.semanticweb.org/research/code_links:http://www.semanticweb.org/research/code_links:http://www.semanticweb.org/research/code_links:http://www.semanticweb.org/research/code_links"" } ] } } ], ""title"": ""Semantic Web Research"", ""paper_lbl"": [ { ""title"": ""Semantic Web: Semantic Grammar"", ""citation"": ""http://www.semanticweb.org/research/semgml/semgml1.html"""	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT name, id, title, abstract, publication, publication_id, publication_date, publication_key, date, publication_page, reference, publication_year, author, subject, location, country, publication_id, publication_language FROM 'GAD' AS a JOIN 'TREC-6' AS b ON a.publ OR (b.publ='GAD') 1 2 3 4 5 6 7 select name, id, title, abstract, publication, publication_id, publication_date, publication_key, date, publication_page, reference, publication_year, author, subject, location, country, publication_id, publication_language, publication_yearfrom (... ) output (Sparql query): SELECT name, id, title, abstract, publication, publication_id, publication_date, publication_key, date, publication_page, reference, publication_year, author, subject, location, country, publication_id, publication_language, publication_yearfrom ( or : g_adm3s2b ) 3 1 Selecting name, id, title, abstract, publication, publication_id, publication_date, publication_key, date, publication_page	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT SUBSTRING( ?""RAND DQN"" /.(""RADQN DQN"" /)).substring(0,1,1).substring(1,1,1) FROM ?models WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HOC"") ?benchmark; ?model rdfs:label ?model_lbl. OPTIONAL { ?model a rdfs:class ?model_lbl. } } input (English text): You should provide a link to some papers that run the DQN model on the basis of a set of data. output (Sparql query):  SELECT SUBSTRING( ?""N"" /.(""N N"" /)).substring(0,1,1).substring(1,1,1) FROM ?models WHERE { ?dataset a orkgc:Dataset; rdfs:label orkgc; orkgc rdfs:name ?dataset_lbl. } If you have any questions or have any ideas to further add to the tool, please email feedback [at] theannex.io"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT COUNT ( * ) AS COUNT FROM ( SELECT TOP( 1 ) a_a as a, ct as ct; a1.0, ct1.0 AS c, a2.1 AS a2; a2.0, ct2.0 AS c, a3.1 AS a3) a_t as t WHERE a_t.h3 COUNT( * ) > 0; output (Sparql query): SELECT COUNT ( * ) AS COUNT FROM ( SELECT TOP ( 1 ) ct as ct, a1.a1 as a1, a2.a2 as a2, a3.a3 as a3; a1.a1, a2.a2, a3.a3 as a3) a_t as t WHERE a_t.h3 COUNT( * ) > 0; data (Sparql query): The output will be the most recently executed query. output (Sparql query/sql): [COUNT] of the data: 4.9070036 Example: (TensorFlow): input (English text): How does the implementation of the MLDocZeroShot english-to-french dataset compares to existing implementations. output (Sparql query): SELECT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCH"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""1-of-100 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?value FROM ( SELECT ?value FROM  SELECT rdfs:insert_dataset(""rte_test_data"") ?model_lbl_1 WHERE rdfs:insert_t"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code_lbl ?code_lbl_lbl FROM ?code_lbl WHERE { ?code_lbl_lbl. { ""code"": ""P1"", } } output (Sparql query): SELECT DISTINCT ?code_lbl ?code_lbl_lbl FROM ?code_lbl WHERE { ?code_lbl_lbl. { ""code"": ""P13"", } } output (JSON): output(LongShortTransform) { ""summary"": { ""lbl_text"": ""P1"", ""rank"": 1 }, ""cores"": [], ""rows"": [ { ""code"": ""P21"", ""rank"": 20 } ], ""sort"": { ""rows"": 'asc' } } output (HTML): output(LongShortTransform) { ""description"": ""A Benchmark for LSTM Conditional Selection (A/B/C)"", ""lbl"": ""P21"", ""type"": ""long_long_lstm"", ""rank"": 0, ""cores"": [ { ""code"": ""P21"", ""rank"": 20 }, { ""code"": ""P12"", ""rank"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT top_scoring_model FROM Triviaqa.tr1_model_score ORG Outputs the most recently benchmarked model. If more then one model matches the provided metrics. This query is especially helpful when the results could span across several models with different metrics. Inputs an array of the model variables for the specified metric. These variables will be used to generate the LSTM activation functions and parameter values. Each value in the array will appear above the model line in the query when running the query. If the model does not have a metric on that list, the current model will be used. If the input value is not an array, then the variable will be used to generate the parameters for the model. In either case, each parameter will use the same name and function in the LSTMs. Note this will only make sense when multiple factors in the model are changing. This can be used to filter the outputs of the LSTM for each model, or to return the maximum score for a model. Input variables for the input column of the query. If multiple inputs match the query, each will not be reported as multiple criteria. Parameters can also contain more than one column. If all of the inputs are NULL, then only the ones with a NULL as a type will be reported. When only a single input is provided, then a subset of the parameters are reported as all parameters at a single column (as it would be if all the parameters were defined as one). This allows developers to limit the results to the most popular criteria, similar to what BatchSets can provide. The parameters can also be separated by a comma or another delimiter. This is useful for using a specific model to train on a certain dataset, and then later filtering it on another dataset. This will only be possible if the model is unique	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is Raman spectroscopy?
 output (Sparql query): "	"SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, ""Raman spectroscopy"")) } "	 ----------------- (in-range -3.8 -9.3) ? spectroscopic-frequency_p:0.000873(c = 0.0016) ? spectroscopic-frequency_k:0.001084(c = 0.001) ? spectroscopic-frequency_v:0.00084(c= 0.0018) ? spectroscopic-frequency_f:0.00079(c= 0.0029) ? spectroscopic-frequency_t:0.0024(c= 0.0031) orkgc ? spectroscopic-frequency_p:0.000873(c=0.0016) ? spectroscopic-frequency_k:0.001084(c= 0.001) ? spectroscopic-frequency_v:0.00084(c= 0.0018) ? spectroscopic-frequency_f:0.00079(c= 0.0029) ? spectroscopic-frequency_t:0.0024(c= 0.0031) orkgp ? spectroscopic-frequency_p:0.000873(c=0.0016) ? spectroscopic-frequency_k:0.001084(c= 0.001) ? spectroscopic-frequency_v:0.00084(c= 0.0018) ? spectroscopic-frequency_f:0.00079(c= 0.0029) ? spectroscopic-frequency_t:0.0024(c= 0.0031) orkgp ? spectroscopic-frequency_p:0.000873(c=0.0016) ? spectroscopic-frequency_k:0.001084(c= 0.001) ? spectrosc	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 If the metric was not provided or the criterion is not appropriate (e.g., the output is the value of the metric without providing the metric definition, or a query to the metric definition is returned, then the output is not reported, instead output should be set to 0 or 1 depending on the input.) It is recommended to provide all criteria necessary for performance evaluation in the output. input (English text): - You can use ?dataset_lbl and/or ?dataset_lbl_rdfs to determine the appropriate metric. - You can use ?dataset_lbl_rdfs and/or ?dataset_lbl_rdfs_benchmark to determine the appropriate metric score. - You can use ?dataset_rdfs to determine the appropriate metric score. - You can use ?dataset_rdfs_dataset to determine the appropriate metric and metric score. - You can use ?dataset_rdfs to determine the optimal metric score using the highest benchmark score, and/or ?dataset_rdfs_metrics to determine the optimal metric metric score using the best metric score. output (SQL query): You can configure an option of the MetricMetricRDF class (for example, the option of selecting the metric metric from the metrics table) and then use MetricMetrics for the metric's behavior. For information about using MetricMetricRDF in SQL 2012, see MetricMetricRDF and MetricMetrics in SQL Server 2012. output (Sparql query): input in English text: Can you list the metrics used to evaluate models on the RTE dataset ? output in English text: - Please provide the metric that is calculated for	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 select p from benchmark p1 p2 where p1. score >= p2. score output (Sparql query): output (Sparql query): Select the top performance metric from the CINIC-10 dataset output (Sparql query): output (Sparql query): Conclusion Since the previous section exposed some of these shortcomings, let's examine a few of the additional metrics that are still not implemented: I found it interesting that I needed to look for certain types of metrics to understand which metrics were used or not to see if the model was really good—which lead me to realize that I needed to have a very clear objective. What I found in my testing of the RTE_AFC method is that a few basic metrics were used, and then I had to check a little further and discover the specific one.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?dataset ORK.ID, ?task 7.ID, ?dataset ORK.ID, ?dataset-lbl.ID, ?benchmark.ID, ?dataset.name, ?project.id FROM ORK INTO ?task 7 ; Let's assume that the benchmark data collection is performed in Java and a graph has to be returned from the database. In Java, the code for query is given like this: public void queryBenchmark(long list, Database db, String title, Long number) throws SQLSTATEOUT if (list<int> = db.query(title, number)) { List<int> benchmarklist = db.results(list); List<int> benchmarks = DBHelper.selectBenchmark(benchmarks); int [] i = benchlist.size(); if (i == benchmarklist.length) { i = DBHelper.findBenchmark(benchmarklist[i].task, i); } List<int> id = benchmarks.size(); if (id == benchmarks.length) { i = DBHelper.findBenchmark(benchmarks[id].id, i); } A few observations are made from the above code: 1. The problem dataset has to be sorted. If dataset  has  a fixed size, it is impossible to find some subset of that dataset that can be sorted. 2. If list is sorted by sorting the value by key  then query is not efficient. Therefore query has to be divided into smaller code blocks and returned at the end of each one, e.g. 3. A good idea for benchmarking code is to reuse existing functionality. If you are using similar functionality, reuse the corresponding query to get a better result. 4. This query is executed with SQLSTATEOUT, since	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Output: Output: output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER ( ? ( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } output (English text): Evaluate two metrics: ?model RDFS:label ?model_lbl. ?"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?p_score ?dataset ?dataset_lbl. output (Sparql query): SELECT DISTINCT ?p_score, ?dataset ?dataset_lbl., ?dataset_lbl., ?dataset_lbl., ?dataset_lbl.., ?dataset_lbl., ?dataset_lbl., ?dataset_ncl., ?dataset_ncl.,., ?dataset_ncl., ?dataset_lbl.., ?dataset_lbl. Output: P32: 1.0% of total (1/2 test) orkgp: 0.0% of total (0/2 test) orkgp : 1.0% of total (1/1 test) orkgp : 0.0% of total (0/1 test) orkgp : 0.0% of total (0/1 test) orkgp : 0.0% of total (0/1 test) orkgp : 0.0% of total (0/1 test) ORkgp: 1.0% of total (1/2 test) orkgp : 0.0% of total (0/2 test) ORkgp : 1.0% of total (1/1 test) ORkgp : 0.0% of total (0/1 test) ORkgp : 0.0% of total (0/1 test) orkgp : 0.0% of total (0/1 test) ORkgp : 0.0% of total (0/1 test) ORkgp : 0.0% of total (	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?id * FROM ?paper ?id WHERE ?""id = ?title ?id;"" = ?id ORDER BY :id; I've created a new project. If you have trouble compiling sqlite files, or want to contribute to the project check out the issues."	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCN Hybrid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?model ?model_lbl ?dataset_lbl_lbl_dataset ?c:ModelType ?c. { ?dataset c orkgc:Dataset; rdfs:label ?dataset_lbl_lbl_dataset. ?dataset orkgc:Dataset. ?dataset rdfs:label ?model_lbl. ?dataset rtfs:count ?model_lbl_lbl_dataset. ?model rtfs:label ?model_lbl. } ORDER BY lc, orkgc output (Sparql query, results in JSON): 2 2.0 0.87 2.1 2.2 0.87 0.91 2.2 0.90 0.89 2.6 0.91 0.95 Question 3: Tell me if your benchmark model can be used to identify which features the model needs to learn to predict which feature the test score should be based on.  input (English text): Is there a way to determine whether a model's prediction should be based on the features that it is best suited to learn, rather than some more generalized feature knowledge ?  output (Sparql query): Question 4:   How can your model be used to train a prediction engine which can predict which features it will learn, rather than some general knowledge, for each of	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What data format does CHEMDNER corpus have?
 output (Sparql query): "	SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } 	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?bencher orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. ?bencher orkgp:HAS_PAST_HITDOWN_LATE ?benchmark. ?lab_id orkgp:HAS_LAB_FILE_FILE ?benchmark. ?cid orkgp:HAS_CHEM_CID ?cid. output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. ?benchmark orkgp:HAS_PAST_HITDOWN_LATE ?benchmark. ?lab_id orkgp:HAS_LAB_FILE_FILE ?benchmark. ?cid orkgp:HAS_CHEM_CID ?cid. If we look in CHEMDNER_BETA.RX, the CHEMDNER corpus data has the following attributes: select name, description"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	-------------- CUB-200 (2011) -------------- Title ------------ ------------------- CUB-200-2011 Benchmarking and Data Mining for the RTE Dataset -------------- Benchmarking and Data Mining for the RTE Dataset -------------- Test Case1 ------------ -------------------------- Test Case2 ------------ -------------------------- TEST CASE3 ------------ -------------------------- Test Case4 ------------ -------------------------- Benchmarking and Data Mining for the RTE Dataset (2010) ------------ Benchmarking and Data Mining for the RTE Dataset (2010) ------------ Test CASE1 ------------ CUB-200-2010 Benchmarking and Data Mining for the RTE Dataset (2010) ------------ ------------------------- Benchmarking and Data Mining for the RTE Dataset (2010) ------------ ------------------------- Benchmarking and Data Mining for the RTE Dataset ------------ Benchmarking and Data Mining for the RTE dataset (2008) ------------ Benchmarking and Data Mining for the RTE Dataset (2008) ------------ Test Case1 ------------ RTE Dataset ------------------------- ------------------------- CUB-200-2008 Benchmarking and Data Mining for the RTE Dataset ------------------------ Test Case1 ------------ CUB-200-2008 Benchmarking and Data Mining for the RTE Dataset (2008) ------------------------ Test Case1 ------------ CUB-200-2008 Benchmarking and Data Mining for the RTE Dataset (2008) ------------------------ Test Case1 ------------ CUB-200-2008 Benchmarking and Data Mining for the RTE Dataset (2008) ------------------------ orkgc:Test Case1 ------------ CUB-200-2008 Benchmarking and Data Mining for the RTE Dataset (2008) ------------------------ orkgc:	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the SEE model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SEE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Note: ""Lebron-Heimlich"" and ""Watson-Watson"" do not include any additional metadata with the request, that is they are both ""BRIEF"" and ""UNDUE"". The ""BRIEF"" and ""UNDUE"" parameters are not necessary for results. A URI will be provided for you to download both models (see above), and the ""BRIEF"" parameter is simply an extended ""URL"" of the paper/problem/document: output (Sparql query): For example, the paper ""Semantic Web"" from LBNL (2014) and the corresponding problem-report are provided above in an ""Sparql query"", you can simply click on it to see the corresponding solutions. See the following link in the response section that ""Sparql query"" has been redirected to : Output (Sparql query): Note: the paper ""BRIEF"" URL can be changed in the response section. In some cases, a model file that benchmarks a specific feature may also be provided: such as ""lazy-minimalist"" or ""semantic-graph"". Response (output): The model file can be viewed in PDF form here: Response (input): Any additional information returned by a link will be automatically inserted in the response. Response (output): A ""documentation"" is supplied in a JSON body. Output (output): Note: The output can be modified by editing ""document_header"" and/or ""document_body"". Response (output): The document can be viewed in HTML form here: Other models ? A few models have been added to LBNL's library: semantic-logical-logical-cluster (SPL), semantic-"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?column ?column_lds FROM ?LBD ?LBD_lds WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?lbd_lbd. } output (Sparql query): SELECT ?column ?column_lds FROM ?LBD ?LBD_lds WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?lbd_lbd. } output (English text): Choose one publication of the following types: (1) Published in Elsevier's Journal of Computational Analysis and Applications (TCA) (2) Non-peer-reviewed and not yet published in Elsevier's Journal of Computational Analysis and Applications (TCA) (3) Published in Elsevier's Journal of Computational Analysis and Applications (TCA) (4) Published only in Elsevier's Journal of Computational Analysis and Applications or Elsevier's Journal of Computational Analysis and Applications (TCA) (5) Published in Elsevier's Journal of Computational Analysis and Applications (TCA) (6) Published in Springer Scientific Products (SPI) (7) Published in Elsevier's Journal of Computational Analysis and Applications (TCA) output (Sparql query): SELECT ?column ?column_lds FROM ?LBD ?LBD_lds WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""R"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" DISTINCT ? paper..., ?dataset, ?paper_lbl, DISTINCT orkgp:HAS_DATASET ?dataset. DocRED GAD has been designed to be compared in this way. Here, there are two datasets: one is human-annotated, the other is not. The authors have designed a number of datasets that include both human-annotated and not-human-annotated datasets. But now the author has selected two such datasets and given their performance on GAD. Let's start with the GAD dataset. First, here is the top result: top result: A paper ranking highest in the dataset DocRED is the paper that is the paper that is ranked second in the dataset. In our dataset, DocRED has the highest performance (see the last column in the table). This is very different from the result in the previous benchmark. So why are these two different ? The second dataset that is the GAD benchmark that the authors have selected consists of a document of at least three pages (it is about 300 words). In this dataset (DocRED), the paper is ranked fifth with a score of about 400. GAD benchmarking only uses data in the first 3 pages of a paper, not in the whole paper. This is important. In order to have a consistent top result across papers, the authors should choose the right dataset for benchmarking. This means that the GAD paper should be benchmarked on at least one dataset. Note that in this case the authors could have chosen to benchmark in either the full or the third page. In other words, they could have said ""compare this paper to this other paper, which is about 300 words"" (if not the entire paper) and ""compare this paper to this other paper, which is not about 300 words."" We did not choose"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 If you found this document useful, please consider becoming a patron. It only takes a moment to make a donation. Thank you! I would LOVE to talk to the developers of any of those models, whether it's your own or another developer. Just drop me a line.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?train.dataset ?benchmark ?benchmark_lbl. train.dataset_lbl = ?benchmark ?benchmark.lbl.dataset_lbl = ?benchmark ?bookmark ?benchmark.lbl.dataset_LBL ?benchmark. ?book orkgp:N_BOOK ?bookmark.lbl input (English text): Which are the titles and IDs of research papers that are being evaluated on the RDF GraphNet dataset ? output (Sparql query): SELECT ?train.dataset ?benchmark ?benchmark_lbl. train.dataset_lbl = ?benchmark ?benchmark.lbl.dataset_lbl = ?benchmark ?bookmark orkgp:N_BOOK ?bookmark.lbl input (English text): Which are the titles and IDs of scientific papers that used the RDS software for classification using GAD ? output (Sparql query): SELECT * FROM RDS_SCHED RESULTS FOR TEXTS WHERE ""N"" = ? N (1) input = ? GAD_ID output (Sparql query): output (Sparql query) SELECT RDD [N]( 1, N) FROM RDD [STATS](RDD[N]( 1, N));   input (English text): This query can be shortened to ? GAD_ID output (Sparql query) SELECT ?training.dataset ?lbl_lbl. training.dataset_lbl = ?lbl (N); RDD [N](1, N) FROM RDD [STATS] WHERE ""N"" = ?train (N); This query can be shortened to ? GAD_"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl; So that's the very first thing that we'll do. But what about the performance ? You know that you'll be able to run the GAD benchmarking code twice while the dataset is being updated, but in reality that's not what we'll be doing (I already did the first test by hand, but I wanted to find a way to make sure the code always found the same dataset when benchmarked for the second time!). So first we'll check for the existence of the bench1.log file and output it to g/benchmark1.log in the directory we ran the benchmark code in: output (${GIR}/Benchmarking/Bench1.log) The benchmark is running, congratulations! This is actually the first time that we're going to run the benchmark and is our last check on how fast the benchmark runs. There's almost nothing on how fast it runs, as we only have a few hundred lines of code running, but since we're running the benchmark for our first time, we're going to make it run about 6 times using the ""benchmark"" variable we just set. Let's quickly run the code: output (${GIR}/Benchmarking/Bench1.log) Benchmark, version: 0.0.1, run (min) : 4.072 ticks output (${GIR}/Benchmarking/Bench1.log) [Benchmark: 0.0.1, run(min) : 4.072 ticks] We've got six lines of code, which should be close to the time that GAD takes to run. How about the GAD file itself ? data-gd/GADBenchmarking_GAD.df #################################### # GAD Benchmarking File # #"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT reference( ?model_lbl/ ?dataset_lbl). FROM ?model, ?dataset_lbl, ?model_lbl WHERE reference( ?model_lbl) < ?modeline. For more detailed information about this model, see  https://www.hoc.ch/model/ For benchmarking purposes I want to use a model that is not fully implemented yet. To do that I should use mmodel with a model that is very similar to my current benchmarking model. This is a good decision: It enables me to run many different benchmarks over a wide range of datasets. The dataset that was used for my benchmarking was very similar to the majority of large-scale datasets used in computational chemistry, which is what is of interest here. To get really fast results it is not reasonable to make the same benchmark twice, but given my current needs it seems like this is a good choice to be more rigorous about. In the following section you can find details for a simple and fast benchmark I run over a dataset of several million molecules, the BN5-H20 dataset and the mmodel dataset:  https://www.hoc.ch/model/ The benchmarking model for the BN5-HY5 system consists of a first-principles approximation for the molecular mechanism of the reaction between hydride ions, using a molecular model of the reaction (Bin2 in the picture) and an accurate experimental data set. The model corresponds to a reaction for only a few of the active ions, which will be used to simulate the BN5-HY5 reaction, but because there is a large number of other ions involved in the reaction this model contains enough information to be reasonably accurate. The data set for the BN5-HY5 system is available from: http://researchgate	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?dataset orkgp:HAS_DATASET orkgp:HAS_P32 ?dataset_lbl; ?benchmark, ?constraint_lbl. input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset ? output (Sparql query): The benchmark results have been available here and here. To use these output files into a SQLite file, simply copy the output file into the query. To run the query, run the following SQL statement: SELECT output.data_schema.id, output.data_schema.list_schemas(), output.datacampath(output.datacampath().strip_tags( 'dataset_lbl' )) as * FROM output AS ******** *,  output.datacampath(output.datacampath().strip_tags( 'dataset_lbl' )) AS ********_ * GROUP BY output.data_schema.list_schemas ORDER BY output.datacampath(output.datacampath().strip_tags( 'dataset_lbl' ))    , output.data_schema.id ROWS. 	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT nlm ?c = count( ?c_id ) as model_c AND nlm ?c_c = count( ?c_id ) as model_c_c or nlm ?c_c_c = count( ?c_id ) as model_c_c_c OR nlm ?c_c_c_c = count( ?c_id ) or nlm ?c_c_c_c_c ?eval. { ?eval } ?metric ?metric_dbl. OPTIONAL { ?metric ?metric_lbl. } As you can see, no default options have been specified to use the RTE input and the RTE dataset and that is why we use some filters to make some filters possible. These additional steps will be documented in the next post and is based on experiments we tried to compare performance when compared with two different training set and model methods (RTE and Random Forest). I also have a script that can test performance for RTE and Random Forest that can be used as a starting point. However, to try it on different RTE datasets and models, you have two options – you can use RTE directly or you can use SRTM and if you decide to use SRTM also get an analysis from SRTM. The method by which SRTM should be evaluated is that you should evaluate results after training your model on your dataset and compare it with other validation set performance of the same model on different data. Here is my R script which generates a model to train under the RTE dataset. Since this analysis is quite long, I have added a table with the training data so that you can see the different performance of your models on different datasets. You can use your R script to generate the model and your SRTM script to	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where did the study with maximal geographic scale take place?
 output (Sparql query): "	SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } 	 WHERE DISTINCT ?paper GADS_LEVEL = ?benchmark Note: All the parameter names are shown in their human-readable form, so the parameters are more or less self-explanatory (which should not turn into a problem for developers). We'd like to remind you now, even though you'll probably not have experienced it directly, how important it is to be careful with your data types. Use them wisely and for what you need and the results you expect when generating all your data types and exporting results. Step 7 – Using GAD Data types to construct a database model¶ As already explained above, the GAD data types provide a very handy way to convert between any two data types and back, so for example, if that particular dataset is an abstract object, you can transform it into another data type called a struct, without changing the structure of the original data types in your database. To do this simply import the GAD types and, using the 'type' function, write a GAD function of the specified types: import pygdb import psycopg2 # We can convert any GAD type with 'type' function def function (_: GAD_FMT): def __init__ (self): # Read an existing GDBLAST object by specifying'read' in the # GAD_ID argument. if'read' in 'GDBLAST' in self.read: return self._read_object() def read (self, object ): return json.loads(obj) You can read and write the following GAD types and their corresponding SQL functions in either Python or Pygdb. The output format used for the GAD functions described here is as follows: GAD_NAME_ID_FIELD_TYPE GAD_NAME_ID_FIELD_FUNCTION | GAD_NAME_ARRAY_NUM	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 -------------------------------------------------------------------------------------------------- | Outputs: | title | id | title.id | Id. Title | Id. ID | -------------------------------------------------------------------------------------------------- WLPC_BENCHMARK.1.1 | 1.1 | 0 | WLPC_BENCHMARK.2.1 | 2.1 | 0 | WLPC_BENCHMARK.3.1 | 3.1 | 1 | WLPC_BENCHMARK.4.1 | 3.1 | 1 | WLPC_BENCHMARK.5.1 | 4.1 | 2 | WLPC_BENCHMARK.6.1 | 4.1 | 2 | WLPC_BENCHMARK.7.1 | 7.1 | 2 | WLPC_BENCHMARK.8.1 | 8.1 | 3 | WLPC_BENCHMARK.9.1 | 9.1 | 3 | WLPC_BENCHMARK.10.1 | 10.1 | 4 | WLPC_BENCHMARK.11.1 | 11.1 | 4 | WLPC_BENCHMARK.12.1 | 12.1 | 5 | WLPC_BENCHMARK.13.1 | 13.1 | 5 | WLPC_BENCHMARK.14.1 | 14.1 | 6 | WLPC_BENCHMARK.15.1 | 15.1 | 6 | WLPC_BENCHMARK.16.1 | 16.1 | 7 | WLPC_BENCHMARK.17.1 | 17.1 | 7 | WLPC_BENCHMARK.18.1 | 18.1	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?model ?model_lbl WHERE { ?label ?dataset_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?dataset a orkgc:Dataset; ?dataset a orkgc:Dataset; ?dataset a orkgc:Dataset; ?dataset a orkgc:Dataset; ?dataset a orkgc:Dataset; } ?dataset rdfs:label ?dataset_lbl. ?label orkgp:ClassName. } } ORDER BY DESC( ?values) LIMIT 1 Example 3 (from Appendix A), which uses the F1 test procedure on R-SCAN, is a modified version of the previous example 4 (see Appendix A), that provides an API for querying the dataset, as with the F1 test procedure, without requiring the need to access additional data such as the data.xml file. R-SCAN is an open source network science database and network analysis toolkit. It is now run by the Max Planck Institute for Computer Science (MPIC) of the German Research Foundation.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" The above R script is very simple, but it does a simple but valuable job. The reason why I didn't use the above script for my research at hand is because (at time of writing) it was pretty small - about 200 lines of code, and it was the first time I've ever asked a single person for help. It was a very small project, and I wanted this project to be a little more ""real"". That's why I wanted to expand the script slightly. I'd love to have people try out SciERC in their own R session, and report back on their experience. I could add a few lines of code to help the user generate a report, and maybe provide a link to the report if it's generated on another machine. So, I've added this function to try out the model, and the result is below (the link will be sent to the user who reports back). library(rsc2) library(SparseSparseClustering) library(parquet) library(rtlib) n=100000 # number of random samples for testing data <- data, lapply(c(lapply(nrow(data), xrange(n))), col=""data"") # select a subset of the dataset cdata <- data[, -1:4][, c] # convert the dataset data into the form of random samples rata <- rsc2(mapply(nrow(data), cdata)) rata <- rsc2(mapply(nrow(data), cdata, drop=FALSE), num=cdata) # create a data set for testing of the model cdata[[,c]] # create random samples from the dataset data = data[[3]][-3]] * rata # create a cluster of data clusters <- data[[0]] * rata"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?dataset ?test_dataset_hits ?dataset_lbl; output (Sparql query): SELECT ?data_hits ?data_lbl FROM ?dataset_hits BY ?id Note: Input fields are required, and outputs are optional. Inputs an input file for the output. This file is provided with the output and should be named differently depending on the output file format used by your R implementation. All records from the input file will be merged into the specified output file. (data for the output file) (data for individual test cases) select test_id, output_file, score, number of observations from results select test_id, output_file, score, number of observations from results select test_id, input_file, score, number of observations from results select * from tests; Outputs an R object. This object is only generated when the R test files are read. You may have to adjust some of the output settings. Most of the time you don't need to, but if you have made changes to the score column and need to specify the correct number of observations, and have other requirements, you should override the test-id option in the output. output_file: An encoding to save the R object to. Supported encoding formats are: ISO-8859-1, Microsoft PostScript (*.ps), and Microsoft Text (*.txt). Outputs standard output. (Standard output) input_file: An encoding to save the R object to. Supported encoding formats are: Microsoft Word (*	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?dataset orkgc:Problem AS problem, ROWS ( ?dataset ) as rows FROM { ?dataset_lbl } LEFT JOIN ( SELECT DISTINCT ?problem_lbl ) on { ?dataset_lbl } ( a orkgc:Problem ) { ?problem_lbl } ORDER BY problem DESC; input (English text): What is the benchmark for the Atari Games data set ? output (Sparql query): SELECT DISTINCT ?dataset, ?dataset_lbl FROM ?dataset JOIN ( SELECT DISTINCT ?problem_lbl ) on ?dataset_lbl on { ?dataset_lbl } ORDER BY problem DESC; input (English text): Describe the parameters used to configure the data set. output (Sparql query): SELECT ?dataset_lbl.LBL, ?dataset_lbl.LBL_s, ?dataset_lbl.LBL_s', ?dataset_lbl.TBL, ?dataset_lbl.TBL_s, ?dataset_lbl.TBL_s', ?dataset_lbl.BBL, ?dataset_lbl.BBL_s, ?dataset_lbl.BGL, ?dataset_lbl.BGL_s, ?dataset_lbl.BGL_s', ?dataset_lbl.S2, ?dataset_lbl.S2', ?dataset_lbl.S2', ?dataset_lbl.C2, ?	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" // row 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 206... from 'hocbench': import numpy as np, df as dsql, orkgp as hoc, sxrs as stxrs, model as m Here, a variable called stxrs is defined, and a model named m is stored. Step 2 The above step uses a variable of type numpy. The model m is defined by the following code: import numpy as np import os rng = np. random. randn ( 12 ) x = np. arange ( 5, 5 ) ax = ax. set_xaxis ( 1 ) # create the x axis ax. imshow ( 'SEMAPHORE', ax. yaxis ) ax. set_color_scale ( 1.5, 1.5, 1.5, 1.5 ) # set color scale. ax. set_title ( 'SEMAPHORE (sorted by value)' ) ax. set_ylabel ( 'Metric' ) ax. set_visibility_map ( True ) ax. set_title ( 'ABSENCE' ) # remove visibility map and label. ax. set_title ( 'SEMAPHORE', axis = 0 ) ax. set_ylabel ( ""SEMAPHORE, sorted by value, with 0=least,"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1 day) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- This is more than a technical question. If you think it would be helpful, please don't be shy to ask in the discussions. What about other Semantic Web problems ? I have a friend that wants to use Semantic Web for research which doesn't seem to fit well in A3C because it doesn't have many data driven features which is a drawback not relevant for A2C. If you have experience with other libraries I'd be very grateful if someone could help me improve it to make it do that. I also hear this issue was also brought up before, also by some other people in the community. How does this particular problem fit for A2C ?	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 T test:test.test:10.5:16:10.6,848,1066,1650,1685,1826:25:7.8:30:5.7 TEST:test test.test:10.5:16:5.7,7.0,9.3,3.0,2.8,5.2,7.2,3:50:10.6:16:11.1,1069,1658,1823:25:7.8:52:4.8 TEST:test test.test:10.5:16:10.8,7.0,8.1,2.7,4.9,4.0,7.2,3:68:10.6:16:11.3,1073,1656,1828:25:7.8:57:4.4 TEST:test test.test:10.5:16:8.4,8.5,9.0,3.0,2.9,6.1,6.0,2:81:10.6:16:11.7,1074,1661,1832:25:7.8:58:4.5 TEST:test test.test:10.5:16:9.0,8.4,7.0,2.9,6.0,6.1,5.4,2:117:10.6:16:10.7,1076,1657,1833:25:7.8:61:4.9 TEST:test test.test:10.5:16:10.9,9.0,8.0,2.9,6.0,5.9,3	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * FROM wos.dataset  GROUP BY ?rf. RANK : ?rf; output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem. RANK: ?problem_lblade. RANK : ?problem;  @( @( (#RANK RANK ) (#problem RANK ? ? ? ? ?problem_lbl )) @(  @( @( @( @( @( @( @( @( @( @( #problem RANK A ? ? ? ? ?problem_lblA )) @( @( @( @( @( @( output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem. RANK ? ? ? ? ?problem_lblA. output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem. RANK _S3 output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem_R3. output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem _R _S3. output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem _R _S3. output (Sparql query):  SELECT * FROM wos.dataset  GROUP BY ?problem _R_S3 output (Sparql query):  SELECT *	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP 100 REPORTS ( ?metric) ?metric from TOP 100 RESULTS where { { ?Metric ?metric_lbl ?RESPONSE; } { ?metric rdfs:label rdfs. ?name rdfs:ID 1/2 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 { ?METRIC rdfs:label rdfs. ?name rdfs:ID TOP 1 RESULTS 0: 1 4.98 6: 3 3.88 9: 1 1.77 10: 1 2.19 12: 1 1.64 20: 1 3.22 24: 3 2.05 } }!!! SELECT TOP 5 RESULTS ( ?metric) ?metric from TOP 5 RESULTS where { { ?Metric ?metric_lbl ?RESPONSE; } { ?metric rdfs:label rdfs. ?name rdfs:ID 1/2 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50!!! SELECT TOP 6 RESULTS ( ?metric) ?metric from TOP 6 RESULTS where { { ?METRIC rdfs:label rdfs. ?name rdfs:ID TOP 1 RESULTS 0: 1 4.96 8: 1 1.82 11: 1 2.02 14: 1 1.72 20: 1 4.96 24: 1 2.02 36: 1 3.77 } }!!! SELECT TOP 7 RES	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT p.kappa + p.m_test_sigma + COUNT(P.kappa) + 1 + ?benchmark orkgp: ?benchmark from data_matrix; output (Sparql query): SELECT p.kappa + p.m_test_sigma + COUNT(P.kappa) + 1 + ?benchmark orkgp: ?benchmark_criteria For more details, please refer to this document GAD1: A Benchmarked Measure of the Effectiveness of the Knowledge Base in High School English Language Learners (GAD1) [2016] GAD2: A Benchmarked Measure of the Effectiveness of the Knowledge Base in Spanish Language Learners (GAD2) [2016] References	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?dataset ?dataset_lbl WHERE { orkgc:Question ?dataset_lbl. ?name ?question. } input (English text): Which is your benchmark for Natural Questions (short) ? output (Sparql query): SELECT ?dataset ?dataset_lbl WHERE { orkgc:Question ?dataset_lbl. ?name ?question. } input (English text): Which is your benchmark for Natural Questions (short) ? output (Sparql query): SELECT ?dataset ?dataset_lbl WHERE { orkgc:Question ?dataset_lbl. ?name ?question. } input (English text): What are the models that have been benchmarked on the Tetris dataset ? output (Sparql query): SELECT ?dataset ?dataset_lbl WHERE { orkgc:Question ?dataset_lbl. id:9. FILTER (str( ?question_lbl ) = ""Tetris"") ?dataset a orkgc:Dataset; rdfs:label ?question_lbl. ?label id. } input (English text): Which is your benchmark for Tetris dataset ? output (Sparql query): SELECT ?dataset ?dataset_lbl WHERE { orkgc:Question ?dataset_lbl. id:11. FILTER (str( ?question_lbl ) = ""Tetris"") ?dataset a orkgc:Dataset; rdfs:label ?question_lbl. ?label id. } input (English text): Which is your benchmark for Tetris dataset ? output ("	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP(250000) ROW_NUMBER( ? ?) COUNT(1) AS bn(ROW_NUMBER( ? ? ?)) FROM __TIMESTAMP__ WHERE 1/24 '' ? ? ? { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } When you ask for the highest score, you need to return the lowest possible score. You typically use the ROUND(result, score) function (e.g. 2.5 from the R statistical package on R 3.2.3). R 2.1.0 and later (and R 3.2.3) use the RAPI function round with a limit of 100. In this example, we set the round to a value of 5 as it looks much worse than 100 (it is close but actually not so much). You can choose not to pass an argument to round so you can easily round numbers in the R output without affecting the input data. In R 2.1.0 and later, the output with RAPI can be used as a data model or as a data loader such as the R CMD interpreter which contains functions including (without any special settings) to plot plots with multiple variables, create dendrogram (linking the data and the labels to a plot, without an extra parameter) and many more. Here are some examples of use of RAPI or the standard R output (R 3.2.0 and later) to build your own plot or dendrogram (or plots and dendrograms) using a number of basic and advanced features, for instance: Plotting with multiple variables (Note: You can use the plot() function only if you use R 2.1.0 or later. R 3.2.0 and later have not	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT * FROM tdb2_lbl_h.datasets t; output (Sparql query): UPDATE t SET id = ?, title = ? WHERE data. ? is specified; output (Sparql query): SELECT TOP( ?column) ?title, ?data. ? FROM t2_h1.datasets t1, t2_h1.datasets t2, t1_h2.datasets t2, t1_h2_h2.datasets t3, t2_h2_h2_h1.datasets t8; output (Sparql query): UPDATE t SET id = ?, title = ?, model = ?, data = ?, ?_dataset = ?, ?_model_rdfs_lbl_rdfs.cvs = ?, ?_dataset_lbl is specified; output (Sparql query): SELECT TOP( ?column) ?model, ?data. ? FROM fk2k.datasets fk2k ON fk2k.datasets.fk2c.indexed_index = ?, fk2k.datasets.fk2q.indexed_index = ?, fk2k.datasets.fk2k.indexed_index IS NULL LIMIT 1; output (Sparql query): UPDATE fk2k.datasets SET id = ?, title = ? WHERE models. ?a = : ?, model ON fk2k.datasets.fk2k.model = ?, models. ?b = : ?, model ON fk2k.datas	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 18 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" 19th January 2011 - First version implemented This is the 1st version of All-attention. The implementation is based on a new way of training the model. You can already find this in the work paper and a nice write-up in one of the latest LWN posts of the work ( http://www.lwn.net/article/268077/ ). It is based on the training process proposed here http://github.com/aleksey/AllAttention-and-Recurrent-attention-layers. This is an example of the training process: - Define a single hidden layer to work for each feature-type - Generate a graph for each feature-value - For each feature, pick a random subgroup of the graph - We use a simple strategy where we draw a feature-value from a list. If we have a feature-value for X, we will pick all the values from ""X-1"" to ""X-N"". Once we get some random value from the list, we check the output of the model against that value and choose the one we should use. If it is not the right one, we stop the training and go back to the beginning. If after 100 samples, we are happy with the value, we stop and generate one more feature list. - We check the output of each model-training step against our training value and pick the one which is closest to the training value. That value is used to train the next layer. - After each step, we generate another feature value from our training list. - We combine all the features from all the previous steps. - We then return a feature-value for this output layer. - If we are unhappy (for example, we are not able to find a set of values that have the corresponding feature), we stop the training and keep on checking if we should"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Does paper ""Disambiguating authors in citations on the web and authorship correlations"" employe Cosine similarity?
 output (Sparql query): "	"ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), ""Disambiguating authors in citations on the web and authorship correlations"", ""i"")) } "	SELECT author; author output (Sparql query): ?author Now we'll create a report of this report which we can compare to the GAD dataset. 1. Make Sqldatabase as Big as GAD Dataset by using LSTM (Linear Time, Size, and Performance) The original idea of using LSTMs to speed up GAD operations is very promising and a lot of applications of this technique have already been applied to some applications. To accelerate GAD operations in the case of Sqldatabase we'll use a different algorithm. LSTM is a time-efficient and sparse linear machine learning algorithm which runs on Sqldatabase very fast. For this benchmark we're going to use one LSTM running on Sqldatabase, which is a linear graph. To create a dataset we need four attributes: data frame: each of the documents has a title, a author, a date and a date range. each of the documents has a title, a author, a date and a date range. key : the document has a key and we'll use	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT d.lbl_id, d.lbl_id, c.lbl_id, c.lbl_id, d.lbl_id, c.lbl_id, c.lbl_id, d.lbl_id, c.lbl_id, d.lbl_id, c.lbl_id, st.id, lst_id, lst_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, ?c.lbl_id, cntxt; d.lbl_id, c.lbl_id, c.lbl_id, d.lbl_id, c.lbl_id, d.lbl_id, c.lbl_id, c.lbl_id, c.lbl_id, orkgc:Problem, cntxt; If the selected models didn't have sufficient training data at all, then they were not evaluated. If you have any additional query, please use one of the methods listed at the previous section. For more details on the evaluation process of the Atari Games dataset, read the previous section.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): How do you want the score to be calculated ? output (Sparql query): 2,3,7,8   input (English text): For a given dataset, which of the following algorithm can be used ? output (Sparql query): { ?test_probability ?score ?prob ?value ?samples ?score_val ?confidence ?val ?confidence_val } input (English text): Which of the following algorithm must be performed ? output (Sparql query): test_probability, confidence, score, value, samples NOTE: If one or more of the items in the drop list are null parameters (e.g.  'prob'), the algorithm will be chosen based on the current session's score. For example, if score_prob is zero, then the algorithm will use the original score as the score. SCHEDULER:	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Inputs for the HoC database were sorted according to the date the paper was posted. After we had sorted through and categorized the papers, all documents were submitted in September 2013 and published online in August 2014. All but one paper in the H1b paper were of the NICE scale as there was no standard for publication and validation. NICE standard is that at least one study should have been the outcome. All non-H1b publications except those from the NICE-based studies were also peer evaluated. For the H1b comparison, we extracted three sets of characteristics from the papers to enable us to judge the quality of the publications. For each paper we took and extracted information on the subject: the size of the dataset, date of inception of the paper in the database, publication year, title of the paper. Each article was assigned a score on the following scale ranging from 0 to 12.0 based on these characteristics. It should be pointed out that there was no significant change for each parameter after the first year and no clear increase for them after 7 years. A number of factors had to be taken into account to determine the accuracy of the scoring. We found that two of the papers had multiple authors, and a third was published with less than 7 years of experience. The other authors were the most experienced researchers, and this should have led to a lower score. In total there were 17 papers of the 17 that were included, of which, 4 (22%) had no data, 1 (7%) had some data, but very little, and 1 paper (2.9%) had not been published yet and some other information, such as data or code for the analysis. These 4 articles had very little data and hence the corresponding papers did not score any higher for the H2. All of them were published in English. For the HoC paper, the first author is	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 (Data) Model # Name Predicted % Input-Time Output-Time Predicted % Input-Rate Output-Rate Input-Rate p p p p p p p p/N 10 1 1 2,091,250,000 5.7 946,822,947 9.6 3.67 0.01 0.97 0.96 0.76 p p p p p p p p p/N 22 1 1,749,037,000 20.4 583,095,867 16.1 4.49 0.10 0.97 0.98 0.77 p p p p p p p p p/N 38 1 1,039,000,000 30.7 937,631,847 21.6 4.59 0.00 0.98 0.97 0.78  p p p  p p p p p  p/N 45 2 1,038,000,000 44.4 1,019,744,000 30.2 4.41 0.00 0.97 0.97 0.78  p p p  p p p p p p p/N 52 4 1,021,100,000 55.1 800,250,000 27.2 4.40 0.00 0.98 0.97 0.78  p p p Model # Name Predicted % Input-Time Output-Time Predicted % Input-Rate Output-Rate Input-Rate p p p p p p p p/N 10 1 1,061,000,000 1.7 1,002,000,000 0.3 1.05 0.01 0.95 0.95 0.75 p p p p p p p p p p/N 26 1 1,	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT() as score FROM (SELECT rdfs as input, score  FROM orkgp.parms SET score = ?dataset- ?criterion COUNT(*) OVER(PARTITION BY ?) ORDER BY score); The results are summarized in the table below. Sparql query result: Score (1 - 100) Score (101-200) Score (201-300) Score (301-400) Score (401-500) Score (501-1000) Score (99-200) Score (201-300) Score (301-400) Score (401-500) Score (501-1000) Score (99-501) Score (201-300) Score (301-400) Score (401-500) Score (501-1000) For more information about the datasets and the associated scores on each benchmark please visit the official project page.  The benchmark datasets are provided below. Additional datasets are in the works. Contact info for the researchers and reviewers: Dr. R.S. Visser, Purdue University Department of Biomedical Engineering, Purdue University, IN. Dr. R.S. Stroup, Purdue University Department of Biomedical Engineering, Purdue University, IN. Dr. V.P. Shrestha, University of Michigan, Ann Arbor, MI. Ms. A.	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT + BiLSTM + CRF Decoding"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT authors, -------------------------- Authors -------------------------- Rene Duhaime, Kevin Macdonald, Martin Oehms, Nils Schuurman, Richard Stadtler, Rene C. L'Houzel, Martin Knutson, Stefanie Schulz, Thomas Daehn, Sven Schuurmann, Thomas Schulz, Christoph Bauer, David Scholz, Hans-Ulrich Schmitt, Johannes D. Wackernagel, Dirk Hulstang, Martin Lutz, Christo M. Neugebauer, Tobias Fähnle, Stefanie Schulz, Thomas Schulz, Rene C. L'Houzel, Christian N. Gerber, Martin Bündgen, Markus Gerber, Christian Dachs, Christian Griesch, Christoph Bauer, Johannes V. Hulstang, Markus Schulz, Martin Düyük-Völker, Martin M. Leip, Alexander Härtlich, Florian Meyer, Matthias Känzermeier, Heinz Kitzmann, Henk M. Klotz, Matthias Stadtler, Rene C. L'Houzel, Martin Wackernagel, Christian P. Wüsthof, Christopher J. Borsig, Matthias Hölzle, Sebastian G. Neugebauer, Andreas M. Küllnitz, Jan H. Jansen, Christian Griesch, Matthias M. Schellinger, Andreas Bockwoldt, Christo M. Neugebauer, Wolfgang Bügen, Andreas Kiecolt-W	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?dataset ?dataset_lbl FROM { ?problem ? ; ?benchmark ?problem } Input format: data [ "" data "" =>... "" problem "" =>... "" benchmark "" =>... "" benchmark_n "" =>... ] Result format: colnames: name: col1: col2:... pk1: pk2: varchar(200); If you are using SAS version 9.1+ or R version 1.8.10 it can be downloaded from r-data_forge_1.0.2. Input format: text [ "" text "" =>... ] Output format: output [... ] Parameter: rdfs: dataset orkgc:Dataset Parameter: lbl: lbl1: lbl2:... Parameters: ?dataset : the data set your analysis has to find solutions for, ?dataset_lbl : name of the problem dataset as its name in English, ?dataset : the data set your analysis has to locate solutions in, ?dataset_lbl_count : the number of solutions in your dataset ?dataset_lbl : name of the lbl ?dataset_lbl_count : the number of lbl ?lab : lbl1: lbl2:... orkgp: HAS_DATASET ?dataset Parameter: pk1 : pk2 : lbl1: lbl2:... orkgp: HAS_DATASET ?dataset_lbl_count : the number of solutions within each lbl ?lab : lbl1: lbl2:... orkgp:HAS_DATASET ?probe : true - if true then a probe will"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Note that the above parameters accept two arguments. p : the query data, it must end with a \ (e.g., \v ). orkgp : the query query for the data provided, it must end with a \ (e.g., \v ). The output can also be a list of one or more records for the result. The query is treated specially by the R client. the parameter p accepts a set of records of the given type. The query is processed at the start of the query, and the query is treated specially, including the type of the record in question. It must be the same type as the type of parameter p (note that the type of parameter p is treated specially). For example, the first query below may result in a record of the following type: type p p 1 1 1 1 1 1 1 1 1 1 1 In the third query, the question has been altered to add any of the records to the output. Note that the default (single option) is to provide output of the same type as the query. In case it is desired to provide output of a different type than the query (i.e., in which case it is possible to supply separate output files for each query), there can be an optional second and/or third (unspecified) parameter. This is used as a special case, and any kind of output option may be specified, as long as it is not used as the default output option. The only other special case is to define a separate option in the orkgp parameter that should be used for a list of records. In this case any record in the output file that also appears in that query will be output. Furthermore, this option may be one of the lists that is stored in the orkgp parameter. The only other special case for this parameter is in case	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""linear-chain CRFs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT COUNT(*) AS total_rows FROM tbl_crfa input (English text): Can I search for a paper with a particular CRF model by title ? Output (Sparql query): SELECT title, CRF_NAME, MAX(title - ?linestart - ?columnstart) AS model, DISTINCT ?linestart input (English text): What's the last thing written by a person connected to a study of the CRF in any of the code links ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the HoC dataset ? output (Sparql query): SELECT ?model_lbl FROM test.crf input (English text): Can you list the models that have been evaluated on the HoC dataset ? output (Sparql query): SELECT ?dataset a, row_count, row_labels, num_model_lbl, count(model_labels) AS total_lbl, ?linestart, ?columnstart, CRF_NAME input (English text): Can you list the models that have been evaluated on the HoC dataset ? output (Sparql query): SELECT ?model_lbl FROM test.crf How much money did you spend on the CRFs validation study ? How much did you spend on the HoCs validation study ? input (English text): How much was the cost to you ? You will choose one of the following items: You paid no money to anyone. You paid $10 to a company to help with the validation. You paid $20 to a company to do	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the DrQA model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT abstract, title, paper_id, benchmark ? paper, data_id, publication_date, author, abstract_date, paper_id, and id as paper_id, code as code, abstract as abstract, title as title, paper_id as paper_id, code as code from papers n in papers n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 [ { ? paper { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } ] { } [ { ? paper { } { } { } { } { } { } { } { } { } { } { } { } [ { ? paper { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } { } ] ] output (Sparql query): SELECT pct, papers : ORDER BY id, paper_id, paper_id, title, abstract_date, paper_id, code, abstract, paper_id, journal as title, abstract, and id as abstract, papers_count for codes, paper_count for papers, and publications_count for journal in sets(pct= ? and papers= ?, papers_count= ? and publications_count= ? ) n in sets(pct= ? and papers= ?, pct= ?and publications_count= ? ) In this example, the input texts are all either English or French texts,	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT count( ?feature_cstr) ?count ?size FROM ?table ; count( ?feature_cstr ) output (SQL query): input (English text): Which was the first to implement the concept of semantic web ? output (Sparql query): SELECT idx ?start ?end = start ?id = id ; idx = ?start ?end input (English text): Can you list benchmarked problems in the area of semantic web ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField ; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Which project is currently the most active in this area (with respect to number of commits) ? output (Parallel query): input (English text): Which project is currently the most active in this area (with respect to number of commits) ? output (Parallel query): output (English text): What task are you currently working on ? output (Sparql query): SELECT COUNT( ?task ) ?task_id FROM ?task_lbl to ?task; count( ?task_id ) output (Sparql query): input (English text"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" After running the tests, I was curious about whether there is an approach to improve the result of this test. I decided to use the ""Sparse"" option because I was familiar with the approach of David Mitchell of the ORL group at the University of Toronto. He used it to benchmark a few of his machine learning models. After running the benchmarks again, it was clear the algorithm was still performing well. My next step was to look for better ways to achieve improved performance by running only more models. One of the ways to increase performance is through an optimization: to run fewer models. And I can see how a small number of such optimization would improve the accuracy of the resulting model. I found some Python libraries for batching operations. Although not the best performance optimization, it will certainly make a big difference in the future. When I was writing this blog post, I was still not aware that there is also a way to benchmark in parallel. I think there is no reason to wait for this method to be made available. All of the code provided in this post was written using NQP in Python 3.3. I didn't know which version the NQP was running at the moment, so I was using the PPC compiler (as provided with NQP) which doesn't have support for parallelization. At the moment, I have not found a method to improve performance in the other ways described in this post. One alternative that I do believe will have improved performance is to use a model-driven training algorithm. It still needs to be tested. But I think this may be very useful to evaluate with a parallel model. I hope that all of the examples used in this post are interesting enough to make people interested in learning about this new field of research. But they are not enough: A well-designed testing framework will greatly improve performance and quality of the results"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which QA benchmarks incorporating more than 10000 questions?
 output (Sparql query): "	"SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, ""[+]"", """") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, """","""", """")) AS ?questions_number) FILTER( ?questions_number > 10000) } "	"SELECT RTE; c; select all input (English text): Can you list the metrics used to evaluate models on the RTE dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } select all input (English text): Can you list the metrics used to evaluate models on the RTE dataset ? output (Sparql query): SELECT RTE; c; select all input (English text): Can you list the metrics used to evaluate models on the RTE dataset ? output (Sparql query): SELECT RTE; c; insert into metrics (key, key, val, val_lbl, t_lbl, t_rfl_lbl, metric_lbl, metric_g, max_g, max_rfl_rfl_g, max_l, max_g, rl, rfl ) values ('test', 'test_lbl', 'test_g', 'test_rfl_lbl', 'test_g_lbl', 'test_rfl_rfl_g', 'test_l_g', 'test_rfl_l', 'test_g_l', 'test_rfl_l"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT IF ( ?dataset = ?dataset_lbl and ?dataset = a or kgc:Dataset and ?dataset_lbl.datasp.titles is set to ""Atari Games"")) ?dataset orkgc:Dataset; ?dataset orkgc:Data. ?dataset_lbl.datasp.titles. { ?problem a orkgc:Problem; } In order to select the desired variables or metrics, the user's SQL query would be composed as follows: SELECT ROWS(t1:p1, ?t2:p2) as [s1,..., sn]; The syntax is a little more complicated for the R2 syntax, with R2 being a more limited subset of R : SELECT t1:p1, v1:n1,..., vn as [t1,..., vn]; The first argument (a variable) is used in the argument list, with no additional values being allowed. The second argument (the actual argument) should be used as a new name for the variable, e.g. ?ts2 instead of ?dts2. The three arguments of the R2 syntax are: The second argument specifies the numeric value (usually a list of numerical indices of type varchar ). The third argument specifies a boolean value (also usually a list of numerical indexes of type varchar ). You can also specify other values that you'd like to use as argument for the R2 syntax, which are as follows: The fourth argument is used as a new name for the variable that should be returned. If it is not present, the first argument must be used. The fifth argument is used to specify the variable used to populate the rows"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Question 3) A paper titled ""Competitive Driving: A Novel Analysis of Virtual Traffic Safety"" (SDS) has used the ""BENCHMARK"" model. Do the results you obtained with the ""PRICE"" model compare any further with the results you obtained with the ""BENCHMARK"" model ? Question 4) In a paper titled ""A Unified Model of the Driving Experience of Mobile Systems"":  (page 9) It says that you used the ""BENCHMARK"" model to compare the ""PRICE"" model results with the ""BENCHMARK"" model.  Is that correct ? Question 5) In a paper titled ""Instrument-Based Multimodal Approach to Autonomous Traffic Regulation and Safety: A New Approach from the Driving Experience Perspective"" it shows you used the ""BENCHMARK"" model. It also says that you used the ""PRICE"" model. Can you comment on this ? Question 6) In a paper titled ""Safety, Autonomy, and Traffic Management: A Review Paper"" it says that you used the ""BENCHMARK"" model to compare your data using the ""PRICE"" model. It also says that you used the ""BENCHMARK"" model to compare your data using the ""BENCHMARK"" model. Can you comment on this ? Question 7) In a paper titled ""Driving Behavior, Vehicle Speed and Vehicle Autonomy: A Review Paper"" it shows you used the ""BENCHMARK"" model to compare your dataset using the ""PRICE"" model. It also says that you used the ""BENCHMARK"" model to compare your dataset using the ""BENCHMARK"" model. Can you comment on this ? Question 8) In a paper titled ""Probability and"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" --------------------- --------------------- --------------------------------------------------- -------------- ------------------------- --------------------- | Title | ID | -------------+------------------------------------------------------------------- --------------------- -------------- ------------------------- --------------------- | ""Tutorials in Datenwerk"" | 42 | 10.28 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Datenwerk"" | 41 | 10.27 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | PIKA | 39 | 10.27 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Zum Einsteins-Testbed. Vormeiner der Zusammenagern"" | 39 | 10.27 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Zusammenagern. Wien. Datenwerk"" | 36 | 10.25 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Wien. Datenwerk. Datenwerk. Vormeiner"" | 34 | 10.22 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Wien. Datenwerk. Datenwerk. Datenwerk"" | 32 | 10.21 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Wien. Datenwerk. Vormeiner nach Vorlagen!"" | 30 | 10.21 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | ""Datenwerk. Vorlagen: Datenwerk"" | 31 | 10.20 | --------------- -------------+--------------------------------------------------- -------------- ------------------------- --------------------- | """	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): Which metrics were observed on evaluation of the Cars datasets ? (Note: The results of the benchmarks refer to benchmarks used by the authors to determine if the problem was found as a good case and how accurate the algorithm was). output (Sparql query): Here, the text of the first question is in French, as is the query itself, so, the output will indicate that the dataset was used for the analysis. A simple alternative to the above approach would be to use the dataset itself, rather than the output of the RNN (because the RNN itself produces a new dataset, of course), but that also makes the RNN's output non-deterministic! This can be avoided by simply using an RNN that can read the file in binary! The code for querying a dataset using the above syntax is below the fold.  This code does not use many RDBMS-related features, and the result is a RNN in its simplest form, as it outputs a simple CSV file for output to SparseText. This is then passed on to the input function to construct a new dataset, and a new RNN can be created from each new dataset. Here is the source of the program: /* readdata.py _________ / /  /  /    /  /  / /  /  /   /   /  /  /  /  /  /  /   /  /  /  /  /  | --  | A. Carcassonne S. Paris S. Zurich  | --  |  |  |  |  |  |  |  |  | |  |  	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE ?title || TITLE || TITLE. TITLE | ID | NAME || TITLE. TITLE | ID | NAME || TITLE. Here's a complete set of questions you can add to your own queries: # This can be added to query (or any line starting with `...`): # SELECT TITLE TITLE AND (title || TITLE) TITLE TITLE TITLE, TITLE, NULL, NULL, N'*', '', ' ?', NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL, N'*', '', ' ?', NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL N'*', 'N'* (e.g., `N', ` ?', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', ` ? ', # SELECT'' OR'AS'SELECT'' NOT'FROM'(' (' (' N ') OR ( ', )'' ) OR ( ',')' (') AND ('' AS') ORDER BY'' ON (''AS')') AS''  |'' AND'' OR''  |'' AND'' AND''  |'' AND''  |'' AND''  |'' AND''  |'' 	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): For what piezoresistive material the least response time was obtained?
 output (Sparql query): "	SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1	" Input: S Paragraph: Input: input (English text): What is the status of the P20/P3 project ? The P20/P3 was originally slated for initial release for early 2018.  It was delayed for an ""unanticipated"" reason. input (English text): If we can go ahead with our release, will it involve making any further changes to the code base ? output (Sparql query): Input: S Paragraph: Input: input (English text): What can you tell us about the status of the P20/P3 project ? The P20/P3's goal is to start developing a language that will be able to work with data, but is not language agnostic. input (English text): How much more progress are you guys making on the P20/P3 ?   output (Sparql query): Input: S Paragraph: Input: input (English text): What is your goal with the Semantic Web ? While we don't yet have a language specific standard or standard for general web development or use, we are already developing languages specifically for the Semantic Web.   output (Sparql query): Input: S Paragraph: Input: input (English text): Are you in the process of working on your second standard ?  Are there currently any plans for adding or modifying existing standards ? output (Sparql query): Input: S Paragraph: Input: input (English text): What will be the relationship between the Semantic Web and P30 ?  Are there any plans to work on a technical standard ? output (Sparql query): Input: output (Sparql query): What is the status of the P30 project ?  It looks like there is"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?code ?code_link FROM papers ORDER BY ?code ?code_link DESC; input (English text): Model score for the GPT2 (small) model. output (Sparql statement): { ?name ?model ?value ?cnt ; ?metric ?cnt ; ?rdf ?code_link ; ?key ?key ?cnt ; ?pch ?pch ; ?model ?model_lbl ; ?labels ?labels %cnt; } input (English text): A list of all the papers in the reference database. output (Sparql statement): SELECT ?reference ?reference_lbl ?datafrom ?data WHERE { ?ref ?ref ?ref_lbl; } LIMIT 2 input (English text): A list of all the papers from the literature in the reference database. output (Sparql statement): outputs: SEND TABLE 'cite_data' WITH FULLTEXT outputs: SEND TABLE 'cite_clust' WITH FULLTEXT outputs: SEND TABLE 'cite_data_lbl' WITH FULLTEXT outputs: SEND TABLE 'cite_clust_lbl' WITH FULLTEXT outputs: SEND TABLE 'cite_data_lbl_lbl' WITH FULLTEXT outputs: SEND TABLE 'cite_clust_lbl_lbl_lbl' WITH FULLTEXT outputs: outputs: SEND TABLE 'data' WITH FULLTEXT outputs: CUSTOMER 'jkoe'@'localhost' IDENTIFIED BY '92233983@' WITH 'hc_pk_t' FORMAT '1.1	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?benchmark, ?model, ?data_source; output (Sparql query): SELECT ?model, ?data_source, ?benchmark( ?input_data_source = ?output_data_source ) ; output (Sparql query): SELECT ?model, ?data_source, ?benchmark( ?input_data_source = ?output_data_source, ?metrics = ?stats. ?stat_ ?stats: ?stats_ ?stats_lbl_lbl_re, ?metrics = ?stats. ?stat_ ?stats: ?stats_re, ?metrics = ?stats. ?stat_ ?stats: ?stats_lbl_re, ?metrics = ?stat. ?stat_ ?stats: ?stats_lbl,. ?output_data_source = ?benchmark, ?metrics = ?stats. ?stat_ ?stats: ?stats_lbl ); select _model from ?benchmark ... select ?sport from ?data_source ... select ?dataset from ?cont ... select ?test_data_source from ?benchmark ... select ?data_source from.benchmark_lbl_re_stats_todos ... select ?dataset from.benchmark_lbl_re_todos ... select ?test_data_source from.benchmark_lbl_re_todos_stats_stats_coding_list ... select ?data_source from.benchmark_lbl_re_stats_	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 output (Sparql query): A total of 4,738 results were found. The top 10 models are: 1. DeepMind, 0.099s. 2. Microsoft, 0.046s. 3. Baidu, 0.047s. 4. Google, 0.061s. 5. Kaggle, 0.060s. 6. Google, 0.046s. 7. RNN, 0.029s. 8. IBM, 0.022s. 9. Amazon, 0.027s. 10. Microsoft, 0.026s.   A total of 7,769 results were found. The top 3 approaches that scored highest for the GAD benchmark are: 1. DeepMind, 0.811s. 2. Microsoft, 0.866s. 3. Google, 0.881s.  Note: a lower score gives one an edge, as the problem to be solved is less complex than the GAD problem.  The GAD benchmark is often used as a benchmarking tool only, and not as an actual GAD model. A  result here from Google : To evaluate GAD, we have designed a framework that has an independent set of features, and then we use supervised learning to combine these independent features to create an explicit feature map. As we increase the computational complexity, that model gradually becomes more difficult to train. For example, a model that tries to model 100 features would take up to 18 days to run, and would take 15 months to train on 20 million labeled images. But if an algorithm uses one feature on an image, its input (input to the algorithm) has limited applicability: every pixel has to be recognized at regular intervals. Google's GAD benchmark model (click to enlarge)   output	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. INPUT { ?dataset a orkgc:HAS_DATASET ?dataset; } { ?paper orkgp:HAS_DATASET ?model. } { ?model rdfs:label ?model_lbl. } } output (Sparql query): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. INPUT { ?dataset a orkgc:HAS_DATASET ?dataset; ?model rdfs:label ?model_lbl. } { ?model rdfs:label ?model_lbl. INPUT { ?dataset a orkgc:HAS_DATASET ?dataset; } { ?model rdfs:label ?model_lbl. INPUT { ?dataset a orkgc:HAS_DATASET ?dataset; } } So the following table is helpful, because it contains quite a few parameters that are related to the model output: input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the code links in papers that use the BiT-M model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT CODE_LINK(a,b,r); ||| code_link(a,b,r) ||| code_link(a,b,r); ||| code_link(a,b,r) ||| code_link(a,b,r); ||| code_link(a,b,r); ||| code_link(a,b,r) OR| code_link(a,b,r); Output (RTE code (a,b,r,c)) RTE code (a,b,r,c,0.03) 0.05 0.01 0.05 0.02 0.00 0.00 RTE code (a,b,r,c,0.01,0.05) 0.00 0.00 0.00 0.00 0.50 0.00 0.00 Output (RTE code (a,b,r,c,0.02,0.00) RTE code (a,b,r,c,0.05 ) 0.40 0.13 0.00 0.00 Conclusions You're welcome if you've seen any of my previous posts on the RTE datasets above!	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?book ?book_lbl FROM { ?db ?db_lbl } UNION ALL SELECT DISTINCT ?author ?author. ?title ?title. ?field ?labels ?labels_lbl. The following two columns are optional: ?dataset — an alias/function, provided in the query statement for the dataset to be compared to the gamedata, for example - { ?dataset a orkgc :Dataset; rdfs:label ?dataset. ?contor orkgp:HAS_DATASET ?dataset. } The result is in the two columns labelled ""DISTINCT""!. When provided, the first argument can be a number from 0 to 9, representing the number of different dataset entries in the benchmark query; in this column, the number is given after the column separators. With the second argument, a variable length string of delimited fields is provided, one or more per row of the benchmark query, separated by commas. For example: result (Sparql query): SELECT DISTINCT ?dataset ?. ?dataset, * (1, 2, 3) ? ? ?labels ? ? ? ?labels(1, 2, 3) ?labels(1, 2, 3). ?field ?labels ? ?labels_lbl. output (Sparql query): SELECT DISTINCT ?book ?book_lbl FROM { ?db ?db_lbl } UNION ALL SELECT DISTINCT ?author ?author. ?title ?title. ?field ?labels ?labels_lbl. The following column is optional: ?cont — for the benchmark query, a variable length list of constraints in"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the NASCell model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NASCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------ | A | B | C | B | C | B | C | |--------------------------------------------|--------|-----------|----------|--------------------------------------------| | A | C | C | A | C | A | A | C | |--------------------------------------------|--------|-----------|----------|--------------------------------------------| | A | B | C | A | B | B | A | C | |--------------------------------------------|--------|-----------|----------|--------------------------------------------| | A | B | C | A | B | C | A | B | C | |--------------------------------------------|--------|-----------|----------|--------------------------------------------| The outputs are in the form: | A | B | C | B | A | B | C | B | |--------------------------------------------|--------|-----------|----------|--------------------------------------------| | GAD|DOT|GBD|GBT|GPCN|GPCR|GPCP|GPR|GPS|GSS|GSSV|GTS|GTE|GC|GCS|GCSV|GCI|GCSV|GCSS|GB|GMP|GMPD|GMPP|GMPP|GMPB|GMPP|GMPV|GMPV | | GAD I|DET|GBD|GBT|GPCN|GPCR|GPCP|GPR|GPS|GSS|GSSV|GTS|GTE|GC|GCS|GCSV|GCI|GCSV|GCSS|GB|GMP|GMPD|GMPP|GMPP|GMPB|GMPP|GMPP|GMPV|GMPV | | GADK|DOT|GBD|GBT|GPCN|GPCR|GPC	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): How do I import the data from the TCN model from your dataset to the SQLite database ? output (Sparql query): input (English text): How do you use the F1 score to measure performance in the FIA Formula 1 World Championship ? output (DML): source: http://www.fia.com/motorsport/sport/news/2016/f1-announces-new-technology-for-test-cars_p1735 input (English text): If we choose DML, in which version do you recommend ?	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" input (English text): What does the paper state about using HoC data ? output (Sparql query): NOTE: This query would have been better suited for the online test of the HoC paper and could have avoided the problem of selecting the last column of the result table. In the paper: ""For many important questions in bioinformatics, such as the prediction of gene function, the input-output problem is a prime candidate for improvement. On many such occasions, the output may be very rich (e.g., the paper reports a number of data sets of high quality [e.g., Drosophila, Escherichia coli, and RPE3]; the input data in general is not very important for the question but is of interest to the bioinformatics community for technical reasons)."" output (Sparql query): select paper.dataset from paper where paper_id = ?benchmark.select( { ?dataseta : ?dataset_lbl }, { ?paper } ) ?modela if ?modela  [7] This query would have been better suited for the online test of the HoC paper and could have avoided the problem of selecting the last column of the result table. In the paper: ""However, the use of a dataset is often not relevant to the question at hand in the scientific publication. This is why the paper recommends the use of the output for the benchmark (in particular the paper on the development of E. coli, p. 1029 to 815)"" output (Sparql query): select paper.dataset from paper where paper_id = ?benchmark.select( { ?d : ?dataset_lbl }, { ?lbl : ?model } ) ?modela if ?modela  ["	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------> TOP { ?dataset orkgc:Dataset/rank } { ?metric orkgc:Dataset/rank } --------> 10.6 <5.0 <1.9 <1.6 input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0 ? output (Sparql query): --------------> TOP { ?dataset orkgc:Dataset/rank } { ?metric orkgc:Dataset/rank } --------> 10.6 <5.0 <1.9 <1.6 input (English text): What is the average benchmark score over the dataset SQuAD2.0 ? output (Sparql query): --------------> TOP 12.8 <3.95 <3.2 <4.0 input(English text): What is the average benchmark score over the dataset SQuAD2.0 ? output (Sparql query): --------------> TOP 12.8 <3.95 <3.2 <4.0 output (Sparql query): --------------> TOP 12.8 <3.95 <3.2 <4.0 input (English text): Why are most metrics positive ? Is there a known algorithm like maxima ? output (Spiral plot): --------------> RANKS<> { ?rank/rank ?value orkgc:Dataset/vars : }< = ?value ; orkgc:Dataset/vars : } input (English text): Why are most metrics positive ? Is there a known algorithm like maxima ? output (Spiral plot): --------------> RANKS<> { ?rank/rank ?value orkgc:Dataset	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?model ?model_lbl, ?dataset ORkgc:Dataset AS model_lbl, rdfs:label ?dataset_lbl. ?metric rdfs:label DISTINCT ?dataset. ?value rdfs:label DISTINCT ?value. ?value DISTINCT ?value. ?value DISTINCT ?value_lbl. In order to get a clearer picture on the results we can also run the test on some other dataset. test (HIPOP_H3_F1_F1_B:HS2_HPS_M1:HIPOP_H3_F1_F1_L_P) SELECT DISTINCT ?model ?model_lbl, ?dataset P_dataset_lbl, ?dataset HPP_dataset_lbl, ?dataset ORkgc:Dataset AS model_lbl, rdfs:label RDFS_Lbl, ?dataset HPP_dataset_lbl, rdfs:label ORAG_Lbl, HPP_dataset_lbl RDFS_Lbl. FILTER (str( rdfs:label)) { RDFS_Lbl RDFS_Lbl ORAG_Lbl } ORDER BY DESC ( ?value ) LIMIT 1 } This time we find that, although the test on the first dataset, does not show any significant differences in performance compared to the second dataset, on the second dataset the second dataset performed much better, and was the best dataset for the F1 benchmark. I am curious as to how the H3 graph compares when compared to the benchmark score, and	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?code, ?benchmark, ?index, ?value if(!index ||!value ||!value_in( ?code, ~~.test ) || ~~.benchmark ) { ?cont orkgp:HAS_BENCHMARK, ?model } After a few days, I found out that the benchmarking code was also used to evaluate models with different level of support for support aggregation for each of the test parameters. It's important to specify the level of support for support aggregation, otherwise performance might vary. input (English text): List the metrics that are used to evaluate models with different level of support for support aggregation for each of the test parameters ? I already found in this query the code reference for the benchmarking code, which includes parameters for the support aggregation level used by the model. I'm not sure how to extract, from the code reference, the value of the test parameter for the model. Can anyone shed any light on this ?	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Who has contributed to the largest number of articles about coronavirus?
 output (Sparql query): "	"SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), ""2019-nCoV"")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), ""2019-nCoV"")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)"	 input (English text): Where can I find documentation about this process ? output (Sparql query): input (English text): Where do I find more information about coronavirus or other viral infections ? output (Sparql query): input (English text): I would like to make any changes to how coronavirus is benchmarked and published in peer-review journals. In particular, I would like to make it easier for peers who already have coronaviruses and infections to review and comment. output (Sparql query): input (English text): What is the role of peer review for peer review papers ? output (Sparql query): input (English text): How is coronavirus used in the peer-review process ? output (Sparql query): input (English text): The purpose of peer-review papers is to get input from reviewers so that these authors can provide more useful and accurate commentary. Why are coronavirus papers not peer-reviewed ? output (Sparql query): input (English text): What types of publications and conferences are reviewed ? output (Sparql query): input (English text): What is the purpose of the peer-review process for the peer-reviewed paper ? output (Sparql query): input (English text): My colleague at MIT and I are looking for researchers working on the computational analysis of coronavirus outbreaks. What can I do to help ? output (Sparql query): input (English text): What is the purpose of the peer-review process for the peer-reviewed paper ? output (Sparql query): input (English text): Any questions about preparing the paper ? output (Sparql query): Please	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?b b. OPTIONAL OR ( ?max orkgp:HAS_METRIC MAX( ?value )). OPTIONAL OR ( ?max orkgp:HAS_EVALUATION max( ?value )). OPTIONAL OR ( ?max orkgp:HAS_BENCHMARK max( ?value )). OPTIONAL OR ( ?"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What coolant does ASTRID reactor use?
 output (Sparql query): "	"SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label ""ASTRID""^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, ""coolant"")) ?coolant rdfs:label ?coolant_label. } "	SELECT ?rdfs:label ?rdf. ?rdf:fmt ?frm_filter ?fml_sorter ? WHERE { ?dataset a orkgc:Dataset ?dataset_lbl. } ORDER BY ?rdfs. ?rdf sorter ?fml_sorter and ?fml_sorter DESC input (English text): What coolant does ASTRID reactor use ? output (VBA script): 1) Selecting all the models. 1. Select the one you want to examine. In this section, we will learn how these models interact, and we will find out about their various characteristics and how they differ from each other. We will then create a detailed report of the interactions, compare the models, create a list of the models, and use these results in our VBA script above to analyze the models further. First, lets see what's happening with the models: 1. Selecting models from the database 2. Loading an input dataset into a model with ModelFactory 3. Outputting the resulting model to a model table 4. Createing a list of all the models we want to analyze 5. Checking the model table for any results We start with the models loaded and loaded into the ModelFactory. All models are initialized with the names of the Dataset and Dataset_lbl attributes. This means that all the models need to have the first name, then the last name, then the model_lbl attribute. Note that we have not seen the last name and model_lbl of the model - this comes when we get the models from the database. 1. A model is loaded and then loaded into the ModelFactory. 2. The output for the first model is stored in the first model_lbl in the	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?benchmark orkgp:HAS_DATASET ?benchmark. Let's use this benchmarking result to create a dataset for the GAD dataset. The name of the dataset is id_d-berzerk. Here's the function definition for the benchmark function: function benchmark():  select { ?dataset a orkgc:Dataset; } printbenchmark({ ?benchmark { ?dataset a, ?key { ?dataset t, ?benchmark t ; } } }):  Prints the benchmark result to the console For each input : output :  Outputs a list of data structures (dataset and key) and their scores, including the GAD metric, for each of the benchmarking datasets ( ?dataset,  1:5, 3:10, 5:50) SELECT DISTINCT   ?benchmark  ( ?dataset a )( ?key { ?dataset t, ?benchmark t ; ?dataset a, ?key { ?dataset t } ?dataset. ?key { ?dataset t, ?key { ?dataset t } ?dataset. ?key { ?dataset t, ?key {   ?dataset t, ?key { ?dataset t, ?key { ?dataset t, ?key { ?dataset t, ?key { ?dataset t, ?key {   ?dataset t, ?key { ?dataset t, ?key { ?dataset t, ?key {   ?dataset t,.   ?key {   ?dataset	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT id, name, score FROM ski:model.pairs WHERE id= ?id input (English text): Have you evaluated the same model on your Raspberry Pi ? output (Sparql query): --------------------- ------ ORDER BY id ASC, name ASC, score ASC --------------------- ------ SELECT id, name FROM ski:model.pairs WHERE id= ?id input (English text): I have seen that you have been using the ""Foldable Dataframes"" tool on Raspbian. What is the purpose of doing so ? output (Sparql query): --------------------- ------ SELECT id, name, score FROM ski:model.pairs WHERE id= ?id ---+---------------------------+--------------+---------------------------+ | id | name | score | ---+---------------------------+--------------+---------------------------+ | 0 | id1 | 1 | 896 | 0.6 | 1 | | 1 | id2 | 2 | 987 | 0.4 | 1 | | 2 | id3 | 3 | 1001 | 0.4 | 1 | | 3 | id4 | 4 | 1013 | 0.3 | 0 | | 4 | id5 | 5 | 1073 | 0.3 | 0 | | 5 | id6 | 6 | 1087 | 0.3 | 0 | | 6 | id7 | 7 | 1114 | 0.2 | 0 | | 7 | id8 | 8 | 1114 | 0.2 | 0 | | 8 | id9 | 9 | 1132 | 0.1 | 0 | | 9 | id10 | 10 | 1132 | 0.1 | 0 | | 10 | id11 | 11 | 1200 | 0.1 | 0 | | 11 | id12 | 12 | 1216 | 0.1 | 0 | | 12 | id13 | 13"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 ----------- ------------- - ?dataset 1 orkgc:Problem 1.0 496 (100%) 264748 (100%) 276612 0.00% 0.00% 1.0000 2.000000 ----------- ------------- - ?dataset.hdf ? 1 orkgc:Problem ?dataset_lbl ?dataset.dataset_lbl 1.00% 0.00% 50.0000 (100%) 0.00% 50.0000 1.100000 The output should look like this: Question 19. Can you give an example of the question format you will use when asking and answering questions ? An answer to Question 19 asks a question and then provides the answers. It is also suitable for answering questions in the form of a short introduction, because some of the questions are long and complex and will take a long time to answer. Answer: Question 19 is about a small number of people that made significant contributions to the field. To answer Question 19 correctly, you need to understand a bit more about the field of Text Summarization, the method that is employed to convert a collection of text items into a single word or phrase. Question 20: What are some recent findings of your laboratory in the field of Text Summarization ? An answer to Question 20 is a brief overview of recent discoveries in the field of Text Summarization. Answer: The recent field of Text Summarization has been driven by a variety of topics and research directions. One of its most recent and compelling developments is the use of text-based metrics to improve the results of text-based machine-learning models of text. The authors have conducted a series of studies which have demonstrated how these metrics can enhance human error by up to 95% (see for example our paper, Quant	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?dataset ?dataset_lbl. ?b orkgc:Benchmark OR ?dataset ?dataset_lbl. ?cont orkgp:Benchmark ?dataset. ?cont orkgp:Benchmark. ?crit orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:P32. ?test orkgp:BENCHMARK. input (English text): Which of the Atari 2600 projects were the most impactful that you have observed in your evaluation process ? output (Sparql query): input (English text): What are the metrics used to define a good Atari 2600 project ? output (Sparql query): SELECT ?dataset ?dataset_lbl. ?average orkgp:AVG ?dataset. ?bench OR ?dataset_lbl. ?cont orkgp:Benchmark. ?average ?lbl. ?bench OR ?dataset_lbl. ?cont orkgp:Benchmark. ?average ORtgp:AVG ?dataset. ?average *total ortgp:AVG ?dataset. ?average ?lbl. input (English text): Which of the Atari 2600 projects were the most impactful that you have observed in your evaluation process ? output (Sparql query): input (English text): What is your estimate of time needed for development of a reliable evaluation method over all of the Atari 2600 projects (for all games except the 2600 version of Star Ocean) ? output (Sparql query): input (English	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?value_score, ?value_score, ?score AS (SELECT DISTINCT ?value_score, ?value_score, ?""( ?:name:id)""|| ?name || ?id|| "" )"", "" ?""( ?:count:counts-counts, "","" count|| "" )"", "" ?""( ?:value_score_mean, "","" value_score_mean, "","" value_score_median, "","" value_score_stddev "" )"", "" ?""( ?:value_score_minimum, "","" value_score_minimum, "","" value_score_median, "","" value_score_mean, "","" value_score_stddev "" )"", ?""( ?:score_rank, "","" score_rank, "","" score_rank1, "","" score_rank2 "" )"", "" ?""( ?:score_age, "","" score_age, "","" score_age1, "","" score_age2 "" )"", "" ?""( ?score[1]- ?value[3], "","" value1&&"" "" )"", "" ?""( ?score[2]- ?value[4], "","" value2&&"" "" )"", ?""( ?:score_weight_mean_stddev, "", "" score|| "" )"", "" ?""( ?score_weight_median_stddev_mean, "","" score|| "" )"", "" ?""( ?score_weight_mean_stddev_median, "","" score|| "" )"", ?""( ?score_score_stddev_mean, "", "" score|| "" )"", "" ?""( ?score_score_mean_stddev_median, "","" score|| "" )"", "" ?""( ?value_score_mean, "","" value1&&"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT { data} FROM { datasource } ORDER BY {!datasource } WHERE {!data } {	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT TABLESPACE ?model_lbl. ?model_lbl rdfs:label ?model_lbl. ?namespace rdfs:value input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (short) dataset ? output (Sparql query):  SELECT TABLESPACE ?model_lbl. ?model_lbl rdfs:label ?model_lbl. ?namespace rdfs:value Input data for the ""top performing"" model, and data for the ""best performing"" model. The data that is necessary to create an RTE model are (from left to right, in the order shown): Metric data (where is the metric value of the benchmark-generated question in the English text); A query for querying the dataset, for example for searching for the top model in terms of F1 score. The question "" What is the best performing model "" is answered by the following query: select query, ?metric_lbl, RTE from orkgc_queries where query.MetricType LIKE ?metric_lbl RTE model of the best performing model. An RTE model will usually contain a query to select all models named with the model_name column, or by selecting a subset."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the metrics of evaluation over the CommitmentBank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT IF (date = datetime(2013, 03, 2, 31, 30, 3)) AS project_id FROM [com.hockeyanalytics.voting] WHERE (project_id = ?); select project_id, project_type, project_name, project_name_lbl, project_name_id FROM [commitmentbank3m_crowdfunder_revenue ] WHERE project_id = ?; select project_id, project_type, project_name, project_name_lbl, project_name_id, count( ?) as project_id, count( ?) as project_type, count( ?key) AS project_id, count( ?key) AS project_type, count( ?key) AS project_name, count( ?key) AS project_name_index WHERE project_id = ?; count( ?key) AS project_id, count( ?key) AS project_type, count( ?key) AS project_name, count( ?key) AS project_name_count FROM [commitmentban_voting] WHERE project_id = ?; select project_id, project_type, project_name, project_name_lbl, project_name_id, project_name_index FROM [compactprojects_voting] WHERE project_id = ?	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input (English text): What are the main results and limitations of this paper ? output (Sparql query): input (English text): What is the output of the paper ? output (Sparql query): input (English text): Is this paper about a particular target field of expertise, such as cellular automata ? output (Sparql query): input (English text): How does the work differ from previous research ? output (Sparql query): input (English text): What is the major claim of this research ? output (Sparql query): input (English text): What is the title and abstract or abstract plus the page numbers, and where are they from ? output (Sparql query): input (English text): The main abstract and pages for these pages are: ""What are the key areas in functional neuroanatomy ?"". output (Sparql query): input (English text): What is the URL of this paper ? output (Sparql query): Input (English text): Any additional references relevant to the above questions ? output (Sparql query): Input (English text): The abstract of the paper is: ""How can we identify functionally relevant neuronal circuits in neural models of language, visual consciousness, memory, and emotion ?"" output (Sparql query): Input (English text): What has been published about this paper ? output (Sparql query): Input (English text): What is the most recent version of the paper ? output (Sparql query): Input (English text): What is the journal associated with this paper ? output (Sparql query): Input (English text): Where"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model_lbl. FILTER (str( ?model_lbl ) = ""VPN"") ?baseline_1_1_1. ?baseline_1_1_2. ?baseline_1_1_3. ?baseline_1_1_4. ?baseline_1_1_5. ?baseline_1_1_6. ?baseline_1_1_7. ?baseline_1_1_8. ?baseline_1_1_9. ?baseline_1_2_1. ?baseline_1_2_2. ?baseline_1_2_3. ?baseline_1_2_4. ?baseline_1_2_5. ?asn1_1. ?asn2_1. ?asn3_1. ?asn3_2. ?asn4_1. ?asn5_1. ?asn6_1. ?asn7_1. ?asn8_1. ?asn9_1. input (English text): For each data collection, what are some additional parameters that I should be considering before using a model of that type, and how could they affect performance ? output (Sparql query): input (English text): The number of ACE models used in the benchmarking exercise is relatively small. What is the reason for this ? output (Sparql query): When comparing models, it is important to keep in mind that one can measure performance and model performance in parallel, or, a priori. It is"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 I have used the same dataset but the benchmark results are listed below along with the number of times the benchmark suite is run: benchmark (test) ############################ 0.000001% 10.000000% 20.100000% 30.000014% (2 iterations) benchmark (test) ############################ 2.000000% 3.100000% 16.100000% (3 iterations) benchmark (test) ############################ 100.00000% 2.100000% 0.160000% (4 iterations) benchmark (test) ############################ 4.000000% 2.160000% 0.0640000% (4 iterations) benchmark (test) ############################ 100.00000% 2.000000% 0.00000000% (2 iterations) benchmark (test) ############################ 0.000001% 5.000000% 5.000000% (2 iterations) benchmark (test) ############################ 0.00000001% 10.000000% 18.100000% (2 iterations) benchmark (test) ############################ 0.00000001% 20.100000% 30.00000% (2 iterations) benchmark (test) ############################ 0.00000001% 100.00000% 2.000000% (2 iterations) benchmark (test) ############################ 100.00000% 2.000000% 0.000001% (4 iterations) benchmark (test) ############################ 0.00000001% 0.00000000% 50.00000% (2 iterations) Benchmark suite ############################ 6,999 benchmarks There is no need to modify your code from above since the code contains no new code and would already appear in such a benchmark suite. This is actually a great example in which I feel it is OK to add new features and code. The code changes are really just for making	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): what evaluation metrics is commonly used when benchmarking models on the STEM-ECR v1.00 dataset ? output (Sparql query): input (English text): What criteria are commonly used to evaluate training models on the Atari Games dataset ? output (Sparql query): Input (English text): What is the name of the dataset ? output (Sparql query): input (English text): What is the purpose of the dataset ? output (Sparql query): input (English text): Does the dataset provide a variety of data elements to perform evaluation comparisons ? output (Sparql query): input (English text): What features of the Atari Games dataset has researchers found particularly interesting ? output (Sparql query): input (English text): What other databases can be used for the comparison ? output (Sparql query): Input (English text): What performance metrics is commonly used to evaluate models on the Atari Games dataset ? output (Sparql query): input (English text): What performance metrics is commonly used to evaluate models on the LEGO dataset ? output (Sparql query): input (English text): What other databases can be used for the comparison ? output (Sparql query): input (English text): What performance metrics is commonly used to evaluate models on the LEGO dataset ? output (Sparql query): Input (English text): What is the purpose of the LEGO dataset ? output (Sparql query): input (English text): What metrics are commonly used to evaluate models on the LEGO dataset ? output (Sparql query): input (English text): What other categories of datasets would be useful for comparing Atari Games and LEGO datasets ? output	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 38	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the GAD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT model_lbl. ?model_lbl. FILTER (str( ?model_lbl ) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont. ( ?model_lbl orkgp:HAS_BENCHMARK ). ?benchmark. ?cont orkgp:HAS_MODEL ?model ; orkgp:HAS_SOURCE_CODE ?code. ?db ?s_test. ?dbl ?s_test_tbl. ?dbl ?s_test_tbl. In the output of the command you can read the following information:The "" model_lbl "" or "" VPN "" module should be installed on "" org/apache/gi "" and runs on the following platforms:1. Apache2. OpenWRT3. Solaris4. Linux5. FreeBSD6. OpenWRT7. NetBSD8. BSDNow you need to run "" org/apache/gi "" to run the "" testbench"" command on your machine.Now you can make a connection to the network and use "" testbench "" to check that the measurements are valid. This command displays some information about the network connection:Now read the following text and run "" testbench "" to get a quick view of the "" testbench "" output:The result of the testbench is displayed in the list of results displayed below. You can get the test results using the command "" testbench testbench_all.""If you want to compare what you got using the above command with the results with an alternative model that is optimized for testing, you can use "" testbench testbench_all "" in combination with the following command:The testbench command makes the measurements look very similar in every case, but in the following cases it seems to"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT 'benchmark' (code) FROM 'benchmark (code);' WHERE 'benchmark' is a valid term. The current list of valid terms: ?code:code of the selected code ?code:codes in the sample dataset ?cohort:cases from the study ?cont:cases in the dataset Or you could take the above examples and apply them to your own dataset. This can be done by creating new queries with the ' ?setmetric' option but do so only after you have established a baseline performance metric and also before you evaluate the new setmetric query. For help in defining a benchmarking metric, see the 'Benchmarking Metrics Toolkit' in the R package 'benchmark.mqrc'. 1.6.2 What is the maximum number of benchmarks ? The maximum number of benchmarks is not set at each time point, rather, the number of benchmark queries and metrics is limited by your dataset/benchmark setup. How many queries and metrics you run is determined by the size of your test set/benchmark set. 1.6.3 Can I run multiple benchmarking queries on a single dataset ? No, multiple benchmarking queries will not be successful. For more information, see section 1.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * FROM gmp.models R2['GAD_TEN_MATH_MOVEMENT'] AS G # Generate statistics using a multi-level gmp model output (Sparql query): SELECT gmp.model R2['GAD_TINY_MATH_GRAPHICS'] AS G # Update GAD models with new benchmark data output (Sparql query): SET GLOBAL_POWER_LIMIT = 3000000 SET GAPL_RESOURCE_LIMIT = 100000000 # Output the GAD metrics that are associated with each model output (Sparql query): select (GAPD_N_POWER_EXPANSION, GAPD_N_MOVEMENT, GAPD_N_DISTANCE, GAPD_N_PERCENT, GAPD_N_INITIAL_COUNT, GAPD_N_LOG_COUNT, GAPD_N_CPR, GAPD_N_THROUGH_DIST, GAPD_N_RANDOM, GAPD_N_RAND_LOG ) AS gpdefs # # The code from http://www.rfc7231.org # # Author: # Michael W. Fowler <wfowler@nist.gov >	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title || ?id || ?dataset || ?benchmark || ?lbl || ?test || ?metric || ?metric_lbl In this example we chose the title of a paper we've seen (we did a Google Scholar search on that title and ID). So we are adding more information. Note that we don't know the title, id, and data set of the paper. We have to read the article. But now, we could add this paper in the search text: ""benchmarks on the AAPD dataset"", so the whole text of the paper would be the text of the paper. It would probably be hard to extract the full text of the paper from this source! What's a benchmark ? It's a measure of the performance of a machine learning model. The performance of a model varies with the number of observations in the training data and can be measured as a percentile (for example the 95th percentile). It's a measure of a model's efficiency. If I know how many observations I have in a dataset, I can test it against a collection of other data sets (where I have more data, of course). I can then determine whether a model performs well. If it's really good, then it should be very fast as well since it uses all the data. If it doesn't provide much information about the model, then it isn't a very good choice. A quick tutorial on benchmarking, by John Horton and Matt Yglesias in Data Mining (pdf), might be of interest: ""The Benchmark, The Test, and the Benchmark. A Practical Way To Define Your Data Science Mission"". So, the main purpose here might be to find out how well a search for ""benchmark"" returns results. If you know very, very little about AI, then using this"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AxCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes ?"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?benchmark ?benchmark_lbl WHERE { ?dataset ?dataset_lbl; rdfs:label ?dataset_lbl. } input (English text): Can you link to the source code for benchmarks that benchmark the AWD-LSTM model with respect to a particular metric ? output (Sparql query): input (English text): Can you link to the source code for benchmarks that benchmark the AWD-LSTM model with respect to several of the GAD metrics ? output (Sparql query): SELECT ?benchmark { orkgc:GAD_METRICS ?dataset_lbl. ?cont ?dataset_lbl. ?lbl ?cont; rdfs:label ?dataset_lbl. } input (English text): What are the titles and IDs of research papers that included a benchmark for the GAD model ? output (Sparql query): SELECT DISTINCT ?papers ?papers_lbl WHERE { ?dataset ?dataset_lbl; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgc:Dataset; } output (Sparql query): Select first 3 columns and column names in a given row, using the given delimiters to limit to the maximum number of values, and then use a delimiter: name: title: (from table WHERE c.dataset = ?datas"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?id ?title FROM ?book ?book_id ?book_title JOIN ?book_lbl { ?book ?book_id } ON ?book_book_id = ?book_id WHERE rf:status = ?rf and ?frq = ?frq orkgs:status = ?frq orkgp:status = ?frq OR ?cj:status = ?cj orkgp:status = ?cj orkgp:status = ?cj orkgp:status = ?cj and ?tobol_id = ?tobol OR ( ?tobol_id = ?tobol ?tobol_id WHERE ( ?tobol_id = ?tobol_id ) ) AND ( ?tobol_id = ?tobol_id AND ?cj:status = IS_UNIQUE or ?cj:id = ?cj:id ) AND ?tobol_id = ?tobol_id AND ?term_id = ?term_id AND ( ?term_id = 1 AND ?title = ?question OR ?article_ref = ?article_ref ) AND ?"" Sorting Sort, by name: query { :sort! ?search ?columns sort_order ?"" } input (English text): How do you sort (by name or by rank, in other words, how do you sort by a specific attribute or group of attributes) ? output (Sparql query): A query like this would return results in two logical forms: (1) If you have many pages, you can look at each one of them individually and add/remove items. (2) You can use only one or two attributes at the time. Input (English text):"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT TOC( ?problem ?, ?dataset ?, ?dataset_lbl) FROM TOC ?LAST; name (required) (Optional) (Optional) output (Sparql query): SELECT TOC( ?Problem ?, ?dataset ?, ?dataset_lbl) FROM TOC ?LAST; name (required) (Optional) (Optional) As the output file will be written, we will see one or two new rows in Parquet of the dataset and the corresponding index in RDFS. You can now start the database queries by inserting the following code into main.routine.scalar : # database.scalar data ""RDF"", ""citation_type"", ""citation_text"", ""source_id"", ""citation_reference_id"", ""citation_dataset"", ""citation_tables"", ""record_id"", ""name"" name, ""text"", ""date"", ""record_dataset_id"", ""record_dataset_tables"", ""record"" This is a ""DBLOOKUP"" example, use the ""TEMP"" variable to make use of the full database table name, This code will be executed every time we need to run a new query with the names of all the datasets that have been used to benchmark the Atari Games problem. If the name, text,date and record_dataset_id are missing, then our request will fail. If any of the datasets is empty (not included in the benchmark list), then none of the benchmarks will be run for that dataset. I am using both the data and the index names to get the performance for both the training and the testing datasets. # database.scalar data"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" select ? 1st row (model) select ? 1st row (dataset) select ? 1st row (hasa_model) result Here, we can see the variable number of rows with a test result with the F1 metric. On the other hand, we see the variable number of rows with no test outcome (and a variable number of rows having one of the metric, but not one of the score). We also have the following score: output (Sparql query): order by ?matric_lbl_cnt : F1 select ?metric. ?metric rdfs:label ?metric_lbl. { ?metric orkgc:Metric; rdfs:label ?metric_lbl. RDFS:Trait(rdfs) ?metric. { ?dataset orkgc:Dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC""); rdfs:label ?dataset_lbl. ?dataset orkgc:Dataset ?dataset_lbl. } RDFS:Trait(rdfs) ?dataset_lbl. orkgc:HAS_DATASET ?dataset_lbl. rdfs:label ?dataset_l bl. Here we have a single row in which the metric was ""F1."" The next four rows are all missing metric scores, so that we can compare our scores to those of the default F1 metric produced by the default HoC metric package. Then in the last five rows, we get our score: output (Sparql query): order by ?test_metric_gibson point_goal navigation cnt from"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Which is most accurate on the Cheetah, run (DMControl500k) benchmark dataset ? output (Sparql query): * * * output (Sparql query): input (English text): Which model on the HoC dataset is the longest running ? output (Sparql query): * * * output (Sparql query): * * * output (Sparql query): * * * output (Sparql query): input (English text): Which model is the most accurate, run (DMControl500k) benchmark dataset ? output (Sparql query): input (English text): Which model on the Cheetah, run (DMControl500k) benchmark dataset is the most accurate, run ? output (Sparql query): * * * output (Sparql query): * * * * * * output (Sparql query): which models rank highest on the HoC dataset, run (DMControl500k) benchmark dataset ?	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT CASE WHEN ?benchmark IS NULL THEN ?benchmark. MIN_SOMETHING_BETWEEN 300 OR  CHECK ROW_NUMBER ()  THEN ?benchmark. MIN_SOMETHING_BETWEEN 600 OR WHEN ?eval IS NULL THEN ?eval. MIN_SOMETHING_BETWEEN 870 OR WHEN ?hgtr IS NULL THEN ?hgtr. MIN_SOMETHING_BETWEEN 870 OR WHEN ?model IS NULL THEN ?model. MIN_SOMETHING_BETWEEN 850 OR WHEN ?svmx IS NULL THEN ?svmx. MIN_SOMETHING_BETWEEN 880 OR WHEN ?vgtc IS NULL THEN ?vgtc. MIN_SOMETHING_BETWEEN 890   input (English text): Where was the first benchmark run ? output (Sparql query): output SQL Query:   select ?benchmark as ?benchmark, ?id as ?id, ?datasetName as ?datasetName, ?modelName as ?modelName, ?lblList as ?lblList or ?modelTypeName as ?modelTypeName, ?valList as ?valList ?benchmark = NULL ; ?id = NULL ; ?datasetName = NULL ; ?modelName = NULL ; ?lblList = NULL ; ?modelTypeName = NULL ; ?valList = NULL input (English text): How many dogs from each breed have been evaluated on the HoC dataset ? output (Sparql query): query ( ?name = 'Dogs', ?n = '200', ?c = '0', ?nval = 200, ?hgtr = 0.5, ?	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Input (Sparql query): Indicate the model that performed best in terms of Senseval 1 metric on the Unsupervised: benchmark dataset ? output (Sparql query): Input (English text): Indicate any results from the ""Analyzing models using S2 and S1 scores: Benchmark."" column ? output (Sparql query): Input (English text): Indicate the model which performed best on the unsupervised supervised dataset ? output (Sparql query): Input (English text): Indicate any results from the ""Analyzing models using S2 and S1 scores: Unsupervised."" column ? output (Sparql query): Input (English text): Indicate which models performed best on the Unsupervised supervised dataset ? output (Sparql query): Output: Column 1 Number of samples that were run: 10 Number of clusters that were created. Cluster was created at:  #of_clusters_cluster  1 Number of clusters in which the model is included: 5 Number of clusters in which the test set(s) was calculated: 5 No. of classes in the classification: 20 No. of class in the validation set: 20 No. of class in the supervised dataset: 2 Number of classes with high level of significance: 2 Number of class with low level of significance: 3 Column 2 Number of samples that were run for each model: 10 Number of clusters formed from the clusters: 1 Number of model clusters that were made: 1 Number of unsupervised clusters formed from the unsupervised clusters: 3 Number of test sets in each model: 10 Number of samples taken from each model: 10 Column 3 Class of the model: 50.0% Number of model classes: 5.0% Number of tests in each model: 50.0"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Provide output data of the wikitext query. output (Sparql query): output (Sparql query): input (French text): Indicate the model of each project (e.g. Research Labels) that is based on an existing semantic model, i.e. a model that is already present in the framework. output (Sparql query): output (Sparql query): input (English text): Show the model output of the database schema. output (Sparql query): input (French text): Show the model output of the semantic query. output (Sparql query): input (English text): Do you believe that the current model is strong enough to be used for Semantic Web ? output (Sparql query): output (Sparql query): input (English text): Do you think that the current model is robust enough to handle the current workload ? output (Sparql query): output (Sparql query): input (English text): What is the current capacity of the software, and what is the total codebase ? output (Sparql query): input (French text): Do you agree that the current model is strong enough to be used for Semantic Web ? output (Sparql query): output (Sparql query): input (French text): What is the current capacity of the software, and what is the total codebase ? output (Sparql query): output (Sparql query): input (English text): Which of the following projects makes up what is meant by the Semantic Web Framework ? output (Sparql query): output (Sparql query): 	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl ) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Provide a list of benchmarked datasets related to the Atari Games research area ? output (Sparql query): input (English text): Provide a list of benchmarked datasets related to the Atari Games research area ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl ) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. orkgc:RDFs:LBL ?dataset. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } input (English text): Provide a list of benchmarked datasets related to the Atari Games research area ? output ("	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the most common lead compound?
 output (Sparql query): "	SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1	SELECT DISTINCT ?lead GROUP BY orkgc orkgc GROUP BY lead; orkgc orkgc leadleadleadleadlead input (English text): What is the most common lead compound ? output (Sparql query): SELECT DISTINCT ?lead GROUP BY orkgc orkgc GROUP BY lead; orkgc orkgc rank lead leadleadleadleadleadlead input (English text): What is the most common lead compound ? output (Sparql query): SELECT DISTINCT ?lead GROUP BY orkgc, orkgc, orkgc GROUP BY lead, orkgc, orkgc, orkgc; orkgc, orkgc, orkgc rank lead_lead input (English text): What is the most common lead compound ? output (Sparql query): SELECT DISTINCT ?lead GROUP BY lead, lead_lead, lead_lead, orkgc, orkgc, orkgc, lead, lead_lead, orkgc; orkgc, orkgc, orkgc rank lead_lead input (English text): What is the most common lead compound ? output (Sparql query): SELECT DISTINCT ?lead GROUP BY lead, lead, lead_lead, orkgc, orkgc, lead, lead_lead, orkgc; orkgc, orkgc, orkgc rank lead_lead input (English text): What is the most common lead compound ? output (Sparql query): SELECT DISTINCT ?lead GROUP BY lead, rank, lead_lead, orkgc, orkgc, rank, lead_lead, orkg	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?""GAD"" :score( ?dataset) :score( ?dataset_lbl) AS ?score_score } Now, let's say you run your query to get all results. Then you have the data in another, bigger table. If you wanted to, you could use the original table data and compare the output from the query to the original result table data. But that is not what I want to do. What I want to do is get the scores of each model by adding up the scores for all of the models. This means I want to write a large query, which would fit on a single process server at most. Fortunately, this is easy to do. Let's say the task is to compute a Score, defined as the weighted average cost of all the models in a top-scoring list. The model table is given below. input (English text): GAD Score: { ""C0"": 4.2, ""C1"": 2, ""C2"": 4, ""C3"": 2 } output (Sparql query): SELECT ?""CNT"": GAD Score: ?""CNT_{top_scoring}"": scores( ?dataset) output (Sparql query): SELECT ?""CNT"" : score( ?dataset. ?cont) score ( ?dataset) Now, lets run the query on both of my processes: # for this project, the model names are the same except for the table name, so we don't have to input them into the query. SELECT ?""CNT"" : score( ?dataset)/score( ?dataset_lbl) score ( ?dataset) And as you can see, we compute the same number of scores"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. }  1.5.2. A Simple HoC to R Database Migration (HoC Migration) [ ] How do we migrate our dataset to another database, but on the same dataset ? We can do so using a simple data transfer tool. I will give three example examples, showing how to take a model defined in an external file, and convert the model to a data frame of a variable type from the HoC model source file. Example 3.2: Create and Move Model from an external file to C language or R for a DataFrame Migration (DataFrame Migration)  1.5.3. Using DataSource and Import Model [ ] The HoC modeling tool source file (C51 and Numpy models) can be accessed from the HoC spreadsheet source file (HoC-LTS-R). In this example we will use the C51 model file to migrate the dataset from our R and C sources. 1.5.3.1. The original VHMM, loaded in R with 'Rforge' tool is located in my_data.csv. A.     The original VHMM that is used in the original C51 simulation, named ""VHMM"" (named from the C code), used Rforge to load this file into R and write it to the file format ("	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the FABIR model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FABIR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?data[ ?dataset_lbl] ?routes. ?benchmark orkgr:HAS_DATASET ?dataset ; ?routes Input: a table structure containing the labels used for each parameter; Output: a RTE query on the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output data RTE query to the output	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the emergency type of AMBER Alert?
 output (Sparql query): "	SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } 	--------------------------- AMBER Alert Emergency | No Emergency > 6 pm EST. > 6 pm EST. No Emergency Emergency | No Emergency > 18 pm EST. > 18 pm EST. No Emergency Emergency | No Emergency > 5 am EST. > 5 am EST. No Emergency Emergency | No Emergency > 1 am EST. > 1 am EST. No Emergency Emergency | No Emergency > 0 am EST. > 0 am EST. No Emergency Emergency | No Emergency > 1 am EST. > 1 am EST. Yes Emergency No Emergency ------------------------------------- input (english text): What is the emergency type of AMBER Alert ? output (Sparql query): --------------------------- AMBER Alert Emergency | No Emergency > 6 pm EST. > 6 pm EST. No Emergency Emergency | No Emergency > 6 pm EST. No Emergency Emergency | No Emergency > 18 pm EST. > 18 pm EST. No Emergency Emergency | No Emergency > 5 am EST. > 5 am EST. No Emergency Emergency | No Emergency > 1 am EST. > 1 am EST. Yes Emergency No Emergency ------------------------------------- input (English text): What is the emergency type of AMBER Alert ? output (Sparql query): --------------------------- AMBER Alert Emergency | No Emergency > 6 pm EST. > 6 pm EST. No Emergency Emergency | No Emergency > 6 pm EST. No Emergency Emergency | No Emergency > 18 pm EST. > 18 pm EST. No Emergency Emergency | No Emergency > 5 am EST. > 5 am EST. No Emergency Emergency | No Emergency > 1 am EST. > 1 am EST. Yes Emergency No Emergency ------------------------------------- input (English text): What is the emergency type of AMBER Alert ? output (Sparql query): --------------------------- AMBER Alert Emergency | Yes Emergency > 6 pm EST. > 6 pm EST. Yes Emergency No Emergency ------------------------------------- input (English text): What is the emergency type	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" input (English text): What is the ratio of model results to those of other RPE models ? output (Sparql query): RESULT: ratio (max/mean) =.01 (or ""1"") input (English text): What is the average of the average of all models ? output (Sparql query): RESULT: (max/mean) =.12 input (English text): How many degrees of freedom can you specify for a dataset ? output (Sparql query): RESULT: Number of degrees of freedom = 10 input (English text): What is the number of features (machines) used in a model ? output (Sparql query): RESULT: Number of features = 6.00 input (English text): What are the inputs for the optimization algorithm, SVD(i, j)) ? output (Sparql query): RESULT: RESULT: [ ?feature mfq:Feature: ?] [ ?feature mrq:Feature: ?] [ ?feature ma:Feature: ?] [ ?feature mj:Feature: ?] input (English text): What are the outputs for the SGD algorithm, SVD(i, j)) ?"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?dataset ?problem_lbl. ?problem_lbl. output (Sparql query): SELECT ?dataset_lbl. Output: A common question was ""what to use in my solution ?"". For now I'll discuss my own solution in an analogy to a game. Note: The model I am using can be downloaded from here. It is a subset of his ""P3-GAL"" model. The most powerful part of this is that you can easily add your own functions based on the output, so don't be afraid to add your own implementations of features and labels... The code has a bit of additional code, but you can skip that part for now. The model has already been implemented and published, but there is still quite a bit left to do: The example here is a simple game that involves two players: player one wins when the number of points on the board is equal to the maximum number of points players can have. The player with the most points at the end of the game wins. The following code is the full code of the model: // A simplistic example that shows the power of data structure manipulation // In the following code, I will use the model to represent a board // populated by the rows in a database without // any special code beyond the main function: const auto grid = query ([[ 'id', 'Player#1','score', '1.00' ], [ 'name', 'David' ], [ 'points', [ '10.00' ]]}); auto myGame = grid. create ( 5, 20 ); auto myPoints = myGame. score. map (( a, c ) => d * ( a + c )). sum (); // As long as the game's score is not <= 0, the player is // considered to have won, and myPoints is"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE, ID, TITLE, ID* FROM ?tables WHERE id = ?ID OR ?id = ?title To extract the titles and IDs of research papers that provide benchmarks for the Atari 2600 Defender dataset, see Extract titles and IDs of research papers. The following sample queries may be helpful in performing this query on the dataset. data, title_list, id_list, paper_list, score_list, id_rdf1, score_rdf1, score_rdf2, id_rdf2, score_rdf3, score_rdf3, id_rdf4, score_rdf4, score_rdf5, score_rdf5, id_rdf6, score_rdf6, score_rdf7, score_rdf7, id_rdf8, score_rdf8, score_rdf9, score_rdf9, score_rdf10, score_rdf10, id_rdf11, score_rdf11, score_rdf12, score_rdf12, score_rdf13, score_rdf13, score_rdf14, score_rdf14, id_rdf16, score_rdf16, score_rdf17, score_rdf17, score_rdf18, score_rdf18, score_rdf19, score_rdf19, id_rdf19, score_rdf20, score_rdf20, score_rdf21, score_rdf21, score_rdf22, score_rdf22, score_rdf23, score_rdf23, score_rdf24, score_rdf24, score_rdf25, score_rdf25, id	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer + Coverage + EntailmentGen + QuestionGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?value AS ?value; rdfs:label ?value. optionally : OPTIONAL ?value, ?value rdfs:label ?value. optionally : OPTIONAL ?target_rdf_lbl, ?target_met:Rdf; OPTIONED ?target_rdf_met rdfs:label ?target_met_lbl.	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT 'Switch Transformer'.*, 'RTE'.*, 'HASH'.*, 'DATA'.*, 'SQUARED'.*, 'LOGIC'.*, 'PERCENT'.*, 'EVALUATE'.*, 'INVARIOUS'.*, ?tbl ?.*, ?rdfs:log ?* FROM 'Switch Transformer, RTE' AS a LEFT JOIN 'RTE' AS b ON a.input = b.input GROUP BY a.input ORDER BY b.rank DESC LIMIT 20 Note: This task provides an opportunity to test our model at other scale and type configurations that you may find interesting, such as an application-scale model. Let's now switch to a dataset containing the same number of articles as with the RTE analysis. The Switch Transformer model outputs a table containing the following columns: input (English text): Can you list the metrics used to evaluate models on the RTE dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code ? output (Sparql query): SELECT 'Switch Transformer'"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" For this post, I've included a sample of the test data and the queries I wrote to test my models on the sample data. I haven't gone through the full query, so I won't go over the details of each. However, I will list out the results and provide some details for the readers to think about, including: 1. My results Here is the output from the SQL query for this sample table that came from an SqpContext with my model, and the query that was run for the full output. The two columns I marked are ""problem"": (columns) and ""paper"" : Rows Rows = 7 Problem=Lets test a model for the common problem of the Semantic Web and the paper print the papers. Paper= Question 1: The paper with the most results. This one has more problems. Problem=Lets find a model, and run it on the Common Problem model. (This one has about 6 problems. Problem=Lets run the model on the Common Problem model once. Problem=Lets evaluate the model one time at a time). 2. The query that was run for the full output Sql Error: You are not authorized to use functions from this table in a query. It's pretty difficult to read the text of the query, so let's do a textual re-run in SQL Express. First, I define a variable called id. I've used the SQL Express method to build in the required variable, and then I tell the sql query to ""make a note of it"". Here are the resulting two-character strings for the query text, which I ran in an SQL Express session: #print id '<print id=""{0}{1}""> I won't go into the details of how the query works, and I won't show you what I"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ( ?benchmark_p <= ?dataset ?dataset_p) AND ( ?cron { ?dataset ?dataset_cron orkgc:Quadrant(dataset) ?dataset_cron } OR ( ?dataset ?dataset_cron) { ?dataset_cron ?dataset_cron } OR ( ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. OR( ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ) OR( ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ) OR( ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ) OR( ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. ) OR{ ?dataset_cron. ?dataset_cron. ?dataset_cron. ?dataset_cron. } ?CORE_CARD ? Output: CASE WHEN ((ERROR_NUMBER OR ERROR_RULE < 1 AND ?sys_max_number < 1) OR ?max_score	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?cid = ?cid FROM ?code WHERE { ?cid = ?code_code; a2c_lbl:Class = { ?code_code = c, ?category = p ?cid } } If you want to run the benchmarks yourself, please download these scripts as the scripts for these benchmarks in the Github repository http://github.com/filippov/A3LSC. Note that in Linux the benchmarks can be run interactively from the terminal with the tools run-a3l.sh. There may have been differences between what is available in the published research and what is available on the open download page Now let's look at the differences for different network architectures and architectures. The results are given in the following table: Differences between VPN vs. A3C-CTS with different architectures/architectures Network A3C VPN A3C-CTS Comparison Time (s) A3C VPN A3C-CTS Comparison Difference 1 8192 14.0 26.8 19.7 21.4 2 6144 14.7 26.4 17.9 18.1 3 16384 14.6 26.8 15.6 17.2 7 23200 14.7 26.6 14.4 17.4 12 40960 14.9 26.9 14.4 17.4 13 48448 15.2 26.8 14.7 17.5 15 64256 14.9 26.7 14.4 17.3 21 12000 8.5 25.3 10.3 15.2 24 10048 8.8 25.5 9.6 15.2 25 12536 8.6 25.9 9.5 15.3 25 15080 7.9 25.4 9.5 15 2200 25.0 24.9 12.0 5.8	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 The only thing I can see is that this is no longer public anymore. The code I made some changes to the precompiled version and added a bunch of options related to benchmarking like : --configurable_host: Sets the host for the database. By default, it will be port 6379. default: '10.0.0.2' --configurable_port: Sets the port for the database. By default, it will be 2028. Default: '2028' --configurable_user: The username to use when making connections to the database. By default, it will be 'admin'. Default: 'root' --compile_dir: Creates the files used by the benchmark framework. Default:./ orpkg_benchmark --compile_file: For benchmarking, creates a './' folder. Default: /home/pi/Hutter_Pitney/Benchmark/benchmark.py --compile_dir_prefix: Creates a prefix to search, in case the benchmarks are made in several directories.  (I like to run the benchmarks in parallel as there are a lot of concurrent connections to the Hutter model being made) Now that the benchmark framework is configured I can run the benchmarks. I use pybench to run the benchmarks. import random db.update() db.destroy() try: while True: benchmark.py benchmark.py: BenchmarkBenchmarkTests.run_all(1000, 500, 5000, 5000, 5000, 10000) except Exception as e: break try: benchmark.py benchmark.py: BenchmarkBenchmarkTests.run_all(1500, 3000, 7000) except Exception as e: break	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?F1 ?F2 ?F3 ?F4 ?F5 ?F6 FROM ?RDBMS ?RDBMS_NAME WHERE ?RDBMS = ?RDBMS and ?F1 = ""text"" ?F2 = ""quot"" ?F3 = ""multigroup"" orkgc:Researcher ?F4 = ""Rf"" ?F5 = ""multigroup"" ?F6 = ""text"" orkgp:Person ?F7 = ?resort ?person orkgp:Resort ?res.resort SELECT ?R2 DESC WHERE ?R2 = ?RES. Output : SELECT ?R2 DESC WHERE ?R2 = ?RES.   No problem! If you check it yourself, you'll see that the SPSS SQL command is executed using the SELECT argument set out by the program. But a better way of selecting the row is to use the WITH clause ""text"", the ""text"" parameter is the output query. This is the second part of the program: SELECT ?F2 ?F3 ?F4 ?F5 ?F6 FROM ?RDBMS ?RDBMS_NAME where ?RDBMS = ?RDBMS and ?F2 = ""text"" ?F3 = ""quot"" ?F4 = ""multigroup"" orkgc:Researcher ?F5 = ""Rf"" ?F6 = ?resort ?person orkgp:Resort ?res.resort select ?RESort ?res from ?resort And that's all, I hope you enjoyed the tutorial. Well, in the end, the goal: to create an article query tool using the RDB"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	-------------------------- NAME TESTED DESC 1. RTE-DATASET-COUNT 1.946 2. RTE-DATASET-LBL-1.946 3. RTE-DATASET-LBL-3.946 4. RTE-DATASET-PV 1.942 5. VALUE 1.903 6. MAX 1.912 7. SUM 1.913 8. SUBSTR_VAL 1.918 9. RESULT 1.920 10. PARAMETER 0.960 Scikit-learn also has built-in support for RTE datasets. The default configuration includes a model for each of the different RTEs: RTE-1: RTE-DATASET-LBL-1.945 - rdfs:model(RTE-DATASET-LBL-1.945) - lba_data RTE-DATASET-RTE: RTE-DATASET-PV-0.945 - rdfs:model(RTE-DATASET-PV-0.945) - lba_data_2b RTE-DATASET-VTAB: RTE-RTE-DATASET-COUNT-0.945 - rdfs:model(RTE-DATASET-COUNT-0.945) - lba_data RTE-RTE-VTAB-0.945: RTE-RTE-DATASET-STANDARD-0.945 - rdfs:model(RTE-RTE-DATASET-STANDARD-0.945) - lba	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?
 output (Sparql query): "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Artificial Intelligence"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	"SELECT DISTINCT ?code WHERE { ?model a ORkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ( ?model_lbl = ?model. ?model_lbl = ?model_lbl_id OR kgc:model_id OR rdfs:model_id) ?compare OR model_lbl_id ? OR a OR kgc:model_id OR orkgc:model_id OR cont OR a OR kgc:model_id OR orkgc:model_id OR b OR kgc:model_id OR cont OR a OR kgc:model_id OR orkgc:model_id OR b OR kgc:model_id OR cont OR b OR kgc:model_id OR cont OR a OR kgc:model_id OR orkgc:model_id OR b OR kgc:model_id OR cont OR a OR kgc:model_id OR cont OR a OR kgc:model_id OR cont OR a OR a OR kgc:model_id OR cont OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR a OR"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Output - Output - Metrics	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 # # A tibble: 6 x 2 x 0 # Model score metric_lbl value_lbl metric_lbl 2 test_mrt_2013.txt 1 3.1 0.7 0.04 2 test_mrt_2014.txt 1 5.0 0.2 0.02 3 test_mrt_2014_01.txt 1 4.3 0.3 0.05 4 test_mrt_2014_02.txt 1 3.2 0.4 0.04 5 test_mrt_2015.txt 1 5.1 0.5 0.04 6 test_mrt_2015_01.txt 1 5.6 0.6 0.02 7 test_mrt_2015_02.txt 1 4.4 0.8 0.01 8 test_mrt_2015_03.txt 1 3.3 0.1 0.03 9 test_mrt_2016.txt 1 3.5 0.5 0.03 10 test_mrt_2016_01.txt 1 3.2 0.4 0.03 11 test_mrt_2016_2.txt 1 3.3 0.1 0.04 12 test_mrt_2016_3.txt 1 4.1 0.2 0.03 13 test_mrt_2016_4.txt 1 3.4 0.1 0.05 14 test_mrt_2016_5.txt 1 4.0 0.2 0.03 15 test_mrt_2016_6.txt 1 3.6 0.2 0.07 16 test_mrt_2017.txt 1 4.6 0.2 0.24 17 test_MRT_2017-01.txt 1 3.1 0.2 0.08 18 test_MRT_2017-01_	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The output of benchmark file, which is an index of articles where the model or attention algorithm has the highest F1 score, is also available on SDSS. The data for the benchmark is provided by a list of articles available on this site. To run a single-threaded model-based experiment: Clone and run a single-threaded BICSTM, using the same SPSS-based model name. You do not need to run multiple iterations so that the output and predictions are uniformly distributed. Start a second experiment, running the same model name. This time, run a second BICSTM with a different size BICSTM_size. In either case, try to get 50 trials out of each. Run multiple BICSTMs at the same time and see how well each improves on the first one. If the model outperformed a similar one, it will be included in the benchmarking dataset, as illustrated below. The output of benchmark file, which is an index of articles where the model or attention algorithm has the highest F1 score, is also available on SDSS. The data for the benchmark is provided by a list of articles available on this site. The outputs of the BICSTM experiments, along with the scores, can now be shared with colleagues. Or you can distribute them as PDF files. Alternatively, you can print the results, of course, or upload them to a web server, for people to use in a lab. This project was developed over the course of about two years for use by an undergraduate computer science student. The algorithm is quite simple. Just feed it an input text file of articles, and see what happens. If it outperforms the same model on all trials, that means the model works just as well as it should. If it's less successful than predicted, there is something going	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------------------- RTE_GALLERY : - 4.8 s STS_GALLERY : 11 s FTRUNCATE : 10 s BINS : 1 fstr1 : 23.3 f1 : 1075 rdfs:id rdfs:index rdfs:datastore rdfs:preamble rdfs:name rdfs:key rdfs:value rdfs:index rdfs:categorical rdfs:time rdfs:source rdfs:subscriber rdfs:target rdfs:id rdfs:group rdfs:year rdfs:score rdfs:metric rdfs:lbl rdfs:class rdfs:classifier rdfs:object In the above query, we are comparing the best benchmark to our database. The STS dataset consists of a mixture of three datasets: a test dataset, a test indexing dataset and a test subset. The test dataset of these three datasets consists of three tests: all row, 50% row and 50% column. The column of these datasets is the same for all three datasets, but the test value differs for each dataset. The benchmark score of the test dataset is the overall score of these three datasets, i.e. the largest result of all three. The benchmark score of any of these three datasets is based on the average of the three scores for that dataset. The STS dataset contains 50 rows and five columns for the test dataset, which corresponds to the average of 150 rows and five columns for the three most popular queries of the test dataset. Using these metrics for the benchmark result, we can now construct a table of the highest benchmark of each dataset on each table. select benchmark, benchmark_lbl, rdfs:label + ','	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Shake-Shake (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?dataset SAM.LBL as ?dataset, ?model SAM. LBL, ?labeling SAM. label_lbl, SAM. benchmark; from ""HICPI: Statistical Analyses of Complex Natural Systems"" LIMIT 1,5. INNER JOIN '' AS ?sam_model_lbl_1 AS ?labels_1 AS ?batch AS ?benchmark ON ?sam_model_lbl_1. ?dataset SAM.LABELLBL on ?dataset_1. SAM. LBL in ('Sampling Processes;Sampling;Keter;Sampling'::text) ORDER BY ?benchmark DESC LIMIT 1,5; orkgp:HAS_DATASET SAM.EVALUATION SAM.BENCHMARK. orkgp:HAS_MODEL SAM.BOARD. I hope these are enough to demonstrate one or two of the possibilities. If you have any more, please let me know in the comments."	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Joint Entity and Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT ?dataset ?dataset_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset rdfs:label ?. ?dataset rdfs:label ?. ?dataset ?dataset_lbl. ORDER BY DESC( ?value ) LIMIT 1}. { ?dataset RDFS:label ?dataset_lbl. } ORDER BY DESC( ?value ) LIMIT 1}. output (Sparql query): What are the most commonly used model/task pairs/assignments of the Joint Entity and Relation Extraction research field to date (the question is, the type of use, i.e., a subset, subset-based assignment, a subset/partition/split-based assignment, a partition/split/group assignment, or a partition/split/group ?) ? input (English text): Which one should we call the model X (X is a data source to extract model from) ? output (Sparql query): SELECT ?model X { ?metric a:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""X"") }. ?metric rdfs:label ?metric_lbl. ORDER BY DESC( ?value ) LIMIT 1}. input (English text): What is the value of X using the above model ? output (Sparql query): ? model { ?statistic a:Metric; rdfs:label ?metric_lbl. DISCARD ?metric_lbl if ?metric_lbl==""X"" ?metric rdf"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?cont orkgq:CASE ?model_lbl. FILTER (str( ?model_lbl) = ""NAT"") ?benchmark (NAT case orkgp:HAS_DATASET ?dataset. ?) ?benchmark. input (English text): Which method do you most often use at your organisation for training and assessing new features in your models ? output (Sparql query): SELECT ?cont orkgp:HAS_DATASET ORKO ?dataset. ?cont orkgp:HAS_IMAGE ?model; orkgp:HAS_SOURCE_CODE ORKO. ; input (English text): What methods do you most commonly use at your organisation for deploying and maintaining your models ? output (Sparql query): SELECT ?cont orkgq:CASE ?model_lbl. AND ?cont orkgp:HAS_DATASET ORKO ?dataset. ) input (English text): What is your experience with the multi-rfc2195 project ? output (Sparql query): { ?model:categorical ?dfs:{ ?name ?dataset. ?type ?dataset. ?age ?dataset. ?gender ?dataset. } } input (English text): I would like to ask a question about the following: output (Sparql query): Which models do you have built ? input (English text): I do not have a model to answer your question. output (Sparql query): SELECT ?cont ? ORKO ?dataset. ?cont ? orkgq:CASE ?model_lbl. AND ?cont ? orkgq:CASE orkgp:"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?new ?new_lbl FROM { ?dataset. ?matrics. ?benchmark ?benchmark_lbl. ?benchmark_lbl_ldr. ?matrics. ?cluster. ?benchmark ?cluster_lbl. ?cluster. ?cluster_ldr. } ?new ?new_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?new_. } output (Sof database query): SELECT DISTINCT ?changes_lbl ?_changes_lbl. FROM { ?dataset. ?reasons. ?reasons. ?reasons_ldr. ?reasons_ldr_ldr. ?data. ?cluster. ?cluster_ldr. } ?changes_lbl ?_changes_lbl. [ input ] [ input ] [ input ] [ input ] input (English text): How often are data updates available ? output (Sparql query): [ output ] [ input ] [ input ] [ input ] input (English text): How long is it now that I am preparing for the GAD Data Summit ? output (Sparql query):  SQL> select count(*) from gandata; 1 2 3 4 5 6 7 8 9 SQL> select count (*) from gandata; 1 2 3 4 5 6 7 8 9 [ output ] [ input ] [ input ] [ input ] [ input ] input (English text): ...[ output ]...(total 2	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	------------------------- Here we can see that there is no difference to the one in the previous example. However, this test indicates that the model of the HoC benchmark dataset is superior in terms of Score metric. We can also compare the output of the HoC Model with that of the model of Atari data. The HoC model is slightly faster in terms of Score. The graph shown before show the performance of the model of Atari versus the input of Atari. However, the output of the models of the Atari database only display the output for the first few epochs, as we can see in the screenshot below: The most of the performance of the Atari model is displayed in the following graph (the blue bars represent total epoch): The HoC model performs best during initial training epochs, but performs worse during subsequent epochs: But, according to the SPSS report, the Score metric of the HoC model is a poor indicator of the performance of the model. The SPSS report points out that, the first few epochs (i.e. at least one trainee per epoch) of training are more affected by the model compared to second few epochs (i.e. during the test sets run each epoch). Therefore, we can assume that the score will be better on later epochs. For example, in the second and third test sets, there is less impact of the test set. However, we can confirm the results of previous example in our test dataset. Here we can see that the model of Atari is superior over the one of HoC in both the Atari data and in training epochs. It seems that, the model of the HoC model performs better in initial training epochs, due to a better representation of the data that is stored, which can help improve the model's performance for further epochs. But this doesn't	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" { ?problem ?problem_lbl ; ?problem ""Web"". ?paper ?paper_id. ?title ?paper_title orkgp:HAS_BENCHMARK ?benchmark ; ?problem_lbl ?problem_lbl. } output (Sparql query): { ?paper ?paper_id. ?title ?paper_title_rdfs:label ?paper_title. DATE. || ?paper ""Jan 1, 2014"" orkgp:P30 ?paper_id. || ?paper ""Web"" || ?paper_title || ?paper_title_rdfs:label ?paper_title. DATE || ?paper_title_rdfs:label, ?paper ""Mar 31, 2014"" orkgp:P31 OR ?paper_title_rdfs:label ? orkgp:P31 ?paper_lbl || ?paper_lbl orkgp:P31 ?paper_lbl_rdfs:label ? orkgp:P31 } Sql> select ?title from ?paper ?paper_id where ?rf is not null ?; +-------|-----------------------------------------------+ || ?paper || || ?paper_id || || ?paper_title || || ?paper_title_rdfs_label || || ?paper_title_rdfs || || ?paper_lbl || || ?paper_lbl || || ?paper_lbl_rdfs || || ?paper_lbl_rdfs_labels || || ?paper_lbl_lbn || || ?paper_lbl_rbn || orkgp:P30 +-------|-----------------------------------------------+ 0 | ?problem ?problem_lbl || || ?paper_id || 1 | ?paper ?book_"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the SearchQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?code WHERE ""bachelor"" = ?code AND ""degree"" = ?code; input (English text): What is the algorithm used to determine which search filters apply to matching words ? output (Sparql query): SELECT DISTINCT ?filter_name ?filter_lbl, ?idx_s, ?idx_t FROM filters GROUP BY idx_s, idx_t ORDER BY ?filter_name LIMIT 1; input (English text): How many criteria do we need in order to determine what sort of words are being asked for ? output (Sparql query): input (English text): What language(s) do you prefer ? output (Sparql query): For more information on the model and the SearchQA database as well as to see results on different queries, I highly recommend checking out the blog posts that I wrote: search model data search query with the following query to compare the results using the model and SearchQA database: input (English text): I have an idea of the number of topics to be included and are you willing to share the list and you have been willing to try them. output (Sparql query): SELECT DISTINCT ?topic, ?""count"","	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the OntoNotes dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT S1 AS ------------ RTE:SELECT S2 AS ------------ RTE:SELECT SUM(S1)/S2 AS ------------ RTE:SELECT Q1 AS ------------ RTE:SELECT Q2 AS ------------ RTE:SELECT Q3 AS ------------ If you haven't implemented an extension for RTE yet, you can still use the same code at: RTE:SELECT S2 AS ------------------------ (sum(S1)-sum(S2)-q1/q2 as ?sum_sum_log RTE:SELECT SUM(S1)/S2 AS ------------------------ (sum(S1)-sum(S2)-q1/q2) as ?sum_sum_log Other RTE Extensions include: RTE:DEFER(RTE:SELECT S1 AS ?s,  RTE:SELECT S2 AS ?s,  RTE:SELECT SUM(S1)/S2 AS ?s) as -------------   return ""s"" as ?s   return ""s1"" as ?"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 select * from   ?max_score  join Defender  ( ?max_score  ,  orkgp:HAS_DATASET) on  %dataset orkgp:HAS_EVALUATION In order to evaluate, we need to find out its top ranking between 30 and 3099. #2.5 - RTE-RTS Benchmark Benchmarking: The RTE Database 1 : Top 30 (30 - max score/ 3099) -  rte_turtles.csv 2 : Top 10 (10 - max score/ 10,001) 3 : Top 1 (1 - max score/1,10,000) 4 : Top 20 (20 - max score/20,0001) 5 : Top 50 (50- max score/50,0001) 6 : Top 10 (10-max score/10,000,000) 7 : Top 20 (20 - max score/20,000,000) 8 : Top 100 (100- max score/100,000,000) 9 : Top 500 (500- max score/500,000,000) 10 : Top One (One-max score/500,000,000) 11 : Top 5 (5-max score/5,000,000) 12 : Top 30 (30- max score/30,000,001) 13 : Top 50 (50- Max score/50,000,000) 14 : Top 10 (10-max score/10,000,000) 15 : Top 20 (20-max score/20,000,000) 16 : Top 100 (100-max score/100,000,000) 17 : Top 500 (500-max score/500,000	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?criterion ?criterion_lbl. ORDER BY DESC( ?metric ) LIMIT 1 || ?criterion rdfs:labels ?criterion_lbl. || ?criterion k rdfs:metric rdfs:value. || ?criterion W k rdfs:value rdfs:metrics. || ?criterion n rdfs:quantile || ?metric Q rdfs:integer rdfs:float || ?metric R rdfs:rechangable rdfs:complex || ?criterion R k rdfs:kappa rdfs:rechangable rdfs:complex || ?metric S rdfs:complex rdfs:complex || ?metric T rdfs:complex rdfs:complex || ?metric Y rdfs:complex rdfs:complex || ?metric J rdfs:complex rdfs:complex || ?metric D rdfs:complex rdfs:complex || ?criterion D k rdfs:delta rdfs:rechangable rdfs:complex || ?criterion D d rdfs:delta rdfs:rechangable rdfs:complex || ?metric C rdfs:delta rdfs:rechangable rdfs:complex || ?class c rdfs:delta rdfs:rechangable rdfs:complex||   F 1 – Model The F1 metric is based on the top 300 fastest cars over the last 12 months, with a mean score of 5.85. The benchmark dataset is created to mimic real-world F1 testing, measuring F1 performance in the real world. This dataset does not include any races	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 --------------------- --------------------- -------------------------- 1.0.0.0 HoC_PCIe.hoc.benchmark, HoC_PVCe.hoc.benchmark, HoC_PCIe_PVCite.hoc.benchmark, HoC_PVCe_PCIMed.hoc.benchmark, HoC_PVCe_PVS.hoc.benchmark, HoC_DVACe.hoc.benchmark, HoC_DVACe_PVSite.hoc.benchmark, HoC_R3DP.hoc.benchmark, HoC_R3P_R3DM.hoc.benchmark, HoC_R5DP.hoc.benchmark, HoC_R5P_R5DM.hoc.benchmark, HoC_R5DP_R5RP.hoc.benchmark { :dataset DatasetID:0, :model ModelID:1, :benchmark B2DP:HAC0T1:R5P_R5DM_PCIMed_PCIMed, :benchmark B4DP:HAC0T1:R5P_R5DM_PCIMed, :benchmark B4DP_PCImed:R5P_PCIMed_PCIMed, :benchmark B4DP_PCPV:R5P_L2DP_PCPV_PCIMed, :benchmark B4DP_PCPv:R5P_R3DP_PCPv_PCIMed, :benchmark B4DP_R5DP_PCPv:R5P_R5DM_PCPv_PCIMed	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT data_lbl. ?label ?quantity :Rows_per_samples. ?order, ?index :Dbl_columns. ?mean, ?std_dev. ?percentile. ?metric_name ?metric_set > ?metric_lbl. ?metric_name - ?metric_set. SET input (English text): Provide a list of benchmarks to evaluate models on the enwik8 benchmark dataset ? output (Sparql query): SELECT data_lbl. ?label ?quantity :Rows_per_samples. ?order, ?index :Dbl_columns. ?mean, ?std_dev. ?percentile. ?metric_name ?metric_set > ?metric_lbl. ?metric_name - ?metric_set. SET input (English text): Provide a list of benchmarks to evaluate models on the enwik8 benchmark dataset ? input (English text): Provide a list of benchmarks used for testing the enwik8 benchmark dataset ? output (Sparql query): SELECT data_lbl. ?label ?quantity :Rows_per_samples. ?order, ?index :Dbl_columns. ?mean, ?std_dev. ?percentile. ?metric_name ?metric_set > ?metric_lbl. ?metric_name - ?metric_set. SELECT data_lbl. ?label ?quantity :Rows_per_samples. ?order, ?index :Dbl_columns. ?mean, ?std_dev. ?percentile. ?metric_name ?metric_set > ?metric_lbl. ?metric_name - ?metric_set. SET inputs (English text):	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?problem ?problem_lbl; input (English text): What are some of the major problems found with the TempEval-3 dataset ? output (Sparql query): SELECT ?problem ?problem_lbl; Question 1 and 2: Are semantic web approaches to solving problems in areas where language barriers are still an obstacle to achieving the goals of web-scale deployment ? Why are they valuable, and will they become more important in the future ? Answer: Because the models and algorithms needed can be applied to problems whose applicability to a broader range of problems is already known, and these can be addressed more easily and more accurately by an existing database. Question 1 describes the challenges in Semantic Web, which can be addressed by existing databases. A typical application could involve applying semantic web principles to different types of problem domains: (i) language identification; (ii) data extraction problems; (iii) language integration problems. There are many examples of these types of problems in the literature; for those who are interested, these should be considered: (1) Semantic Web for Human Cognition. For example, see: Eichner, K., Varela, V., Veltman, J., and Alchian, N. (1995). The search engine semantic model and human cognition: Problems and theoretical approaches. In Computer Vision and Pattern Recognition with Computers, pp. 193-214. Berlin : Springer; (2) Semantic Web for Multilingualism. See example: Veltman, J. (1999). Semantic Web: A new search engine for multilingual language learning. In Computer Vision, Communications and Computation, Vol. 35, pp. 1541-1545. doi:10.1016/S0197-4540(98)00081-9. Many online resources are available on how to improve how our web	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the most common location in the studies?
 output (Sparql query): "	SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1	" <document> <title>Lecture</title> <time>2004-03-21 15:46:00</time> <address>The Institute for Information about the Internet, University of Copenhagen, Denmark</address> <group>Lecture</group> <time>2004-01-24 15:30:00</time> <contents> I find that the most important criterion in selecting a paper to have, as the title, a research field, the position to which it belongs, and the area in which it relates to, is the author's title. I propose a proposal, so it is important for the author. </document> </p> input (English text): Your name / email address: How does your paper appear in the results: A list of published papers from the area of Semantic Web: This text has the same spelling and grammar as the document itself. You can copy and paste it using the Ctrl-C and Ctrl-V keys. We highly recommend that students paste this text, since it is often more easy to understand than the text you see. We can tell if we are reading a document or an actual paper by the color of the text itself, and the color of red, blue, or green indicates whether we're reading a printed piece or an online document. For more details about the formatting of text, see our help page. Input (English text): The following characters are allowed in the text: ? The characters above and below are escaped. The HTML <div> element does not allow HTML entities, such as quotes ( \ ) and HTML <del> elements. Instead they are prefixed with a's' character to distinguish them from HTML tags like <img> and <a>. \r The characters above the next ""R"" are escaped or replaced with a carriage return ( \r ). This is a special"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ID, COUNT(1), SUM(1/COUNT(1, 1) OVER (ORDER BY ID) LIMIT 1, :dataset a.name_lbl, :eval. ?model :dataset_lbl { ?dataset a orkgc :dataset_lbl; ?model rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BCN+ELMo"" ) ?benchmark orkgp:HAS_DATASET ?dataset. OPTIONAL { ?model :dataset rdfs:label rdfs:label_lbl. } } The input should be a query containing the queries that you want generated for each model, and the output should have a row labeled with the list of the models that you have evaluated. Here is an example: To see a visualization of your program in action you'll need to generate the following query: CREATE TABLE #1 (model_hans1, model_hans2, model_hans3, dataset_hans1, dataset_hans2, dataset_hans3) VALUES (1, 'Hans1'), ( 2, 'Hans2'), ( 3, 'Hans3' ) EXEC sp_data_generate( 'cds_model', MODEL_HANSSON1, NULL, MODEL_HANSSON2, NULL, MODEL_HANSSON3 ) ; SELECT * FROM #1; Output: { ?dataset : ""Hans1"", ?model : { ?dataset : ""dataset_hans1"", ?model_lbl : { ?model : ""dataset_"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model_lbl. ? ?label. ? { ?model a orkgc:Model ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK orkgp:HAS_MODEL ORkgp:HAS_SOURCE_CODE ?code as ?model_lbl. ? ?input ?input_lbl. ? } Output from benchmarking on the TABBA benchmark dataset input (English text): Test if the TABBA model performed best in terms of Pre-Training Dataset metric on this benchmark dataset ? output (Sparql query): SELECT DISTINCT ?dataset. ? ?label. ? { ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK orkgp:HAS_MODEL ORkgp:HAS_SOURCE_CODE ?code as ?dataset. ? ?input ?input_lbl. ? } Inputs from the TABBA benchmarks to the FIM benchmark for validation inputs (English text): What is the model's performance relative to the others ? output (Sparql query): SELECT DISTINCT ?model_lbl. ?; ?db ?model_lbl. ? { ?model a:HAS_DATASET ? ORkgc:Model ?dataset ?cont orkgp:HAS_DATE } ?model_lbl. ? ?input ?input_lbl. ?. ?model_lbl. ? { ?model an:HAS_DATASET ? orkgc:Model ?dataset ?cont orkgp:HAS_D	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl orkg2:HAS_METRICS ?metric. } The output can be viewed in a table view. The SQL command is available in R 2.5.0 onwards. To get the previous commands, select option 4 in the RTE.db configuration file. Here is the output for measuring the number of repetitions at the first sample/note of five seconds/bar of two bar or six seconds from the first two samples : Dump2pairs=1: count(repeat_samples) = 4: #print count(repeat_samples) | #print total. repeat_samples[1][""recur""]=1: repeat_samples[2][""recur""]=0: repeat_samples[3][""recur""]=0 repeat_samples[4][""recur""]=2: count(repeat_samples.seq(0.5))=1: repeat_samples.seq(1.2) = 0: repeat_samples.seq(0.5) = 1: repeat[0][""recur""]=0: repeat[1][""recur""]=1 count(repeat[0][""recur""]) = 0 count(repeat[1][""recur""]) = 0 repeat[2][""recur""]=1: repeat[0][""recur""][0].sort(); repeat[0][""recur""][1].sort(); count(repeat[1][""recur""]) = 0 count(repeat[1][""recur""][2].sort()); repeat[1][""recur""][2].sort(); Count(repeat_samples.seq(0.5)) = 1 count(repeat_samples.seq("	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Indicate the model that performs best in terms of BLEU score metric on the IWSLT2014 Spanish-English benchmark dataset ? output (Sparql query): input (English text): INDEX, INDEX IN OUTPUT ?dataset. OR kgc:VORP_MODEL or model. TO INPUT ?model INPUT ?dataset. OR kgc:VORP_MODEL ? IN OUTPUT ? output (Sparql query): input (English text): Indicate the model that performs best in terms of BLEU score metric on the IWSLT2014 Spanish-English benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performs best in terms of BLEU score metric on the IWSLT2014 Spanish-English benchmark dataset ? output (Sparql query): input (English text ): Indicate the model that performs best in terms of BLEU score metric on the IWSLT2014 Spanish-English benchmark dataset ? output (Sparql query): output (HTML code): HTML code for a HoC model for the IWSLT2014 benchmark dataset (with one record per line). document (C) HoC data for HoC (XLS format) input (C) XLS file containing the dataset dataset. IWSLT2014_ENG.xlsx output (C) XLS file containing the model for the IWSLT2014 model in the IWSLT2014_ENG model in the English language dataset. input (cnn.xml) template and configuration XML dataset for IWSLT2014 dataset (with one record per line). xmlwriter.jar (Java): File for writing to the json format	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT 'F1 model'; output (Sparql query): SELECT model_name, model_lbl_name FROM models; select '^model_name^|' F1 model '; output (Sparql query): select model_name, model_lbl_name FROM models; This question was asked earlier, I want to add it here again for comparison: Question: Which model has achieved the highest F1 metric on the BXE-cancer benchmark dataset ? Informative output: SELECT '*'; Output: ""F1 model"" I've seen a variety of answers to this question over the years.  I know the most common is to say a model that has achieved ""the worst score possible in terms of F1 metric"" is likely a bad model in theory, but I can't cite any studies that compare it in practice.  I do note that in our first F1 metric study of the BC 5CDR-disease dataset  the data was limited, and probably over-counted the worst metric score, but for the purposes of the analysis the score was the least important metric to assess.  (Not to mention the fact that all models tend to behave similarly on many metrics.  If I had to pick one to use for my next paper, I'd choose a model that outperformed the worst score on about 90% of the metrics used.) Another commonly mentioned answer to this question is to just say that the best model will probably show low scores for more samples, but again, there are really only a few models that have been tested that are even close, and none of them have really scored too low for an objective measure. "	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?sales_custid; FROM { ?dataset orkgc:Dataset } JOIN { { select ?sales_custid || ?value as custid } } ORDER BY rdf:RTE LIMIT 1,5 input (English text): Which models show the highest score when benchmarking on the Atari 2600 Dataset ? output (Sparql query): ------------------------------------- SELECT ROUND(ROUND(ROUND(score * 1e5) + 1e5)) AS score; AS score_avg FROM { ?dataset ORkgc:Dataset } rdf GROUP BY ?productID AND ?mnt_id ORDER BY score Here we see a significant improvement in the score but there is a problem. Why ? First, we were unable to find a good representative of our target product in this dataset: The average of scores for RTE is only 1.26. That's the lowest for any dataset. Let's check if this is an artifact from the dataset as represented in the table. output (Sparql query): ------------ SELECT p as p_avg FROM { ?dataset { ""OrkCust"" { ?productID ?mnt_id } } GROUP BY ?productID || ?mnt_id ORDER BY p ASC LIMIT 1,5 ----------------------|-----------|--------|---------------------------------------------------------------------- || || || | | || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 { ?paper ?paper_id; ?product_id ?product_id; ?labels ?labels; ?quantum_id ?quantum_id; ?type ?type; } or kgp:HRS_RADIO, or kgp:HRS_STATS, or kgp:PAGES, or kgp:PASTASK, or kgp:PAGE, or kgp:PIDUS, or kgp:PAGE_MATERIAL, or kgp:PASTASK_TYPE, or kgp:PAGE_TYPE_MATERIAL or kgp:PAGE_TYPE_TYPE_MATERIAL or kgp:PAGE_TYPE_TYPE_MATERIAL, or kgp:PAGE_TYPE_TYPE_TYPE_MATERIAL or kgp:PAGE_TYPE_TYPE_TYPE_MATERIAL or kgp:PAGE_TYPE_TYPE_TYPE_NEXT_TEST or kgp:FANBOATOROG ; output (Sparql query): { ?id ?id; ?retrievers ?retrievers or kgp:PAGE, ?page ?page, ?page_type ?page_type, ?page_status ?page_status, ?page_date ?page_date, ?page_date_in_yyyyMMdd ?page_date_in_yyyyMMdd, ?object_id ?object_id, ?label ?label, ?result ?result, ?res   ?result_res  } or kgp:PAGES_FANBOATOROG, or kgp:PAGES_FANBOATOROG_PAR, or kgp:PAGES_FANBOATOROG_HRS_RADIO, or	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?src and ?dst FROM #{ ?source.src ?source.dest }, #{ ?src.src orkgc:Ljava/lang/reflect/Method; } -------------- output (Sparql query): -------------- input (English text): Where can you find code references in papers that benchmark the COMET - Direct model ? output (Sparql query): -------------- select ?src_lbl, ?dest_lbl, ?src_lbl_lbl, ?dest_lbl_lbl_lbl from #{ ?source[src.src.src] ?source[src.src.src] }, #{ ?dest[src.src.src] ?dest[src.src.src] } -------------- output (Sparql query): -------------- select ?src_lbl, ?dest_lbl, ?src_lbl_lbl, ?dest_lbl_lbl_lbl from #{ ?source[src.src] ?source[src.src] }, #{ ?dest[src.src.src] ?dest[src.src.src] } --------------- output (Sparql query): { ?name ?text } This is one of the best, if not the BEST, implementation I've come across. The code is also really simple to use as well. (I used to write a whole program to benchmark the Direct model too but I got bored :) ). It's not too difficult to understand (well, sort of). Here's the code from the project repository: import java.xml.Serializable ; import java.awt.BorderLayout ; import java.awt.Color ; import java.awt.DataFrame ; import java.awt.Geometry ; import java.aw	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl WHERE { ?modelrdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""F1"") { SELECT ?modelrdfs ?model_lbl. ?dataset rdfs:dataset { ?dataset rdfs:dataset. ?value rdfs:value. } ?method ?method OR { ?hazmat ?rdfs:metrics ?data. ?test orkgp:HAS_TEST ?data. ?test ?rdfs:metrics. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset ? output (Sparql query): input (English text): Which model has achieved the highest F1 score on the BUCC Russian-to-English benchmark dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?modelrdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""F1"") { SELECT ?modelrdfs ?model_lbl. ?dataset rdfs:dataset { ?dataset rdfs:dataset. ?value rdfs:value. } ?method ?method OR { ?hazmat ?rdfs"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 orkgp:HAS_SOURCE_CODE ?code; ORkgp:HAS_STRING ?code_str. ORkgp:HAS_STATISTIC ?code_stat, ?codepage. orkgp:HAS_PROGRAM ?mod, ?num. orkgp:HAS_DUNC_MODEL ?model, ?benchmark ?dataset, ?coder. Scheduled benchmark for benchmark analysis: input (English text): When can I find a benchmark report for the current algorithm that uses the Oracle implementation of the VPN ? output (Sparql query): orkgp:HAS_DATASET, ?mod ?_n ?_mod. orkgp:HAS_DATASET_PROGRAM ?mod_n ?_pro. The benchmark report data is also available here. I have two additional questions I've kept on my shelf for awhile but haven't been able to complete yet. 1. What are the maximum performance figures that an individual machine could generate per second ? 2	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Inputs and outputs have been saved as.sql files. Next, we are going to add two new variables: one is for the initial test set, the second is the final benchmark set. We are going to do it through a second batch of code. I used only a few lines of code in the first batch—it's a basic loop. We'll be adding a little bit of code in the second batch. # --------------------------------- 1. Set your database configuration 2. Test the data for some time, checking for outliers or ""lumps"" in the top 10 3. Clean up, then put the results into a table for other calculations 4. Iterate and collect more records and output the data into a SQL table (e.g. Rows, Columns, etc.) # --------------------------------- # --------------------------------- Define the input data and schema 1. Input data Inputs: F1_Score (the F1 score score on the benchmark dataset) F1_Rating (the average F1 rating out of 10 from 1 to 10) DBSet (the HASS dataset) # --------------------------------- # --------------------------------- Define the output data, which we'll output into a SQL statement 2. Output data Outputs: HoC_Calls (the number of HoC calls in both batches) HoC_Hits (the number of HoC hits in both batches) HoC_Kills (the number of HoC kills in both batches) HoC_Kills_Result (the number of HoC kills and the number of HoC kills_result in both batches, with the number of HoC kills in the final 10 being the same) HoC_Kills_Result_Result (the number of HoC kills-result in the final 10, with the number of HoC kills in the final 10 being the last 10)"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" I also want to give my thank-you to all who helped test, report, and report some more when needed. I'm happy to report that the best result with the SPSS data was the first (and a little bit luckiest). You'll notice that I've made a new ""output"" list of the SPSS datasets (the first and the last) so that the output of this example can be seen with the right mouse button. Here's also a list of useful web resources for others trying to get an IWSLT2015 database of problems. For all those curious, here is all the code needed: # This program builds a database of IWSLT2015 problems # It finds the problems, provides the metrics of their problems, # and generates a query using SPSS 15.0 (Windows) (SPSS) import sys import pandas as pd # This program uses the data in an ""input dataset"" and its metrics # to build a ""output dataset"" of IWSLT2015 problems  dataset = pd. read_csv ( ""t1-dataset.xml"" ) # This can be implemented as a single-statement python program which # computes the metrics of each problem and reports them # here: http://www.reddit.com/r/SPSS2013/comments/2j7m3t/iwwslt2015_spss_metric_calc_sql_sql_sql/ # This program creates a set of queries which are evaluated by # the program upon each run. It also generates the final output # at the end of each run. import sys import pandas as pd df_test = [ (1, 1) ] # The 'in' data and the 'out' data for this example: IWSL"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * LIMIT 10 SELECT * CASE WHEN TITLE_NAME = '' THEN 2 ROW() WHEN TITLE_LOCALE = 'IT' THEN 1 ROW() WHEN TITLE_LOCALE = 'L' THEN 4 ROW() WHEN TITLE_LOCALE = 'E' THEN 8 ROW() WHEN TITLE_LOCALE = 'X' THEN 16 ROW() ELSE 1 ROW() END output (Sparql query): For more information about different metrics, see the Metrics section of the Atari Games Documentation site. For details about how these data have been processed, see the Dataset Processing section of the documentation.	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" --------------------. -----------------. ---------------------. ---------------------. Here, as input text is a text, it is being passed in the query of ""select ?"" which is a filter statement. After all, you can use this in SQL to select text as input, even the text that is a text and being passed as input. To demonstrate, the input text will be a simple text, but I wanted to have a benchmark to demonstrate how a given metric works. To get the metrics, I will define the following metric:  metric id:  1 input (CMS): the datasource that will contain the data (i.e web-page) output (CSV): CSV text with data as rows of columns metric (SQL query) :  select id from { ?data:ROWList{  id = 0; ?rdf:label[1]; ?rsa:label[1]; ?rcb:label[1]; ?xqb orkgp:ROWList|rowlist[1]}; ?rsa, rcb, xqb, orkgp; } To use this metric, I have some variables to test. The ""id"" key value, means that I would get the id of index that the column is being stored in for. The ""type"" key will be ""query"". In this case, I will specify the query to be used in the query. The ""result"" key contains my data that I would expect to be shown on the result. Here, the variables I have for my input text is simple. First, we have a ""datasource"" and then a ""data"". There are two different ways to specify the datasource and the data. The first use the ""dat"" part, which will be a reference to some datasource that is being tested"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 cannot display the image model in view.pl because of a problem in the network stack. If you are using an alternative network such as Caffe or TensorFlow, then you may be able to add it to the current configuration simply by editing the file view.pl and adding the configuration line '... #include<caffe/caffe_model.h>', and then saving the settings as the file view.pl.ini. To run this code with OpenCV with the --network option, be sure you have followed the'OpenCV network configuration and startup guidelines for training'section in the OpenCV documentation. For example, the following configuration would be a good starting point: caffe --network /path/to/view.pl --layers 1 --num_layers 3.0000 --device /dev/input/* To make further tweaks, you can edit the network configuration in the OpenCV console with the -v flag. With this command, you can see the names of the network-specific settings along with their values within a single configurable.py file. After modifying the OpenCV configuration file, run Python scripts that include scripts for converting the Python files into C++ executable or library files. After you have modified OpenCV configuration files, these scripts can be run using the python scripts command: python scripts.py. The best method for configuring your network is to copy and modify your network configurations from the OpenCV Console from the project directory in a text editor such as Notepad (or TextWrangler). You should also make sure you have installed the python 3 interpreter installed (if you have not already done so). If you have not already done so, configure your network by editing the network.py file before you start the network training: C(1,100); #... train_model(C	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?data_p1, ?data_p2 AS c, ?data_p3, ?data_p4, ?data_p5, output (Sparql query): SELECT DISTINCT ?data_p1 AS c, ?data_p2, ?data_p3, ?data_p4, input (English text): Which model has achieved the highest Accuracy score on the Bloomberg Finance benchmark dataset ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?data_p1, ?data_p2, ?data_p3, input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?data_p1 AS c, ?data_p2, ?data_p3, ?data_p4, input (English text): Which model has achieved the highest Accuracy score on the Bank of America Merrill Lynch benchmark dataset ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?data_p1, ?data_p2, Input: Output: output (Sparql query): output (Sparql query): output (Sparql query): 1.0 (7 rows) input (English text): How long has this project been in production and how would you estimate its current use ? (This also includes: how long is the process in terms of time and cost - I'd appreciate it knowing this!) output (S	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?model ?model_lbl, ?dataset ?dataset_lbl FROM ""F1"" GROUP BY ?model ?model_lbl, ?dataset_lbl LEFT JOIN ""Kinetics-600"" on ?dataset_lbl. ""F1"" | ( ?metric ?m/d) / 1000 output (Sparql query): SELECT ?model ?model_lbl, ?value ?value FROM ""F1"" ORDER BY ""value"" DESC LIMIT 1 { ?model_lbl:model_lbl } A Note on Variables I decided to use a variable called ?dataset_lbl, which was a reference to the ""F1"" table from the ""HoC"" benchmark. For the HVS, I also used data from ""HAS_EVALUATION"" - which contains results on the HVS benchmark dataset for a number of different models. I also used a variable called "" ORKG - ORKG"" to test my model on various datasets outside of the HVS, in order to see which datasets can handle our parameters and performance that best."	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT metrics_lbl, metrics_lbl || metric_lbl output (Sparql query): SELECT metrics_lbl || metric_lbl || metric_lbl output (Sparql query): --------------------------------------------------------- 4. Datapoint Results As we said, the dataset with all 3 possible outcome measures was used for the analysis. Let's see how we analyze and present the dataset. We analyzed all the datasets in one row. In the output we will see the number of times that the output is included in the analysis, what's the estimated error we can see and the most important metrics we want to examine. We used the same statistical approach that we used to calculate the ""model performance"" of each models. It is a simple calculation: we find the ""average"" scores and average the errors together, that is: We divide the average error value by the total number of observations and use that as the mean error for each metric. All metrics are taken into account automatically by RTE. RTE provides an RTE method for scoring metrics: ""mean(<value>)"". (You can find these methods by visiting the section ""Statistics"" in the R manual). However, there are many methods based on various criteria. This method is not the result of human coding, rather the method by which RTE tries to determine a good method to score a particular metric. Some of those options are: 1. Metrics are scored in ascending order of ""distribution"" over the dataset 2. The average of the data is chosen to indicate the accuracy 3. This approach shows in the output ""metric error distribution"". We found that the average metric error score for each metric was calculated for each dataset by: 1. All the metrics were weighted by the score value 2. We counted the number of missing observations "	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. { ?cont orkgp:HAS_DATASET ?dataset } = ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. } input (English text): Provide a list of papers that have used the BASS model for benchmarking purposes ? output (Sparql query): SELECT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. { ?cont orkgp:HAS_DATASET ?dataset } = ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. } Input format: Output of a single line - see 'input' section for explanation. Input format: The output of a single line - see 'input' section for explanation. Output format: Line 1: Contains the table description (e.g., ""Classes"") and all the available features (e.g., ""class: "" (all features are tagged with class names), ""id: "" (class id), ""rank: """" (rank to 1, all scores are 1-based),...); line 2; is the list of rows of data in a table. Line 3: The 'output.' column is the list (one row) of columns specified in 'output' section of the documentation but not yet defined for all the tests of a benchmarking paper. Line 4 is a blank line. Line 5: Outputs a result row (as a S3 document), in the specified format. If any required columns are empty, this is equivalent to not filling them out; this option is ignored. This"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What is the top benchmark score and its metric on the Multiscale (long) dataset ? output (Sparql query): input (English text): What is the top benchmark score and its metric on the Multiscale (multidimensional) dataset ? output (Sparql query): input (English text): How did the number of questions in an English-speaking audience in this test compare to the number of questions in a non-English-speaking audience ? output (Sparql query): input (English text): How should the model be benchmarked ? output (ParseResponse): 1 input (English text): How should the model be benchmarked ? output (ParseResponse): 2 input (English text): How should the model be benchmarked ? output (ParseResponse): 3 input (English text): Please describe the method you used to extract data from the datasets, such as columnar tables and linear models. output (ParseResponse): A columnar table is a table that contains more than one row per attribute; an ML model uses rows to build features and columns for training and testing. We typically use only data on the table and none on the cell with no feature value. input (English text): Can you give a brief description of how you created the model ? output (ParseResponse): For the long dataset, we used a convolutional neural network (CNN) model, which can learn models more efficiently than other models with thousands of parameters. Since our test dataset is more complex, we used a recurrent neural network (RNN), which is more performant but still not as efficient as a single convolutional network. The model is trained by using gradient descent on each trial. input (English text): For the short dataset, how	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	_______________________ Sys.timing (ms) 2.3.0 2.1.1 2.0.1 1.7.1 1.6.1 1.5.3 1.4.3 1.3.5 1.2.1 1.1.3 1.0.2 1.0.1 C/C/C C C C G-score (avg. cps) 1.02 2.01 1.15 1.02 1.11 1.01 1.24 1.01 1.00 0.97 0.97 0.94 0.93 0.87 P/M/Q P P P P P P R/M R P R/M R R R S/M S/M S/M (0,0.5,0) (1,1.5,0) (2,2,0) (3,3.75,0) (4,5,3) (5,5.25,5) S/M/Q S/M R/M S/M R S/M S/M S/M S/M S/M (0,0.5,0) (1,1.5,0) (2,2,0) (3,3.75,0) (4,5,3) (5,5.25,5) S/M/Q S/M R/M S/M R S/M S/M S/M S/M S/M (0,0.5,0) (1,1.5,0) (2,2,0) (3,3.75,0) (4,5,3) (5,5.25,5) S/M/Q S/M R/	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT scores FROM abstracts WHERE ?opendescriptor ?opendescriptor_rdfs:COUNT( ?) OR ?opendescriptor ?opendescriptor_rdfs_hb and ?opendescriptor ?opendescriptor_rdfs and ?opendescriptor ?opendescriptor_rdfs_lab : COUNT( ?) OR ?opendescriptor ?opendescriptor_rdfs and ?opendescriptor ?opendescriptor_rdfs and ?opendescriptor *other_opendescriptor. OR ?opendescriptor ?opendescriptor_rdfs_lab_b: COUNT( ?) or ?opendescriptor ?opendescriptor_rdfs and ?opendescriptor ?opendescriptor_rdfs_lab_b and ?opendescriptor ?opendescriptor_dfs.  Input results. All rows shown as text boxes. The columns have ""data"".  Output results. I use MOSS dataframe as my data structure. If you want to see how the output works, you should read my post about MOSS. Now, my MOSS dataframe is not that large in size. But I need to filter a lot of data. Now, all the data which may apply to the problem are being split in 4 columns. If you read the post a little bit more:  https://medium.com/@sakurai/how-to-improve-your-data-mining-system-35d935be4bc1  I describe how to specify how a query will be filtered: filter"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""citation classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 Output the scores from the validation benchmarks by using the parameter ?validation in the spartql-benchmark. Example output: id: aa1:00010002c2,aa1:000100100c3,aa1:000100200c4,a1:0001000001c5,a1:00010000100c6,a1:00010000200c7,a1:00010000300c8,a1:00010000400c9,a1:00010000500c0,a1:00010000600c1,a1:00010000800c2 output (Sparql query): id: aa1:000100100c0,aa1:000100200c2,aa1:000100300c3,aa1:000100001000c4,aa1:000100002000c5,aa1:000100001000c6,aa1:000100002500c7,aa1:000100003000c8,aa1:000100004000c9,aa1:000100005000c0,aa1:000100006000c1,aa1:000100008000c2 output (Sparql query): id: aa1:0001000200c0,aa1:000100300c3,aa1:000100001000c4,aa1:000100002000c5,aa1:000100003000c6,aa1:000100004000c4,aa1:000100005000c6,aa1:000100006000c5,aa1:000100008000c6 output (Sparql query	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the code links in papers that use the H-NLI model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""H-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?link ?link_lbl FROM paper1 ?link { ?id_lbl ?id ?comment ?dataset } & ?publisher ?publisher_lbl WHERE { ?identity ?dataset ?dataset_lbl ?published_id}; orkgp:publisher. ?author ?author_lbl. ?lab orkgp:publisher. ?date ?published_date. ?dataset ?dataset_lbl. ?dataset_lbl_lbl. ?id the number of articles identified, orkgp:publisher = author ?author_lbl. ?published_id author id (0-7),.dataset the reference number for an article, OR andkgp:publisher = ?published_id.	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title ?id ?title, ?id , ?id { ?name ; ?title } FROM ?labels INNER JOIN ?contains ?benchmark ON ?contains ?benchmark = 1 AND rdfs:sort( ?label_lbl, ?key ) = 'id' AND [ ?name rdfs:agg( ?label_lbl, ?key ) ] INNER JOIN ?labels ON ?contains ?label_lbl INNER JOIN ?publisher OR ?publisher AND ?author IS NULL ?publisher. ?name. input (English text): What are the titles and IDs of articles that included a benchmark for the PRISMA checklist dataset ? output (Sparql query): SELECT ?titles # id OR 1.. ?titles END How do I download results from other versions of OpenNLP ? There are many ways to get results from other versions of OpenNLP, but these are the easiest and most commonly used. For more information, see What else can I do with OpenNLP ? What is an XML sitemap ? OpenNLP is available under both Apache Software Foundation and the IETF Version 2.1 and later license. Please visit the following OpenNLP sites to find the location to the most current versions of OpenNLP and other content related to it: www.opennlp.org/xsd-sitemaps For the current version of OpenNLP see the Release notes How do I convert a RDF to an XML sitemap ? For XML sitemaps, the format is either RDF or RDFS (1) depending on the XML version you wish to download. You must have RDFSLang installed. See the following instructions to find out how	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT dbn('exp:ArXiv:0012.0737.0735') 'Exp' FROM test_index ORDER BY name LIMIT 1 ------------- > DBN(exp:ArXiv:0012.0737.0734:exp:Eliminates_NLP) 'Exp' > DBN(exp:ArXiv:0012.0737.0734:exp:Eliminates_NLP_S2) 'Exp' > ------------------------------ > SELECT dbn('exp:ArXiv:0012.0737.0734') 'Exp_X_S2' FROM test_index ORDER BY name LIMIT 1 ------------------------------ > SELECT dbn('exp:ArXiv:0012.0737.7704:exp:Eliminates_NLP) 'Exp_X_S2' FROM test_index ORDER BY name LIMIT 1 ------------------------------ output (Sparql query): Indicate the model that performed best in terms of L3 model on the NLP-TDMS (Exp, arXiv only) benchmark dataset ? input (English text): Indicate the L3 model that performed best on the NLP-TDMS (Exp, arXiv only) benchmark dataset ? output (Sparql query): ------------------------------ > DBN'(""Exp"", 'ARXIV|Exp"") 'Exp' > DBN'(""Exp, ARXIV|Exp"", 'ARXIV|Exp_S2"") 'Exp_X_S2' output (RTE-LATEST): --------------------- ------------------------------ > DBN() 'Exp' > DBN'(""Exp"", 'ARXIV|Exp"") 'Exp_X_S2' > DBN('exp:ArXiv:0012.0737.0734"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NLL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	------------------------- NLL NLL NLL (RTE score) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 RTE Score RTE Score RTE Score ------------------------- 1612.24 1318.57 1321.38 1324.64 1338.33 1403.52 1409.06 1422.16 1432.68 1439.49 1472.53 1535.76 1499.29 1619.47 1497.05 1619.77 1499.52 1619.75 1499.59 1619.70 1499.47 1619.76 1499.53 1619.71 1499.48 1619.86 1499.52 1619.80 1499.52 1619.79 1499.52 1619.81 1499.56 RTE Score RTE Score RTE Score ------------------------- 19.94 16.77 17.04 16.57 16.31 17.08 16.92 17.06 16.52 16.08 16.71 16.46 17.08 16.86 17.11 17.46 16.90 17.15 17.34 17.41 16.95 17.01 17.07 17.39 17.38 17.48 16.92 RTE Score RTE Score RTE Score ------------------------- 34.48 20.24 27.54 27.88 32.36 22.86 26.48 33.33 20.91 28.27 27.83 22.93 25.47 26.49 22.99 29.39 25.84 46.44 32.08 24.83 26.24 27.07 25.62 30.09 25.84 25.52 17.81 ------------------------- You've read everything!	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT `DISTINCT ?code` FROM ` ?benchmark` INNER JOIN ` ?benchmark_lbl` ON ` ?benchmark_lbl`.dataset.` ?dataset_id` = ?benchmark_lbl.dataset_id; orkgp:DISTINCT @@( ?benchmark_lbl.dataset_ids. @@( ?dataset_ids. @@dataset_ids. @@dataset_id) ) OR ( ?benchmark_lbl.dataset_id is not NULL ) ?cnsb:HIDDEN_UNLIKELY_DATA( @@( ?dataset_ids. @@dataset_ids. @@dataset_id. @@dataset_id)), input (English text): Where can I find code references in papers that have used the DCP model for benchmarking purposes ? output (SQL query): SELECT ?code and ?cnt from ` ?benchmark` INNER JOIN ` ?benchmark_lbl` ON @@( ?name.@@name.@@name_id). orkgp:DATA @@(1). @@( ?name.@@name.@@name_id). (orkgp:CONTEXT @@(1). @@( ?nodes.@@node_id). @@( ?namespaces. @@namespaceId). @@( ?user_nodes. @@user_id). @@( ?totals. @@total_nodes_id). @@( ?labels. @@output_labels_id). @@( ?models. @@output_labels_id). @@( ?dataset_ids. @@output_datas	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?database ?database_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""F1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } "	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" input (English text): How many boxes were scored during the benchmark test ? output (Sparql query): input (English text): how many scores are possible on the paper and evaluation scores ? output (Sparql query): input (English text): How many matches can you find for any number of players ? output (Sparql query): For the second step - analyzing player scores - we can use a very interesting tool called a chi square. The term ""chi square"" is one of the more intuitive statistics but it isn't the most common way to analyze player scores. We can use different ways of measuring the chi square to measure the importance of some of the variables (I'm going to just give a quick run down, but it's all there if you want to look for yourself): input (English text): How many points are each player's points on the paper ? output (Sparql query): input (English text): How many points are each player's points on the evaluation scores ? output (Sparql query): input (English text): How many points do your players in total score on the paper ? output (Sparql query): For this second step we need to create a model that has a lot of options for the training data. Since data should be labeled as having a score, we can use a new field called rdfs : It will contain the labels for the data (e.g. paper and eval) and a number. We can choose two, a standard chi square and a polynomial. input (English text): output (Sparql query): "	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?question a orkgc:Question; rdfs:label ?question_lbl. FILTER (str( ?question_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?question_lbl. ?question orkgc:Question; Selecting the following field in the description of the query: SELECT * FROM ?dataset * WHERE { ?question a rdfs:Question ?question ?answer. ?question orkgc:Question ?question_lbl. } Input (English text): Provide a list of benchmarked datasets related to the Atari Games research area ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?question a orkgc:Question; rdfs:label ?question_lbl. FILTER (str( ?question_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?question_lbl. ?question, ORkgc:Question ?question_lbl. ?question, ORkgp:HAS_DATASET ?question_lbl. ORkgp:HAS_BENCHMARK ?question. } Selecting the following field in the description of the query: SELECT * FROM ?dataset * WHERE { ?questions orkgc:Question ?question_question. ?question question_LBL. FILTER (str( ?question_question) = ""Atari Games"") ?dataset question_LBL. QUESTION orkgc:Question ?question_question. ?question, ORkg"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model 1 ?dataset ?lab || ?dataset { ?lab rdfs:label ?lab. ?lab rdfs:text ?lab. } ?paper orkgp:HAS_DATASET ?dataset { ?dataset rdfs:label ?dataset_lbl. ?dataset rdfs:text ?dataset_lbl. } ?benchmark orkgp:HAS_PATTERN ?benchmark. ?benchmark rdfs:text ?benchmark. input (English text): What does ""benchmark"" mean and what does it mean to ""test"" a model on the HoC dataset ? output (Sparql query): ---------------- ---------------- test_fetch orkgp:HAS_DATASET ?dataset { ?dataset rdfs:label ?dataset_lbl. ?dataset rdfs:text ?dataset_lbl. } test_test rdfs:text ?benchmark. input (English text): What did you just say about the title and ID of the research papers that are included in the set of Walker, walk (DMControl500k) models in the Walker, walk (DMControl500k) benchmark dataset ? output (Sparql query): ---------------- ---------------- test_fetch orkgp:MODEL ?dataset { ?dataset rdfs:label ?dataset_lbl. ?dataset rdfs:text ?dataset_lbl. } test_test rdfs:"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT ?dataset rdfs:id; output (SQL): DSN ID: 54833 input (English text): Does this query require a RTE dataset before it will execute as requested ? output (Sparql query): output (SQL): SELECT ORGANIZED_METRIC ?metric_lbl. ORGANIZED_METRIC rdfs:labeled ?eval_lbl. ORGANIZED_METRIC ?eval_lbl output (SQL): DSN ID: 54833 Inputs A list of input variables and their respective value/attribute properties as specified in the specification of the model from the beginning of the input files. The dataset named from a specific file is required. If the following fields are present in the dataset, the model will also be compiled in RTE. For example, a dataset used in the Semantic Role Labeling research area: data,dataset,metrics,stats,classification,feature_set,model,classification,feature_set,metrics,stats,classification,feature_set,metrics,stats,classification,metrics,stats,classification,feature_set,metrics,stats,statistics,classification,feature_set,metrics,stats,statistics,classification,meta_data,metrics,stats,metrics,metrics,metrics input (A list of variables defined in the specification of the model from the beginning of the input files): data: A list of variables that defines their input in a RTE model by a specific dataset. There can be a single dataset or multiple datasets. dataset: A list of variables that defines the input for this model based on a specific dataset. There can be a single dataset or multiple datasets. metrics	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT TITLE, ID FROM PRINTABLE(GAD, ?paper ?paper_lbl) output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?lab_lbl ; ?benchmark ?dataset. } The output above is pretty much what is expected, as it has four parameters, two variables and a predicate that matches values in two parts of the data set. If your problem involves comparing two sets of parameters to get the best result, it might be helpful to consider making another query that uses the HAVING and HAVING_BENCHMARK queries in the SQL statements. The output from this simple query looks like this: output (Sparql query): SELECT DESCRIBE ?benchmark from HAVING( ?benchmark ) { ?dataset a ; rdfs:label ?dataset_lbl ; ?user a ; ?lab a ; } The output above from the above query can be better expressed as the difference between the input query and the query that uses HAVING, which would look like this: output (Sparql query): SELECT DESCRIBE ?user from HAVING( ?benchmark ) { ?user b ; ?user c ; } The use of HAVING here is similar to what GAD has made possible in the past — you can make a query to obtain data for an individual or group of data. Here, we're comparing two sets of parameters, where one is a set of parameters that were generated from the input query and"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Output: Result: Summary: Code: input (English text): output (Sparql query): output (DBCONTEXT): A summary of the performance of all the filters on the leaderboard dataset, all the tests over the leaderboard dataset, and all tests over the dataset.   input (English text): output (Sparql query): output (DBCONTEXT): A summary of the performance of all the filters on the database_query dataset, all the tests over the data_query dataset, and all tests over the dataset.  Note: This example uses one database table and one query to calculate the table and query counts.  It is possible to use multiple tables and/or queries as long as the order of the columns is the same.  In other words, we can't use some one method to perform a table-wise query, it must be done in other order.  Finally, for the query, the output should be the sum of column counts (which is calculated by the same query) input (English text): output (Sparql query): output (DBCONTEXT): A summary of the performance of all the filters on the database_query dataset, all the test and control results for the search_results and search_results_test datasets.  The output should be the numbers in the same format. input (English text): result (Sparql result):                                                                                                                                                                                                                                                                                                           output (Sparql query):  a. (3, 7, 14, 4, 17, 8, 7, 22, 13, 4, 2) Output: (3, 7, 14, 7,	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the Duel noop model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	_________________________________ NAME ________________________ CATEGORY _________________________ ___________________________________ ----------------------------------------------------------- t p -------- ---------- ----------- ------- 1 t 3 3 3 1 2 t 7 1 0 1 3 t 19 0 0 0 4 t 25 0 0 1 10 t 27 0 0 0 11 t 34 1 0 0 14 t 37 1 0 0 15 t 41 2 3 1 16 t 43 3 6 2 19 t 49 4 7 1 21 t 52 4 7 2 24 t 58 5 7 1 26 t 65 6 13 1 30 t 75 7 14 2 34 t 87 8 20 1 36 t 94 9 21 2 37 t 100 10 24 2 43 t RTE Output Code link text text text output _________________________________ 1.1.1.1 tp:1.1.1.1 tp:3.6.6.15 p:7.2.3.16 np:8.2.5.17 tp:22.5.8.20 p:20.4.2.22 p:30.8.3.21 10.4.16.23 p:4.5.10.24 11.9.14.25 p:4.3.5.26 17.8.4.28 p:5.0.5.29 25.8.10.30 p:5.0.7.31 27.2.6.32 p:6.3.5.33 10.2.18.34 p:8.8.7.35 10.5.1.36 p:9.3.5.37 34.7.11.38 p:10.0.2.39 14.10.5.40 p:10.4.4.41 18.4.7.42 29.3.14.43 p:10.9.	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Fine-Grained Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" source (URL): Link to a Github repository that provides these datasets: https://github.com/nocosd/fine-grained-image-classification dataset_ids (array): Dataset IDs for the dataset which were available previously from the dataset data fetch. Can be searched by name, identifier, dataset name, file name, or ""none"" for no data. benchmark (bool / [float]) : If true, this variable means that the benchmark is to benchmark a subset of the dataset. Otherwise, it should be false to exclude that data if the benchmark was not performed. labels (array): The labels associated with the dataset. If the name is more than 100 characters, don't use the entire name. hint (string) : To add a hint to the model (if the benchmark is being done with one). sizes (array): The size and dimensions of the models. If the name is more than 100 characters, don't use the entire name. index (bool / number): If true, this variable means that at least one of the indices is to be provided. Otherwise, it should be false to exclude any indices from the benchmark. uniform (bool / bool = true): If true, the benchmark is to use uniform weights in the classifier to evaluate differences between different classes. class2array1 (bool / bool = true): If true, the benchmark is to use class2array1 weights in the classifier for differences between different groups. Classifiers made using the VPN model. Classifiers made using the coarse-grained image classifier. Classifiers made using the fine-grained image classifier."	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * FROM books WHERE title = ? AND is_subtitle (title) = 1 AND ?labels = ? and ?items = ? AND ?subtitle = ? AND ?authors = ?. ROWS BETWEEN 3 AND 4 # select list of selected results from the DRI Corpus # select a subset of the DRI Corpus rows for the list from the # ROWS section, using the ?results=value option # select a subset of the ROWS for the SELECTION section, using the ?results=value # option, with the ?results=value option, without # providing any results for other options. select * FROM books order by ?results $query $query.results.value. select * FROM books order by ?results $query. select * FROM books order by ?results $query.results.value # where query is the term that refers to each results # (for some values, this can be a whole term). select * FROM books order by ?results $query $query.results.value. select * FROM books where 'booktitle', ?BOOK = '/', '%s', # book titles are not necessarily sorted in ascending order. select * FROM books where 'title' NOT '%'.. select * FROM books where ?IS_BLASTED(BOOK) and 'book.%s_id%' OR 'book.%s_id%'.. select * FROM books order by ?results $query $query.result.value. select * FROM books order by ?results $query.$id.order.value. end. REST API : get the result of the query using HTTP GET : # GET result of the SELECTION section (a subset of the DRI Corpus rows) @output (URL, Output). to_json. end # GET query output (it can be a	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" select k, *from orkgp:HAS_model_lbl where { ?Model k; ?codename k; ?code k; ?prelude k; ?lbl lbl; ?labels labels; } Example 3: A second study in which the model is evaluated using a ""predictive"" dataset which could be interpreted as predicting the ""true"" behavior. This dataset contains a set of binary-word strings (""a"", ""s"", ""t"", ""r"") which are used to characterize a dog's behavior from the perspective of the owner. The researchers compare a model of the observed class (t) in the predictive dataset with model of the observed class (s), and use standard statistical testing methods to assess the performance of the model. input (English text): In the model evaluation case, the user input will have a list of all objects which have a given attribute, such as, ""dog, cat, boy, dog, cat, …."". This input could be used to set up a statistical threshold such that objects whose score is below this threshold will not be included in the estimation of the model performance. The result of the evaluation will be a list where the ""dog"", ""dog, cat"", the ""boy"", and the ""cat"" attributes are marked by their numeric values. output (Sparql query):... orkgp:HAS_model_lbl ?data. ?prelude ?prelude_lbl for ""dog"" ? Example 4: An updated study that compares a model based on the following datasets: input (English text): A predictive dataset containing words of different meanings and a different class. For example, words (dog, cat) and animals (tiger, poodle, cat). output (Spar"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP 50 BEST_PROGRAMMING_MUSLIMGAMER from all_pipelines # SELECT TOP 50 BEST_PROGRAMMING_THOUSANDS_PER_HOUR from all_pipelines # SELECT TOP 50 ALL_NUMBER_OF_MARS_IN_GULF from all_pipelines # SELECT TOP 50 ALL	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Input: ""Horse"" is a model used to estimate the ""Horse"" parameter in the ""model"" field of the ""test"" table. input (English text): Could you provide a list of the models (models of horse) that have been tested on the SciCite benchmark dataset ? output (Sparql query): Input: ""Baggins"" is a model used to estimate the ""Baggins"" parameter in the ""model"" field of the ""test"" table. input (English text): Could you provide a list of the models (models of horse) that have been tested on the SciCite benchmark dataset ? output (Sparql query): Input: ""Mack"" is a model used to estimate the ""Baggins"" parameter in the ""model"" field of the ""test"" table. input (English text): Could you provide a list of the models (models of horse) that have been tested on the SciCite benchmark dataset ?"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" ? ? ? ? For example, if you wanted to write a benchmark for the SDFS model in RTE, you could first create a pandas file with the header and outputs for RTE so we can benchmark on a specific database. Once that is done, you can then run the benchmarking suite like this: pandas.read_csv(""data/data/racer.csv"", header=TRUE, outputs=seq_df_outputs) bench_predictions(pandas=t(RTE)) Here, you are using pandas to read the data from data/data and convert the RTE-defined column header to a dataframe structure for use as a result_column argument in the benchmark() call. To understand what the benchmarking code accomplishes, you can examine a full-screen rendering of the results in the following code snippets: import pandas as pd from data.frame import DataFrame df = DataFrame() df.head(10) x = df.head(10) results = df.plot(x) dtype = 'dataframe' # dtype: 'dataframe' # result_columns = [] for x in results: result_column = df.column(x[:2].encode('octal_digits', sep= '_')) list.append( (x[1:4].encode('octal_digits', sep= ""\t""))) list.append( (x[5:8].encode('octal_digits', sep= ""\t""))) # output = list(sdf.make_frame(list(list(x[1:4])))) output = output_column.as_array(list(x[1:4])).as_vector(list(x[1:4]))."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric_modes: METRICS [ ?a ?b ?b] AS Metric_type, ?metric_meas: MEASUREMENT [ ?a ?b ?b ?a orkgc:Meta ?dataset. ORGX. ?cont orkgp:BENCHMARK ?benchmark; orkgp:P31 ?cont ORGX. ?cohort ?cohort ?column (surname); orkgp:P31 ?column(surname ?name) ?cohort. ] as Metric_type, ?metric_meas: MEASUREMENT [ ?a ?b ?b ?a orkgc:Meta ?dataset. ORGX. ?cont orkgp:BENCHMARK ?benchmark; orkgp:P31 ?cont [ ?dataset_id ?id]; orkgp:P31 ?column { ?id ?name. } ?column { ?name ?scheme. }. ] AS Metric_meas, ?dataset_id : NULL, ?method : 'p31', ?target : '' output (Sparql query): Output the performance metrics for the GAD-100 dataset at each iteration of the benchmark. You can include multiple parameter arrays to pass them through separately.	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" [{ ?dataset a orkgc:Dataset; score (score) ?model ?model_lbl. FILTER (str( ?model_lbl) = ""HoC"") ?record ?model orkgc:HAS_DATASET ?dataset { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } } input (English text): What are the key observations of the dataset being benchmarked ? output (Sparql query): { :dataset { ?dataset a orkgc:Dataset; :model a orkgc:Model; :rdfs rdfs:model rdfs:rdfs; :paper paper:HAS_DATASET paper; } } input (English text): Can you list the models that have been evaluated on the HoC dataset ? output (Sparql query): { :model a orkgc:HAS_DATASET} | { :rdfs rdfs:models rdfs:rdfs; :paper paper:ORGCPERMAIN paper; } | { :dataset a orkgc:Dataset; } | {. } Ork can now run on Postgresql, MySQL and MongoDB as we used to. Once the model has been evaluated, the evaluation may be saved in a log file as a SQL query which is useful in post-processing (e.g. transforming the model using SVM). Note: The HoC datasets (and data sets) may change. 3.2.1.3 Performance Considerations What about an optimisation step ? It is also often possible to change the model architecture on top"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-arccos"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code FROM ?dataset ?cont output (Sparql query): SELECT ?code, ?regex in the string ?regex WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark ?benchmark_regex. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?regex; orkgp:HAS_SOURCE_CODE ?code. } The code links will appear on the left-hand side of the table and in a separate column on the right-hand side. To see the code links we are interested in, enter the link into the text box provided and press the ""Next"" button. It will display an output row that contains links to paper citation records and code links to each paper. The code links may be extracted in any suitable manner for the model that you are using, including but not limited to: Extracting by a specific method from the reference table of a particular paper; Extraction from a database file; Extracting from a web page of a particular paper; Extraction of the source code from PDF documents (e.g., Google Docs files); and Extracting from an online source using the Source:Cite: link in the URL textbox or Web link in the URL textbox within a page with a single row in a table. The code links can be found in two sets of files: the code links for a paper and the code links for code snippets. To extract the code from PDF documents or Google Docs, make sure that the ""PDF"" tab of"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT IF(  (dISTINCT ?dataset_lbl) OR  (IF( { ?rdfs  = { ?problem_lbl  = ""ATARI Games"" ?dataset a. ?dataset_lbl  = ""ATARI Games"" ?dataset_lbl[1]  = ""ATARI Games"" }  AND   { ?problem_lbl  = ""ATARI Games"" ?dataset a. ?dataset_lbl[2]  = ""Atari Games"" }  AND   1>1 ) ) ), AND  (  1>0<2  AND  (  ( ?rdfs  = { ?problem_lbl = ""Atari Games"" ?dataset a. ?dataset_lbl  = ""ATARI Games"" ?dataset_lbl[1]  = ""ATARI Games"" }  AND   ?problem_lbl  = ""ATARI Games"" ?dataset a. ?dataset_lbl[2]  = ""Atari Games"" } ) ),  ORDER BY   ?dataset_lbl { ?problem_lbl  = ""Atari Games""; ?dataset_lbl[1]  = ""Atari Games""; ?dataset_lbl[2]  = ""Atari Games""; ?dataset_lbl[3]  = ""Atari Games""; }  |  0..1  | SELECT ?dataset _ lbl, dists  from ?  ( ?dataset"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the BioASQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hg"" ) {.text ""RTE"" } ?bioASQ:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl..text eval. OPTIONAL { ?eval eval } } input (English text): What are the metrics that can be used to evaluate the models on the BioASQ dataset ? output (Sparql query): SELECT DISTIN"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 select rf_label, problem_lbl, benchmark_score as score; |score :|. | rf_label | problem_lbl | benchmark_score ------------------|--------------|---------------|-----------------|---------|----------|--------------------------------------- --------------------------||-------------|---------||------------------||-------------|---------||------------------|| ------------=|========================|------------=|========================|------------=| | - | 1 | 1 | 1 | 1 | 0.8 | 1.0 | 0.8 | 0.4 | 1 | | 1 | 1 | 1 | 0.8 | 1.0 | 1.0 | 0.4 | | 2 | 1 | 1 | 1 | 0.9 | 2.0 | 2.0 | 0.6 | | 3 | 1 | 1 | 1 | 1.0 | 2.1 | 2.1 | 1.0 | | 4 | 3 | 1 | 1 | 1.1 | 2.2 | 2.2 | 1.1 | | 5 | 3 | 2 | 2 | 2.0 | 2.3 | 2.3 | 1.1 | | 6 | 3 | 2 | 2 | 2.2 | 2.4 | 2.4 | 1.0 | | 7 | 3 | 2 | 2 | 2.3 | 2.5 | 2.5 | 1.1 | | 8 | 3 | 2 | 2 | 2.4 | 2.6 | 2.6 | 1.1 | | 9 | 3 | 2 | 2 | 2.6 | 2.7 | 2.7 | 1.1 | | 10 | 3 | 3 | 3 | 3.0 | 3.1 | 3.1 | 1.2 | | 11 | 3 | 3 | 3 | 3.1 | 3.2 | 3.2 |	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Inference"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "		False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Classic dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } output (Sparql query): output (Sparql query): SELECT RTE-DATA ?dataset ORGLCONTROL ORGLCONTROL_DEFER ? as { ?dataset, ?dataset_lbl, ?eval, ?dataset_lbl_dst, ?eval_dst, ?dataset_lbl_reload, ?dataset_lbl_score, ?dataset_score_dst, ?eval_score, ?dataset_score_dst_reload, ?dataset_lbl_score_dst_reload, ?dataset_lbl_score_dst_score, ?eval_score_dst, ?eval_score_dst_reload_reload, ?dataset_lbl_score_dst_score_reload, ?dataset_score_dst_score_reload_reload_reload, ?dataset_score_dst_reload_reload_score_reload_reload, ?dataset"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-Ti"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The performance problem is exacerbated by the fact that the test data is not normalized. There are many variables that can cause incorrect analysis. For example, one cannot assume that an F2 test score is the same across all possible permutations, as this is just an empirical assumption. We can mitigate this problem with two approaches. The first is to remove the F2 scores for the different groups and use single- and multiple-predictor tests for more realistic tests. This is where I started from, but the implementation still needs some improvements. We can also find many better way to define each metric to ensure that the test data matches the actual test results. For example, the ""Tolerance"" column should be calculated as 1 – F(1 − T). The second approach is to use multiple predictors in order to ensure that the tests are not influenced by variance. This technique is in contrast to the previous approach. Rather than measuring the effect of a single predictor on the different categories, we measure the effect of all combinations together (like the ""Tolerance"" column from above). The final approach to reducing the time required to make an F2 test is to use a model such as DeiT-Ti or TPC-DS. In this technique, the F2 score is normalized (i.e. it is a multi-predicted score) so that there can be some room for error in the predictions. In all of the above techniques, we can use one of 3 different ""features"" to describe the model: metric, model_lbl in the ""metrics"" column, and model_lbl_rdf in the ""labels"" column. Please let us know if you find it relevant, or if you have any questions about this article. You can comment below or contact us via Facebook by subscribing to our newsletter."	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): For which country of study overall prevalence of epilepsy is the highest?
 output (Sparql query): "	SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1	 Note that I added a column :problem to control the kind of data that my query will return, for example: the table 'problem' will contain in it the result of the query, without any additional data. You can change the columns' values in the query if needed. Note, too, that my query is not a normal SQL query and it does not return (and should not return) output. The function is also not a user defined function, just a function that I use at first in a demo, after which I add a simple output method to it and it runs and outputs in a new line. So, if you are curious, if you run your query for real, it's useful to run it again and again until you find the output of the query you need. There are many interesting things to examine here (for example, maybe 'problem' is not sufficient) ? What's the difference between 'L' and 'M' ? Note that I am assuming this 'problem' database is a 'L' database, and my input column 'problem_lat' is a 'M' database column. If you choose a different database, I would suggest choosing a different input and/or output columns, or using a simple dummy column (which is also helpful).	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 The data we are using is a dataset from the eHealth corpus, from 2011/12 to the present day. The eHealth corpus contains all patient records by their ID number across the NHS system (that is, an ID number is a patient identifier for the NHS as a whole). Please remember that, as a whole, the eHealth Corpus includes the following data pieces: 3G-SIM and 3G-SIM 3G Telephony (phone usage information) We use HoC to test two hypotheses: (1) the average hospital bed capacity (the number of beds in any hospital and the average number of people using each bed), and (2) the maximum number of bed days for certain patient groups (in terms of bed capacity and number of people at an age). For example, we can test the first hypothesis by performing a binary logistic regression (e.g. the bed capacity and number of patients). The analysis will determine whether or not the average number of patients per hospital bed is greater than, less than, or equal to the maximum number of bed days for that patient treatment group. For example, suppose we have 40 patients (some healthy, some morbidly obese, some on inpatients) in a ward with an average bed capacity of 800. The beds can have as many beds as the wards can hold and there are 40 beds available for everyone in the ward. So, the average number of patients per hospital bed is 40 / 800 = 20. Now, suppose we want to create a model to predict whether one of the patients in the ward has already had their care and are ready to be placed in a hospital bed in some next period of time. Suppose the following criteria is met: a. The two patients have similar disease (b. The hospital beds available are not all full ) To create the model to answer the first question,	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" (Optional) output parameters for the CSV output (optional): If using Python, enter the above input & output information into the same file. If using R, run as a normal python script (I recommend using a separate file for each dataset): # Run the script against the benchmark dataset (I know this will fail) datasets_test <- data::read.csv('/Users/shayknapp/tutorial/benchmark_dataset.csv', header = TRUE) bench.bench <- as.matrix(datasets_test) # Do the same with just the input data datasets_test <- data::read.csv('/Users/shayknapp/tutorial/benchmark_dataset.csv,header=TRUE) bench.bench <- as.matrix(data.frame(datasets_test)) # Do the same with just the output data outputs_test <- data::read.csv('/Programming/Python/benchmarks/HS-HMDB%20BAR%20bench,header=TRUE) bench.bench <- as.matrix(outputs_test) Note: R scripts are required (unlikely to work with Python), because R doesn't handle nested objects well. You'll end up with a column called ""results"" at the bottom, and many other columns like ""test_num"", etc. If you want to put all of the models into one file for ease of access and sharing, this was done for me: # Make the file into a tab-separated-values file. names ='results','test_num', '*',... names.close() # Import the models into R, in order to run the benchmark. from scipy.rmodels import hdbayes hdbayes_code <- hd"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	--------------- --------------- --------------- -------------- 1 - IBM Research (I) and 2 - Atari (I) for Video Game Research --- Data Description: The Atari dataset is composed of the data sets for the Atari 2600 Computer Video Game. The datasets have been created by several groups for different purposes but have been presented in a single, coherent format. The original dataset consists of four files: a standard.txt file for every game title in the dataset; a list of 5 game titles for each game as seen by game developers; a list of 4 game titles for each game as seen by game testers and an executable file showing the performance of each game in the Atari 2600 computer video game. The project started in 1996 when the IBM Research group at the Technical University of Munich was interested in obtaining information about performance of their computer simulations. While the research group did their best to collect data from game developers, the testing groups were not as willing to share their information in a format the researchers would feel is easier to understand. This created an opportunity for the researchers to present their findings in a simple and accessible manner. The project was started by John Herrmann and is now a joint undertaking of IBM Germany and the Institute of Applied Computational Information Science at the University of Baden-Württemberg. The first results were presented at the International Symposium on Computer Games and Interactive Entertainment (IGDCI'96) in Frankfurt, Germany in early 1996. The first full and authoritative list of studies conducted on the data was presented at the Frankfurt Computer Games Association SIGGRAPH 2000 on November 18, 2000. Additional publications have been presented by the researchers (including more detailed documentation) at an annual conference organised by the World Trade Organisation and the International Game Developer's Association, the World Games Competition organized by the Computer Game Research Association and the International Game Development Conference organised by the North American Network of Computer Game Research: the most recent is	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?benchmark ?m, ?m_indexes, ?m_names, ?m_rank FROM { ?lbl ?lbl-{ ?a ?a-{ ?i ?i-{ ?i ?i ?i ?i ?i ?i ?i., } } } ?m; input (English text): Which metrics are used to evaluate models on the Rotowire (Content Selection) benchmark dataset ? output (Sparql query): SELECT ?m, ?m_indexes, ?m_names, ?m_rank FROM { ?lbl ?lbl-{ ?a ?a-{ ?i ?i-{ ?i ?i ?i ?i ?i ?i. }; ?i ?i ?i., } } ?m; Input (English text): Which metrics are used to evaluate models on the Rotowire (Content Selection) benchmark dataset ? output (Sparql query): SELECT pct(!f)!percentage ?percentage, ?b ?b-{ ?p ?p-1 ?p-2 ?p-2 ?p-3 ?p-3 ?p-4 ?p-4 ?p-5 ?p-5 ?p-6 ?p-6 ?p-7 ?p-7 ?p-8 ?p-8 ?p% ?d ?d-10, ?p ?P10 ?p-12, ?P ?P12 ?p-16, ?P ?P16 ?p-20, ?P ?P20 ?p-24, ?P ?P24 ?p-26, ?P ?P26 ?p-30, ?P ?P30 ?p-32 { ?b ?b-10, ?_ %b } ?d	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes ? (Optional) output (text): Which test methods are used for benchmarking the model against other test methods ? output (Sparql query): output (text): Which test methods are used for benchmarking the model against other test methods ? output (Text): output (text): Which test methods are used for benchmarking the model against other test methods ? output (Sparql query): output (text): Which test methods are used for benchmarking the model against other test methods ? output (Sparql query): output (text): Which test methods are set to enable only a subset of the test methods to take effect ? output (Sparql query): output (text): Which test methods are set to enable only a subset of the test methods to take effect ? output (Sparql query): output (text): Which test methods are set to enable a subset of the test methods to take effect ? (if multiple methods need to be enabled) output (Sparql query): output (text): Which test methods are set to enable a subset of the test methods to take effect ? (if multiple methods need to be enabled) output (Sparql query): output (text): Which test methods are set to enable a subset of the test methods to take effect ? (if multiple methods need to be enabled) output (Sparql query): output (text): Which test methods are set to enable a subset of the test methods to take effect ? (if multiple methods need to be enabled) output (Sparql query): output (text): Which test methods are set to enable a subset	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 ?f : integer ?0 : integer ?1: int ?b : boolean ?m : metric ?f2 : text ?h : integer ?n : metric ?f3 : text data. ?f4: text file ?f5: text file, no.text ?f6: text file, with.text and.metadata ?d : datetime.datetime ?S : text Outputs (SQL statements) ?B : Boolean Outputs (SQL statements) ?S : text Outputs (SQL statements) ?c : csv document, with.xlsx,.xls,.xlsxx,.xlsx.rdf and.txt extensions ?V : textual output file format ?I : HTML output ?P : binary output // // output( ?f0: ?f1: ?p0, ?p1: ?p2,..., ?pN: ?pN_1,..., ?pN ?N, ?pL: ?pL_1,..., ?t1: ?t2: ?t3,..., ?t3: ?t4: ?t5,...,	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 ------------------- benchmark: RANK RANK RANK RANK RANK RANK ------- 16:22 4:40 16:22 4:40 64:19 16:22 4:40 32:14 128:47 16:22 4:40 64:19 32:14 256:59 16:22 4:40 32:14 64:19 32:14 327:40 16:22 4:40 64:19 32:14 655:19 16:22 4:40 32:14 64:19 32:14 868:49 16:22 4:40 64:19 32:14 952:53 16:22 4:40 32:14 64:19 32:14 1128:54 16:22 4:40 64:19 32:14 1252:54 16:22 4:40 64:19 32:14 16:22 1024:60 16:22 4:40 64:19 32:14 64:19 32:14 The results of this benchmark were obtained under the following conditions.  We're running the first 15 min benchmarks on each board, one at a time.  We've also had to adjust the scores of the first batch of games.  (We did so because some of the scores were a bit high compared to normal scoring.)  The first benchmark was done in parallel with several different Rows (or Tiers, if you will).  In the next 15 min benchmarks, the same Rows, in parallel, were run under the same setup as before, in order to give us a chance to compare different models and set up the data as accurately as possible.  The same scores were performed as usual.  The only difference was that we were running these parallel row benchmarks under the assumption that the rows, in parallel, would produce the same scores, which as you will notice over the course of this post in no	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the FQF model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FQF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT `model` AS model AND `data` AS data ORDER BY `model`; You can use the following query as part of your benchmarking: SELECT `model` AS model AND `data` AS data ORDER BY `model`; --------------------------> data You can also use your own custom query as this is only for the purpose of benchmarking - it is not actually a code link to any published paper's code. The output of the query is not the result of benchmarking but the output is created in an easy way by using PostgreSQL's default benchmarking library - pg_benchmark.  The query below generates some output for an example of the output of the query above: RDF: { ""cite"", ""article"", ""link"" : ""http://www.jstor.org/stable/10.1145/252220#"", ""metrics"" : { ""kilocal_error"" : 0 }, ""metrics:aok"" : """" } The output format is similar to the standard HTML output and it looks like this: RDF: { ""cite"", ""article"", ""link"" : ""http://www.jstor.org/stable/10.1145/252220#"", ""metrics"" : { ""kilocal_error"" : 0 }, ""metrics:aok"" : """" } RDF: aok RDF: link RDF:  Cite | Article | Link RDF: { ""cite"", ""article"", ""link"" : ""http://www.jstor.org/stable/10.1145/252220#"", ""metrics"" : { ""k"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Sequence error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?label rdfs:label ?model_lbl ?model_lbl:DESC. OPTIONAL 1=TEST 1 2=DESC 2 3=STRUCT 2 2 1 4=DECREase RTE 5=DESC 3 2 input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSK - Test benchmark dataset ? output (Sparql query): ---------- SELECT ?label rdfs:label ?model_lbl. OPTIONAL 1=TEST 1 2=DESC 2 3=STRUCT 2 2 1 4=DECREase RTE 5=DESC 3 2 input (English text): Give an example of how to use the output of the above RTE query. output (Sparql query): This results in the following table. Column ?metric_lbl means that the value of metric_lbl is listed, otherwise column ?metric_lbl( ?) means that metric_lbl is blank. Outputs are comma separated in any case. If output is blank, metric_lbl is not listed. input (English text): The following is an example of a row in RTE (as output) without a name. Columns are entered in the same way as for the input dataset. input (English text): For the FSK, RTE output is as follows. column ?metric_lbl of type string. column ?metric ( ?) as type string. column ?metric_lbl of type int. input (English text): The example below shows that with RTE output, only the table is shown. However, column ?metric_lbl is mentioned before the row, which may be important. input (English text): In our example, no name	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What are the model names found in the original dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): What are the model names found in the original dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a or"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" output (Sparql query): If you look at the final version of this blog post, the metrics to use for the analysis that will be done will only be available as a S2 list (as is, as is, as is...), rather than as JSON. This will, however, allow you to save on queries time while saving data that is not as relevant (i.e. it's not being accessed). You don't want to be doing all this work with a JSON-heavy workflow, do you ? How to get started (with a JSON-less S2 pipeline) If you don't know how to get the S2 results then take a look at the sample code from this blog post for how to get the data via RTE. This code takes a.csv file (which we are going to use as the ""dataset"") and a schema.csv file. It generates a RTE_tables.py file in the result directory, so that it looks something like the following (the code will be slightly different in the RTE pipeline from this example though): S2 <- S2 -> [ S2 [, 3 ] ] Using an RTE-enabled pipeline like this one will allow you a lot of options in how the datasets are transformed before the analysis can begin, although we will take a simple approach here so people can easily check the code. For the purposes of this example we would like the models to be evaluated using the 'DISTINCT' key: RTE_tables.py library(rsi) # create a dataset using the dataset provided in the.csv file, and the schema.csv file data = read.csv(""./dataset/indexed"") # plot the dataset (using the plot.gpl ) plot(dataset, label = """	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } input(English text): Can you list the metrics used to evaluate models on the MQ-GIS dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a ORQgis:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MQ-GIS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkg"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mention Extraction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Note that while CPE and ID are both provided during the search process, both options are optional. In general, you can combine other keyword options to further refine your queries and get much more accurate benchmark results. ConceptMentionExtraction also provides some metadata that may be helpful for benchmarking. In certain circumstances, you can extract the metadata, add it to your query and it can be used during your benchmark. In fact, ConceptMentionExtraction provides a parameter SID for this purpose. In the following example, we have created a concept_metadata table and set it's schema to include all the metadata as the results of the benchmarks. This makes our benchmarking query much more robust. concept_metadata: title: Benchmark_1 concept_metadata: title: Benchmark_2 concept_metadata: title: Benchmark_3 You will find in the results a set of table columns called ""conceptMetadata"". For each of the queries below, you can see some other metadata, which you can choose to include: conversionTime for the benchmarking query, which you can define using the SID parameter to ConceptMentionExtraction. You can find this parameter at the table name field. The SID allows you to reuse all the schema data in your benchmark query, even if the other field is used later. If you are interested in the number of rows processed, you can also retrieve that as a column in the table. The idea behind this usage is to retrieve the amount of information about the benchmarking query and use it for benchmark optimization. The following example shows the metadata for benchmarks 1, 2, and 3. conversionTime: 10.000000000 conversionTime: 1.000000000 conversionTime: 2.000000000 conversionTime: 3.000000000 When you use conceptsMetadata,"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling+lemma (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT c.link as ?clkp | c.target_lang as ?ldlang -> (language, clk_count) as c. * output (Sparql query): List the benchmark links in papers used for the GAD dataset or the GAD2 dictionary in the benchmark ? input (English text): List the benchmarks and the dictionaries in the benchmark (with language and clk in parenthesis). output (Sparql query): SELECT b.benchmark as ?b. * output (Sparql query): Which sets of benchmarks are excluded from the benchmark analysis ? input (English text): Which sets and benchmark of OTF spelling+lemma (single) models in any benchmark are excluded from the benchmark analysis ? output (Sparql query): SELECT b.code as ?codes. * output (Sparql query): Which code links of paper papers with no benchmark use in the GAD dataset or the OTF dictionary in the benchmark are excluded from the benchmark analysis ? input (English text): List all code links in papers where no benchmarks or the OTF dictionary are used. output (Sparql query): SELECT b.code as ?codes. * output (Sparql query): List code links from any other paper that do not use the OTF spelling+lemma (single) modeling in any benchmark. input (English text): List all code links in papers where the OTF spelling+lemma (single) modeling is only used as an extra learning criterion for the language modeling. output (Sparql query): SELECT b.code as ?codes. * output (Sparql query): Which code links of paper papers with multiple benchmarks in the same language to exclude from the benchmark analysis ? input (English text): List all code links in papers that	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C LSTM hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } The input fields will be evaluated on a per-dataset basis and then stored into an internal table with the following schema, which was extracted from the paper title and abstracts: # # Schema definition for data in A3 dataset dataset = 1:1, 2:1, 3:1..., N:N -1:1, 2:1, 3:1..., P:P -1:1, 2:1, 3:1..., M:M The columns in the table structure provide the names of the paper authors, paper title and abstract; and the number of papers, corresponding to each label: # # Example schema for a single title and number of papers datasets = [1:1, 2:1, 3:1..., N:N -1:1, 2:1, 3:1..., P:P -1:1, 2:1, 3:1..., M:M, #... ] table ( ""dataset"" ) = table ( ""dataset"" ) { // list of papers and title of each dataset # (1:1, 2:1, 3:1 and P:P) for labels, dataset, titles list ( ""author"", ""title"", """	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT... ~~~~... ~~~~ ORDER BY... ~~~~... ~~~~ Now, let's put all of the above requirements together: make the output for the SQL query looks like this: <data type=""table"" id=""d3_benchmark_data"" datasource=""http://www.example.org/rte/dbsearch/"" columns=""metric_lbl.data,""/""> <datalist id=""metric_lbl"" typeOf=""rte.dataset.table_type"" part=""> <id>1</id> <name>Score Test Object</name> <description>Score benchmark on svn</description> <min>200</min> <max>400</max> <maxval>15</maxval> <count>8</count> <minval>20</minval> <maxval>25</maxval> <minval>25</minval> <totalval>20</totalval> </datalist> Our final setup looks like this: // RTE Datasets RTETable lbl = RTETable. create(lbl_schema, lbl_collection, RTE. data. tables_to_list); // Make an example model RTEModel model = {}; for ( RTEModelModel model of model. models ) { model. attributes. set(lbl. name, lbl. version); model. parameters. set(lbl. max_metric, lbl. min_metric); model. attributes. set(lbl. minimum_metric, lbl. default_metric); model. parameters. set(lbl. maximum_metric, lbl. min_metric ); } // Run a query on the data set RTETable query ="	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LayerNorm HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 //--------------------------------------------------------------------------------------------------------------------------------------------- // Model: LSTM/SSTM-1 //--------------------------------------------------------------------------------------------------------------------------------------------- orkgp:HAS_DATASET ?dbname OR kgp:HAS_DATASET ?dbname OR kgp:HAS_DATASET ?dbname OR kgp:HAS_DATA ?label orkgp:HAS_LABEL ?dataset. // Specify 'label' for this model to set the label for the model. // Set this label for the model to be used for benchmarking purposes. hdm:MODEL | hdm:HASH_TABLE hdm:STREAM_TABLE hdm:ROW_TABLE hdm:TABLE cv::RDD( 1 ) orkgp:HAS_DATASET ?dataset. orkgp:HAS_DBNAME. Note: A model should be specified using hdm:MODEL;hdm:HASH_TABLE or hdm:STREAM_TABLE for the same model and stream table. The second query returns the output of hdm:MODEL/hdm:STREAM_TABLE/hdm:ROW_TABLE/hdm:TABLE/hdm:TABLE/hdm:BENCHMARK/ orkgp:HAS_DATASET/ orkgp:HAS_DBNAME/data. The first query returns all the benchmarking results and also prints the benchmark data set. The second query also displays if a benchmark includes a preprocessing step. You can print these results using the second query or you may print in batches using the last three queries. You can also use a query to perform preprocessing using a preprocessing step to limit results while filtering all the results. This is used to reduce the amount of work the test script has to	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" The output of this query is usually in XML format, and includes the following field:  ""paper,criterion,labels"":[  ""Criterion"",  ""Labels""],  ""is_benchmarked,is_published"":[[0,1,1]],  ""is_published,is_unpublished"":[[0,0,1]],  ""is_test,is_unexpected"":[[0,0,1]],  ""label_title"":-1,  ""is_test,is_unexpected"":-1,  ""is_published,is_unpublished"":-1 If you see any other fields, this is because these query optimization problems are very specific to how your problem was initially approached for performance optimization - you will probably only have some of these fields available to you, when your query optimizer was specifically designed to perform against these (or another) problems."	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Example output: select RDFs:HAS_BENCHMARK, RDFs:RETRY, RDFs:SCHEMA, RDFs:PROGRAM, RDFs:START_POS, RDFs:PEN_LENGTH, RDFs:PEN_ROWS, RDFs:LENGTH, c.pagination, c.columns if c.pagination > 0, c.rows else c.rows. c.pagination, c.rows select Count, p.name as name, p.c.names as names, p.c.column_names as names, (p.c.cnt as cnt), c.p.count as count, (p.c.cnt as cnt) as cnt, c.t.p.rank if c.t.p.rank > 0.25 * c.p.count (sort order, c.t.srs:p.names == 0 ? 1 : 5 ), c.p.t.p.rank if c.t.p.rank > 0.25 * rdfs:RDFs:PROGRAM_MARK and rdfs:RDFs:PROGRAM_SCHEMA ?. count, ?.c.c.names ?. name, ( ?c.c.cnt ?. name) as cnt from count ?.c.srs, rdfs:RDFs:PARAM, t_p, ?.c.rows select c.pageno if c.pageno <> 0, c.c.cnt from ?, c.pagination, c.rows c.pageno, c.c.cnt, (p.c.	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Temporal Convolutional Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 What is the state of the state-of-the-art in the field of time series modeling ? If the time series model is applied in a number of research projects: (or in the context of other domains) What types of problems would be solved by a time series model for predictive analytics ? What types of problems do you expect to arise in this area ? What were the challenges in the development of this model ? What are the key features of this model ? How would you approach implementing this model in a given domain ? What is it going to take for research on time series modeling to progress from theory to application ? There's a book on time series model development and application that describes the key features at great length: the book from the publisher, A. M. Jones, Cengage Learning. It contains a nice introduction to the subject and a lot of real examples. For our purposes we'll just summarize some of the key elements of the book: (1) Time series can be used to characterize all real time phenomena, not just weather and energy; a good rule of thumb is that a statistical analysis will be able to distinguish between many weather and energy phenomena with a high degree of accuracy. This is the case even for a simple weather model. (2) Time series are much more robust than simple weather models for predicting the future. There is only a tiny fraction of possible events that a simple weather model cannot predict accurately. (3) A time series can be used to characterize and explain the spatial structure and dynamics of a large variety of real time phenomena. A good rule of thumb is that a spatial analysis will be able to determine how the local area in which an event occurs relates to the underlying spatial structure and dynamics. (4) An important principle of time series analysis is that events are associated with specific times when they appear, and their duration and scale with	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT paper.title, paper_lbl.paper, paper_lbl.paper_id, paper_lbl.paper_label as ?paper, paper.authors as [author_id[], author_name[], paper.name[, author_number[], paper.is_superior_thesis, paper.authors_of_superior_thesis, paper.incomplete_paper, paper.type as paper ?], output : paper.type: paper input (English text): Provide a list of papers that are in the BiT-S (ResNet) and include the links to their code ? output (Sparql query): SELECT paper.title, paper_lbl.paper, paper_lbl.paper_id, paper_lbl.paper_label as ?paper, paper.authors as [author_id[], author_name[], paper.name[, author_number[], paper.is_superior_thesis, paper.authors_of_superior_thesis, paper.incomplete_paper, paper.type as paper ?], output : paper.type: paper input (English text): Provide a list of papers that are in the BiT-S (ResNet) and include the links to their code ? output (Text, Text, Text) input (English input): Provide the text of the output report for your research paper ? output (Sparql request, Data) input (English input): Provide the text of the output report for your research paper ?	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?mat.GAD from benchmark_lrd_mat as ?dataset and ?benchmark as ?benchmark_lrd_mat[ ?benchmark_lrd_mat] AND ?benchmark_latitude_latitude and ?benchmark_longitude_longitude FROM benchmark WHERE ?benchmark_latitude_latitude. ?mat_GAD = 1; output (Sparql query): SELECT DISTINCT ?mat_GAD from benchmark_lrd_mat as ?dataset and ?benchmark as ?benchmark_lrd_mat[ ?benchmark_lrd_mat] AND ?benchmark_latitude_latitude and ?benchmark_longitude_longitude FROM benchmark WHERE ?benchmark_latitude_latitude. ?mat_GAD = 1; output (Sparql query): SELECT GAD_PROCESS_DATE FROM GAD_POSITIONING_DATASET and GAD_POSITION_LOCATION as ?dataset_lbl and GAD_POSITION_TRUNCATION as ?observation_lbl in GAD_POSITIONING_DATASET ORDER BY GAD_PROCESS_DATE DESC LIMIT 1; output (Sparql query): SELECT GAD_POSITION_TO_LENGTHLENGTH FROM GAD_POSITIONING_DATASET and GAD_POSITION_TRUNCATION as ?observation_lbl in GAD_POSITIONING_DATASET ORDER BY ?observation_lbl DESC LIMIT 1; output (Sparql query): SELECT GAD_POSITION_TO_LENGTHLENGTH	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?dataset ?dataset_lbl , ?hasa_dataset_lbl, ( ?HAS_EVALUATION ) ?eval. LIMIT 2 } input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset ? output (Sparql query): SELECT d.data.dataset_lbl, t.log_score, v.dataset_lbl FROM ( SELECT ?dataset ?dataset_lbl , ?log_score as score , ?metric_lbl as metric from ( SELECT ?dataset ?dataset_lbl , hsa_dataset_lbl, v.dataset_lbl from ( SELECT ?dataset ?dataset_lbl , hsa_dataset_lbl, v.dataset_lbl from ( SELECT ?dataset ?dataset_lbl , hsa_dataset_lbl, v.dataset_lbl from ( SELECT ?dataset ?dataset_lbl , hsa_dataset_lbl, v.dataset_lbl from ( SELECT ?dataset ?dataset_lbl , hsa_dataset_lbl, v.dataset_lbl from ( SELECT ?dataset ?dataset_lbl , hsa_dataset_lbl, v.dataset_lbl , orkgc:Metric as metric from hsa_dataset_lbl ) ) ) ) ) dt WHERE { ?metric a orkgp:HAS_DATASET ?	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the FTD dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 [[1]] In this query you can easily see which of the two different metrics are being used in the test dataset, or you can use the advanced filtering and sorting options. Also notice that one of the two criteria is being chosen for filtering because the other is not used in the model. The result will be a sorted list of models. You can get the first three models using the orkgp:HAS_DATASET query or the orkgp:HAS_EVALUATION query and the last three using the data in the dataset using the orkgp:HAS_METRIC query. For example let's assume that you want to filter models using one test metric per group and you want to use the results of the BLAS.fuzz test or the orkgp:HAS_DATASET filter to sort a table in order of the least performance. select name, model, eval, desc0, descl from dataset where metric and type='Fuzzy' and count(name) > 1 and desc2= ?name select name, model, eval, desc0, descl from dataset where metric and type='Lossy' and count >= 1 AND descl== ?desc0 and desc2= ?name select name, model, eval, desc0, descl from dataset where metric and type='Lossy',... output (Sparql query): { name : 'ftd', metric : 'ft', model : 'fp', model_lbl : orkgp:HAS_DATASET, eval : eval } select name, model, eval, descr1, descr2, desc0 from dataset where metric is lossy and type='Lossy',. select name, model, eval, descr	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Output: Output: The first two tasks in the paper require two different models, one labeled training data with the label 'image', the other labeled training data with the label 'training-problem'. You just use the label you want, the other model will do the conversion and make sure that the correct label is assigned. We need to change our CIFAR-10 training dataset to have a set of labeled data. The classification can't do the conversion, no matter what kind of label we use for training, we need a conversion. The conversion steps in the paper are so easy to understand. I'm not going to describe them here. I want you to practice the step of converting your pre-trained CIFAR-10 model to the training-problem. Let's start from your CIFAR-10 dataset and convert the pre-trained CIFAR-10 model to the training-problem. The first thing to do is to convert it to CIFAR-10. I recommend you to use a linear algebra algorithm and not a matrix math algorithm, a linear algebra algorithm will speed up the training a lot. (If you are using a matrix math algorithm, you're going to waste time on the conversion, make sure the label is a string) The code shown below is the most common way to convert to CIFAR-10, but I think it is worth pointing out that the best choice is using the code, if you want to keep the code and can change it, the code works. First thing we need to do is to insert our pre-trained model of image classification, the CIFAR10 we just converted, at the end of the dataset. The dataset has one field, the set of labeled data which we want to train on. Now let's have a look at the CIFAR-10 column in the	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------ 1) DESC(c.c.l.l.o.1:c.c.m.d) 1) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) 1) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) 1) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) 1) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) DESC(c.c.l.l.o.1:c.c.m.d) 1) DESC(c.c.l.l.o.1:c.c.m.d) DESC(	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark_lbl_lse. ?benchmark_lbl orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:P31 orkgp:HAS_BENCHMARK ?benchmark_lbl. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. ; } output (Sparql query): SELECT * FROM ?""benchmark_lbl"""" WHERE rdfs:label"" ?labels_ldb1_lc3_b1"" = '' OR rdfs:label"" ?labels_ldb1"" OR rdfs:label"" ?labels_ldb2_lc3_b1"" = '' OR rdfs:label"" ?labels_ldb2"" OR rdfs:labels_ldb3_lc3_b1"" = '' OR rdfs:label"" ?""labels_ldb1"" SAS input (English text): What are the title and ID of all standard SAS packages that are publicly available ? output (SMS query): input (English text): What is the maximum value of a column in a table defined by SAS ? output (SMS query): output (SMS query): SELECT MAX(column, 2) FROM column1 ORDER BY column; MaxValue = 0 TIP: We also provide the TIPs, a text listing of the most common T"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the code links in papers that use the POP3D model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 All options and examples are included together in the main README file inside the project folder. The Atari game design documentation has a table with an index for that data. You can download the first part of the data here. You can download the second part here. The results have all been analyzed on SPMage Pro (and, of course, SPMage). You can download the data you need to do this analysis (including all benchmark data and results) here.	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" { rdfs::HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp::HAS_DETECT ?metric_lbl. } } How to use it to make your code more robust¶ For example, consider a model that can determine which player scored more or less than the opponent based on a given set of score information, then use a Monte Carlo simulation to find which optimal strategy to use. However, this particular model might fail if the player's score information depends on the opponent and the opponent's statistics; if the player's statistics are not available in the data set, this model might need a special strategy to work correctly. As an easy example, consider whether the given model does the right thing when the model's best-in-class score is 0.00. How do we make a model that does not need a complicated set of strategies when such scores are not available ? There are two ways. One option, called the best-in-class strategy based approach, uses a simple strategy to choose the best strategy for the given Score metric: In this case, the user can specify the appropriate score metric on the model. The other method, called the strategy-based approach, uses many different strategies. For each strategy, we use one of them to determine the appropriate score metric for the given Score metric; this is called the strategy's probability function. Here is the code that uses the strategy-based approach: # This is the basic example: This model selects the best strategy according to # Score based on the user-specified metric, returns the optimal # match strategy for the Score metric. let best_best_score = Score ( ""RTE"" ) # The given score metric is inputted to the strategy-based approach. { rdfs::HAS_DETECT { ?dataset a. RTE"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT  { ?data ?dataset _id ( ?dataset_id )  , ?dataset :label ?dataset_lbl.   ?labels ?labels_{ ?dataset_id } orkgp:HAS_DATASET ?dataset.   ?cont orkgp:HAS_BENCHMARK ?dataset.   ?paper orkgp:P31 ?cont; orkgp:HAS_DADVENTORY ?dataset_lbl. } output (Sparql query):  SELECT  { - ?data ?dataset_id ( ?dataset_id )  , - ?dataset :label ?dataset_lbl. SELECT  - ?labels ?labels_{ ?dataset_id }  , - ?benchmark ?benchmark_{ ?dataseteorkgp:HAS_DATASET ?dataset. } The output from this query is the title of each paper mentioned in the query. There would also be another query to compare performance of models based on different metrics.  I like to imagine a set of competing benchmarks, and we would be selecting a new benchmark based on a set of metrics.  This query shows an interesting way to make the benchmark list more complete: $ > gbench benchdb2.py Benchmarks (GAD). Benchmark(name = 'GAD'),  [2],... $ > $ > runbench_gad.py Benchmarks (GAD),  Benchmarked	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): Select 1 from the papers with 'Formal Criteria for Statistical Testing' paper id output (Sparql query): SELECT ?id, ?name, ?status as title; ?id, ?name, ?status as title; ?name, ?status as title; input (English text): Which of the models submitted by the authors of the formal criteria papers can you evaluate ? output (Sparql query): SELECT ?id, ?id, : ?model id as ?max, : id as ?min, ?status as ?matches, ?date as ?published, ?title as ?finalTitle, ?paperspec as ?paperspec, ?name as ?finalName, ?finalYear as ?finalDecade, : ?max as { ?max.name ?max.ratio ?max.value ?max.rank ?max.statistic ?Max( ?matches"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Inputs: BODY: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3.7 RTE_SCRIPT1 v1.0.0 Input: 0 1 2 3 4 5 6 7 8 9 10 11 12 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3.7 RTE_SCRIPT1 v1.0.0	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?lbl ?name ?problem ?lbl ?name_lbl || ?obd || ?nvs || ?sus || ?scss || ?std || ?pub || ?convert || ?convert_lbl || ?convert_paper || ?convert_abstract || ?convert_annot { ?a:Type; ?b:Field; ?c:Term; ?d:Field; ?h:String; ?p:Pair; ?b_id:id; }	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT max(c.idx) + ?max(c.idx), -1, max(c.idx) + ?max(c.idx), max(c.idx) + ?max(c.idx), ?max(c.idx) - ?max(c.idx) as ?bestbenchmark ?mat, ?mat as ?best, ?best as ?best_score, ?best_score as ?best_rating, ?best_rating as ?paper, ?paper as ?paper_score, ?best as ?paper, ?paper_score as ?paper_rating, ?best_rating as ?foolscap input (English text): Can you list benchmarks performed by the paper fields ? output (Sparql query):   SELECT min( ?best. ?best_score, ?best_score ), max( ?best_score, ?best) as ?best, ?best_rating as ?best_rating, ?best_rating as ?paper, ?paper_score as ?paper_score, ?paper_rating as ?foolscap input (English text): Can you describe the most interesting problem ? output (Sparql query):  SELECT CATEGORY as problem, CATEGORY_NAME as difficulty, MAX( difficulty_factor ) as num_easy, MAX( difficulty_factor ) as num_common, max( difficulty_factor ) as num_hard, MAX( difficulty_factor ) as num_stupid as num_easy, MAX( difficulty_factor ) as num_common, MAX( difficulty_factor ) as num_hard as num_common, MAX( difficulty_factor ) as num_stupid as num_hard, MAX( difficulty_factor ) as num_stupid as	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"_____________________________________________________________________________________________________________________________________ First, we need to create the data. _______________________________________________________________________________________________________________________________________ setopt, (type=routines), {.data_dir = ""./data"";.dataset = ""./dataset"";.data_index = 1;.dataset.data_index = 10; } ____________________________________________________________________________________________________________________________________ getopt, (type=routines), {.data_dir = ""./data"";.dataset = ""./dataset"";.data_index = 1;.dataset.data_index = 10; } ____________________________________________________________________________________________________________________________________ lapply, (type=routines), {.data_dir = ""./data; "";.dataset = ""./dataset; "";.data_index = 1;.dataset.data_index = 10; } ____________________________________________________________________________________________________________________________________ ""ROTATE"" the queries The RTE datasets contain an array of variables with a number of variables in each. For performance purposes we have divided our dataset into three categories. - RTE: a set of two-dimensional arrays, each object with four values. In the RTE dataset we have not added any dimension or label. - RTE: a large set of one-dimensional arrays, each object with three values. - RTE: a second big set of one-dimensional arrays, each object with four values. Once you create the datasets, you can also set various parameters to your RTE queries. You can also set several parameters for the RTE query. But, to make the calculations in real-world scenarios you find it necessary to have them separately. RTE queries are performed within a query that we call ""rotate"". The basic RTE query: select * from rows The ""select"" RTE query: select row from"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT MIN( ?value) AS ?metric FROM ��.�� tbl WHERE ( ?value < 5) ) when the metric is not specified. 6.3.4. The following parameters have special meanings in a query planner or an Oracle RDBMS value When this parameter is used, the value used to produce the value returned by the metric in the output of SQL statements, as determined when the query is executed. If this parameter is not set, the value used to produce the value returns in the result of the query, as determined in the execution plan. If the default value is supplied, the default value in the execution plan used to determine the value is based on the metadata on the metric. There are four possible values for this parameter. set The default value for this parameter should be set to SQL_1. setmetric The parameter may be used to alter the metadata on the metric's default value, resulting in a different result set as follows. The metadata on an RTE query is processed through Oracle's Oracle Query Language. For RTE queries, the metadata defines a default RTE metric that defines the primary metric for the query. For unordered RTE queries, the metadata defines the set of primary metrics, based on the metric order and data type, that define what is most important to the performance of the query. This, in turn, uses the primary metrics as the default and selects the next primary metric based on	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE {... ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE {... ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) LIMIT 1 } F1_2.6.2 SELECT DISTINCT ?model ?model_lbl WHERE {... ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE {... ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVAL"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?solution ?solution_lbl ?solution_id FROM ?solution table. output (Sparql query): input (English text): Can you list benchmarked problems in the area of Semantic Web ? output (Sparql query): SELECT ?id ?task ?task_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""SemanticWeb"") ?project. ?project. ?task. ?task_lbl; orkgp:P30 ?RFID. ?project orkgp:P31 ?project. orkgp:HAS_BENCHMARK ?task. ?task rdfs:label ?task_lbl. } output (SQL query): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset ? query (English text): SELECT ?id ?task ?task_lbl ?solution_id FROM ?solution table. output (Sparql query): input (English text): How long does it take to finish benchmarking in seconds ? output (SQL query): query (English text): SELECT ?id ?task ?task_lbl ?solution_id FROM ?solution table. output (Sparql query): input (English text): Can you list benchmarked problems in the area of Semantic Web ? output (SQL query): SELECT ?id ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FIL"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT COUNT( ?) OF { ?dataset ?dataset_lbl; ?benchmark } FROM { ?dataset_lbl. ?labels_lbl; ?benchmark_lbl; } WHERE { ?labels_lbl!~ ?labels; ?benchmark_mld!~ ?benchmark; } Input: Name of the dataset, title OR title of the benchmark, ID of the analysis paper, or a plain string to be used for identification purposes. Output: SELECT... FROM ( SELECT... FROM { ?dataset_lbl } AS { ?labels_lbl } FROM { ?dataset_lbl } AS ?labels_lbl WHERE { ?labels_lbl >= ?labels_lbl ?dataset_lbl: ?labels} AND { ?dataset ?dataset_lbl }!~ ?labels; ) as { ?dataset_lbl, ?benchmark_lbl, ?benchmark_mld } OUTPUT: [ { ?dataset: ?labels}, { ?dataset: ?labels_lbl}, { ?labels_lbl >= ?labels_lbl ?dataset_lbl: ?lbl} ] Example: For further information about the methods involved in the comparison of performance, see this paper from the same researcher with more details. For example: Input: Name of the dataset, title OR title of the benchmark, ID of the analysis paper, or a plain string to be used for identification purposes. Output: SELECT... FROM ( SELECT... FROM { ?dataset_lbl } AS { ?labels_lbl } FROM { ?dataset_	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?model ?model_lbl ; input (English text): Which model has achieved the Highest F1 score on the Atari 2600 Alien dataset ? data (Alien): Metric. Object. Score. Object. Score................... F1 input (English text): Which model has achieved the Highest F1 score on the Atari 2600 Alien dataset ? output (Sparql query): SELECT ?model ?model_lbl ; input (English text): Which model has achieved the Highest F1 score on the Atari 2600 Alien dataset ? data (Alien): Metric. Object. Score. Object. Score.................... F1 input (English text): Which model has achieved the highest F1 score on the Atari 2600 Alien dataset ? output (Sparql query): SELECT ?model ?model_lbl ; input (English text): What is the highest F1 score you have achieved on the Atari 2600 Alien dataset ? output (Sparql query): data (Alien): Metric. Object. Score. Object. Score.................... F1 input (English text): What is the highest F1 score you have achieved on the Atari 2600 Alien dataset ? output (Sparql query): data (Alien): Metric. Object. Score.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 The benchmark results for benchmarks, both at the very low and high levels, are shown below: The benchmark results for benchmarks, both at the very low and high levels, are shown below: The data used to perform the benchmarking of Birdsnap are as follows:  Birdsnap 1.1.3  Birdsnap 1.2.0 Birdsnap 1.3.0 Birdsnap 1.3.1 Birdsnap 1.4.0  Birdsnap 1.5.0  Birdsnap 1.5.1 Birdsnap 1.6.0 Birdsnap 1.6.1 Birdsnap 1.7.0 Birdsnap 1.7.1 Birdsnap 1.8.0  Birdsnap 1.8.1 Birdsnap 1.9.0  Birdsnap 1.9.1  Birdsnap 2.0.0  Birdsnap 2.4.0  Birdsnap 2.5.1  Birdsnap 2.5.2  Birdsnap 2.6.0 Birdsnap 2.6.1  Birdsnap 2.7.0 Birdsnap 2.7.1  Birdsnap 2.8.0  Birdsnap 2.8.1  Birdsnap 2.9.0  Birdsnap 2.9.1  Birdsnap 2.10.0  Birdsnap 2.10.1  Birdsnap 2.11.0  Birdsnap 2.11.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?dataset ?dataset_lbl WHERE { ?question ?dataset. ?answer, ?problem a orkgc:Question, ?question question = ?problem_lbl ?question :Answer; ?test1 ?dataset ?dataset_lbl ?dataset_lbl ?label [ ?label. ?test1. ?test11. ?test18. ?test3. ?test5. ?test8. ?test15. ?test3. ?test15. ?test38. ?test5. ?test8. ?test15. ?test3. ?test8. ?test15. ?test38. ?test17. ?test3. ?test32. ?test8. ?test15. ?test3. ?test39. ?test5. ?test8. ?test15. ?test38. ?test17. ?test3. ?test32. ?test8. ?test33. ?test8. ?test38. ?test17. ?test3. ?test32. ?test8. ?test33. ?test8. ?test37. ?test8. ?test15. ?test3. ?test32. ?test8. ?test37. ?test7. ?test18. ?test37. ?test17. ?test3. ?test35. ?test8. ?test15. ?test3. ?test33. ?test8. ?test38. ?test7. ?test18. ?test39. ?test17. ?test3. ?test33. ?test4 ?dataset ?dataset_lbl. } input (English	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?con orkgp:HAS_MODEL ?model_lbl. } ORDER BY DESC( ?value) } GROUP BY ?metric ?metric_lbl ORDER BY DESC( ?value) Output from the previous query: Input data: Model name: orkgc : Driver model name: test2 : Target: Vehicle ID: 2   Target vehicle: 746   Target test: 3124   A test in a vehicle and 746 in a vehicle. 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1 The last query is a bit more difficult to explain but is worth mentioning because it shows how to make the same query in the RTE format: input (English text): RTE : 1. Data: vehicle: 746"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 To get the output of your query query, you can use the function queryOutput to call your query by name. For example, if you had the following definition of the output of a query query: ( ?metric ?dataset) | -------------------------- | Benchmark | Your query is a function containing the following: [ ?rdfs:filter. ?metric ] [ ?rdfs:label. ?criterion ] ; ( ?dataset is the argument for the function name queryOutput). You can use a parameter pack for parameter packs (see Parameterers). When you call a function, the function is invoked, and a response is received (if specified). This response is the result of the function invoked. A response may be a variable (usually, a list) or an object. Variables (variables that look like RDF objects) are passed to the function function by name, and an RDF object (a RDF object that looks like a model) is returned if the function returned an object. You can return an RDF object by calling the object method on a dataframe or by returning a list of RDF objects (list or flat). By default, RTE uses a dictionary-like structure to store metrics. By default, RTE stores all the variables in the table dataframe. For each metric variable in the table, if it exists, the metric is stored as a key and the variable is stored as a tuple. You may pass a name to the metric function to store the name and metric in the function's output (by assigning to it a value by returning a value by returning a function or named argument by passing a function argument to the function). For this reason, the format of the output is slightly different from the output from standard RTE. For example, RTE may use different formats for the output	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	  user@host:~/Data$ sqad1.1 --summary metrics | +----------+------+------+| | ID|Class|Metric|ModelID|ModelType | | 1| ?b | orkgp|RTE | | 2| ?a| orkge|RTE | | 3| ?d| ?g | orkgc:Dataset | | 4| ?k | ?m | ?n | | 5| ?n| ?s | ?n | | 6| ?s | ?l | ?w | | 7| ?w | ?l | ?w | | 8| ?g | ?n | ?x | | 9| ?d | ?c | ?x | | 10| ?l | ?s | ?x | | 11| ?b | ?c | ?c | | 12| ?c | ?m | ?n | | 13| ?s | ?c | ?h | | 14| ?h | ?l | ?l | | 15| ?g | ?n | ?u | | 16| ?i | ?m | ?e | | 17| ?h | ?l | ?e | | 18| ?l | ?e | ?y | | 19| ?m | ?e | ?z | | 20| ?g | ?n | ?z | | 21| ?c | ?e | ?x | +----------+------+------+ user@host:~/Data$ After some searching it seemed that the metrics in the Dataset of the SQuad1.1 dataset is not used, but I could not find the exact number. And the results were very confusing. I first tried to find the number of metrics in the SQuad1.1 dataset, for me its not	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT TOP(25) ( ?name, ?cont, ?benchmark, ?dataset, ?metric, ?metric_lbl, ?metric_lbl_rdfs, ?metric_lbl, - ?metric ) as ?list, ?value, ?metric, ?metric_lbl, ?metric_lbl_rdfs FROM papers WHERE author IN ( 'Andrew Ng ', 'Hwang Hyeop Choi' ) AND authors_code IN ( 'r/lmstm' ) ORDER BY DESC( ?name )  ; orkgp:HAS_MESSAGE ?message, ?message_descr, ?metric, ?metric_lbl, ?metric_lbl_rdfs ORDER BY ?dataset, ?metric, ?metric_lbl, ?metric_lbl_rdfs ) ROWS = 1;  select ?name, ?cont, ?benchmark, ?dataset, ?metric, ?metric_lbl, ?metric_lbl_rdfs FROM papers WHERE author IN ( 'Andrew Ng ', 'Hwang Hyeop Choi' ) AND authors_code IN ( 'r/lmstm' ) ORDER BY ?matrix, ?matrix_descr, ?metric, ?metric_lbl ORDER BY ?dataset, ?metric, ?metric_lbl, ?metric_lbl_rdfs ; orkgp:HAS_MESSAGE ?message, ?message_descr, ?metric, ?metric_lbl, ?metric_lbl_rdfs, ?metric_lbl, - ?metric	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the ACL Anthology dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?metric ?metric_lbl ------------------------- ------------ ORDER BY RANK DESC ( ?rank) -1 1.00 2.00 3.00 4.00 5.00 7.00 ----------------- ----------- ----------------- Roles Roles Roles Roles Roles Roles -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- Numeric rank: 1 2 3 4 5 ------------- ----------------- ------------- ----------------- ------------- ----------------- ------------- ------------- -1 1 2 3 4 5 -2 2 3 4 5 -3 3 4 5 6 -4 4 5 6 7 -5 5 6 7 8... ------------- ----------------- ------------- ----------------- ------------- ----------------- ------------- ------------- 1.1 2.3 3.5 45.6 700.7 701.8 702.1 3.2 5.4 7.7 8.9 11.0 12.1 12.4 12.8 12.9... ------------- ----------------- ------------- ----------------- ------------- ----------------- ------------- ------------- ------------- ------------- ------------- 7th rank: 1 2 3 4.6 3.7 5.8.9 714.10 715.11 716.12....... ------------- ----------------- ------------- ----------------- ------------- ----------------- ------------- ------------- ------------- ------------- ------------- ------------- 7th name: Tom B. Thomas The above example returns a rank of 971 out of 11,041,919. The output is formatted as follows, with each row consisting of a column-name followed by two numeric columns, one row containing a row-index, and the other containing the row-index of the second row: RANK 102130 RANK 971.0 2nd rank: Tom B	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" outputs: { :score (string, integer); :model String; :target Platform, integer; :benchmark Score; :query ""query the database"", String; :type DbQuery; :fields Fields, integer; :fraction (float, int, int, int; float[], int[], int[]; int[], int[], int[]; long[], int); :id id; :name (string, Integer); :source Source, Integer; :predictors Predicates, Integer; :processor Processor, Integer; :numPredicates NumPredicates; :predicted Predicates) ; :query_fctr (String, Integer, Integer, Integer, Integer, Integer);} Note That there are at least two possible ways to set up SQL queries in Postman : either it will be configured automatically (e.g. if you specify an environment variable in your POSTMAN setup) or you can use the manual configuration method described in the manual section of this document. For details of which method is used, please refer to the manual : manual Section 3.6. What databases Postman supports ? A SQLite3 database is supported, although Postman is configured to always use MySQL if there is a MySQL database available. How to set up Postman to use a database ? See manual Section 3.7. The process of building and using Postman is described in Section 3.6.1. Postman can search database names within a range. The search string is the name of the Database as a colon-separated list of words separated by spaces. How to search a database in Postman ? In this section: search_type = search_method, which is one of simple_search, simple_search_regex, or find_with_name. , which is one of,, or."	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Note that this is not exactly comparable to the NCBI's own ""Model Selection for HPC"" (www.ncbi.nlm.nih.gov/pfss/html/models.html): Model Selection for HPC Model selection is the process of constructing a database that is representative... Model selection is the process of constructing a database that is representative of high performance computing algorithms... HPC algorithms have been used to perform many applications including research on human cognition and behavioral sciences. The first one will be the most challenging, but it is also most interesting: How do you select a relevant data set that has already been selected by different algorithms in the past ? And how do you select this data set without making it boring and difficult ? Now we will see how one such selection can be achieved in Hive, by using a special kind of dataflow – dataflow analysis – that helps to overcome the data-driven limitations, limitations caused by the limitations in Hadoop. Figure 1 presents an example of dataflow analysis by using a special kind of Hive file ( Figure 1 ): it can be viewed as a simple SQL SQL database file in Hadoop. As in many other examples in this book, this file should be loaded into Hive with the ""pprint"" or ""import"" commands. The contents of the file shown in Figure 1 are the data for two models: one, chosen for the benchmark, and one randomly chosen for another. In the benchmark dataset, the data is available in several different formats (Text format, Excel format,.json,.csv) but in the random dataset, only the Texts are available. This kind of file creates a special Hive-specific version known as the ""HDF"" file, in addition to a standard output file. In this post, we will see two very simple approaches for dealing with Hive's standard output and"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 select output. score, output. count, output. avg., output. min., output. max., output. stddev., output. score, output. avg., output. min., output. max., output. stddev.;, output_lbl; In addition, you can also add additional data in a single query - see the reference below. To have a quick look at a few of the additional metrics and scores, see the following:                                            - Benchmarking scores with the new API This reference was originally created in the time period when the Atari Games software was being developed, and as such was not necessarily a complete list of the data that you could use, other than perhaps the overall metric and score. However, there are other sources published by other users, including  the.c.y.xml file from David Haddad,  the    (modified) DDP-2000 dataset    from  Brian M. Johnson,  and the    .d.xml   from Steve Siegel, all of which are provided in the same folder as the.c.y.xml file I included above. I encourage any interested user to use the new API to provide their own benchmarking information, and I will update this entry as additional datasets are published. As always, my apologies if there is a misunderstanding on the list, and happy benchmarking! I look forward to hearing from you.	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT * FROM ?""""|""{ ?dataset a, { ?label ?model orkgp:HAS_EVALUATION ?eval. ?paper ?iso-8859-1. || ?iso-8859-1. || ?iso-8859-1. || ?iso-8859-1. } ?model ?dataset_lbl. ?model_lbl. || ?model rdfs:label ?model_lbl. \ orkgp:HAS_BENCHMARK ?benchmark. \ orkgp:HAS_MODEL ?model_lbl. \ orkgp:HAS_PATTERN ?batch_size ?dataset. \ orkgp:HAS_PATTERN ?batch_size ?dataset. \ orkgp:HAS_RANK ?eval. \ orkgp:HAS_MODEL ?eval rdfs:label orkgp:HAS_RANK. \ orkgp:HAS_RANK rdfs ?: OR ( ?dataset_lbl orkgp:HAS_RANK orkgp:HADATA. \ ) ?dataset. \ orkgp:HAS_PATTERN ?dataset_lbl. \ ?dataset_lbl. \ orkgp:HAS_PATTERN ?dataset. \ orkgp:HAS_PATTERN ?dataset. \ orkgp:HAS_PATTERN ?dataset. OR ?dataset_lbl orkgp:HAS_BUNDLE_MARK. \ ) ?dataset. \ OR ?dataset_lbl orkgp:HAS_"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT y, x FROM test x WHERE y > 5 AND x < 10 AND y > 20"")), metric_rdf (rdf ( ""SELECT x, y FROM test x WHERE x > 5 AND x < 10 AND y > 20"")), y_metric (rdf ( ""SELECT y, x FROM test x WHERE y > 1 AND x < 10 AND y > 20"") ), x_metric (rdf ( ""SELECT x, y FROM test x WHERE x > 5 AND x < 10 AND y > 20"") ), x_score (rdf (). c (x_score)), y_score (rdf (). c (y_score)), metric_rdf ( ""select c(y, x) FROM test x GROUP BY c('y'), c('x')"" ),  x_score (rdf (). c (x_score)), y_score (rdf (). c (y_score)), """", metric_rdf ( ""select c('Y,'x), y, x, y, x' FROM test"" ))) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP (3) TITLES (1) SELECT TOP (2) ID, BASTARD_ID, HASH_STAT, R2_BASTARD, GAD_COUNT, AVERAGE(R2_BASTARD) SELECT TOP (3) DESC ( A (1), C (1), 'c ', '2'), BASTARD_NAME, HASH_STAT, R2_BASTARD, GAD_COUNT, AVERAGE(R2_BASTARD) SELECT TOP (3) DESC ( A (2), C (2), 'c ', '1'), BASTARD_NAME, HASH_STAT, R2_BASTARD, GAD_COUNT, AVERAGE(R2_BASTARD) SELECT TOP (3) DESC (A (1), C (1), 'c ', '1'), BASTARD_NAME, HASH_STAT, R2_BASTARD, GAD_COUNT, AVERAGE(R2_BASTARD) SELECT TOP (3) DESC (A (2), C (2), 'c ', '2'), BASTARD_NAME, HASH_STAT, R2_BASTARD, GAD_COUNT, AVERAGE(R2_BASTARD) SELECT TOP (3) DESC (A (2), C (2), 'c ', '0', '1'), BASTARD_NAME, HASH_STAT, R2_BASTARD, GAD_COUNT, AVERAGE(R2_BASTARD) 	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fine-Grained Gating"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): What are the authors and first names of the collaborators of the papers that benchmark the Fine-Grained Gating model ? output (Sparql query): input (English text): What methods, data sets and tests are used in these benchmarks ? output (Sparql query): input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model ? output (Sparql query): input (English text): What are the name and number of publication authors, coauthors, and abstracts ? output (Sparql query): input (English text): What authors and number of authors worked on each of the papers that benchmark the Fine-Grained Gating model ? output (Sparql query): input (English text): What are the names and number of authors of the collaborators of the papers that benchmark the Fine-Grained Gating model ? output (Sparql query): input (English text): Why is the number of publications of this paper higher than the reference count ? output (Sparql query): input (English text): What is the paper title ? output (Sparql query): input (English text): What is the abstract ? output (Sparql query): input (English text): What is the title of the paper ? output (Sparql query): input (English text): What were the abstracts about ? output (Sparql query): input (English text): What were the title and abstracts of the paper that used the benchmarks for this paper ? output	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Average Return (NoOp)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT STATEMENT_TIMESTAMP( ?timestamp ?), MODEL_TIMESTAMP( ?dataset), MODEL_BASE ( ?model) AS ?model_base FROM ( SELECT CASE WHEN STATEMENT_TIMESTAMP.datetime LIKE '/dev/urandom' THEN STATEMENT_TIMESTAMP.datetime ELSE STATEMENT_TIMESTAMP.datetime, CASE WHEN STATEMENT_TIMESTAMP.timestamp LIKE '/dev/urandom' THEN STATEMENT_TIMESTAMP.timestamp ELSE STATEMENT_TIMESTAMP( ?timestamp); END ) AND STATEMENT_TIMESTAMP.timestamp LIKE '/dev/urandom', MODEL_TIMESTAMP_REVIEW( ?dataset, ?model), MODEL_TIMESTAMP_RESULT ( ?dataset), MODEL_TIMESTAMP_RESULT ( ?model; ?dataset.hcid)) AS ?dataset_result; input (English text): What are the top 10 results in the TCP benchmark ? What do they show ? output (Sparql query): Output from the TCP benchmark: ------------------ --------------- Total Time 10.936s Connections 2514 Failed Connections 3132 Finished Connections 139875 ------------------ --------------- ------------------------------------------------	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT _LIMIT, ?dataset. ?cont OR ( ?dataset. ?cont OR ?benchmark OR ?cont OR ?benchmark. ?cont OR ?benchmark. ?code OR ?model ) ?dataset._LIMIT, ?model. ?codes OR ?model. 1. OR 3. 	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT CONTAINS( ?data ) AS ""Best performing model"", COUNT( * ) AS ""Number of data objects"", DIFF( ?b ) AS ""Average BLEU score"" FROM TOP( ?b, ?dataset. ?lbl_lbl ) LEFT JOIN( ?data, WMT2016_EUROPEAN_DATA) ON ( ?lbl_lbl. ?lbl_lbl_lbl ). ?lbl_lbl. ?lbl_lbl. ?lbl_lbl_id = ENABLE, ( SELECT ""RFE"" FROM ( ?lbl_lbl, ?lbl_lbl_lbl ), ?cust = CUST_BBL_CUST_LOOKUP, ?diss = CUST_BBL_LOOKUP, ?min_score = CUST_BBL_LOOKUP ) AS ""Best scoring"", COUNT( * ) AS ""Max score"", GROUP_BY( ?metric) AS ""Count"" GROUP BY ?metric ORDER BY DISTINCT ?metric output (Sparql query): What are the top ten models for BLEU and the best performing models on the RTE dataset ? output (Sparql query): ------------------------- TOP10 FOR BLEU RESULTS (10) ------------------------- 1. WMT2016_EUROPEAN_DATA 2. BLS01_YU_WMT2016 3. WMT2016_EUROPEAN_DATA 4. WMT2016_EUROPEAN_DATA 5. BLS01_RU_WMT2016 6. BLS01_RU_WMT2016 7. BLS01_RU_EVAL"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?metric ?metric_name ?value FROM ?Labs.model_test IF NULL OR FALSE EXISTS (""RTE"") GROUP BY ?metric ?metric_name. LEFT JOIN ?Labs.model_test ON ?metric_name.id = ?metric_id LEFT JOIN ?Labs.models.model_test AS model_test ON model_test.id = ?test.model_test.ID input (English text): Which method of measuring performance on RTE dataset are you using ? output (Sparql query): SELECT { ?metric ?metric_name }!!! WHERE RTE = 1; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 2; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 3; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 4; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 5; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 6; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 7; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 8; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 9; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 10; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 11; 1 row SELECT { ?metric ?metric_name }!!! WHERE RTE = 12; 1 row SELECT { ?metric ?"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value)))) ) AS ?dataset_num, RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value,RTE_SEQUENCE( ?value	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?Model ?Model_lbl WHERE { ?rdfs:labels ?labels. FILTER (str( ?labels ) = ""F1"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?labels ) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } Notice that the model is a unique model for each dataset, which saves space even in the largest data sets. In fact, you can access the model name in the Sql Query using the ?label argument. This can be useful for a query like: ""SELECT "" ""model"" """" { ?model_lbl a orkgc:Dataset; ?labels a. } "" WHERE { ?rdfs:labels ?labels. FILTER (str( ?labels ) = ""HoC"") ?dataset a; ?dataset_lbl a orkgc:Dataset; "" } "" ORDER BY { ?value } LIMIT 1 } "" which can be made more readable if you remove the comma: "" AND {.name a."	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ?dataset. ) AS nd2lbl, ?dataset_lbl as ?dataset_lbl * 1.2, AS nd4lbl, ?dataset_lbl as ?dataset_lbl ?dataset_lbl as ?dataset ? FROM ( SELECT nd2lbl, ?dataset_lbl AS lng, 1.1 AS nd2lbl ** 2, rss as rsp, 1.2 AS nd4lbl, 1.2 AS nd8lbl, 1.2 AS nd12lbl, 2.0 AS nd21lbl AS sb_hq, 2.0 AS sb_pk, 5.0 AS nd50r, 5.0 AS nd52r, 5.0 AS nd80r, 5.1 AS nd100r, 6.0 AS nd106r, 8.0 AS nd112r, 5.0 AS nd120r, 9.5 AS sb_pk, lng_lng, nd8r, nd12lng, nd32lng, nd60r, nd75r, nd100r, nd120r FROM sbl AS sb_lng INNER JOIN ( SELECT * FROM (SELECT rss AS rsp, nd2lbl ** 2, rss as rsp FROM hcasas AS hcm HCSAS AS ai JOIN theanode AS th EASR AS ai ON theanode. hc_base_path = ai.	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Output is used to analyze the model's performance. To evaluate how well the model executes the RTE data, you pass the RTE results to the orkgp variable. For example, to evaluate the Y-MISSILE-RTE model on all datasets in the RTE data, you can run the following: orkgp:RTE( ?dataset); to be passed the result of orkgp:YMISSILE_RTE_EXPLAIN. Finally, we need to pass the output of orkgp to the benchmark variable. This can be done with the following: orkgp:RTE( ?dataset) ; output = RTE( ?dataset). orkgp=BENCHMARK. After the benchmarking is done, the model receives an overall score. The most important number is the overall score with the sum of the individual scores. If for example output (X) = (10+1)/100 then the model scored 10 points. BENCHMARK stands for Benchmark-Average-Impact. It is a metric that provides a representation of a model's performance relative to the benchmarking result, with a maximum score of 100. For example, the best score of 20 means the model scored 20% more than the benchmarking result. The following command outputs the performance of the single output variable, which is RTE. output (X) = RTE. You can also use RTE in any of the two existing datasets you want to use. You can run the following with RTE as the first output variable and then run RTE in the second variable: RTE( ?dataset). If you want to use a different set of datasets then just pass the RTEs through the new <- parameter	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): Is there a problem with the FSNS - Test dataset ?, with the metrics, or not ? If so, what is it ? output (Sparql query): OK. Input : ( ?problem ? ) Question : Output : Some time later, the same user submitted a similar question from a different thread. This time, the questions were posted with additional information. This time, the questions had different answers, and had slightly different answers to the user-response questions. However, the user's original question was still correct. As with the previous two posts, we decided to examine the user submission, in order to identify how common the same user submitted questions in this manner. The results of this look at a subset of users in a certain subreddit: (Note: I used the keyword 'user'. That is, to be exact, we used the word 'user_id', and 'user_name' because both the original questions, and the question rephrased in the second thread were all on the same user, and not distinct users.) If the user who submitted the three questions to reddit were a human, we would expect to see one of them in the top 20% of all submissions, at about 17% chance. If we used a Bayesian approach, we would be able to conclude that at best, we saw one percent chance, if not 2% chance. Of the 22 results, we saw two questions that appear to be from the same user in the top 20%. It is worth noting that a few of the queries, such as, 'how many people are in China today,' appear in two different threads, so they will be counted twice. What follows is an analysis of the likelihood that these two queries, from completely different perspectives, were submitted by the same user, and whether this indicates the same user	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPAD-path"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?problem1 ?problem2 ?p. ?p. 1. ?p. 2. ?p. 3. SELECT ?pp1. ?pp2. ?p. 1. ?pb. ?2 SELECT ?pp1. ?pp2. ?p. 1. ?p. 2. ?ps. ?2 SELECT ?pp3. ?p ?p1. Selecting "" ?"" in output format. input (English text): Which of the following problems is one of the benchmarks that was used in the Semantic Web benchmarking ? output (Sparql query): { ?hr ?hr1 ?hr ?hr2 ?hr ?hr3 } This output includes both the benchmarked problems and all their problems. You've probably had this error before and no help was given. It's especially frequent if you're testing multiple MPAD domains. Make sure to include the 'MPAD-path' option in your query. The MPAD path is an MPD module name that consists of a prefix and a path separated by a colon (':'). It is a prefix that you must have installed, but not the path from which you must use. This is because the name is used as the prefix for each MPAD process, not the process itself."	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the BoolQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?score ?metric ?metric_lbl (MAX( ?value) AS ?score) INNER JOIN ( ?label ?dataset_lbl JOIN { ?retval ?data} AS ?value) ON ?dataset_lbl.dataset_id = ?label. ?dataset_lbl VALUES (4, 2) (4, 2) --------------------------- 1 | CIDR 2.14 3 | GLSL 1.94 4 | RPE 2.34 5 | RTE 2.18 6 | DBSK 1.67 7 | CIDR 2.06 8 | GLSL 1.53 9 | RPE 2.08 6 6.8.1: 1. RTE 3 3.2.1: 1. CIDR 1.97 2. CIDR 1.68 3. DBSK 1.65 4. RPE 1.68 5. DBSK 2.15 6. CIDR 1.58 7. DBS	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?countries FROM ?datasets. ?countries ?countries_lbl (SELECT ?countries FROM ?datasets. ?countries. ?countries_lbl ?countries_lbl ORDER BY ?countries_lbl. ?countries_lbl. ?countries_lbl. ?countries_l	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl ?dataset_wcd ?dataset; output { ?dataset rdfs:label ?dataset_wcd. ?dataset lbl-kp:WAS_DATASET ?dataset. ?dataset wcd if ""Dc:WICd_Kp_PCR-2-kp.HOC"" then ?dataset rdfs:labels ?dataset_wcd. ?dataset dmcontrol100k if ""dmc_100k.HOC"" then output (Sparql query): OPTIONAL { ?dataset rdfs:label ?dataset_wcd. ?dataset wcd if ""Dc:WCBd_Kp_PCR-1-kp.HOC"" then orkgp:HAS_CATCH ?model. ?model rdfs:labels ?model_lbl. ?model if ?model. ?model rdf:labels ?model_lbl. } options: ?model rdfs:labels, :label ""Dc:WCBd_Kp"", ""Dc:WCBd_PCR-1-kp"". ?dataset rdfs:label, ?dataset_wcd, ?dataset_wcd. :dataset wcd ? Dc:WCd_Kp ?dataset, rdfs:labels return [{ ?dataset rdfs:label ? ""Dc:WCBd_Kp"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM papers IN JOIN orkgp ON papers.code_lbl = orkgp.code_lbl INNER JOIN orkgp ON (papers.classification xs-2s.classification) INNER JOIN orkgp ON (paper.code_lbl xs-3s.code_lbl) WHERE { ?hl in (HL.idx or HL.name, HL.idx.field) ?code_lbl.h_labels ?code_lbl.labels. ?hl rdfs:id.code_lbl.code_lbl. OR ?hl and ( ?code_lbl.h_labels.1 ?code_lbl.h_labels.2, ?code_lbl.h_labels.2.3, ?code_lbl.labels.2.4, ?code_lbl.labels.3.1, ?code_lbl.labels.3.2, ?code_lbl.labels.3.3, ?code_lbl.lbl.label ?hl in (HL.idx or HL.name, HL.idx.field) ?code_lbl.hs_labels ?code_lbl.hs_labels. OR ?hl and ( ?hl.idx.field ?code_lbl.hs_labels ?h_labels.1, ?hl.idx.field ?hl.hs_labels ?e_labels.1, ?hl.idx.field ?hl.hs_labels ?e_labels.2, ?hl.idx.field ?hl.hs_labels ?e_labels.3, ?hl.idx.field	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per dim"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT 'best' AS 'best', ?a AS 'a', ?r AS 'r', ?id AS 'id', ?benc AS 'bencher', ?paper AS 'paper', ?paper_lbl AS 'paper_lbl', ?benchmark AS 'benchmark', ?benchmark_lbl AS 'benchmark_lbl', OR kgc:Benchmark orkgp:Benchmark_lbl ?benchmark. output (Sparql query): input (English text):... Input to ROC curve (from the GAD2 dataset) for a simple 2-layer recurrent convolutional neural model. The output is a single value for the best (most likely very wrong) prediction of the output, but the input is actually 2 input vectors, where the 'i' stands for prediction. In summary, for the 2nd half of the convolution chain (that is, the convolution of the input vectors and the one for the output), it will depend on the input layer, it will depend on the input layer's size, it will depend on the input layer's accuracy, and it will depend on how the output is computed, and it will depend on the type of the output itself, for the 3d training set. The most important factor is, of course, how the input is shaped—how is the input shaped to solve a given classification problem ? It may seem like there's a ton of information here, but don't let this confuse and overwhelm you. One thing that jumps out at me is that I don't hear people talking about how the output is computed, but the structure of the input. For instance, here's one of the first papers I found on this topic—which doesn't have a great paper quality reputation—called ""Training on Classifier-Based Non-Linear Regression for"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Output (Text based): This example can be extended. Example 2: Evaluate multi-object metrics on multiple datasets. This example shows a simple way to evaluate multi-object metrics and then use them in conjunction with the model results. import numpy as np import numpy.ndarray as np # Create datasets based on the RTE Metrics. dataset_1 = ""dataset_1""; dataset_2 = ""dataset_2""; dataset_3 = ""dataset_3""; # Determine the metrics of the multi-object datasets. NGRmsExpectations = None; NGRmsExpectations_1 = None; NGRmsExpectations_2 = None; NGRmsExpectations_3 = None; for num in dataset_1: NGRmsExpectations_1 = NGRmsExpectations[num].fit; NGRmsExpectations_2 = NGRmsExpectations[num].fit; NGRmsExpectations_3 = NGRmsExpectations[num].fit; NGRmsExpectations_1 = np. nn. NN. fit ( dataset_1, ""RTE"", 2, num ( dataset_1 )) NGRmsExpectations_2 = np. nn. NN. fit ( dataset_2, ""RTE"", 2, num ( dataset_2 )) NGRmsExpectations_3 = np. nn. NN. fit ( dataset_3, ""RTE"", 2, num ( dataset_3 )) NGRmsExpectations_1 = NGRmsExpectations[num].fit; NGRmsExpectations[num].fit = np. mean ( dataset_1, df = [ 0, 1 ] *"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Are children examined in the studies?
 output (Sparql query): "	"ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, ""ildren"")) } "	"SELECT DISTINCT ?publisher OR ?publication_timestamp ORDER BY author_id END AS published_timestamp; input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model_lbl orkgl:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE || ?code; } input (English text): Are children examined in the studies ? output (Sparql query): input (English text): I want to review the publication history, in cases when some papers used the VPN Model for benchmarking purposes. Can I use this functionality ? output (Sparql query): SELECT DISTINCT ?publisher OR ?publication_timestamp ORDER BY author_id END AS published_timestamp; input (English text): Where can I find code references in papers that have used the VPN Model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model_lbl orkgl:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # -*- coding: utf-8 -*- # ---------------# # ------------== # # | Example | # # | ---------------------------------- # # | 1. A 1-gram word with its 1-gram representation as an index: # # (in the English language) 'fr' 'ge' # # | 2. Comparing two 1-gram words that do not contain the same # sequence of phonemes: # 'ger' (english) 'ger' (german) # # 2. Comparing two 1-gram words that are different in two ways: # `1' (1 letter) and `2' (2 letters) # # 3. Comparing two 1-gram words that are similar in many ways: # `2' (1 word/phrase) and `3' (2 words/phrases) # # 3. Comparing two 1-gram words that are similar in many ways: # `1' (1 thing) and `2' (1 expression/word) # # 4. Comparing two 1-gram words that share some characteristics: # `2' (1 sentence/sentence) and `1' (1 piece of speech) # # 5. Comparing two 1-gram words that do not share any characteristics: # `1' (1 entity) and `2' (2 entities) # # 6. Comparing two 1-gram words that share some characteristics: # `1' (1 thing) and `2' (1 phrase) # # 7. Comparing two 1-gram words that share some characteristics: # `2' (1 sentence/sentence) and `1' (1 sentence/phrase	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 We used SPSS for analyzing the datasets (Lime and GADs data) and the Kaggle competition for data analysis. To be able to explore, we took the dataset of GAD and analyzed it using SPSS v16.0.0 and SPSS v17.0.2. In the figure below I have annotated data of GAD and the Kaggle competition. SPSS v16.0.0 The figure below shows the benchmark (Benchmark) score for the GAD dataset on the ImageNet V2 network-based classification. You can clearly see the difference between the performance to the reference dataset and the GAD dataset with their same features. Although there is a wide gap it is not very obvious to the readers. It looks like there is a difference of 200% on the benchmark with the difference of 200% on the GAD dataset. The following figure shows the SPSS plot of benchmark performance for the GAD dataset on the network-based classification. Again we see that not only the benchmark score differs between the reference dataset and the GAD dataset but also the difference is 200% in the performance. This means that the Kaggle competition dataset may well be good for data analysis but it is still not the best dataset for an analysis.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 For more information on the results, please see the paper's GitHub repository. An associated blog post is available at   http://blog.fasterat.org/2014/11/the-gad-benchmark-2014-the-new-datasets/.	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl( ?value) FROM ?met ( SELECT count( ?name ) FROM ?met ) RDATA: { ?metric orkgp isp ?val. :metric rdfs:label ?metric_lbl} To add custom queries to queries in the RTE dataset, use the RTE_QUERY function. Using the RTE_QUERY function provides a number of conveniences, including a standard column order to group by and, a way of aggregating metric values and some general function to create custom queries using the metrics that are returned by a query. For some examples of the different kinds of queries that might be created using the RTE_QUERY function, see Table 5 below. You can find a comprehensive description of what the functions provide, along with examples in the documentation, by visiting the source code. 5.1. Example 4.1. The SUM formula To sum the scores for a query using the RTE_QUERY function, you create the SUM function: (SELECT value FROM ?met || ?rank ORDER BY ?value DESC LIMIT ?metric_lbl) The standard definition of SUM includes the following columns: val (variable) The result variable. The standard way of specifying the sum of scores is with the VAR or CVAR syntax. The argument given to RTE_QUERY is used by the default formula parser in the RTE R package. If you type in RTE_QUERY(val, RTE_QUERY_SUM). It should produce the following output:  [ ?name  =  ?value  , ?r  =  ?rank  , ?m  =  ?metric  ,   ?c  	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?model orkgp:HAS_MODEL_SYS ? OPTIONAL { ?model orkgp:HAS_MODEL ?model_lbl RDFS:LABEL ?model_lbl. } The last step involves the creation of a Hadoop-specific partitioning table. The partitioning file provides two different types of partitions. A primary partition is a partition of data that does not contain any other data. For example, an RDF database does not contain any tables. A partition of data called a secondary partition contains a single table or a set of tables. In the default example, the partitioning file in the Multimodal PISA dataset contains one primary partition and three secondary partitions. A secondary partition also can contain duplicate primary partitions. The following are some examples: primary partition: rows: 1 2 3 4 5 6 7 8 9 10 rows: 1 2 3 4 5 6 7 8 9 10 secondary partition: rows: [2, 3, 4, 5], [6, 7, 8, 9], [2, 3, 4], [3, 5, 6], [7, 8, 9] rows : 3 4 5 6 7 8 9 10 To create a secondary partition, you should put the secondary partition in the database as a separate table before the primary partition. For example, if you wanted to create a secondary partition"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT { }_SUM( ?metric, ?value ) AS { score > 200 ?value:N ?(N=150, ?value= 0 ?scale orkgc:Dataset; orkgc:HAS_TRAIN ?label ?labels. ) } FROM { ?dataset orkgc:TIMELINE } GROUP BY ?metric ?metric_lbl ; ?cont orkgc:HAS_BENCHMARK ?benchmark. Input (English text): Can you list the different metric used on RTE datasets to evaluate models ? # -*- coding: utf-8 -*- # -*- # from  https://raw.githubusercontent.com/tutorial/ts-dataset-models/master/ts.yml # from datetimes import datetime # import pandas as pd # import sklearn as skl # import numpy as np # from scipy as np # as numpy # from sklearn.datasets import dataset # from sklearn.pandas import mtcars # to avoid copying scipy objects from'scipy.optimize.scipy' # -*- # The function kw_test, used to test if models can learn to predict, is imported from scipy.optimize.scipy # -*- # sklearn defines three types of data types in the package: # 1  -  'float': float # 2  -  'string': string # 3  -   'boolean': boolean # The most common use of these types here is to define the 'class' of models you want to train/test. # To define a class a list	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Zaxxon"" ) ?benchmark orkgp:HAS_DATASET orkgp:HAS_EVALUATION orkgp:HAS_METRIC ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. } } I hope the example above illustrates the concept of the parallel benchmark and how to use Parallella benchmarking with RTE to evaluate the performance of RTE based programs. In my next tutorial, I provide a step by step guide to implementing the parallel benchmark described here and evaluating the results. In particular, the algorithm for selecting the RTE-based metric from the set of all possible metrics is included in the previous tutorial. Further reading For further reading, I wrote a book on parallel computing called Parallel and Distributed Programming for Embedded Systems."	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(1)   ?model ?model_lbl* ?metric/value. orkgp:HAS_DATASET :   ?dataset_lbl  orkgp:HAS_EVALUATION :   ?model_lbl.   ?model rdfs:input(English text): Can you list the models that have been evaluated on the HoC dataset ? output (Sparql query): SELECT TOP(1)   ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"")) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model._lbl. } What is your answer  to the following question ? 1. Can you find a dataset where your top model has a good testability score as good as your best model from other benchmark datasets ? 2. What is the difference in testability between the best model from these two datasets and the best model from the dataset I've chosen on random testing and all other databases ? 3. For each of these three comparisons, how much of a difference can you measure ? If the best model can be found in the dataset without any problems, does that mean it must be the best model for all datasets ? (I.e. If your best model can be found in a dataset, that should mean it must be the best for all"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Sentence Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT c FROM (SELECT x FROM test_text x WHERE x.length > 6 && x.length <= 18), (SELECT y FROM test_text y WHERE (y.length > 6 && y.length <= 18) AND x.length > 6) AND (SELECT x FROM test_text x WHERE x.length > 18 AND x.length <= 18) AS DISTINCT_F1 output (Sparql query): SELECT ?dataset ?dataset_lbl. name AS c. name, c. score, ?metric, ?value, (SELECT x.length, ?metric_lbl. name FROM TestF1 x JOIN d2d rdfs c2 d1 f1 f2 rdfs c3 c4 rdfs c5 c6 ROW_NUMBER(x.length) AS c. score, ?metric, ?value, (SELECT x.length, ?metric_lbl. name FROM TestF2 x JOIN d2d rdfs c2 d1 f2 f3 rdfs c3 c4 rdfs c5 rdfs c6 ROW_NUMBER(x.length) AS c. score, ?metric_lbl. name FROM TestF3 x JOIN d2d rdfs c3 c5 rdfs c6 ROW_NUMBER(x.length) AS c. score ), ?cont (select x FROM (SELECT x.length FROM TestF1 x WHERE x.length > 6 && x.length <= 18), (SELECT q FROM TestF2 (SELECT z FROM TestF2 z WHERE z.width > 6 && z.width	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT 'Benchmark', ?benchmark ?benchmark_lbl, ?lbl1, ?lbl2, ?lapd, ?lpd, ?kmeans, ?logrank, ?pkf, ?pkp, ?pkq, ?plr, ?pkm, ?pkq, ?pkw, ?pkt, ?pqd, ?qd, ?qdf, ?qpk, ?qo, ?qr, ?qw, ?qx, ?qy, ?qzu, ?ry, ?rz, ?sd, ?sys, ?tsc, ?tz, ?tzm ( ?dbf ?)) ?dataset. pkf. pkp. pkc. pkm. pkmf. pkpf. pkpk. pkq. pkqq. pkqq. pkqp. pkqpk. pkqp. pkqqq. pkqqq. pqd. cntr. rbw. rbs. rbf. rbx. rbp. rbq. rbq. rbq2. rbq2. rbq3. rbq3. rbq4. rbq4. rbqp. rbt. rbqr. rbt2. rbt3. rbt4. rbt4. rbtp. rbtq. rbk. rbtp. rbkp. rbkp. rbkq. rbkqp. rbkqq. rbkw. rbtx. rbpw. rbs. rbq. r	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } The output is much more readable with less red font :<  The raw data in this case is taken from my blog posts. As you can see from the table on the left, the model score shows very good correlation with other parameters (as you can see the model score is very close to the sum of all parameters, the difference between the model score and the sum of the scores shows it is an excellent fit, and thus it is a very good classifier). It's interesting that, in this one case, the raw datasets are already quite clean, so I can't find any hint of other metrics being used to evaluate the model, however, the one metric of interest is the one with the largest error in the graph above (the one with the largest error in the output from the graph). The output from the graph with this metric, that is, the error in the data, is more than 10% of the score - it's not uncommon to see models with many error. The one metric that I was interested to see how the model would perform with was the (log 2 ) loss, that is, the squared bias loss. I thought that it might show a good model with high error that didn't need much tuning, so"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tokenlearner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Include the token learner reference for each paper ? output (Sparql query): input (English text): Which of these benchmarks has the highest success rate ? (1 point for success, 10 points for failing) output (Sparql query): Inputs: p1, p2, p3, p4, test, n, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16, p17, p18, p19, n, p20, p21, p22, p23, p24, p25, p26, p27, p28, p29, p30, p31, p32 Outputs: n | success | failure | p1 | p2 | p3 | p4 | test | p5 | p6 | p7 | p8 | p9 | p10 | p11 | p12 | p13 | p14 | p15 | p16 | p17 | p18 | p19 | p20 | p21 | p22 | p23 | p24 | p25 | p26 | p27 | p28 | p29 | p30 | p31 | p32 | p33 | p34 | p35 | p36 | p37 | p38 | p39 | p40 | p41 | p42 | p43 | p44 | p45 | p46 | p47 | p48 | p49 | p50 | p51 | p52 | p53 | p54 | pr| Outputs: n | success | Failure | (1)p1 | (20)p2 | (20)p3 | (20)p4 | (20)p5 | (60	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the MedSTS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT MODEL_CONSTANT_LENGTH ?test_list ; name rdfs:name orkgp:HAS_DATASET ?test_list. name orkgp:HAS_SOURCE_CODE ?test_list. name orkgp:HAS_TYPE_TYPE orkgp:HAS_CODE ?test_list. name orkgp:HAS_CODE ?test_list. name orkgp:HAS_NAME. If you wish to embed a model, a model must be specified. In the next tutorial, part two, we will dive deeper into the MedSTS dataset to develop more complex models.	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT t.count(r.c_freq * t.m_i) as F1_frequencies; t.count(r.c_freq * t.m_i); What is the best performing model benchmarking your algorithms ? input (English text): What was the maximum run time for the Benchmarked Model ? output (Sparql query): What was the maximum run time for the Benchmarked Model ? input (English text): Tell me about some of the code examples that you used in your benchmarkings. output (Sparql query): Tell me about some of the code examples that you used in your benchmarkings ? input (English text): What was the number of times that you had to update or delete the data files in the model ? output (Sparql query): What was the number of times that you had to update or delete the data files in the model ? input (English text): What are the features used in your Benchmarked model ? output (Sparql query): What are the features used in your Benchmarked model ? input (English text): Are there any other Benchmarked models that are more efficient for Benchmarking CoNLL datasets ? output (Sparql query): Are there any other Benchmarked models that are more efficient for Benchmarking CoN	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT RANGE BATCH_NUMBER 1 ?lasso AS LAGO_PERCENTAGE ?lasso AS LAGO_CONSTITUTION ?lasso AS LAGO_CONSTITUTION_LAMBDA ?lat, _, qmin_lat, _, qmax_lat, _, rmin_lat, _, rmax_lat, _, q_min_qmin, q, q_max_qmin, _, q_rmin_qmin, q, r_max_qmin, _, q_max_rmin, q, r_target ?p, q AS PERCENTAGE_MIN ?p AS PERCENTAGE_MAX input (English text): How many tests have been done on the Lasso tool from LÖV ? output (Sparql query): input (English text): How many tests have been done on the Lasso tool from LÖV ? output (Sparql query): Input values: 10, 60, 120, 80, 120 output values: input (English text): Given a sequence of numbers 0, 10 and 70, how many times do you expect there to be a sequence that is identical to the previous sequence ? output (Sparql query): SELECT COUNT(DISTINCT id) FROM NUMBER_OF_NAMES WITH 1 AS id, 0 AS name FROM NAMES input (English text): Given a sequence of numbers 0, 10 and 70, how many times do you expect there to be a sequence that is identical to the previous sequence ? output (Sparql query): SELECT COUNT(N	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	--------------- --------------------------------------- { ?test paper_ID( ?dataset_ID ) ?data %>% select!is_null ?labels \ case ?labels when 'hoac' and ?labels.~ ? 'hoch' ?data[ ?dataset ?] \ when 'hoac' and ?labels.~ ? 'hoch' & ?dataset_lbl } \ <dbl:id=42> { ?test paper_ID( ?dataset_ID ) ?data %>% select!is_null ?labels \ case ?labels when 'hoch' ?data[ ?dataset ?] \ when 'hoac' & ?dataset_lbl } \ <dbl:id=43> { ?test paper_ID( ?dataset_ID ) ?data %>% select!is_null ?labels \ case ?labels when 'hoac' ?data[ ?dataset ?] \ when 'hoac' and ?labels.~ ? 'hoac' & ?dataset_lbl } \ <dbl:id=43> { ?test paper_ID( ?dataset_ID ) ?data %>% select!is_null ?labels \ case ?labels when 'hoac' ?data[ ?dataset ?] \ when 'hoac' & ?dataset_lbl } Output: [test/hoac/hoch/hoch/hoch/hoch], ... and their IDs ? output (Sparql query): ------------- --------------------------------------- { ?test paper_ID( ?dataset_ID ) ?data %>% select!is_null ?labels \ case ?labels when 'hoch' ?data	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ImageNet + iNat on WS-DAN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	-------------- ---------------------------- RTE-10.0 ImageNet/RTE/1 v. 1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/2 v. 1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/3 v. 1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/4 v. 1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/5 v. 1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 imageNet/RTE/6 v.1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/7 v.1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/8 v.1 Output (Sparql query): ---------------------------- ---------------------------- RTE-10.0 ImageNet/RTE/9 v.1 output (Sparql query): --------------------------- --------------------------- RTE-10.0 ImageNet/RTE/10 v.1 output (Sparql query): ------------------------- ---------------------- output (Sparql query): ------------------------- ----------------------- RTE-10.0 ImageNet/RTE/11 v.1 output (Sparql query): ------------------------- ----------------------- RTE-10.0 ImageNet/RTE/12 v.1 output (Sparql query): ------------------------- ----------------------- RTE-10.0 ImageNet/RTE/13 v.1 output (Sparql query): -------------------------	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } In this query the first option is ""benchmark"" and the 2nd option ""evaluation"" with one row for each metric ""model_lbl"" and with a fourth row for each metric ""benchmark"". I thought the output format seemed appropriate to me. I am a little hesitant to assume that the output is generated by the same data scientist, but I can understand your concern. After all, the data scientist is the only person in the world that will be able to generate the data. You are probably one of those few that you believe has the right skills set. So we can see in that table that for each metric, the author generated the model model_lbl and has published them to GitHub. So it makes sense to assume a similar level of skills are required for a model. The issue I have is the ""ORGKP"". If we consider the output of the benchmark data set, model_lbl looks"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset.. ORG. rdfs:categorical ?code ORG. categorical ?code. ORG. rdfs:categorical ?code. ORG. categorical ?code. ORG. categorical ?code. ORG. categorical ?code. ORG. categorical ?code. ORG. categorical ?code. ORG. categorical ?code. ORG. categorical ORG. omm :categorical. ?code ORG. dbo :categorical. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG. ORG	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP ( 1 ) code. avg_score The model is tested on four datasets: http://arxiv.org/abs/1407.0913:SUN:LSTM:COUNTRIES:CURRENTS-1.csv https://pbs.twimg.com/media/CJE0LXr1AACg.jpg (note the URL is an arbitrary one, we're just using it to indicate which dataset is being tested) http://arxiv.org/abs/1101.0565:STANDARD:BLASL:COUNTRIES:COUNTRIES.csv https://pbs.twimg.com/media/CJZcDnY5Z7D1o.jpg (note the URL is an arbitrary one, we're just using it to indicate which dataset is being tested) https://pbs.twimg.com/media/CJeKFQJ6uPw4M.jpg (note the URL is an arbitrary one, we're just using it to indicate which dataset is being tested) http://arxiv.org/abs/1101.0565:STANDARD:BLASL:COUNTRIES:COUNTRIES (note again the URL is an arbitrary one, we're just using it to indicate which dataset is being tested) The benchmark score is the mean of each score taken by the model. The average score is the sum of all scores. The performance metric is the average performance on different datasets. The benchmark score is usually interpreted to be the highest average performance on a dataset, which I would expect would indicate a high-end model. With this metric, the model is generally considered to have a	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT RTE.ORKGCOST orkgc:Dataset; rdfs:label ?metrics ?metric_lbl ?benchmark ?metric_lbl ORDER BY RTE.ORKGCOST orkgc:Dataset; orkgc:HAS_DATASET ?metric_lbl; orkgp:HAS_LIMIT ?metric_lbl orkgp:HAS_DATASET orkgp:HAS_EVALUATION orkgp:HAS_METRIC orkgp:HAS_LIMIT > ORKGCOST orkgc:Dataset> ""ORKGCOST"" > ORKGCOST orkgc:Dataset > ORKGCOST > ORKGCOST orkgc:Dataset > ORKGCOST"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl. WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CASTRIAN"") ?statistics_lbl. { values int, name text, description text, performance int } } For non-trivial projects, I prefer to write a couple of SQL statements (and run them in some kind of environment) that make use of the database connection. There are many other ways to write your queries: if/if... constructs, database-specific expressions and even conditional logic. None of them are hard for someone to implement and it's all nice to have a choice. And then there is the question of which queries are supported on each interface and how they are processed with the other data. For a lot of scenarios (especially production), an ORM doesn't help; a relational database doesn't exist. Or maybe the ORM and/or a language like Java would support the right query when you write it manually (that's where the ORM part ends). It's an extremely complicated question and is often a question I am asked in the comments of the blog posts. I think the problem is with the developers, not the data model. To keep it simple,"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT SINGLE INDEX"". Please note that this query is not always a good choice. If there are many other queries to be executed, it becomes difficult to have a good overview. The following table summarizes the differences between a high and low cost model. As the number of iterations increases and the cost for a model increases, the performance of the model decreases. As the number of iterations is low, the model performs well. Cost vs. Score Performance of High Cost Models C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score C.Efficiency Score 100% 0.25 0.125 0.125 0.125 0.125 0.000 0.125 0.125 0.125 0.125 0.125 150% 0.50 0.125 0.125 0.125 0.250 0.000 0.125 0.125 0.125 0.125 0.125 0.125 200% 0.75 0.125 0.125 0.125 0.250 0.250 0.250 0.091 0.375 0.125 0.125 0.125 250% 1.00 0.125 0.125 0.125 0.250 0.150 0.150 0.092 0.375 0.125 0.125 0.125 270% 1.25 0.125 0,125 0.250 0.250 0.180 0.180 0.092 0.625 0.125 0.125 0.125 0.250 300% 1.50 0.125 0.250 0.250 0.180 0.180 0"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * FROM SPAGE WHERE ID= { ?dataset_lbl | ?model| ?rdfs_lbl ? } -------------------- ----------------------- select * from SPAGE input (English word): What is the performance of the model on those metrics ? output (Sparql query): -------------------- ----------------------- SELECT * FROM RDFSLBL AS SPAGE FROM HONEST-OF_CLASS_MIMETERS AS RDFSLB AS SLB.LSLC AS SLBLRDFSLRDFSLC.LSSASLC Rdfslc.lssaslc; -------------------- ----------------------- select * from RDFSLB.LSSASLC input (English word): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip dataset ? output (English text): -------------------- ----------------------- SELECT * FROM SPAGE WHERE ID= { ?dataset_lbl | -1} -------------------- ----------------------- select * from SPAGE input (English text): Which of the input file (.pdf) datasets is considered representative for any of the individual features ? output (Sparql query): ------------------------------------------- select * from SPAGE 1 4.6 4.6 1.0 8.3 8.6 8.8 8.3 8.6 1.0 0 1.0 ------------------------- select * from SPAGE 2 3.6 3.7 * ----------------- -------------- select * from SPAGE 3 4.6 4.7 3.3 8.3 8.6 8.8 8.3 8.6 * ------------------------- select ------------------------- from SPAGE 4 4.6 4.7 3.3 8.3 8.6 8	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Input input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset ? output (Sparql query): Input output (Sparql query): What is the lowest F1 score on HoC dataset ? The low F1 score for an object on the HoC benchmark dataset is defined as: lowest, lowest-best, least, lowest average, highest average. Note: Only ""lowest"" has the right interpretation. The top-most and bottom-least models were given low scores because they achieved a higher F1 score than the others. The average of the data of"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What are the measures for the Atari 2600 Hero benchmark dataset ? output (Sparql query): ------------------------ [ Top 50 Scores ] ------------ ------ Score Name Value ----------- ----------------------------------- : 1.6"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?dataset ?dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; ORG.SYS 'Benchmarking Amazon Elasticsearch'| select name, benchmark_id, benchmark_lbl,  @{ Here is a sample of an output SQL statement in which data is included in the output. A lot of SQL examples involve more than just setting the table or database column names. The performance analysis example just shows what can become available from a given query, if the user knows in advance which metrics are available. SELECT id, benchmark_id, benchmark_lbl,  ( ?) as ?lbl,  select ?vk,  @{benchmark_id= ?benchmark_id; ?vk= ?vk ?vk_lbl= ?vk_lbl ?vk. ?vk_lbl. }; id|benchmark_id|benchmark_lbl| The advantage of query trees is that they are extremely simple, they have no overhead to insert more columns, and they are extremely easy to debug. They are typically easier to set up than a normal tree structure. SQL queries have access to many more common attributes. While we all try to achieve the maximum performance from a simple query, query trees are typically very efficient. They can scale up, down, and out without much cost. It would be great if we could achieve that in order to build complex SQL queries. However, query trees tend to be more	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT rdfs:name ?dataset. rdfs:name ?dataset_lbl. We can see that the results are much more predictable -- there are more papers that had the best results that include the Rainbow+SEER model than the original paper. More interesting is the second query, which has a lot of details about the output but is not a valid search. I find the above query to be an example of what I would consider ""search query optimization"" as compared with a good (full-text search) or an invalid (long-range search) query. Because it is so complex, it is very difficult to know the answer to most of the above questions. The question of optimal query optimization would require the use of a more complex statistical technique such as a supervised convolutional neural network (SNL, in our case) or similar. As you can see, it is much more efficient to use the original paper and only remove the relevant parts from it. Unfortunately, as you point out, that requires a lot more time and computational resources as the paper you were querying can cover a vast amount of data. So I believe that it is the result of good research. As a result, I would not advise others to use that technique. But as you mention, the fact that they can cover a large amount of data is not the biggest issue. The big issue is that we do not like to wait for new information to be available in the form of a paper that we had not read yet, but we are willing to read a paper that is very similar to our own research. Now that is a great example on which to build the most sophisticated search algorithm, because the authors are experts in the field of our research area and this knowledge will be crucial in the development of their algorithms. So the fact that a paper is similar"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?dataset as ?dataset1, ?data as ?data11, ?dataset2 as ?dataset2, ?score as ?score1, ?metric as ?metric1, ?metric2 as ?metric2, ?result as ?result1 ORDER BY 1 DESC(metric1); ________________________________________ output (Sparql query): _____________________________________________________________________ Score : 1 511.9% Score : 2 522.1% Score : 3 551.1% Score : 4 562.5% Score : 5 577.6% Score : 6 628.3% Score : 7 639.8% Score : 8 667.6% Score : 9 686.4% Score : 10 699.7% Score : 11 738.1% Score : 12 757.1% Score : 13 760.1% Score : 14 782.7% Score : 15 _____________________________________________________________________ How does the model compute the mean based on the input scores ? input (English text): What is the average accuracy for the RTE dataset ? output (Sparql query): ________________________________________ A: 547.5% B: 522.8% C: 547.6% D: 469.7% RTE : 661.9% How does the model compute the precision based on the input scores ? input (English text): What is the accuracy for the RTE dataset ? output (Sparql query): ________________________________________ A: 522.1% B: 432.6% C: 547.3% D: 461.5% RTE : 586.7% How does	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"--------------------------- Title: ""Model with Weighted Mean of a Weighted Raster of 1/15 of the Text"" Author: T. Rizzolatti and P. Giacomini Date: 12/22/2016 Abstract: Our goal in this paper is to define a weighted mean of the weighted Raster of each model that is defined for a given text, where the weight of each point is computed in terms of the normalized weighting of the other points. We then estimate the weight of a weighted mean given the following distribution: w(x ) = ________________________ for the input x from 0 to 15, where x is an integer between 1 and 15. We computed the weighted mean using an RDFS and a SPARQL query using several datasets, including: the following three-level text: ""1/15 of the text"", the original Roman text, ""Gestalt"", and ""F1/15"", the English-Romanian dataset. We observed an average of 0.2235 points per metric, and the weighted mean of the weighted Raster of a given text, using our proposed method, is 1.2265 points and 0.2154 points. input (English text): Which model has achieved the highest F1 score on the WMT2016 English-Romanian dataset ? output (Sparql query): Select one (1) from -------------- a b c d e f -------------- Model with Weighted Mean of a Weighted Raster of 1/15 of the Text Model with Weighted Mean of a Weighted Raster of 10/15 of the Text Model with Weighted Mean of a Weighted Raster of 50/15 of the Text Model with Weighted Mean of a Weighted Raster of 100/15 of the Text ------------------------------ Model with Weighted Mean of a Weighted Raster of 1/15"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT rdfs:id, result, ids = (result > 0) ?rdf_ids WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } orkgp:P32 ?problem rdfs:id ? Note that the queries used for the indexing of table rows are used when you index the whole dataset. In case the dataset is split into smaller parts, you must split the query on the part in which it is most useful. When a particular query is not relevant to the data structure of interest but only contains information that does not make sense within the dataset we index, we refer to the query as being redundant. After we have gathered some data and stored it on the local database or database on the distributed storage, we are able to call the query and look up the data from the local database using different APIs, either using the CRUD or the REST APIs. It often helps when you can combine these APIs to process a complex data load. The SQL Language defines several methods that can be used to process a query. In addition, there are another four SQL language methods that you can use that have been introduced and defined by the Schema Definition Language (Schema.org). In this exercise, we will explore how to access these methods using the REST APIs. You can read more about all of the methods in the REST Tutorial. First things first: The APIs are described in the HTTP Reference Document for the server"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"--------------------------- ORDER BY c.value DESC(c.name) ----------------------- R2(c.value) ----------- 1.071 (1.0) 5.853 (2.45) p.value ------------------------ 2.788 (1.0) 9.068 (2.22) v.p.value ------------------------ 0.765 (0.1) 0.8908 (0.37) r.value ------------------------ 0.767 (0.0) 0.7340 (0.15) c.value ------------------------ 1.007 (0.3) 2.063 (0.45) c.value (n.transf.value) ----------- 0.00035 (0.0) 0.00010 (0.1) 1.005 --------------------------- ORDER BY c.value DESC(c.name) 1.7 The ESC-50 is used to measure the accuracy of regression-based modeling (e.g., GCP). In addition, the ESC-50 was added as a ""benchmark"" dataset for the R-M model validation dataset to ensure that it is equivalent to the data used in the GCP validation dataset."	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT { ?dataset a orkgc:Dataset ; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HSQ_DATASET ?dataset; orkgp:HAS_DATASET ?benchmark. ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } FROM model GROUP BY ?dataset ORDER BY ?"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl FROM ?model ?Model_lbl WHERE DISTINCT ?metric_lbl. input (English text): Which model has achieved the highest F1 score on the TempEval-3 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ?metric orkgc:Metric; orkgc:Metric OR kgc:Metric. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Which model has achieved the highest F1 score on the TempEval-3 dataset ? output (Sparql query): SELECT ?model ?model_lbl FROM ?model ?Model_lbl WHERE DISTINCT ?metric_lbl. input (English text): Which model has achieved the highest F1 score on the TempEval-3 dataset ? output (Sparql query): SELECT ?model ?model_lbl FROM ?model ?Model_lbl WHERE DISTINCT ?metric_lbl. input ("	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?statistics. ?benchmark. ORG_SUM(benchmark. ?stats ORG_STATUS ) as statistic ; AND { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET { ?statname orkp:HAS_DATASTATE ?statname orkp:HAS_STATUS ?statvalue ?statvalue; ?vstatname orkp:HAS_VSTATVALUE ?vstatvalue; ?valrname orkp:HAS_VALR_VALUE ?valrvalue; ?pstatname orkp:HAS_PSTATVALUE ?pstatvalue; ?wstatname orkp:HAS_WSTATVALUE ?wstatvalue; ?kpstatname orkp:HAS_KPSTATVALUE ?kpstatvalue; } OR ?statname orkp:HAS_STATNAME FROM ?statistics_lbl orkgc ORDER BY ?statname ORG_STATUS; The above query only returns the output of the current benchmarking benchmark for either Orkgp or orkgp, as seen in the output. I chose to use Orkgp as I did not find that the result of the benchmarking was comparable to other orkgp models. The model in the first row had the highest performance. The other models all had their own ranking, which may be different from how you are thinking. Some of the models in the first row may be ""somewhat"" good, and those that are ""a little"" good, which is also good to know. The"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	-------------- Benchmark scores and metrics -------------- Number Score Avg. Score 1 19 4.9443957146692 2 7 1.76497244824063 3 6 1.58995798472778 4 5 1.57995798472778 5 4 1.5695801142897 0 6 1.52360774422988 7 3 1.50886400753744 8 2 1.50886400753744 9 1 1.49886400753744 10 0 0.49886400753744 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 52 53 54 55 56 57 58 59 60 62 63 64 65 66 67 68 69 70 71 72 73 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Lattice Input: input (Numeric: numeric values, decimal: decimal numbers) output (Sparql query): -------------- output -------------- Number Score Avg. Score Number Score Avg. Score -------------------- 9 10 10 7 9 10 10 7 9 10 10 7 9 10 10 7 9 10 10 7 9 10 10 8 8 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -------------------- -------------- Lattice Input: input (String: text) output (Sparql query): ---------------------------------------- output ---------------------------------------- Id Name Type ---------- ---------- ---------- ---------- ------- ---------------------------------- ---------- ---------- ---------- ---------- ------------------ --------------------------- 1 1 COUNT(*) [COUNT(*) - 1	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT COUNT( * ) / COUNT( * ) AS total FROM. H. genia_lbl. dvs. orkgp ORDER BY total DESC LIMIT 100; ------------------------------------ -- select the model from the top of the table -- for the entire dataset SELECT COUNT ( * ) /	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?key ?metric FROM YelpBin ?id. LIMIT ?metric WHERE ?""K"" = ?key. } ?id. LIMIT ?metric WHERE ?key = :key. } ORDER BY ?title input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): (1) Sorting the query time results by average time, then (2) sorting the query time results by % time input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): (3) Using the model's output we can see what percentile we are on this benchmark and what percentage of rows are ranked on each scale input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): (4) Using the model's output we can see what is the top score percentile and what proportion of rows are ranked for this top score percentile input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): (5) How many rows are actually ranked on each of the scales ? input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): (6) We can see how most of the benchmarked models rank and how they are not performing significantly worse (on average) in this dataset input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): (7) We can view the list of models that are currently scored based on the RTE dataset in the ""RTE"" column: input (English text): What is the"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Scheduled Test The following schedule will run tests on all the models which have been evaluated on the Amazon E-Classifier. If no models are set, the schedule will run once per epoch. input (English text): Can you set the interval of the scheduled tests for the HoC benchmark dataset ? output (Sparql query): Results We can find two sets of results in this part. The first set of results represents the accuracy of the model that is actually evaluated on the Amazon benchmark dataset (the other set is also the set for which the evaluation was performed on the HoC dataset). The second set of results are the best model scores that the HoC evaluation ran over the period of time to perform the tests. The final set of results are generated by the Eigen framework and are not really that interesting as far as the HoC evaluation is concerned. The above table shows, in this case, the best model scores (with an integer score, which is a good indication of the accuracy of the model) from the evaluation run which were in the top half (the third row on the table, in blue) of all models with more than 100 tests. We can see that the GIMP model is very close to being the best model on these tests and the fact that the Nurburgring model has a very close to a 1.8 accuracy is very good for it (we'll see when we add it to the next tutorial). The following table shows the best model scores (with an integer score) for tests on the test set (with an integer score, which is a good indication of the accuracy of the model) whose accuracy is above 1, the best model score on the test set with an integer score, and the best model score on the test set with an integer score. The last table is the best model scores with an integer	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title from rdfs_title WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?benchmark. ?title orkgp:HAS_MODEL ?model. ORDERS ?stat. ?stat rdfs:label ?model_lbl. } What are the titles and IDs of research papers that do not include a benchmark ? output (Sparql query): output (Sparql query): SELECT ?title from rdfs_title WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl.FILTER (str( ?dataset_lbl) = ""HoC"" ) ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?benchmark. ?title orkgp:HAS_MODEL ?model. ORDERS ?stat. ?stat rdfs:label ?model_lbl. } What are the titles of a record of research papers that include a benchmark for the Atari 2600 Bank Heist dataset ? output (Sparql query): output (Sparql query): SELECT ?title from rdfs_title WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl.FILTER (str( ?dataset_lbl) = ""HoC"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT SUBSTR(value,10,1); output (Sparql query): SELECT SUBSTR(value,10,2); output (Sparql query): SELECT SUBSTR(value,10,3); Output (Sparql query): SELECT SUBSTR(value in score,10,4); Output (Sparql query): SELECT SUBSTR(value in score,11,5); Output (Sparql query): SELECT SUBSTR(value in score,11,6); Output (Sparql query): SELECT SUBSTR(value in score,11,7); Output (Sparql query): SELECT SUBSTR(value in score,14,8); Output	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 The table below lists the results of the benchmark for each test. Model Metric Score 1 3.8 2 5.6 3 7.6 4 8.4 5 7.8 6 4.6 7 8.4 8 8.8 9 9.0 10 11.8 R - SSE - SEARCH - SSE - SSE - SEARCH - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - SSE - Model Metric Score 1 9.6 2 7.8 3 7.8 4 7.8 5 8.2 6 8.4 7 8.4 8 7.8 9 9.0 10 9.0 11 9.0 12 9.0 13 9.0 14 12.6 15 13.2 16 14.6 17 14.8 18 14.8 19 14.8 20 14.8 21 14.8 22 13.4 23 12.0 24 11.8 25 11.8 1.   (2.4×15.8 )  -5.3 1.   (2.8×17.8 )  -4.7 1.   (2.2×18.8 )  -3.5 1.   (2.7×19.0 )  -4.1 1.   (2.4×19.8 )  -4.3 1.   (2.8×20.4 )  -3.4 1.  	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?counts. ?count_seqs. ?count( ?count_seqs. ?) FROM ""RTE Dataset"" GROUP BY ?count_seqs. ?count_seqs. ?count( ?count_seqs. ?) ORDER BY ?count_seqs. ?count_seqs. ?count( ?count_seqs) If you've used this feature (or a similar one in the future) you should be able to generate output just like the previous examples, i.e.: output (Sparql query) input (English text): Could you tell me if there is a significant correlation between the accuracy (benchmark) and the number of metrics used ? output (Sparql query): input (English text): output (Sparql query) Output format: sql, json, xml It is also possible to specify an output format if you wish to export a table as a file. output (XML) If you specify an output format then the table will be named as output by default. output (XML, xml) If you would like to specify an output format then the table will be named as"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the MFEC model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MFEC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The following data can be included in benchmarking the GAD dataset: paper_label : ID for the paper paper_lbl : ID for the document in GAD dataset_lbl : ID for the dataset in GAD, as a table with id, data and key column names benchmark_inputs : (optional) Input parameters interval : Set when to compare baseline to observed baseline values. Set in seconds from now (UTC time) interval_range : Range where to set benchmark to the observed, specified in seconds interval_range_value : Value where between specified time and baseline value to get the observed value for test_label interval_value : Value where between specified time and baseline value to get the observed value for test_lbl interval_value_range : Range where between specified time and baseline value to get the observed values for test_lbl Interval range is best to start with low values and raise to higher values as your results increase. Note that in order to calculate the observed baseline value to benchmark the MFEC model, there is also a separate benchmark_interval_value to calculate observed pre_interval value to benchmark the model. R-G-S Results You can run this benchmarking with R and RStudio as follows: R Compute bench_gad benchmarking.sh R Benchmark the GAD dataset using R. R run bench_gad,benchmark_interval_lbl or bench_gad,benchmark_value_range. To start R benchmarking, select bench_gad (if you have already done so), set the output_file, include the name of the output file and select benchmark, run Benchmarking (results should be shown in this file) and then press the Run button.	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT *, ?metric OR ?metric_lbl. ) || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || | orkgp:HAS_DATASET || || || | orkgn:HAS_DATASET || || || || || || | orkgp:HAS_DIFORTIES || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || | orkgp:HAS_DESC || || || || || || || RANDOM ?metric_lbl. ?metric rdfs:label ?metric_lbl. output (Sparql query): SELECT CAST( ?metric_lbl) AS metric, RANDOM ?metric_lbl. FROM (SELECT ORGANIZE(ORGANIZE( ?metric_lbl) ) AS rd, CAST( ?metric_lbl) AS metric_lbl FROM (SELECT ORGANIZE( ?metric_lbl) ) AS rd WHERE rd.data_format = ?dataset_lbl. RDD ( ?dataset ) ORGANIZE ( ?dataset ). RDD ( ?dataset ) ORGANIZATE ( ?dataset ) ). rd. ORGANIZATE ( ?dataset ). ORGAN ( ?dataset ). ORGANIZE ( ?dataset )	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT SINGLE TOP N.NAME ?""DENSE GROUP"" FROM seel.cse.lsu.edu/data/re17.zip RERULES RTE: DISTINCT ? METRIC ?value ?""DENSE GROUP"" input (English text): The data are processed as follows: The metrics are stored in the RDF Sitemap. The resulting RDF Sitemap contains a table with all metrics for each metric name in the dataset. output (Sparql query): SELECT DISTINCT ? METRIC ?value. ?factor (MAX( ?value) AS ?score) FROM seel.cse.lsu.edu RTE: DISTINCT ? METRIC ?value. ?factor (MAX( ?value) AS ?score) Rerouter (input (English text): All features are included in the RDF Sitemap. The output RDF Sitemap contains a table with all features for each metric name in the dataset. input (English text): What metric(s) are being evaluated on the seel.cse.lsu.edu/data/re17.zip dataset ? output (Sparql query): SELECT SINGLE TOP N.NAME!factor orkgc:Dataset!factor orkgc:Sensitivity orkgc:Metric orkgc:CASE WHEN RERULE { rdfs:label ?dataset_lbl. AND IF RERULE [1] THEN { ?factor. ?grouping orkgc:Label } ELSE { ?factor. ?grouping ?gmp:Label } END. AND IF RERULE [2] THEN { ?factor. ?grouping ?kgp:Label } ELSE { ?factor. ?grouping ?gp. } IF"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	-------------------------- model Name ------- --------------- ----- orkgc:Dataset orkgp:HAS_DATASET HoC -------------------------------------------------------------- ?model_lbl. ORI : Model (test) ORI : Model with 99% accuracy HoC : Model with 99% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HOX : Model with 100% accuracy HoC : Model with 100% accuracy HoC : Model with 100% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 100% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HoC : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HOX : Model with 0% accuracy HOX : Model with 100% accuracy HOX : Model with 0% accuracy HOX : Model	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" rdfs:label ?label. FILTER (( ?rf_label) = ""Semantic Web"") { ?paper ?label; orkgp ?label; orkgp:HAS_BENCHMARK ?parm. { ?problem ?parm; ?benchmark ?parm_lbl. } } The RDF query will pass the problem and benchmark results of the problem through the model which will then be used to extract the key/value pairs that match the problem (parms or problem_labels ) in the relevant dataset and then the results of the model can be output or filtered to the relevant dataset (this is a typical use case). This is quite similar to what the LLD parser does. The problem with this is that the data needs to be transformed from an ""ordinary"" text into a ""Semantic Web"" format. For this purpose, we have provided a pre-processing pipeline that converts text data in the text file format to a standard ""Semantic Web"" format (an object with a type attribute of ""http://schemas.xmlsoap.org/soap/items/metadata/"" and a text property of ""schema.org"", ""schema.org/schema/titles"" and ""schema.org/titles/object/"" ) and then uses the RDF feature extractor to convert the XML data to the Semantic Web format, for which the RDF query passes the results of the transformation through the model. RDF queries for Semantic Web As illustrated above, this can be used to build a semantically rich search engine that can be used for retrieving relevant text articles that may be related to a specified topic. The Semantic Web is intended to be structured as follows: A web page is a document, generally consisting of"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet-152 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"--------------------------- FAIL ------------------- | ?lbm:ResNet-152 orkgc:Dataset | | FAILURE | --------------------------- If you use RTE, you do not need to specify your ""metric"" for model evaluation. Note: You need this option if your dataset is defined as DIN, and if you want to use DIN as the label type for your model. You can pass an optional ""benchmark"" parameter to your command to be evaluated."	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" To set up our Semantic Web experiment, we use the Parquet package, which is also known as a distributed data structure. In preparation for our experiment, we will import a sample of the Parquet file for the data described in the paper: https://www.cs.ucla.edu/~mcginn/papers/v6/eprint/2016/013020.pdf. We will convert it to a relational data structure which we will use for the analysis.  After that, we will create the schema.xml file: < ?xml version=""1.0"" encoding=""UTF-8"" ?> <schema xmlns=""http://tools.ietf.org/html/html4xml"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://sqlite.org/""> <xsd:schema name=""parquetSchema"" type=""http://schema.org/Parquet""> <xsd:extension base=""http://extension.parquet.org/""> <xsd:sequence> <xsd:attribute name=""id"" type=""http://schema.org/IID"" /> ... </xsd:sequence> </xsd:extension> The resulting schema.xml file (for our test dataset): < ?xml version=""1.0"" encoding=""UTF-8"" ?> <schema xmlns=""http://tools.ietf.org/html/html4xml"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:dsd=""http://www.w3.org/2004/01/rdf-schema-core"" xml"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT COUNT(*) FROM all_statistical_data GROUP BY code; input (English text): Is there more than one dataset for which code benchmarks on the SPSS 1000 ? If so, where can I find codes matching these datasets ? output (Sparql query): SELECT count(*) FROM all_statistical_data,all_graph,all_viz,all_coding input (English text): Does the Gantt chart show only a subset of classes of models ? What if there are more than two classes ? output (Sparql query): SELECT COUNT(*) FROM all_statistical_data,all_graph,all_viz,all_coding ; input (English text): Provide a list of academic papers that use the WALKER/WALK (DMControl) dataset. A copy of the publication will be sent as well if there is a paper number. output (Sparql query): SELECT AUTHOR, TITLE, TITLE, CREDITS FROM all_article_articles LIMIT 10; input (English text): Where is the paper number for this paper mentioned ? output (Sparql query): SELECT COUNT(*) FROM all_article_articles LIMIT 20; input (English text): What publications did you find that used the DATASET parameter ? output (Sparql query): SELECT TITLE FROM all_papers LIMIT 10; output (Sparql query): WHERE TITLE IS NOT NULL AND DOI IS NOT NULL AND TITLE IS NOT NULL AND AUTHOR IS NOT NULL; input (English text): Is	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT SINGLE_REVIEW ?codec ?dataset. ?benchmark asbenchmark, ?code ascode, ?codec ascodec; orkgc:LAW_DESCRIPTION ?code: { ?codec ?dataset. ?benchmark { ?code ?codec ?totals( ?codec. ?matrix ?scale), ?matrix ?scale}; orkgc:STATISTICAL ?code: ?test ?dataset. ?test { ?codec ?totals( ?codec. ?matrix ?scale) ?test, ?scale ?scale}; orkgc:REVIEW ?code asreview, ?codec ascodec; orkgc:NOTES ?code asreview; orkgc:REVIEW_DATA ?code asreview_data; orkgc:HOST ?code asreview_host; orkgc:IP ?code asreview_ip; orkgc:ISRAIL ?code asreview_israil; orkgc:DNS ?code asreview_dns, ?totals( ?host ?host_info), ?host_info ?host_info; orkgc:PIPELINING ?code asreview_pipeline; orkgc:STACK ?code asreview_stack, ?totals( ?stack ?stack_info), ?stack_info ?stack_info; orkgc:BIN ?code asreview_benchmark, ?stack ?stack; orkgc:ROUTINES ?stack ?stack_info; orkgc:STREAM ?stack ?stack_info; orkgc:BATCH ?stack ?stack_info; orkgc:PAGE ?stack_index_page; orkgc:PAGE_NUM	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT MODEL { Model.model_lbl } FROM sys.dm_model_model_lbl INNER JOIN (SELECT MODEL_LABEL.name FROM sys.dm_model_model_lbl JOIN Model_labels ON MODEL_labels.model_lbl = MODEL_labels.model_lbl.model_lbl JOIN Model_labels_values ON MODEL_labels_values.model_lbl = Model_labels_values.model_lbl WHERE MODEL_lbl = MODEL GROUP BY Model.model_lbl. model_lbl. model_lbl. model_lbl || MODEL_lbl. model_lbl. model_lbl. model_lbl. model_lbl. model_lbl; MODEL_labels: Model.model_lbl. model_lbl. model_lbl. Model_lbl. Model_lbl. Model_lbl. Model_lbl. Model_lbl. MODEL || MODEL What metrics are being used ? input (English text): output (Sparql query): SELECT MODEL_VARIANCE( METRIC_SUGGESTED = ?) ?metric_lbl. FROM sys.dm_model_model_lbl. MODEL_labels INNER JOIN (SELECT MODEL_DATA_LABEL.name FROM sys.dm_model_model_labels_values ON MODEL_data_labels.model_labels = MODEL_data_labels.model_labels JOIN Model_data_labels ON MODEL_labels.model_labels.model_lbl = MOD	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"------------------------ The top five models were: (model name) (option name) (parameter name) input (English text): Is there a reason for the ""all test types on the same dataset"" ? input (English text): What is the name of the model used for this test ? output (Sparql query): ----------------------------------------------- from SST_Classifier import model_evalval from SST_Classifier import _model_evalval_from_rte.sst2 import readSST2, SST2, SST2_bbox from sstx import * from sstx import SST2_bbox, SST2_L2_l2 input (English text): What is the name of the model used for this test ? output (Sparql query): ------------------------------------------------- from SST_Classifier import model_value1, model_value2 from SST_Classifier import model_value3 from SST_Classifier import model_value4 from SST_Classifier import model_value5 from SST_Classifier # The model name is SST_Classifier#Model_value1 and the model value is: 0.0 # The model name is SST_Classifier#Model_value2 and the model value is: 1.0 input (English text): Could you tell me how to interpret the parameter value given in your model name ? output (Sparql query): --------------- The parameter is the index (the most recent value) for this argument value input (English text): What is the name of the parameter variable used ? output (Sparql query): ----------------- parameter_name input (English text): What is the model name used for this test ? output (Sparql query): ----------------------------------------- from SST"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" The output of the SQL query can be used to create an Excel file with the results of that query in the ""RDD"" column."	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM _proposal_list.distinct LEFT JOIN _proposal_list.count_pairs ON _proposal_list.distinct.count_pairs.pairs_part_of_ref-1 AS ref1 = ref2  WHERE ref1 = ? and ref2  ORDER BY ref1 input (English text): You can find the source code of a paper listed on this page. output (Sparql query): SELECT ?title, ?author, ?volume, ?issue, ?author_id, ?length FROM PROPOSAL(issue, author_id, length, volume, author_id, ISBN) input (English text): We would like to provide links to papers that report our results. output (Sparql query):   SELECT title, author,  volume,  issue,  author_id, ISBN FROM PROPOSAL(issue, author_id, length, volume, author_id, ISBN) We have a set of topics to address with this proposal. We wish to improve the understanding of statistical learning with RTE and we intend to explore how RTE can be used as a language, a scaffold or as a tool to perform tasks such that the language is learned. The first project we are working on is to improve our understanding of statistical learning, particularly its theory. We need to figure out how data can be understood for a given model, and how the RTE framework can be used to achieve this. The second project we are working on is the learning of the RTE framework. That is, we need to understand how the RTE framework (RTE, RTE2 or both) can be used in the context of training the model. We need to identify the different steps of training from where we should derive the	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT titles AND ID FROM publications; output (HTML): < ?xml version=""1.0"" ?> <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd""> <html lang=""en""> <head> <meta charset=""UTF-8""> <title>Test Data for the TDS-TDMS Datasets in the NLP-TDMS and NLP-TDMS+SPS datasets</title> <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8""> <link rel=""shortcut icon"" href=""http://purl.org/NVYX/""> <link rel=""alternate"" type=""application/pdf"" href=""https://s3.amazonaws.com/nlp-dms-database-software/sparse-rdf-table.pdf""> <link rel=""stylesheet"" type=""text/css"" href=""https://purl.org/nvyx/demo.css""> <link rel=""stylesheet"" type=""text/css"" href=""https://purl.org/NVYX/demo.scss""> </head> <body>, <div> <h3>This document is published under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License </h3> <div id=""content""> <h3>For more details read our license </h3> <div id=""summary""> <p>This software is provided AS IS in the hope it will be useful, but WITHOUT ANY WARRANTY, either expressed or implied!</p> <p"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 output (Sparql query): Select { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } input (English text): Can you list the data sets that have been validated on the ClueWeb09-A dataset ? output (Sparql query): Select { ?dataset ?dataset_gaggle:GAD OR ?dataset ?dataset_cvs:CVS OR ?dataset ?dataset_pkdf:Pkdf OR ?dataset ?dataset_kdf:KDF OR ?dataset { ?dataset ?dataset_rdfs:RDFS OR ?dataset ?dataset_fidf:FIDF OR ?dataset ?dataset_fidf:FIDF { ?dataset ?dataset_gaggle:LAMBDA OR ?dataset ?dataset_cvs:CVS OR ?dataset ?dataset_pkdf:Pkdf OR ?dataset ?dataset_kdf:KDF OR ?dataset { ?dataset ?dataset_kdf:KDF AND ?dataset ?dataset_lbl:LALR2 OR ?dataset ?dataset_lbl:LALR2 { ?dataset ?dataset_dyn:CDF OR ?dataset ?dataset_dfs:DFS OR ?dataset ?dataset_fpkdf:FPKDF OR ?dataset ?dataset_rlfs:RFLF OR ?dataset { ?dat	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?metric a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. input (English text): Can you list the metrics used to evaluate models on the FSE dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?metric fse:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. input (English text): Can you list the metrics used to evaluate models on the GATK dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?metric gatk:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?lbl ?test_id, ?lab ?lab_lbl, ?res ?lab, ?data ?res. ?res rdfs:label ?lab_lbl. ; orkgp:P29 ?lab_lbl. input (English text): Is the model suitable to address the use cases in the Semantic Web ecosystem, i.e., the Web Content Management System (WCM), the semantic search product, etc. ? output (Sparql query): SELECT RESULT AS ?result FROM res. WHERE results. ?res ='Semantic Web '. *If the user presses C-c C-l (copy), the query output becomes a single row in the table, rather then being split by tabs in the table. To get back the original query, press F9 (delete) in the terminal.	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG fine-grained gate"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" And that's the only difference between this code and the ""SCH"" one: The FG gate model uses a new schema from scratch, so as a result it is a completely different language code. Note that only the code to analyze the semantic web is here, the query parsing one can be found here - you can also find it on MIX and the other MIX sites. The key feature in my code is that instead of running a new schema, I use a Schema.js code generator (using JSON.stringify instead of JavaScript's built-in eval ), that transforms all the input text into a nice formatted JSON, similar to the text format used by the Schema.org site. This is very similar to how Schema.org uses the XML-RPC API in a browser without using an XMLHttpRequest object to send a request to Google and download the data from there. It is possible to generate multiple Schema.js schemas per database, for example for a different kind of text (in this case, it is not good practice to provide a schema with a specific character set, however as we will see it is not impossible to implement that). You can do that by using the in-memory SQLite database or the MySQL database by using an in-memory SQLite server. For my own purposes I chose to use the in-memory one because it was available for my server. The other option (also available for MySQL and SQLite) is to use an XS database (for example with the XS-API available in Google's Chromium engine), which has its own internal SQL database and allows to specify how it should look like. Now this is all good enough for the analysis of the semantic web, but it is not very efficient as we can see here on the example here on the wiki: In the case"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT * FROM ?dataset_lbl WHERE { ?db:hq_result ?dataset_lbl. { hq_status rdfs:label ?lbl_lbl. FILTER(str( ?dataset_lbl) = ""NYT29"") ?benchmark. } } output (Sparql query): Input data type: PostgreSQL. { ?db:hq_result orkgp:HAS_DATASET ORkgc:HAS_DATASET. ?dataset_lbl a:Dataset ? (db:hq_result (hq_status a) orkgp:HAS_DATASET (hq_status a)) ?dataset rdfs:label ?lbl_lbl. RDFS:label ?lbl_lbl. FILTER(str( ?dataset_lbl) = ""NYT29"") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset. ?eval ?model ?. } }, output (Sparql query): Output Data Type: PostgreSQL. { ?db:hq_result ORkgc:HAS_DATASET ORkgc:HAS_DATASET. ?dataset_lbl a:Dataset ? (db:hq_result (hq_status a) orkgp:HAS_DATASET (hq_status a)) ?dataset rdfs:label ?lbl_lbl. RDFS:label ?lbl_lbl. FILTER(str( ?dataset_lbl) = ""NYT29"") ?benchmark"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROFESSOR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): The following two queries evaluate the same model. What are the differences between these two queries ? output (RTE dataset): --------------------------- SELECT (value, c.metric) FROM ( SELECT c.name, ?value. ?metric FROM rte1 AS data INNER JOIN rte2 AS data2 ON data.name = data2.node_id AND data2.status = 'DONE'), c GROUP BY c.name ORDER BY 00000000000000000000000000000000000000000000000000000000000000000 || 000000000000000000000000000000000000000000000000000000000000000000000000000000000 ORDER BY 000000000000000000000000000000000000000000000000000000000000000000000000000000000 || 000000000000000000000000000000000000000000000000000000000000000000000000000000000 ORDER BY 0000000000000000000000000000000000000000000000000000000000000000000000000000"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entity F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT F1 score from test set. F1 is based on the most interesting algorithms. input (English text): Can you list the metrics and their definitions in the RTE dataset ? output (Sparql query): SELECT METRIC('data.frame','metry') METRIC('data.frame','metry', 'int64') METRIC('data.frame','metry', 'float64') METRIC('data.frame','metry', 'log') METRIC('data.frame','metry', 'float(2)) METRIC('data.frame','metry', 'float(').^float(3)) METRIC('data.frame','metry', 'float(1)) METRIC('data.frame','metry', 'float(1)+3') METRIC('data.frame','metry', 'float(2)+3') METRIC('data.frame','metry', 'float(1)*100') METRIC('data.frame','metry', 'float(1)+3*100') METRIC('data.frame','metry', 'float(1)*100') METRIC('data.frame','metry', 'int(10)') METRIC('data.frame','metry', 'float(	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ln(X_1:X_N,X_2: Y_1:,Y_2: Y_n) AS X,lm(X_1:X_2: Y_1:,Y_2: Y_n) AS Y,lm(X_1:X_2: Y_1: X) AS X,lm(X_1:X_2: Y_1: Y) AS Y from ?dataset as ln_lbl where x= ? and y= ? SELECT dplyr::benchmark:Y::X::LSTM::Y::X,lm::X:X:LSTM::Y::X,lm::Y:Y:LSTM::Y::X,lm::X:X:LSTM::Y::X,lm::Y:Y:LSTM::Y::X,lm::Y:Y:Y,lm::X:X,lm::Y,lm::X::LSTM::Y::X,lm::Y::Y::X,lm::X::X::LSTM::Y::X,lm::Y::Y::X,lm::X::X::LSTM::Y::X,lm::Y::Y::X,lm::X::X:LSTM::Y::X,lm::Y::Y::X.... output (Sparse-LSTM query): output (Sparse-LSTM query): SELECT X_1: x.c, Y_1: y.c, X_2: x.r, Y_2: y.r, X_1: X1, Y	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 1.5.4, 2016 / 2, 12:00 CET The first table provides the top results. The first column provides the metric and values of the performance of the game for each benchmark. The second column lists the top three benchmark results (metric and value). The third column lists the maximum scores, if any. All of these values are in milliseconds. The table shows the top results for each metric and value. The two columns that contain the number of games are also counted in the results, if necessary. The first table also compares the performance of the top results compared to other benchmarks based on the same metric and value. The second table compares the same benchmark result against the top results for the metric and value of the individual benchmark for which a result is given. The first table compares the top benchmark results to other benchmarks (based on metric and value) for the metric and value. To see the results with a different value, a corresponding benchmark has to be entered into Table C at the end. The third tables displays for each benchmark in the second table the maximum values for that metric and result. The first table also compares the top results, based on the metric and value, against the highest single set of benchmark results, again for the metric and value of the individual benchmark. 1.5.5, 2016 / 2, 12:00 CET The first table presents the top 5 most popular games. The table provides the metrics and values of the games with the highest number of games that can be played during the benchmark. In the table, the metric is identified by the string ?t. In the text of the first column of this table, the metric and value are also provided. The two columns that contain the number of games are also counted also in the results, if necessary. The first table also compares the results against other benchmarks (based on metric and value)	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which sectors are modeled as energy sectors and how often?
 output (Sparql query): "	SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } 	 Sectors are energy sectors 1. The majority of energy consumption in the world is provided by the following energy sectors, of which the number of units consumed per capita is estimated by multiplying the estimated number of people by the reported average annual per capita consumption, by multiplying by the expected average increase observed in the world economy and by assuming a uniform population growth rate. Including the oil, coal, gas, nuclear, and hydroelectric industries as energy sector 1b -3. Inclusion of renewable energy such as solar, wind, hydro and geothermal energy as energy sector 4 - 5. The proportion of production in the various energy sectors has been given in the table below for each country as well as the percentage of total energy produced from the different energy sectors which is considered as energy based on their proportionate share of total and absolute production. Note the overall distribution of energy consumption. Based on the analysis of the data provided, the following categories were identified for this study. Country: The energy consumption in this country is not provided by the energy sector or is made up purely by external demand. So total energy consumption could be calculated from the analysis of the actual or predicted consumption of energy-related products. This category is therefore based on the study of the consumption of energy-related products, such as electricity, gas, steam, petrol, gasoline, oil, diesel, diesel fuel, and coal. Revenues: The energy consumption is based on the electricity, gas, and steam sector of the economy. A country's overall economy is divided into 12 sectors and the economic activities, as well as the total consumption per person, are taken into account in the total energy consumption. The income of the energy sector is taken into account and, as in the previous category, this is used as a proxy for overall economic output. Energy Production: Since the energy production is considered as the share of the country	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?dataset ?dataset_cid. ?dataset_cid. orkgc:Dataset; title: ?metric ?labels_lbl. paperid: ?name. publication_month: ?date. ?dataset_cid. ?dataset_cid. ?metric ?labels_lbl. input (English text): Provide data from data, of the RTE dataset, showing how models performed ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl. ?dataset_lbl. orkgc:Dataset; ?dataset_lbl. input (English text): Identify the key metrics used to evaluate models on the RTE dataset ? Include their name and values ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl. ?dataset_lbl. ORkgc:Dataset ORkgc:Metric ; name: ?name. ?name rdfs:label ?metric_lbl. ?metric rdfs:label ?metric_lbl. ?metric rdfs:label orkgc:HAS_INCOME_LOWEST_MIN. ORkgc:HAS_TOTAL. ORkgc:HAS_TOTAL. ORkgc:HAS_DETREASE_LIMIT. ORkgc:HAS_DETREASE_LIMIT. ORkgc:HAS_GAPS. ORkgc:HAS_LIMIT_LIMIT. ORkgc:HAS_POPULATION. ORkg	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title,id,score from ( SELECT title,id,score FROM ?topicgroup WHERE title = 'Rotowire' AND id = ?topicid ). ?topicgroup Rows ---------- --- (8 rows) output (Sparql query): SELECT ?feature ?feature_id, ?feature_value, ?feature_score FROM ?feature GROUP BY ?feature_id ORDER BY ?feature_value, ?feature_score DESC. id DESC. score orkgp:HAS_BENCHMARK, ?problem ?problem_lbl, ?content_sort_type. ?content_sort_type ROWS ---------- 1 2 4 6 8 1 2 4 6 8 1 2 4 6 8 1 2 5 1 6 8 2 5 2 5 6 1 6 3 7 4 5 1 1 1 2 2 8 3 2 1 1 1 1 1 3 1 6 7 6 1 6 3 1 2 6 3 4 6 3 3 4 6 5 1 1 1 2 2 7 5 5 3 3 6 1 6 3 3 3 6 6 2 8 4 4 1 2 1 2 3 5 6 5 6 2 3 6 7 3 6 6 6 2 7 1 3 8 4 5 6 1 1 3 3 1 2 6 3 4 5 5 3 6 4 6 5 3 6 4 3 7 2 8 5 1 3 3 2 3 1 2 4 4 7 7 8 3 7 6 3 6 1 5 1 9 5 4 1 4 1 6 4 4 2 2 1 1 8 1 5 5 6 3 1 2 1 2 2 1 5 1 8 2 6 6 3 1 6 6 4 6 3 3 1 8 2 5 3 3 3 1 3 1 4 6 3 4 2 4 7 4 5 4 1 6 2 10 6 1 5 2 2 2 4 1 2 1 6	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? 
 output (Sparql query): "	"SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, ""Naive bayes"")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, ""Bag of words"")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, ""Feature request"")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } "	" input (English text): Which database/application did you work with in the experiment ? output (Sparql query): input (English text): If you were a participant in the study which software, that would be ? output (Sparql query): input (English text): If you were a researcher in the study which software would be ? output (Sparql query): input (English text): Which database/application did you work with in the experiment ? output (Sparql query): input (English text): Please describe the steps taken in developing the algorithm to predict user feedback from user feedback for user feedback for user feedback output (Sparql query): input (English text): What do you mean when you say you can make predictions based on user feedback ? output (Sparql query): input (English text): How many rows will this response need to contain ? output (Sparql query): input (English text): What are the expected results ? output (Sparql query): input (English text): The algorithm is using a ""Bayes net"" to predict user feedback, i.e. comparing users responses to an objective function to understand information in the context of the user feedback. output (Sparql query): input (English text): What is the name of your dataset ? output (Sparql query): input (English text): Can you describe how you came up with your hypothesis and how you came up with your results ? output (Sparql query): input (English text): What is your method of evaluation ? output (Sparql query): input (English text): What are the expected results of the method ? output ("	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?dataset and ?model rdfs:filter ( ?class is(T, ?data)  | is(A, ?model)) ?benchmark orkgf:HAS_DATASET ?dataset; orkgf:HAS_EVALUATION ?eval. ? paper orkgf:HAS_BENCHMARK ?benchmark. input (English text): You have a paper ? Output (Hashing script): [ { ?paper  orkgf:HAS_MODEL || ?model rdfs:filter ( ?class is(T, ?dataset)) ?benchmark &&  T!=-1 orkgf:HAS_DATASET || ?dataset rdfs:filter ( ?class is(T, ?model)), ?model rdfs:filter ( ?class is(T, ?dataset)) || ?dataset rdfs:filter ( ?class is(T, ?model))  || ?model rdfs:filter ( ?class is(T, ?dataset))  } ] output (Sparql query): { "" name "" : "" paper "", "" authors "" : [ "" Klaus Kuhn, Philipp Wesseling"" ], "" papers "" : [ { "" id "" : 26, "" title "" : "" The DMLB-30.0 dataset "" }, { "" id "" : 27, "" title "" : "" DMLB-8.0, "" author "" : "" Timo Saarela-Saarela "", "" papers "" : [ { "" id "" : 28, "" title "" : "" Dependency-free ML with DMLB-8.0 model "" }, { "" id"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?benchmark ?test_lbl and print names, dates, paper identifiers, and outputs. output (Sparql query): input (English text): Is a test set available ? Output (Sparql query): output (Sparql query): input (English text): How large did your benchmark run ? Output (Sparql query): output (Sparql query): input (English text): What languages have been used in the benchmark ? Output (Sparql query): output (Sparql query): The table above shows some examples of data output from the HoC benchmark run. The results of this benchmark were highly consistent. The accuracy of the model was nearly perfect. There were very few variations, and most differences were small, like a few hundred samples or very few words in the text. This is clearly better than any previously published model. The results of the model were very predictable. However, the results are a little bit unexpected. The accuracy of the model on the English text was better than the accuracy on the German text. However, the text size was comparable, and the model was not better or worse at discriminating different languages. So, what happened ? The model was trained on a sample language. The model did not have to make any choices about words or words sizes. The model is trained on a normal dataset of English, German and Spanish texts. What happened ? The model is trained on a text set in which the number of words is about 1. What will happen after the model has learned to recognize the text ? An interesting question. What is the probability. We can easily obtain the probabilities by applying Markov Chain Monte Carlo (MC	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?model ?model_lbl. FROM orkgc:Metric; DISTINCT ?model; ROWS 12-19 output (Sparql query): input (English text): What is the highest benchmark result achieved on the Swingup, DMControl700k ? output (Sparql query): input (English text): Which model is achieving the highest F1 score on the Swingup, DMControl700k ? output (Sparql query): SELECT ?model ?model_lbl. FROM swingup:LangU; DISTINCT ?model; ROWS 12-19 output (Sparql query): input (English text): Which model is achieving the highest F1 score on the Swingup, DMControl700k ? output (Sparql query): SELECT ?model ?model_lbl. FROM swingup:Stat; DISTINCT ?model; ROWS 12-19 output (Sparql query): input (English text): What is the highest benchmark result achieved on the Swingup, DMControl800k ? output (Sparql query): input (English text): What is the highest benchmark result achieved on the Swingup, DMControl800k ? output (Sparql query): SELECT ?model ?model_lbl. FROM swingup:Stat; DISTINCT ?model; ROWS 12-19 output (Sparql query): 	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT NIL  * FROM abstracts  NIL WHERE  Abstract.__class__ = 'labels'; Note that all the above queries are performed using the SQL statements ""select * from"" instead of ""select a"". The output of all the above queries is identical, but with the ""select * from"" feature replaced by ""select a"". You can see how a very similar thing is happening here. We don't explicitly call ""select"" the ""output"" parameter for all our outputs, because the original queries for ""select * from"" have been substituted for our ""select a"". So if we perform an ""select * from"" in the output parameter, the old query for ""select a"" will be sent for all the other outputs as well. The result of the ""select a"" query in this case is a SELECT * from an old ""select a"" query to another old ""select a"" query. The output of this ""select a"" query for ""select a"" is what we'd expect it to be. For more examples, see here. But why should we do that ? The ""Select"" clause always returns the same table in a query — if the expression is true, it's used to load it into an SQL database. This is very much like the old SQL WHERE clause, which would always return the same table if the expression was true. As I mentioned earlier, this is because the ""Select"" clause is not executed on anything besides an input string. The output will always be the same table, and the query will always return the same result. Therefore, the result of a ""select * from"" will always be the same table as that of a ""select a"". The ""SELECT"" clause doesn't know anything about the source text. Therefore, it doesn't know your input text. "	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT RDFS_LSTM_LABEL ?model_lbl. ; FILTER (str( ?model_lbl) = ""VPN"") ?benchmark. output (Sparql query): SELECT c( ?key ?value) ?result. as ?kV ?kV; { ?key a v :- ""VPN"" ?value A v ; } Input: LSTM-like network layer model input (English text): Where can I find code references in papers that use the LSTM (Bai et al., 2018) model for benchmarking purposes ? output (Sparql query): SELECT ?dataset ?data. ?model orkgp:HAS_DATASET orkgp:HAS_KEY ?model ; ?dataset:CODENAME ?data; orkgp:HAS_BENCHMARK ?data.. input (English text): list of code references output (Sparql query): SELECT c( ?key ?value) CODENAME orkgp:HAS_BENCHMARK c; input (English text): list of code references output (Sparql query): SELECT c( ?key ?value) ?c ?value; Note that you can replace each keyword with a code-reference: input (English text): What benchmark paper used LSTM (Bai et al., 2018) for benchmarking purposes ? output (Sparql query): SELECT CODENAME ?c ?value. (This example is not too readable but you should get the idea.) Note that code-references are generated for all the possible combinations of data-frame, key, and output-value. So far, the examples below are pretty"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 If you are familiar with the RTE model evaluation process, you can start with evaluating the RTE model and model scores as follows. By using the following query, the evaluation process will begin.	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgp:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Outputs for different model outputs in the model output table. Inputs: Inputs: Inputs: Outputs: Outputs: Inputs: Inputs: Outputs: Outputs for different model outputs in the results table. Inputs: Inputs: Outputs: Outputs: See also	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT c1 as result1, c2 as result2, c3 as result3, c4 as result4 as value1 FROM odata AS w)) a1 a2 a3 a4 bdat1 bdat2 bdat3 bdat4 retval_lbl, odata, retval_lbl, odataread, bdat1, bdat2, bdat3, bdat4 -- -- Output with	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT F_value F_lbl, H.DATASET, H.EVALUATION, a, ?metric, ?metric_size_lbl. FILTER (str( ?metric_size_lbl) = ""F1"") a or kgc:Metric OR: kgp:HAS_DATASET ?metric_size_lbl OR: ?dataset A or: metric. H.DATASET H.EVALUATION a or kgp:HAS_DATASET H.MAX_LBL H.MODEL H.VALUE OR: * ?METRIC_NAME OR: metric_lbl. OR: metrics.dataset_size. input (English text): What is the highest benchmark result achieved on the Unsupervised: dataset, including the metric and its value ? output (Sparql query): output (Sparql score score): F_value = 118640.18, F_lbl = 121640.74 SELECT F_value F_lbl, h.dataset, h.eval, a, ?metric, ?metric_size_lbl. FILTER (str( ?metric_size_lbl) = ""F1"") a OR kgc:Metric OR: kgp:HAS_DATA ?metric_size_lbl OR: ?dataset A OR: metric. H.dataset H.eval H.model H.value OR: * ?MATRIX_NAME OR: metric_lbl. OR: metrics.dataset_size. input ("	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark_orkgp:HAS_DATASET. ORGTC-ASCT ?lab. benchmark_orkgp:HAS_BENCHMARK. ?benchmark oorkgp:N2. We hope you like this new feature! And please don't hesitate to use the feedback form on our website."	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT STATUS as Evaluation, RANK (evaluation_rank) AS rank, MIN (rank, 1) as min_minval, MIN (minval, 1) as min_minval_min, MIN (minval_minval, 1) as min_minval_min_rank, MIN (minval_minval_min_min, 1) as min_minval_minval_min_rank_rank, MIN (minval_minval_minval_min_rank_rank_rank_min, 1) as min_minval_minval_minval_max_rank, MIN (minval_minval_minval_minval_max, 1) as minval_minval_minval_minval_max_rank, MAX (minval_minval_minval_max, 1) as minval_minval_max_rank, MAX (minval_minval_max, 1) as minval_minval_max_min, MAX (maxval, 1) as max_minval_minval Input (Sparql query): select [{ ?model ?model_lbl. MODEL_NAME (p) } : { ?method ?method_name, ?method_link ?method_link_name, ?rdf_link ?rdf_link_name } from ?methods where ?index ?index_cnt. ] as cn where p. ?model!= ""VPN"" Output (Parquet): { ?method ?method_name, ?method_link ?method_link_name, ?rdf_link ?rdf_link_name} Select the models and methods used to create dataset: select[ ?column ?method_name| ?column ?method_link"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" { } orkgp:C25 ?op [{ ?rf a orkgp:RDBMS_PAPER. FILTER (str( ?rf_label) = ""No"", ''') ?comp:Orkgp; orkgp:C26 ?rfe ?op_n; ?rfe orkgp:P30 ?rfe; orkgp:P31 ?prob; orkgp:P32 ?prob_log; orkgp:P33 ?prob_lscf ? ?lab ?} { } orkgp:P31 ?prob_log; orkgp:P33 ?prob_lscf ? } input (English text): When is the next benchmark ? output (Sparql query): { } orkgp:RDBMS_PAPER. FILTER (str( ?rf_label) = ""No"", '') ?op [{ ?rf a orkgp:RDBMS_PAPER. FILTER (str( ?rf_label) = ""No"", ''') ?comp:Orkgp; orkgp:C26 ?rfe ?op_n; ?rfe orkgp:P30 ?rfe; orkgp:P31 ?prob; orkgp:P32 ?prob_log; orkgp:P33 ?prob_lscf ? ?lab ? } { } orkgp:P31 ?prob_log; orkgp:P33 ?prob_lscf ? }"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 A second SQL query, querying the database with a different method, is also shown in figure 4. 3.3.6: Performance and Benchmarking Figure 4. Performance of GAN and Sparse GRU models for TensorFlow with varying datasets and parameter settings. The second query (scattergraph with 10,000 samples on the left) is a linear regression with 3 epochs. The accuracy score (the bar on the left) is normalized and compared with a standard BLEU model. At each point of measurement, the model performed the average for its entire run, and the best run was calculated as the standard deviation of the best run scores (the square root of the standard deviation). The blue line indicates the accuracy of the model for the most extreme values on the right. The performance is the average of the top 10% of the run at 30,000 samples, and the bottom 3% of the run at 30,000 samples. (Click to enlarge.) The best run at 30,000 samples is the 5.0 BLEU model (black arrow), and the best at 30,000 samples is the 10.0 BLEU model (black circle). The top 5% of runs (blue line) is the average of all the runs averaged over all epochs. The bottom 5% of runs (blue box) is the average of the top 5% of runs weighted by the number of epochs (i.e., the number of samples in each epoch). The most extreme runs on the left have a significantly different performance from the worst run in the right. Figure 6. Results at 300K-1M samples (left) and at 400K samples (right) for the BLEU model with three different parameter settings (with values in red): no optimization, optimized, and all parameters optimzied. These results indicate that	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"-------------------------- <meta rdf:lang=""nl_NL"" name=""Lang""> <meta rdf:lang=""en_US"" name=""Lang""> <meta rdf:lang=""en"" name=""Lang""> This query returns a dictionary that has three fields for each paper: title, author_name, and title. Title will determine which dataset this title was submitted to. If you specify the title field, the results will be sorted by descending date-time priority. Author will determine where the author's name appears on the title, and thus, this will change the priority. If you want to view your findings manually, see the output of the SQL query, above. NOTE: The results from this query might not be accurate. To get your own results, make a copy of this answer file first. When you are done with your copy you should be able to run a script on it. Then, run the SQL query again to verify that your queries are correct. 3.8 - Review your data For each paper, you want to take time to determine what you know about the benchmark, what is the best way to interpret the data, and what other methods of investigation are needed to answer your main question. For a general overview of an article or tutorial title, look at it from the title field, in the case of a text that contains a URL, just see if you can find a link. Then, in that way you might be able to figure out where its author's name and title appear, then use that as an example for the other pieces of this query. For each paper, you want to look for information about it on the Wikipedia page and other similar resources. The Wikipedia pages will take you a good deal farther than the paper itself, since they are filled with more information than the paper. 3.9 - Update"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- <article> <citation> <content> <author>( ?pname):<date> <author> :<date> <p_name} ( ?pname) ( ?pname) ( ?pname) ( ?pname) ( ?pname) <cite_author> :<date> <author> :<date> <p_name} ( ?pname) ( ?pname) ( ?pname) ( ?pname) <cite_page> :<date> <author> :<date> <cite> :<date> <cite> :<date> <cite_url} :<date> <cite> :<date> <cite_url} :<date> <p_name} ( ?pname) ( ?pname) ( ?pname) <cite_page> :<date> <p_name> :<date> <p_name} ( ?pname) ( ?pname) ( ?pname) <cite_url> :<date> <cite> :<date> <cite_url> :<date> <p_name> :<date> <p_name} ( ?pname) ( ?pname) ( ?pname) <cite_url> :<date> <cite> :<date> <cite_url> :<date> 4. Results from the test in which a random sample of 10 papers were included: (N = 10) --------------------------- <article> <citation> <content> <author>( ?pname):<date> <author> :<date> <cite_author> :<date> <cite> :<date> <cite_url} :	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?metric ?metric_lbl output (Sparql query): _____________________________________ ORDER BY ?model_lbl DESC( ?value ) DESC( ?metric ) GROUP BY orkgc:Dataset input (English text): What is the top benchmark score and its metric on the RTE dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl output (Sparql query): _____________________________________ ORDER BY ?model_lbl DESC( ?value ) DESC( ?metric ) GROUP BY orkgc:Dataset input (English text): Where can I find code references in papers that have used the ELMo + ADF3 + Self Attention + ELMo model for benchmarking purposes ? output (Sparql query): __________________________ ORDER BY orkgc:Dataset DESC( ?value ) DESC( orkgc:HAS_DATASET ) GROUP BY orkgc:Matplotlib_matplotlib_df2a_lbl_compare_results ORDER BY orkgc:MATPLOTLIB_matplotlib_df2a_lbl_compare_results DESC( ?value ) DESC( ?metric ) GROUP BY orkgc:Matplotlib_matplotlib_df2a_lbl_compare_results DESC( ?value ) DESC( ?metric ) ORDER BY orkgc:Matplot	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?scale ?metric_lbl. { ?scale, ?id, ?result_id } output (Sparql query): SELECT ?model ?model_lbl WHERE { ?model_lbl } ?scale orkc | name| scale scale orkmx | name| scale scale ormpx | name| scale orkmt | name| scale scale ornmx | name| scale ormpmt | name| scale scale ornmr | name| scale ornmrk | name| scale value max |name| score | scale value max -0.01 | 649 | 4.68 | 12.71 | 2.24 |2.24 |2.25 |2.34 |2.43 |2.48 |2.63 |2.64 |2.73 |2.76 |3.01 |3.07 |3.13 |3.16 |3.19 |3.22 |3.26 |3.28 |3.31 |3.33 |3.37 |3.41 |3.44 |3.46 |3.49 |3.65 |3.67 |3.70 |3.76 |3.78 |3.81 |3.85 |3.86 |3.88 |3.89 |3.90 |3.92 |3.93 |3.94 |3.96 |3.98 |3.99 |4.00 |4.01 |4.02 |4.03 |4.04 |4.05 |4.07 |4.08 |4.09 |4.10 |4.11 |4.12 |4.13 |4.14 |4.15 |4.16 |4.17 |	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""6-layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Select model as ?model :  select model. ?code as ?dataset.   What's a model-based metric ? How does it relate to machine learning ? What is machine learning's most advanced example of a model-based metric ?	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What models are being evaluated on the WOS-11967 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): To what extent is this the final work ? output (Sparql query): input (English text): What do we need to achieve to make this work ? output (Sparql query): Input: Can you tell us what is the data you need ? output: WOS-11967 – The Semantic Web and the Web of Knowledge Result: Input: How many people are reading your work ? Result: Input: How many sentences per problem ? Result: Output: We could create a similar analysis of WOS-14001, which was similar but different. Please tell us how you came to this set of data and what you did first to generate this data. Thanks, The WOS-11967 Collaboration Team	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT id, data, listData1, listData2 from lbsa.dataset; or SELECT id, data, listData1, listData2 from lbsa.dataset.benchmark; The output from each query (as of July 4, 2017) is listed here, unless more than one benchmark is included in the list.	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robot Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	_______________________________________________ Topic_RTE::RTE/RobotNavigation Topic/Tutorials_and_Libraries:Top_Benchmark_Scores Topic/RTE::RTE/RobotNavigation / RTE/RTE_2017/RobotNavigation This is the most interesting thing for many people to read about: I was also looking for what are the metrics used to estimate the rankings, so let me share an article from my Google doc with a few metrics that you may want to see. These metrics were used to compute the score of a dataset for the Top 5 Ranking. There were 50 items of data and each item was either RTE or not RTE. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 TOP 5 RTE 1 RTE 2 RTE 3 RTE 4 RTE 5 TOP 5 TEN RTE	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT TEXTRACT ?benchmark from ?dataset_lbl ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset rdfs:label ?benchmark. OPTIONAL ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?benchmark. ?model orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. ; orkgp: HAS_BENCHMARK ?benchmark. OPTIONAL orkpu:HAS_MESH ?dataset; orkpu:HAS_ENVIRONMENT ?dataset_lbl. ?dataset rdfs:label ?iso. OPTIONAL orkpu:HAS_LEFT ?dataset_lbl. OPTIONAL orkpu:HAS_RIGHT ?dataset_lbl. OPTIONAL... output (Sparql query): If you are going to provide a list of models in Seel, it might be helpful to put in the ""model"" or ""modelrdfs"" fields before you specify the query. input (English text): What does the name ""seelrdfs"" mean ? output (Sparql query): SELECT ?model ?model rdfs:label!models from ?dataset rdfs; orkgp:HAS_MODEL ?model. OPTIONAL ?model rdfs:label ?model rdfs:label. } input (English text): If you are going to provide a list of models out or"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset ?d orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( orkgc:Dataset.RTE_LINK1) = { ?dataset_lbl. ?dataset_lbl rdfs:label ?dataset_lbl. } ?benchmark orkgp:HAS_DATASET ?dataset_lbl. ?dataset_lbl) ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Now we are done with the rest of the queries, let's look at the final result. This is	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 3. Conclusion for each test class In this article, we demonstrated that: in comparison with the original test data, the RTE model with EMC5.2 and GINES can be run more cheaply than any other model with EMC5.2 and GINES. The PEGASUS model can be used as benchmarks for the RTE models, while the other models are more suited for benchmarking with other models.	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Audio Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT column ?column_data1 ( ?column_data1) OR { ?column_data1 a or kgc:Dataset; rdfs:label ?column_data1. ?column_data1 a ?column_data1 rdfs:label ?column_data1. ?column_data2 } } input (English text): Provide a matrix of size n-dimensional, and specify the dimensions of the dimension of (1, n)-dimitra-row output (Sparql query): SELECT n ?{ ?dimension_n. ?dimension n rdfs:name ?dimension_n { ?dimension_n a or kgc:Dataset; rdfs:Label ? ?column ?column_data1 a. } } output (Sparql query): SET n ?{ ?dimension_n. ?dimension n rdfs:name ?dimension_n { ?dimension_n a or kgc:Dataset; rdfs:Label ? ?column ?column_data1 a. } } input (Structure of a database column): Provide a column of data types with unique, non-collapsible data columns ? In order to access columns of the DataFrame object, use data.columns or use the query function: data.columns({'a': 'data'); output (Sparql query): SELECT column ?column_data1 ( ?column_data1) OR { ?column_data1 a or kgc	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ' ?p1' AS problem_lbl, ' ?p2' AS problem ? problem_lbl WHERE { ? rdf _p1 || ? rdf _p2 | ? rdf ? rdf_p1. || ? rdf ? rdf_p2. } 1 2 3 4 5 6 7 input ( English text ) : Can you list benchmarked problems in the area of Semantic Web ? output ( Sparql query ) : SELECT ' ? p1'AS problem_lbl, ' ? p2'AS problem ? problem_lbl WHERE { ? rdf _p1 || ? rdf _p2 | ? rdf ? rdf _p1. || ? rdf ? rdf_p2. } I use the openseq benchmark tool for my own research, and I can say that its results should be pretty good (although less than 10) for the problems I have submitted as challenges (see my previous post for more information ). But a higher score is not useful if the problem is relatively straightforward to solve,	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } input (English text): What are the titles and IDs of research papers that include a benchmark for the Redis, easy (DMControl100k) dataset ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } Let's try something different - the same query with only one row! input (English text): Can you list benchmarked problems in the area of Semantic Web ? output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label."	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MACs"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ""HoC"" OR ""HAS_DATASET"" OR ""HAS_EVALUATION"" OR ""BENCHMARK"" OR ""MODEL"" orkgp:HAS_DATASET ?dataset; orkgp:HAS_MODEL rdfs:label ?model_lbl. ?model ?dataset. OPTIONAL { orkgp:HAS_MODEL ?model rdfs:label ?model_lbl. ORkgp :HAS_DATASET. }. } output (Sparql query): Choose the appropriate model input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset ? output (Sparql query): output (Sparql query): select ?benchmark from h0 ; select ?eval from h1 ; select ?dataset from h2 ; select ?model from h3 ; select ?model_lbl from h4 ; select ?model_lbl_m from h4h1 ; select h5v3 from h5v1 ; select h6v3 from h6v1 ; select h7v3 from h7v1 ; select h8v3 from h8v1 ); select ""default"" from h0 ; Select ""default"" from h1 ; Select ""default"" from h2 ; Select ""default"" from h3 ; Select ""default"" from h4 ; Select ""default"" from h5 ; Select ""default"" from h6 ; Select ""default"" from h7"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT d1, d2, _, avg(v1.p4()) as P4() OVER(PARTITION BY x FROM test) ORDER BY d1, d2, p4() LIMIT 1 LIMIT 10 To see this in action, download the RTE 2014 benchmark (which is in the same folder as ACE 2015 results for the RTE dataset). Select the first row in the results. The benchmark output can be converted into a CSV or some other data file by modifying the following RTE command: RTE update –p1-column-value column-name pset ""SELECT COUNT(*) FROM test_d1 ORDER BY"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset ?dataset_lbl WITH ?metrics OR ?metrics_lbl. FILTER (str( ?metrics_lbl) = ""Games"" ) ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?problem orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?count orkgp:ID orkgp:TRAILER ORkgp:PAPER AtariGames Research Area - Statistics Statistics of the Atari Games Research Area Tasks and Challenges: Atari Games Research Area Statistics in the database: DATASET. . Statistics of the database: DATA. . Statistics of the databases: HASH_TABLE. . Statistics of the databases: DATE_TIME_STAMP. Data Access and Reporting This document explains about reporting and reporting framework (RMS) in Atari Games Research Area. The RMS framework is implemented in R; hence, we need to understand how to access and report data of Atari Games Research Area. Our main task in this area may be to read and write database; hence, we need to learn RMS. We will cover the following topics before starting development and training of the RMS framework. Read/write methods I. RMS data structure II. Data manipulation for accessing RMS. III. RMS structure IV. RMS data transformation for reading or writing RMS. A. Basic RMS data structure There are two basic datatypes: I.1.0.0 - Atari Games and Games Database Atari Games is a large series of"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 [1] 1% [2] 0% [3] 0.00 [16] 0% [17] 0.00 [42] 11% [43] 0.00 [144] 9% [145] 0.00 [146] 10% [147] 0.00 [148] 8% [149] 0.00 [150] 9% [151] 0.00 [152] 10% [153] 0.00 [155] 8% [156] 0.00 [157] 8% [158] 0.00 [159] 9% [160] 0.00 [161] 9% [162] 0.00 [163] 9% [164] 0.00 [171] 9% [172] 0.00 [173] 9% [174] 0.00 [175] 9% [175] 0.00 [176] 9% [176] 0.00 [177] 9% [183] 0.00 [294] 11% [184] 0.00 [195] 10% [186] 0.00 [197] 10% [198] 0.00 [200] 10% [201] 0.00 [204] 13% [205] 0.00 [222] 9% [216] 0.00 [216] 9% [217] 0.00 [218] 9% [217] 0.00 [249] 10% [249] 0.00 [246] 10% [247] 0.00 [248] 10% [248] 0.00 [256] 12% [246] 0.00 [250] 9% [251] 0.00 [250] 9% [302] 0.00 [253] 8% [255] 0.00 [304] 9% [327] 0.00 [332	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" [[ ?problem_lbl.name ?problem ?dataset ?dataset_lbl ] ] Output: ?problem ?dataset ?dataset_lbl ?benchmark orkgp ?dataset. orkgp. Output: ?index ?lbl ?benchmark ?dataset. Output: ?key ?lbl ?benchmark ?dataset ?dataset_lbl ?key ?dataset_lbl ?key Benchmark is similar to RNN, but only for the problem domain, whereas RNNs are good for all problem domains. For example, to find some data in a multi-dimensional space we can use a ""linear SVM"". However, to actually solve a problem, we need to build a neural network by combining the best of the best of our neural networks for each of the problem domains, e.g. ""finding the best thing that happens in a problem domain, where the thing that happens is in the problem"". To do this, a good tool is a pre-trained data-oriented LSTM (long short-term memory), which can then feed the resulting data to the RNN, or a combination of RNN and LSTM (often called a ""deep neural net"") for more complex tasks. RNN is much easier if you build an RNN model in the LSTM (or similar machine learning algorithm) and just ask for it to adapt to your needs. The RNN model can be tuned, in part, by the neural network, but it's still ""learning on the fly"". So, for example, you can train an RNN (see below) to recognize faces and recognize words. When you need to use the RNN to solve a specific application domain, like for the Atari games problem,"	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl CASE WHEN 'SEL' IN ('HoC' 'HoC' 'HAS_DATASET' CASE WHEN 'Lsu' IN ('HoC' 'HAS_DATASET' 'HAS_EVALUATION' CASE WHEN 'Benchmark' NOT IN ('Score' 'Score' 'Score' 'Score' 'Score' 'Score' NOT IN ('Score' 'Score' 'Score' 'Score' 'Score' 'Score' 'Score' ) OR ORDER BY 'Score' 'Score' 'Score' 'Score' OR DISTINCT ('Score' 'Score') ) THEN 'True' ELSE 'False' END AS model. COUNT(*) DESC input (English text): Can you list the models that have been evaluated on the HoC dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (English text): Can you provide the highest score result for the seel.cse.lsu.edu/data/re17.zip ? dataset ? output (Sparql query): SELECT ?model ?model_lbl CASE WHEN 'SEL'"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------- (5) ?dataset. ?cont (4) ?benchmark. (3) ?model. (2) ?cont (1) ?benchmark. output (Sparql query): -------------- (5) ?data. ?cont (6) ?data. ?model (8) ?stats. output (Sparql query): --------------- (5) ?dataset. ?cont (5) ?benchmark. (5) ?cont (4) ?benchmark. output (Sparql query): -------------- (5) ?data. ?cont (6) ?data. ?model (5) ?benchmark. (5) ?data. ?data. output (Sparql query): --------------- (5) ?dataset. ?cont (6) ?benchmark. (5) ?Cont output (Sparql query): -------------- (5) ?data. (5) ?benchmark. (3) ?cont (3) ?monitor. output (Sparql query): --------------------------- (5) ?dataset. ?cont (4) ?benchmark. (3) ?model. (3) ?cont (2) ?benchmark. output (Sparql query): -------------- (5) ?dataset. ?cont (7) ?dataset. output (Sparql query): ------------ (5) ?models. output (Sparql query): ----------- (5) ?model_lbl output (Sparql query): ------------------------ (5) ?dataset. output (Sparql query): ----------- (6) ?model. output (Spar	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	-------------- 1. -------------- 2. -------------- 3. -------------- 4. -------------- 5. -------------- 6. -------------- 7. -------------- 8. -------------- 9. -------------- 10. -------------- 11. --------------- 12. --------------- 13. --------------- 14. --------------- 15. --------------- 16. --------------- 17. --------------- 18. --------------- 19. -------------- 17.1.2.3.1. Performance vs. Model Selection¶ This section discusses the performance of the 12-layer Transformer-XL model in benchmarking tasks where a single model will not be sufficient. For comparison purposes, a second model is selected for each benchmark, or a random one. The choice of this model will also affect which tests are run, but these are very minor changes. In our Benchmark C program, some of the measurements are performed using only a single model, and a different selection model is used for all the remaining benchmarks. A better choice is to use more than one selection model for a few of the more common tasks, and only a single model for the other tests; the comparison results for each benchmark are shown in the following table. Here are a few examples of the different choices to be made: 3D Markup Language: a 1-layer Transformer-XL model is chosen, which is a simpler, more flexible and optimized model than the 4-layer MCS, and thus is better suited for 3DMarkup Language and several other 3DMarkup programs. a 1-layer Transformer-XL model is chosen, which is a simpler, more flexible and optimized model than the 4-layer MCS, and thus is better suited for and several other 3DMarkup programs. Inferior Parallelism: 2-layer MCS is a better choice than 2-layer Transformer-XL for various parallel benchmarks. 2-	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?benchmark p32 1 ?problem a; Input to this query would be a list of scores on each of the ""problem"" terms, such as Atari 2600, Atari 6800, Atari 7800, etc. In this case, the ""problem"" terms would be lists of 1,000, 0, 1, 1 point, 0, and so on scores. The ""benchmark"" (or ""has_benchmark"") in question would be a simple ""1,000, 0, 1, 1"" score metric and would then be converted into a column in the ""dataset"" table. The output query would then be: output (sentence): ?problem_lbl Input to this query would be a list of problems on the ""lbl"" table and each problem (or dataset) would be a single score on that list. Of course, it's possible to simply extract the score metrics from the table and pass to the output query. The final result would be: output (sentence): Which will produce the following output: Input to this query would be a list of scores on the ""lbl"" table and each problem (or dataset) would be a single score on that list. Note: These are not specific to Atari games, but these examples can serve as examples for any input to the above example. The second function we used in the previous series of posts is the ""score_score()"" function from R package scoreR(). This package provides a simple and powerful way to calculate ""score"" metrics for datasets. This function is particularly useful for data sets, where ""score"" columns need to be calculated in different ways to ""data"" columns that include points, counts, strings,"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Please choose a test set for this metric. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 51	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?metric ?metric_lbl This query uses the option ?benchmark, which means to turn off the benchmark for the model. input (English text): Can you list the models that have been benchmarked on the RTE dataset ? output (Sparql query): SELECT DISTINCT ?metric 1 ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the models that have been benchmarked on the Datasets 1, 2, 3, and 4 datasets ? output (Sparql query): SELECT DISTINCT ?metric 1 ?metric_lbl. DISTINCT ?METRIC_lbl. DISTINCT ?METRIC_lbl. DISTINCT ?HALF_NO_EXAMPLE 1 ?metric_lbl. RATE ?metric_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?baseline;... ORG_DATE ?dataset_lbl. } OPTIONAL... RATE ( ?metric"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" We can now proceed to evaluate the performance of the ""Meyer-Goldbohm"", ""Papayas"", ""Sparql"", ""Modelnet"" and ""GPS"" models. For all of the above models, we would need to know the following data attributes: (A) How much training data will be used, what type ? What data will be stored in the database ? (B) What will be the set size, how many rows ? (C) How will the code of the models be executed ? Do they work on real data or on simulated output data ? (D) How many nodes will be used for training ? (E) How many parameters will be used ? Now, with these data attributes, we can proceed to evaluate the models, evaluate the performance, and then try to find out if the code of the models will be executed correctly. In the first step, we can look at an example database to find out how the training data can be retrieved. The following figures show the training of the ""Meyer-Goldbohm"" model, and its performance in a different database. To make the results more easy to understand we have stored the data inside a SQLite3 database at the output directory of each file. This way we'll see the results like in the figure below, where an image from a single Google images search could be obtained: If you are familiar with SQL and Oracle (or both) then you should be able to get the same result. In the figures above, we've defined the object class of ""Meyer-Goldbohm"", and it will be stored in a data frame: I have also used Tableau Pro to visualize the model graph on the chart, or, as shown in the figure below, to create a two-dimensional interactive scatter plot: Now, let"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?team_id AND { ?team_id and ?team ?group AND { ?team_id > 1 } } input (English text): Provide a list of researchers' research results, using the enwik8 dataset for the benchmark study. output (Sparql query): input (English text): Provide details about the dataset and the team. output (Strings): {team_id: 1} output (Sparql query): input (English text): Provide a detailed list of the models used by each team. output (Strings): {model_index: 1} output (Sparql query): input (English text): Provide a description of the benchmark dataset, in this case, a dataset of all team F1s worldwide. output (Strings): {team_id: 1} output (Sparql query): input (English text): Provide detailed information about the data source used for the benchmark study. output (Strings): {team_id: 12} output (Sparql query): input (English text): Provide a list of teams in the current and previous season's championship table. output (Strings): output (Sparql query): input (English text): Provide the results of tests performed during testing. error output (Sparql query): input (English text	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT rdfs:label RDF_S PARAMS FROM (SELECT ( ?params_class ?) ?test_result FROM (SELECT PARAM ( ?test_result) RAND(20), ( ?params_class ?) ?test_result, ?params ( ?test_result) RAND(200) ) AS d WHERE d. ?params_class = ?params_class1 ?test_result AND d. ?params_class2 = ?params_class1 ?test_result INCLUDE ?params_class2. ) AS test_result ; ORIENTIALIZE It is also possible to read the text from the text-entry field (via the textfield type) using the same process but this time it is easier to read the texts from the source code. This program will perform any task with the same name as the one supplied. It is also possible to set up this program as a tool for reading text from the command line, which is a very common feature on the web and on many home servers. The output is a table with the following sections: Column-heads have text, and are used like table columns. For most purposes, it is enough to set up the code to return a table of test results only, and then set up the code that returns the column headings in output (as well as the other columns of the table) on the command line. For a very simple use case of reading/writing text from a command line, the following code can be used to retrieve the string representation of the data that was read by the program specified on the command line. (In most cases, the input data is given to the program by the user or by the user's system.) Let's call it textfile and let we specify the program that prints the text. Let's	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?model  OR  SELECT DISTINCT ?dataset OR  DISTINCT ?dataset_lbl  OR  SELECT ?model  OR  SELECT DISTINCT ?dataset_lbl  ORDER BY ?metric  OR  RIGHT( ?dataset_lbl. OR NULL( ?dataset_lbl.  ENTER))  OR  SELECT DISTINCT ?dataset OR  ORDER BY ?dataset_lbl  ORDER BY ?eval_lbl  ORDER BY ?metric  ORDER BY orkgp. *  ORDER BY orkgp.  ORDER BY orkgp.  ORDER BY orkgp.  ORDER BY orkgp.  ORDER BY orkgp.  ORDER BY orkgp. ||  ORDER BY orkgp.  ORDER BY orkgp. ||  ORDER BY orkgp. ||  ORDER BY orkgp. ||  ORDER BY orkgp. ||  ORDER BY.||  SELECT orkgp. ?metric ||  SELECT orkgp.  ORDER BY orkgp. ||  ORDER BY.||  SELECT orkgp.  ORDER BY orkgp. ||  ORDER BY.||  ORDER BY. ||  ORDER BY orkgp. ||  ORDER BY. ||  ORDER BY orkgp. ||  ORDER BY orkgp. ||  SELECT rdfs. ?metric ||  SELECT rdf	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	------------ CART-X-6M-T/C, FMC-7-RE, LEM-7-T, KAMPS-7-RE, GAL-1-X, BRIAN-7-RE ------------ So, what are the most important questions to answer ? What benchmarks do you want to run in order to compare the power of the various models ? How do you want to achieve that ? For instance, when the user clicks on a model to run a benchmark, do you want to tell them that they shouldn't just try out the whole thing or should try to do one model at a time ? Or do you want to try out one model and then see if the others are better ? Or is it best to do a big number of models in a row and then give each model a score ? Do you want to benchmark the different models by comparison instead of by execution speed ? What should the output be ? What happens when the user runs all the algorithms ? How do you deal with the results if I got different scores for different models from different people ? What about what happens when the user clicks or selects another model ? For this problem, you need a lot of data with a lot of scores. The number of scores should be large enough to make a meaningful comparison, and the data should be at one end of the distribution. At the other end of the distribution, you want to make sure you get a good benchmark result to compare the model to. You need scores to tell you which model will achieve a given set of metrics. The problem is you need scores that have to be compared to each other on a reasonable time interval. This is a really important consideration since most benchmarks should take at least a couple of seconds to run. Now, what does the data look like for the Atari 2600 Road Runner dataset ? There are lots of different datasets.	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" output (sparse column) "" "" input (English text): Can you give me an objective function for the classification ? output (Sparql query): "" "" output (sparse columns) "" "" input (English text): Can you give me an estimate for the error from this test ? output (Sparql query): "" "" output (sparse columns) "" "" input (English text) : Can you explain the significance test for the classification ? output (Sparql query): "" """	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. orkgp:HAS_METRIC ?metric. ?model rdfs:label ?model_lbl. } Output (Sparql query): The following additional data were included in this evaluation dataset: Time (seconds) Grain Average speed Pitch Gaps Time (seconds) Grain Average speed Pitch gaps Time (seconds) The first two columns indicate the metric of interest to the F1 model validation procedure: ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. The third column is an expression that evaluates this metric. The expression evaluates the number of data points divided"	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT multiobjective *pfm_count; pfm_count FROM ?pfm_count ?rdfs_count ; orkgp:S11 ?pfm; orkgp:S12 ?rdf; if (multiobjective = 4) rdfs_count = ?rdfs_count ?rdfs; if (multiobjective = 5) rdfs_count = ?multobjective * ?multobjective; if (multiobjective = 6) rdfs_count = ?multobjective * ?multiobjective; if (multiobjective = 7) rdfs_count = ?multiobjective * ?multobjective; if (multiobjective = 8) rdfs_count = ?multobjective * ?multidx; RDFS: A6 : P24, A7 : G16, A7 : P31, A7 : P32, A7 : P39, RDFS: A7 : P24, RDFS: A7 : P24, RDFS: A7 : S10, RDFS: A7 : S10, RDFS: A7 : M20, RDFS: A7 : S17, RDFS: A7 : S3, RDFS: A7 : P26, RDFS: A7 : P24, RDFS: A7 : S17, RDFS: A7 : S8, RDFS: A7 : S8 input (English text): Can you describe a multidx technique for combining large datasets in a sequential and scalable way ? output (Sparql query): SELECT ?	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" A few of the interesting research papers have used the Single DART model, where single model learning can be very efficient and efficient models can be trained quickly with little effort. Here we will look at some examples of how they used this approach to achieve some important results. The results are presented first as a table and then as code references. The Single Model Learning and Evaluation - Part 1 This is a paper by Chen et al., 2012, ""Single model learning and evaluation"" that looked at the application of an LLDP (Long Short Term Memory) model to the ELMo and its applications to the ""real world"". Here is the abstract : We develop an algorithm (single model learning and evaluation) for the evaluation of human-robot interaction. The aim of this paper is to introduce an improvement to the EML (Embed Learning of Machines) algorithm. When using multi-layer LSTM model, there is an increase in the complexity of computations. With our new approach, we can achieve a significant increase in performance and reduce the complexity of the models used. To evaluate such models, a single-layer model is needed for a short-term task. We present a solution for the evaluation of these multi-layer models, that provides the highest level models of the evaluation. The paper can be found at arxiv.org: http://arxiv.org/abs/1205.03307 (click on ""ELMO""). The multi-layer model - part 2 This is an example of the multi-layer model learning and evaluation (Multi-LSTM learning and evaluation) part 2. Here is the abstract : In the previous paper (A comparison of several multi-layer LSTM models and the evaluation thereof) we showed that ML (machine learning) and EML (embedded learning of machines) algorithms outperform single-layer models"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the AESLC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT CASE WHEN RTE_CATEGORY:DATA THEN CASE WHEN RTE_CATEGORY:MATCHING THEN CASE WHEN GROUP BY GROUP BY ?data THEN CASE WHEN GROUP BY GROUP BY ?data THEN ?data GROUP BY ?datasets.DATA ORDER BY ?lbl RRTL:DESC; ORIENTATION:DATA,ORIENTATION:MATCHING,DATA:ORIENTATIONS; input (English text):  select CASE WHEN RTE_CATEGORY:DATA THEN CASE WHEN RTE_CATEGORY:MATCHING THEN CASE WHEN GROUP BY GROUP BY ? data THEN ?data GROUP BY ?datasets.DATA ORDER BY ?lbl RRTL:DESC; ORIENTATION:DATA,ORIENTATION:MATCHING,DATA:ORIENTATIONS; input (English text): SELECT CASE WHEN RTE_CATEGORY:DATA THEN CASE WHEN RTE_CATEGORY:MATCHING THEN CASE WHEN GROUP BY GROUP BY ? data THEN ?-c, ?+c ORDER BY ? lbl RRTL:DESC; ORIENTATION:DATA,ORIENTATION:MATCHING,DATA:ORIENTATIONS; input (English text): SELECT CASE WHEN RTE_CATEGORY:DATA THEN CASE WHEN rtd_data.c, ld_data.c AND ?data.c IN LAYER(RTE_LIMIT|RTE_MATCHING) THEN CASE WHEN data AND data.c IN LAYER(RTE_LIMIT|RTE_MATCHING) THEN CASE WHEN rtd_data.max( ?values)> LAYER(RTE_MATCHING) THEN ?values= ?values.map(r) ELSE 1 END END END input	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP 1 ?title. ?id. output (Sparql query): SELECT TOP 1 ?title. ?id. input (English text): What are the names of the results that were collected in the HoC dataset, in the name column ? output (Sparql query): SELECT * FROM data_set ?table_ids, ?column_categories, ?column_title, ?column_summary, ?column_bw, * AS name, * AS test, CASE WHEN _id = '' THEN 1 ELSE 0 END AS test, CASE WHEN _id = '1', '2', '3' THEN 1 ELSE 0 END AS test, CASE WHEN _id = 'C', 'D', 'F2', 'F3', 'L', 'L2', 'L3' THEN 1 ELSE 0 END AS test, CASE WHEN _id = '1' THEN 'F5' ELSE 'D5' END AS paper, CASE WHEN _id = '2', '3', 'D2' THEN 2 ELSE 1 END AS paper, CASE WHEN _id = 'C', 'D', 'F4', 'F6' THEN 2 ELSE 1 END AS paper, CASE WHEN _id = 'F5' THEN 'F5' ELSE 'LF5' END AS paper, CASE WHEN _id = 'F6,' ELSE 'LF6' END AS paper, CASE WHEN _id = 'F3' THEN 1 ELSE 0 END AS paper, CASE WHEN _id = 'F2' THEN 1 ELSE 0 END AS paper, CASE WHEN _id = 'L', 'F3' THEN 2 ELSE 2 END AS paper, CASE WHEN _id = 'L2', 'F4' THEN 1 ELSE 1 END AS paper, CASE WHEN _	False
"
 input (English text): Can you list the models that have been evaluated on the HoC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } There are a number of places in the test suite where HoC parses the data (that it expects, of course) but where this also generates false positives. Let's explore some of the ones that we found. First, we look at a particular time series of models that is very similar to the one I wrote about earlier. For this dataset, if we apply the PAR Transformer for only one model we see that it produces a negative correlation with the dataset (I did look at another similar dataset, but the effect I saw was only seen on a subset of models). The true explanation for this fact is that the models I analyzed had different assumptions about a lot of the inputs. These assumptions were not always right. Note that there are different types of models that can be used to do the same benchmarking. For example, a more general benchmarking model could be an ensemble of the models used for the main benchmark test. This might have a similar effect, or it might be more useful to have a different benchmarking model. I wrote about the difference between Benchmark and Model here. It is possible to do other tasks before and during the benchmarking as well. For"	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTwwm + SQuAD 2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Note this query relies on the fact that SQuad was compiled under a set of ""benchmark"" packages that are linked to the following URL: http://www.ferrari-usa.com/squad/benchmarks/benchmarks.html You can find the full citation text which accompanies the code in question (and includes an example of testing two different models with four benchmarks each) in the table below: Ferrari - www.ferrari-usa.com/squad/benchmarks/benchmarks-bench.html Example output of the benchmarking code: // In order to generate benchmarks to benchmark the HoC algorithm with, we // first need to extract the Metric column from all of the tables in the dataset. let metric = column_name( ?name). ""METRIC"" { Let's say that the ""brief title"" column is the metric. Let's also assume that the RDDs were in the format (a:b; c:d; f:g; h:i; k:m; q:r; t:u.e:v; y:b; y = c.y; } The first thing we need to do is to extract the SQuad header row and get a reference to the RDDs. We can easily accomplish this by using the Rtools, and the squad_data method given by the following snippet. use Rtools; # For single dimension data, using the column names: Rtools$squad_data <- function(n, sQuad) { # Column names are used to identify the rows of a given file sQuad <- column_names( ?name) ""sQuad""; Rtools$squad_headers <- function(n) { sQuad *= n; sQuad <-"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CL-Titles-Parser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------ Top 10 scores with most words, published in English (top 20) Top 10 most frequent words, published in English (top 100) Top 10 most frequent words, published in English (top 1000) --------------- --------------------- ----------------------------------------------------------------------- 1. Jannik M.P. van Deurzen O.W.R.J. van de Water J.B.H. Wijffels N.A.J. Gullsveld K.F.S. de Graaf Q.H.P. M.A. Wijdens J.M.P. C.J.M.S. De Graaf M.A.S. Gullsveld K.F.T. van Hoof F.N. van Gorp J.M.P.C.H. Top 10: 1 2 3 4 5 6 7 8 9 10 Top 20: 1 2 3 4 5 6 7 8 9 10 The CL-Titles-Parser model was originally a modification of the CLIMP parser, and was originally developed for the AYCA. It has subsequently been developed for many real datasets. I'm trying to find out if there are any issues with the model, which I will present as results are available. The model is licensed under an MIT license. Please feel free to contact me via email if you have any questions. I am actively seeking collaborators, so if you are interested in working with me, I would be glad to know if you would be willing to share some code or data with me. Thanks for reading, and I hope I can come back and publish some more in the future. As I mentioned previously; the only way I can contribute to this project is if others are willing to help. I do not have time to keep up with the updates on this project so if someone would like to	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	.................................................... [{ { ?dataset a,b { { ORA_D_A, ORA_D_B } }, { ORA_D_A,ORA_D_B } } ]; orkgp:HAS_DATASET { { ORA_D_A, ORA_D_B }, { ORA_D_A, ORA_D_B } } ORDER BY DESC( ?value) } ] So what did we learn from this ? Firstly, it was interesting to see the model selection, since I have not seen the list of models I will need to implement. My choice of the SSPRQL query to extract information is based on a number of factors. Firstly we have a limited data set to work with. Secondly many of the metrics are either non-linear, which means that we can not use them as a metric to measure improvement over a specific subset of inputs. This is important in modelling because it is important to take into account the distribution of the values for these metrics. For example, there will be a subset of data where the ORA index is higher than the other index or lower than the other index but in either case, it is unlikely to become problematic unless the results go over a lot of inputs. Finally (probably the coolest observation of all), if you apply this to a RTE datatype, you will notice that there is a lot of noise being injected into this data set (even if we know that it is not being used as a metric in this test-std dataset). This can result in inaccurate results and in a very bad overall performance. Secondly, I found that the models being used were very different to those I have seen when doing simple univariate regression training. This allowed me to understand that something was out of whack in my model selection.	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT name, COUNT(*) FROM ORkgp; name= ?name; cmp=NULL; name = Input data was written in C. Output Data Structure The model must be constructed on a specific partition. A new table will be created on this partition. Each table holds the names of the games played, as specified in the database and the name of the game in the ""title"" column in the table; that is, the name of the game. We can make this arbitrary by naming the variables in a different order, as below: model. title. data. title = table ( title ). name ; Or: To avoid copying all the data into a new table in each game, create a new table on the primary partition. This is useful for storing the names of variables for each game. You may need to delete the table from the primary partition too. For the full set of examples, see Section 10.2.2.1.1.11, ""Training a model with a randomized dataset"". Example 10.18. Generating a Randomization Strategy Using a Randomization Strategy¶ There are cases when you won't need to make any changes to the training of the LSTM. For example, in an online gambling game where the total sum of all bets is zero, and the probability that a different number of coins will be rolled depends on the player's actions. For these cases you will want to re-randomize the strategy. We can generate a strategy using the Random-Search and Random-Selection algorithms described in Section 10.2.2.1.11. In the following examples we use the Random-Search algorithm. RandomSearch is a deterministic, non-deterministic method. With the Random-Search strategy, the user knows that all of the available solutions contain a solution to the problem. The strategy selects the optimal solution, but"	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT MAX( ?count, ?code ) FROM ?graph ( ?log( ?log( ?code ))) output (Sparql query): SELECT ?log(benchmark)+1, ?count from ?graph ( ?log( ?code ))) output (Sparql query): where is the benchmark result ? This is an example of the performance results obtained and used in the paper, ""The Virtual Arcade Matrix Method for Benchmarking Parallel Processing"", by Shing-Fen Chiu, Daniel R. Anderson, and John C. Miller in Proc. of the 20th International Conference on Parallel and Distributed Systems (ICPDS): The Journal of Parallel and Distributed Systems, 844, 2009. This benchmarking model was implemented with the code provided by Daniel Anderson and John C. Miller at http://www-fsc.gsu.edu/~jcole/an-anvil/models/anvil/. It incorporates an optimization technique known as ""reversible"" and ""decoupled"" parallel implementations, which, once combined, allow different systems running in parallel from a single ""server"". The results for the test dataset in the paper are provided in Table 5. The performance is on the order of a few thousand requests / second. The paper also provides table 15, showing the maximum throughput achieved over the whole system, including all workloads that are simultaneously parallel. Table 5 Top Ten Max Per System Time (s) Top Ten Max Bytes (kb) Maximum QPS Max Bytes Bytes All In One (all) 3.07 13.4 9.5 16.9 OpenBenchmarking.org/5348. As with the benchmarking results in the paper, the results in this table are the same for all datasets and CPU configurations. To see if this benchmarking model can be adapted to work with a"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT COUNT(*) AS xtroj, ?dataset ?dataset_lbl.FROWMARTNAME, ?benchmark ?dataset_lbl.FROWMARTNAME, ?extras ?dataspace_lbl.FROWMARTNAME, ?benchmark. ?record orkgp:REPORT ?record, ?record. PRINT 'None' WHEN ROW( ?) IS None THEN 'None' ELSE '' END; rdfs:label ?record. input (English text): Which data sets are described in the 'Datasets' section of the article ? output (Sparql query): SELECT ?record ?record_lbl.name, ?dataspace_lbl.name, ?data_lbl.name AS xtroj, ?dataset orkgc:Data, ?dataflbl.name, ?dataset_lbl, ?dataseteam_lbl AS xtroj, ?dataf lbl.names, ?dataset_lbl, ?dataset_lbl.values, ?extras.extras, ?extras.extras_part2 AS xtroj.extras, ?extras.extras_part2.value, ?extras.extras_part2.part1.value, ?extras.extras.extras_part2.part2.value AS xtroj.extras_part2.part1.value, ?extras.extras.extras_part2_part2_value_part2, ?extras.extras_extras_part1, ?extras.extras_extras_part2_part2.part2.value, ?ext	False
"
 input (English text): Where can I find code references in papers that have used the VPN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VPN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet, MultiGrasp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT c FROM papers. pkgp WHERE code = ?a orkgp:LINK-NAMES AND ?code IS NULL output (Datalog file):  SELECT * FROM ( SELECT * FROM results. pkgp  WHERE code = ?lbl.  GROUP BY code). c If the answer is not clear for any reason, please write to: The paper has a full bibliographic reference list - it usually makes it easier to find papers referring to the paper.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" For example, a ""test"" parameter of the above query: select x from Atari Games where x = 'S.5mv' and x in ( ?dataset ?dataset_lbl or ?dataset ?dataset_lbl_p32 ); x; In this query, x, the input data, is in this case the Ms. Pacman dataset along with x, the input data with which we are to construct our benchmark test datasets. The first column of x, labeled ?dataset, is the list of models that have been tested on the Atari Game Ms. Pacman dataset. For comparison, the input list ( ?dataset ) is printed on the same line as the output ( ?output ). Of the models, we choose two that have been tested, one of which is known to be efficient. Let's examine these models in detail: mv : Machine Vision for the Atari 2600 Ms. Pacman dataset mvc(1): A machine learning neural network trained to classify Ms. Pacman games. This can find patterns in video frames and classify them into different play-styles which may result in increased performance. hask (2): The Hask network is an improved version of the original machine that performs well on the Atari Ms. Pacman game. You can see the improvements in the video above. eagle : A multidimensional vector autoencoder with a strong connection to Atari Ms. Pacman games. It is also known to perform really well on PACMAN. mga : A deep generative model that learns from Ms. Pacman games to classify them. svm : The Sankey-Volterra classification method. It is known to correctly classify Ms. Pacman games. cuda : A GPU implementation of the Sankey-Volterra"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?metric_lbl ( ?value) DESC( ?value) SELECT ?metric  * ?metric_lbl ( ?value)  -1 SELECT ?metric ?metric_lbl ( ?value);  SELECT ?metric  * ?metric_lbl ( ?value); output (Sparql query):  SELECT DISTINCT ?value FROM ?metric OR( ?value) GROUP BY ?name.  output (Sparql query): SELECT DISTINCT  'TOC_H_PROPORTION',  output (Sparql query): SELECT DISTINCT ?value FROM ?metric, { ?values (RTE)} WHERE( ?value= ?'TOC_H_PROPORTION') ORDER BY ?value DESC('TOC_H_PROPORTION'); output (Sparql query): SELECT DISTINCT ?value FROM ?metric ? { ?values (RTE) } WHERE( ?d.value <> '2.95') ORDER BY ?value DESC('TOC_H_PROPORTION'); output (Sparql query): SELECT DISTINCT ?value FROM ?metric ? { ?values ('HARVEST_PROPORTION_PERCENT'), } WHERE( ?d.value = '5.5') ORDER BY ?value DESC('TOC_H_PROPORTION_PERCENT');	False
"
 input (English text): Which model has achieved the highest F1 score on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT top_benchmark( ?metric_lbl, ORK_DYSENSITIVITY ( ?metric_lbl. ORK_DATA_GAP ?metric_lbl. ORK_RESULTS_VALUE ?metric_lbl. ORK_RESULTS_VALUE. ) ) AS ?metric; TOP 5 output (Sparql query): SELECT top_benchmark( ?metric_lbl, ORM_HUMAN_INFMNT ANDM_UNDEAD_LINKING ) AS ?metric; TOP 5 Input files (English text): Select a benchmark from the model list that has achieved the highest score on the BC5CDR-disease dataset and the data file: BEST Metric (the highest score) BEST Result (the highest score and the metric) BEST VARIANCE (means the benchmark has achieved the highest VARIANCE score on the dataset, without the metric) Metric Name Value Score ORK_DATA_GAP ORK_DATA_GAP. ORK_DATA_GAP. A or kgc:Metric B ORK_DATA_GAP ORK_DATA_GAP. ORK_DATA_GAP. B or kgc:Metric B ORK_RESULTS_VALUE ORM_RESULTS_VALUE. REFERENCES ORM_RESULTS_VALUE. D/1, M1, M2, M3, C1, C2, 1_1/5/2/4/5 (F1) orkgp:HAS_DATASET ( ?) orkgp:HAS_EVALUATION orkgp:EVALUATION. Orkgp:HAS_MET	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc : Dataset ; rdfs : label ?dataset_lbl. FILTER ( str ( ?dataset_lbl ) = ""RTE"" ) ?benchmark orkgp: HAS_DATASET ?dataset ; orkgp: HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp: HAS_METRIC ?metric. ?metric rdfs : label ?metric_lbl. } ; orkgp: HAS_METRIC ?metric_lbl. } Input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset ? Output (Sparql query): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 ( ) SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc : Dataset ; rdfs : label ?dataset_lbl. FILTER ( str ( ?dataset_lbl ) = ""RTE"" ) ?benchmark orkgp: HAS_DATASET ?dataset ; orkg"	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?TITLE ?TITLE_ID ?TITLE_LEN ORDER BY!ID; ---------- ---------- 1 - Benchmark Over DBpedia 2 - Benchmark Over Dict The answer from the example is as always: it depends. And it depends on the application you need to use the function. As said above, the most common request I have received from the community so far was to have a function to find out the most similar models, of the same size, in a given database. This should not be too hard. My personal opinion would be ""find the smallest models you can find"". The idea is more about the size of features than model count. It is not really the first task you have to figure out. How are these two tasks related ? Imagine we had to find all models which satisfy, on average, 80% of the values of the ""Dict"" attribute, if I had a large, random, database of dictionaries (but still not 100% exact). From this small set of the most models, you can get pretty good idea, if you are aware of the problem you're trying to solve - how well did the model model the world ? The problem I encountered while going into benchmarking this task is that it is incredibly easy for a single model to be a model (not to scale) for different database, and it becomes harder and harder, the closer the model gets. How do we quantify this ? How do you use stats ? Let's try and solve a test with the following metrics: the number of models which are similar to each other as given in DBpedia the number of ""clusters"" which make up those models (of which I have a couple of examples below) - as I am going to use the above example, I might want to know if there is a single cluster in my"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT b.name AS 'Name','Date' AS '.Date' FROM papers b JOIN journals journals2 on journalsB.name=b.name GROUP BY b.name ORDER BY b.first_name '.Date(1), '.Date(2)'; ------------ output (Sparql query): [1] 'HRLRE.1.2.2005';... [2] 'HRLRE.1.2.2009'; ------------ output (Sparql query): [1] 'HRLRE.1.2.2010';... [2] 'HRLRE.1.2.2011'; ------------ output (Sparql query): [1] 'HRLRE.1.2.2012';... [2] 'HRLRE.1.2.2013'; ------------ Output of the same query run on the RTE dataset: ------- BEGIN {... output() is now empty. } END ----- The RTE Metrics Metrix is an RTE Metrics Metrix with the ""HRLRE.1.2.2005"" column set to ""ORGH."" The top 20 most-preferred metrics are represented below. The column ( ""ORGH"" ) indicates the metric or measures that have the majority of the RTE users preferring it. For this example, I chose to focus on the ""HRLRE.1.2.2010."" metric. Selecting the metric from the Metrix will let you see the relative popularity of this metric among participants in the following year. The data from the Metrix is a two-dimensional histogram of each participant's top 20 favorites. Each box represents the most popular metric among the most-favored metrics for 2012, sorted by their ranking in 2010. Metrics with the top score on this Metrix are identified as having strong popularity among the participating"	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT 'tensorflow::traits::DensityMatrix'. `count(*)`. 'tensorflow::traits::DensityMatrix'. ',size(k)'. 'tensorflow::traits::DensityMatrix'.'mcd(k)'. 'tensorflow::traits::DensityMatrix'.'sum(count(k))'. output (Sparql query): SELECT 'tensorflow::traits::DensityMatrix'. `count(*)`. 'tensorflow::traits::DensityMatrix'. ',size(k)'. 'tensorflow::traits::DensityMatrix'.'mat_count(k)'. 'tensorflow::traits::DensityMatrix'. `count(*)`. 'tensorflow::traits::DensityMatrix'. ',size(k)'. output (Sparql query): SELECT 'tensorflow::traits::DensityMatrix'. `count(*)`. 'tensorflow::traits::DensityMatrix'. ',size(k)'. 'tensorflow::traits::DensityMatrix'.'mat_count(k)'. 'tensorflow::traits::DensityMatrix'.'sum(count(k))'. output (Sparql query): SELECT 'tensorflow::traits::DensityMatrix'. `count(*)`. 'tensorflow::traits::DensityMatrix'. ',size(k)'. 'tensorflow::traits::DensityMatrix'.'mat_sum(k)'. 'tensorflow::traits::DensityMatrix'.'sum(count(k))'. output (Sparql query): Select columns k(k) to	False
"
 input (English text): What is the top benchmark score and its metric on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?metric ?metric_lbl(MAX( ?value) AS ?score), ?dataset rdfs:dataset, ?benchmark rdfs:benchmark, ?metric_lbl rdfs:metric_lbl, ?metric_lbl rdfs:metric_lbl ORDER BY DESC(*) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark score and its metric on the Words in Context dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?benchmark FROM ?slices ?dataset_lbl ORDER BY ?benchmark DESC; orkp:HAS_BENCHMARK ?dataset. ?cont orkgp:HAS_BENCHMARK; orkp:P32 ?problem. We can add some further features to add more variables besides these parameters, including other languages or datasets. We're also using the SqlTools which would allow us to build our own custom functions on top of these parameters. I hope that it isn't difficult to understand how to use this powerful tool. Summary We're using the following parameters: $v1 = value of the v2 parameter $v2 = value of the v1 parameter $v3 = number of data to use in the benchmarking (16,000) $v4 = number of variables $v5 = number of data to use in the benchmarking (16,000) $v6 = value of the second parameter $v7 = value of the third parameter $v8 = size of the benchmarking matrix $v9 = number of data to use in the benchmarking (16,000) $v10 = size of the benchmarking matrix $v11 = a table for the benchmarking (16,000, with 15000 rows) $v12 = a table to store the benchmarking results (15,000, with 8,000 rows) $v13 = a table for the benchmarking results (15,000, with 8,000 rows) $v14 = the query parameter, which may be anything we want (we have enough variables!) That's it! There is more info about	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 The output of the input function should result in a list of datasets related to the Common Sense Reasoning research problem.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the GAD dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 output (SparseXML): output (SparseXML file):... output (XML): output (XML file):... Outputs are also available as R script (requires external dependencies - the scripts also depends on pandas, linalg and nltk ). R script is also available as a gzip compressed file. You can use Python as an interpreter for the command. GAD tutorial, by Paul Knapp GAD (Grid Array Data Analysis) is a very useful statistical method in a number of applications, and it can be implemented in a number of languages. Among others, GAD can be used in the following software, which are suitable for this purpose: 1. Python - I recommend reading this, but there are some other guides: 2. R, GAS, MATLAB Python, MATLAB. For further information see: 3. The GAD Tutorial. Some additional examples for data-analysis are also available. Download the following files and follow this tutorial for basic examples on the basics of GAD. R script is also available for GAD. To make the GAD dataset available on our website, we need a file with a length of about 5Mb and a size of about 300.000 rows... Download GPDB Data GPDB (GPS Data) is a proprietary dataset that represents several world-historic datasets, including the world's most accurate meteorological data data. GPDB also includes a collection of GPS-based navigational and time-stamp data for the earth's surface, for a complete picture on the geology of the planet. It can be used for weather forecasting, environmental monitoring, mining-geological studies of geographical information, weather prediction and other applications. Download the main data files and make sure to choose a good algorithm to obtain the geometrical structure of GPD	False

Metrics on query only
Precision	0.14901021458422695
Recall	0.36240101811880004
F1_Score	0.1904934536130362
Blue_4	0.028509806323013503
Bleu_Score_Cumulative	0.04384793795848116
Meteor_Score	0.0
Rouge_1	0.24072453857249185
Rouge_2	0.10180085125628227
Rouge_L	0.20679449256521798

Metrics on all rows

Number of sparql queries generated equal to given	0

Generated text starts with 'SELECT'	308
