Questions	SPARQL in Gold Standard	SPARQL generated	exact match
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the CNN / Daily Mail dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric { ?metric ?metric_lbl } FROM ?eval; (This example is an older algorithm that is no longer supported by orkgp) output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER(str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL{ ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English): Which metric in the data set is usually used to evaluate the quality of the CNN / Daily Mail model ? Output Select the metric that you want to evaluate. First, check if a metrics is available for this metric: SELECT rdfs:metric || ""CNN / Daily Mail""; OR rdfs:metric || ""Image_1_L"" IF rdfs:metric.metric.lbl. is NOT NULL; OR rdfs:metric || ""CNN / Daily Mail"" If rdfs:metric.metric.lbl. is NULL You have selected an item with a data set object for the metric. You can find additional Metrics for this type of models in the Metrics article. You can also check this document and Metric_Comparisons section of the orkgp documentation for further details. Also check the orkgp website."	False
"
 input (English text): List the code links in papers that use the Nyströmformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Nyströmformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?dataset AS title, ?dataset AS id, ?benchmark AS data ?category, ?bibliography_id FROM ?benchmark JOIN ?dataset on ?benchmark. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. ?dataset. } The output is: The output is: # Load the nyströmder model from a.csv file # # Please note that you have limited storage space # for this long output CSV = '$(data_output-format)a.csv' # load the np2 model from a.csv file model = numpy.load(csv, 'n_model2_samp_dataset.csv','samp_',... ) # load some data from a.csv file f = open('sample_dataset') for line in f:... # write the output into a long file for row in df: print('LF:', line) # read the CSV in from disk while True: line = f.readline() f.close() data_out = f.read() csv_out = model.parsed(csv_out) f.close() # If they include one of your articles in a benchmark the output is: # else, the result will be the same thing but only includes the papers that # include one benchmark in the benchmark list, not those over it. # No need to worry about this. output = '#Benchmark_LF(Nyström-Deterministic)', # Output to the standard output. output.append	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DrQA (Document Reader only) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA (Document Reader only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the UrbanSound8k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): What models are being evaluated on the UrbanSound8k dataset ? output (Sparql query): If you have any questions or comments, please contact me at drquaker AT gmail DOT com.	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?benchmark, ?benchmark_lbl, rdfs_value, ?benchmark_vars, rdfs:type, rdfs:id, ?paper_lbl, ?paper_vars, ?n orkgc:L_BAR, ?dataset_lbl, ?cont, ?lab, % Results may be obtained by querying the following command: output (Sparql query): SELECT ?n as ?n ?cont for ?lab with ?:str( ?lab ) ?dataset_lbl, ?cont as ?cont, ?lab with ?lab_lbl, ?normal as ?normal, ?normal_lbl as ?normal_lbl, ?test as ?test ?benchmark, ?""benchmarks"" as ?benchmarks,!""cont"" as ?""cont"", ?""benchmarks_lbl"" as ?benchmarks_lbl,!""benchmark_vars"" as ?benchmark_vars, ?""benchmark_vars_lbl"" as ?benchmark_vars_lbl, ?dataset_lbl as ?dataset_lbl, ?""benchmark_vars""_lbl as ?benchmark_vars_lbl, rdfs"	False
"
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the JNLPBA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JNLPBA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the MAZEA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP(1) ?benchmark AS top_score, ?metric AS ?metric_tbl, ?scores AS TOP(1) ?scores_tbl, ?metric_lbl AS top_metric_lbl, ?measurements AS TOP(1) ?measurements_tbl FROM benchmark SELECT ?model as (, { ?metric_lbl} as ?metric_lbl), ?eval as ?eval, ?meas as ?measurements, ?score as (SELECT STATEMENT( ?metric_lbl) WHERE { { ?metric }. ?metric_lbl } END) AS score, ?scores AS TOP(1) ?scores_tbl, ?metric_lbl AS top_metric_lbl, ?measurements AS TOP(1) ?measurements_tbl FROM measurements GROUP BY ?measurements GROUP BY ?measurements_tbl ORDER BY score DESC, ?score DESC Mixed Data. One can use mixed results but this is not recommended. For more details check the section Mixed Data Design and Implementation. 2 – Mixed Data with Multiple Metrics A mixed outcome of two or more metrics can be achieved using simple statistical tests like those described in this post. 3 – Mixed Data with Multiple Values of One or More Metrics In some cases a mix of some or all the metrics may be needed or desired. This can be achieved by creating a mix using R. 4 – Mixed Data with Additional Metrics In some case additional metrics would be required or desired. For example, one might want to calculate the number of passes over the different time spans. 5 – Mixed Data with Additional Data In many cases additional metrics can be needed or desired. For example, one	False
"
 input (English text): Can you list the metrics used to evaluate models on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the TDMSci dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Which tests measure the quality of models evaluated ? output (Sparql query): input (English text): Please describe the parameters used to evaluate models on the TDMSci dataset. output (Sparql query): input (English text): What do you define as a quality level of a model ? output (Sparql query): input (English text): Please describe and describe how you define a model as being 'underpredictable' ? output (Sparql query): input (English text): How do you interpret the results of these models ? output (Sparql query): input (English text): The results of these tests, which measure the generalization capability, are very good. However, these tests have a lot of low-confidence inferences because of the statistical assumptions of those tests. What can you do about this ? output (Sparql query): input (English text): Can you specify additional parameters to be tested using more empirical studies of the model, at least in order to allow some confidence in the results ? output (Sparql query): input (English text): Can you specify additional parameters to be tested using the TDMSci dataset and more empirical studies of the model, which would allow some confidence in all of the results ? output (Sparql query): input (English text): Can you discuss the relationship between test performance and overall accuracy ? output (Sparql query): input (English text): Would you like to include this analysis as part of the documentation for this project ? input (English text): Can you define additional parameters to be tested using more empirical studies of the model, at least in order to allow some confidence in the results ? output (Sparql query): 	False
"
 input (English text): What models are being evaluated on the BC5CDR-chemical dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the mean capacity of a carbon-based fuel?
 output (Sparql query): "	"SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, ""([0-9]+),([0-9]+)"", ""$1$2"" ) ), ""([0-9]+).*"", ""$1"" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), ""(fuel|CO2)"")) } "	SELECT DISTINCT ?miles of fuel ?miles. OPTIONAL {} input (English text): Which is the best predictor of future fuel consumption ? (This query is not in the SQL-RDBMS output file). output (Sparql query): SELECT ?min orkgp:HAS_GRAPHS ?max orkgp:HAS_MINUTE ? ; input (English text): How do the outputs of the BCRMs compare with those of the BCS ? Please see the output table of the BC5CDR data on the BCRMs. The BCS output data files are very small and contain all the data that can be stored in a database without re-encoding. Note that I am not aware of a tool whose SQL-RDBMS output can be viewed in PDF format. Most importantly, I am not aware of a SQL-RDBMS with large files (many hundreds or thousands of rows) in which a spreadsheet program can be run without losing any data.	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the WOS-11967 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?table ?table_id ?table. ?table_tid, ?table_text, ?table_date_no, ?table_date, ?table_author, ?table_reporter, ?table_location AS ?table, ?table_poster, ?table_titles, ?table_notes, ?table_timestamp, ?table_id, ?table_subsys, ?table_sys, ?table_field, ?table_dataset, ?table_model, ?table_lbl FROM ?table_schema ?table_dataset_schema WHERE ?table_object_schema = ?schema WHERE ?table_schema = ?schema2 ?table_dataset_schema2 AS ?table_model2 FROM DATASET ?table_model. ?dataset_schema2. WHERE ( ?type!= ?dataset_type2 and ?dataset_schema2!= ?schema)) AS a INNER JOIN ?table_titles and ?table_notes1 on ?table_titles. ?table_note1, ?table_notes1. FROM ?table_notes as ?t1 INNER JOIN ?table_titles1 as ?t2 ON ( ?table_notes. ?table_note1 = ?t1. ?table_note2. ) AS (t1. ?table_note1 = ?t1. ) on ?table_date1. ?table_date2 and ?table_month and ?table_day1. ?table_day2. AND ?table_month1. AND ?table_year1. ORDER BY DESC( ?value) LIMIT 1 ? output (Sparql query): SELECT SUM( ?table_titles	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT dp. ?dataset OR kp. ?dataset_lbl FROM ?dataset_lbl JOIN ?lab ; dp. ?lab_lbl_i, kp. ?lab_lbl_i, dp. ?lab_lbl_i, kp. ?lab_lbl_i, dp. ?lab_lbl_i, kp. ?lab_lbl_i_lbl, kp. ?lab_lab_lbl_i_lbl as ?lablbl_i, kp. ?lablbl_i_lbl, dp. ?lab_lab_lbl_i_lbl_i as ?lablbl_i_lbl Ranking The scoring that each experiment in a pair will receive is: Accuracy: The most accuratiou (or likely result) among the experiments that are run and the average (or likely result) among all the experiments that are run in the pair. Quality: The least accuratiou (or likely result) among the experiments that are run in the pair. Parity: The least accuratiou (or likely result) between pair of experiments. Score: The least accuratiou (or likely result) on each experiment that is put into the pair. To calculate the accuracy score, the score of each experiment will be divided by the accuracy score of the other experiments in the pair. The score is measured with the following formula: P *( Accuracy + Quality) / Accuracy + Quality If the experiment that performed well in the other experiments, is more likely to rank highly in a ranking, the score for this experiment should be bigger. If the experiment that performed poorly is more likely to rank badly, the score for this experiment should be smaller	False
"
 input (English text): Which model has achieved the highest F1 score on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Note that the models are ordered in order of preference. We have used the words ""model"" with the right parameter, as there is a lot of confusion on this question. You can see the full table here: Data Model Order in WMT2016 Romania-English Benchmark dataset Note that ""model"" has been assigned 1 as an index on a table that has no model (as seen in the previous results), and all other terms have been assigned 0. This is because it is impossible to compare two different models. However, if you wish to compare different models only if there is a chance of a crossover, you can define a different index (in S-Exp or SQL) that only contains models, not indexes. For the reasons given earlier (i.e. a mismatch between two models with crossover possibility!), to solve the crossover problem, you should also create a partition between two models (to ensure that the model with the highest score is a binary model). This is described in more detail when the question of ordering the models comes up, later. A note on RDF and SPARQL In order to explain the concepts involved in the following examples, you'll need to know a bit of RDF and SPARQL. Let's start by briefly describing RDF. RDF is an open standard for unstructured metadata that can be retrieved as a vector or an object in XML format. SPARQL is an advanced query language which is primarily intended for user interface applications, allowing developers to query information about information content. SPARQL defines a query language that allows you to query information about an object or dataset based on a simple syntax, and the SPARQL operator "":"" will be used to construct and extract data from a structured query language (XML, JSON, …). This allows you to easily communicate information about data"	False
"
 input (English text): List the code links in papers that use the GGCNN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GGCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GGCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } Note: it is possible to specify as many rows as your dataset supports. For"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AI-KG dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AI-KG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?metric ?metric_lbl } FROM dbo.sparql.models dbo ; with select (name; id); and then use GROUP BY (name; id1); (18 rows) This also makes sense: in this case, if I wanted to specify my metric as a comma separated values field, I can do it by specifying a grouping clause between the select terms: select { { SELECT ?metric ?metric_lbl } FROM dbo.sparql.models dbo where { ?dataset a orkgc:Dataset } ?group by { { SELECT ?metric ?metric_lbl } FROM dbo.sparql.models dbo group by { ?dataset a orkgc:Dataset }... } AND { { SELECT ?metric ?metric_lbl } FROM dbo.sparql.models dbo where { ?dataset a orkgc:Dataset ?dataset } ?group by { { select ?metric ?metric_lbl } FROM dbo.sparql.models dbo and { ?dataset a orkgc:Dataset ?dataset }	False
"
 input (English text): Can you list the metrics used to evaluate models on the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 TABLE ( ?name ?ID) -- title: orkgc-benchmark ( ?name ) ID -- title_id: orkgc-benchmark.txt orkgc-data.tar.bz2 orkgc-file.tar.bz2 output (sql) (output_file, data, column_values): { orkgc-data.tar.bz2|orkgc-file.tar.bz2|orkgc-file.tar.bz2} or kgc-data.tar.bz2 output (stdout) [ : { orkgc-data.tar.bz2 | or kgc-file.tar.bz2 |or kgc-file.tar.bz2} | or kgc-file.tar.bz2 | or kgc-file.tar.bz2 | or kgc-file.tar.bz2 | or kgc-file.tar.bz2 | or kgc-file.tar.bz2 | or kgc-file.tar.bz2 | or kgc-file.tar.bz2 } or kgc-file.tar.bz2 ] or kgc-file.tar.bz2 output (stderr) [ : { orkgc-data.tar.bz2 | orkgc-file.tar.bz2 |or kgc-file.tar.bz2} | or kgc-file.tar.bz2 |or kgc-file.tar.bz2 |or kgc-file.tar.bz2 |or kgc-file.tar.bz2 |or kgc-file.tar.bz2 |or kg	False
"
 input (English text): Which model has achieved the highest DISTANCE_TO_GOAL score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""DISTANCE_TO_GOAL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 The model which performs best on the TDM Tagged Corpus benchmark dataset is: As an example, if one had a choice between the following models for the Habitat 2020 test-std benchmark dataset, it is not clear to me that the first choice is worse: Model Name (metric) Quality (metric) R2(r) P Score Error (r) 1.0 CATEGORY V 1 0.071 0.021 -0.004 0.0350 2.0 EGG1 V 2 0.043 0.032 -0.012 0.0750 3.0 AIG V 2 0.046 0.031 -0.011 0.0749 4.0 AIG2 V 1 0.083 0.023 -0.022 -0.0849 5.0 HAT V 2 0.044 0.0301 -0.017 0.0924 6.0 IAG V 1 0.099 0.027 -0.015 -0.0851 7.0 IFV V 2 0.048 0.026 -0.016 -0.0851 8.0 ORV2 V 1 0.069 0.040 -0.051 -0.0909 9.0 ORV3 V 2 0.047 0.026 -0.015 -0.0851 10.0 ORV4 V 1 0.063 0.032 -0.006 -0.0852 If it were my choice, the first choice would be the better choice. In fact, in some cases, the second choice is better than the first one. What do you think ? As can be seen by the figures above, this is very suggestive and it is possible that many more metrics related to these model functions could be added in the future.	False
"
 input (English text): What is the average efficency for experiments?
 output (Sparql query): SELECT AVG( ?efficency_values) WHERE { orkgr:R155266 orkgp:compareContribution ?contrib. ?contrib orkgp:P43156 ?efficency. ?efficency rdfs:label ?efficency_labels. BIND(xsd:float( ?efficency_labels) AS ?efficency_values) } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 The output has the following attributes: - A list of models that have outperformed the benchmark on the BC5CDR-disease dataset by the specified number of standard deviations. - A short description of the analysis that led to the benchmark prediction accuracy. - An S3 document that contains the model, associated test statistics and R-DAE results. The R-DAE results is the R-DAE value compared to the default R-DAE, which provides an indication of the accuracy of the prediction.	False
"
 input (English text): What models are being evaluated on the Car speed in Liuliqiao District, Beijing dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Car speed in Liuliqiao District, Beijing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ? model ?model_lbl WHERE { ? dataset a orkgc : Dataset ; rdfs : label ?dataset_lbl. FILTER ( ? str ( ?dataset_lbl ) = ""Car speed in Liuliqiao District, Beijing"" ) ? benchmark orkgp : HAS_DATASET ?dataset ; orkgp : HAS_EVALUATION ?eval. ? paper orkgp : HAS_BENCHMARK ?benchmark. OPTIONAL { ? paper orkgp : HAS_MODEL ?model. ? model rdfs : label ?model_lbl. } } The second output tells us how to run our query again on the new input data. As expected we get the same results: output (Sparql query): UPDATE DATASET, _ (a, b) SET ?a = ?rfa(a,b) ?rfa(a,b)= a rfa(a,b)= b OPTIONAL { ? paper _ ?model } The second time to run, we can see that the model was faster: output (Sparql query): UPDATE DATASET, _ (a, b) SET ?a = 1 RDFS(a,b)=a RDFS (a,b)=1 OPTIONAL { ? paper model } The model is faster because we could extract the first part of the data from the original dataset and then do an alternative prediction. The"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the IMDb-M dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" Example 5: a comparison with LSTM Let's look at the LSTM-like feature-related code which, as the output suggests, is based on the model of the MIST model on the IMDb-M dataset: @datasets.examples.LSTM import scala.collection.mutable as collections def benchmark(a, c, d): """"""Benchmark a model on the MIST-M subset using the LSTM."""""" for x in range(1, 5): a = a + (1.0 * c) d = sum(x) for k in range(1, 5): c_ = k - 1 d_ = d * k if d: return(a[i, j]) else: return c * d_ Note that the code is also similar to benchmark above but used only for the LSTM-like performance test. This is because we decided to use the same LSTM in the benchmark that in the LSTM-M part of the model example. Using the LSTM on the IMDb-M dataset is quite comparable to using the LSTM on the test dataset, i.e. LSTM outperforms both of them. It would be much more suitable to benchmark on the data set directly, i.e. the test dataset. Unfortunately, LSTM performance is not fully solved as yet. The LSTM in the benchmark is not the same as the LSTM inside the implementation. We will be able to address this in the future. Example 6 [3] Using only the LSTM in the benchmark for comparison  Example 7:  Benchmarking on both the LSTM and LSTM-M subset  One way is to use the combination of LSTM and LSTM-M data (e"	False
"
 input (English text): List the metrics that are used to evaluate models on the Ohsumed benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 We may now generate a query that retrieves the metrics from the benchmark dataset (e.g., the metric_lbl variable in the original code example). In doing so, we may be prompted for the dataset name and the metric for each score: input (English text): Show title and ID of the most popular papers that discuss the benchmarks ? output (Sparql query): The best possible approach is therefore to explicitly specify the metric to be used, then provide the ID or title of a paper for the metric to be applied to. 4.4. Benchmarking your algorithm based on score or metric We now return to our example from above: input (English text): Show the metric score that was used to evaluate our algorithm and the result of our optimization ? output (Sparql query): ( You can now add this score to the benchmarking results to compare how close you really are to the best possible score. If you are using the default settings for the metrics variable, the best score computed was 1.24. You might think that the best possible score will not be very different from the metric score of 1.24, but in reality, you might very much be closer! In my opinion, this is an important area where optimization is critical: it will result in different results depending on the measure(s) used. 5. Performance analysis In this section we are going to analyze our benchmarking performance, and determine whether it was worth the extra effort to use SPSS. SPSS is very common to use in computer science as it is powerful and very widely installed. However it is also more difficult to use for speed testing tasks, due to limitations on its use. However, there are a number of other alternatives that can be used to analyze your performance: the same technique is also valid for	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?label ?input orkgg:CHARSET ORkgg:GENESIS. FILTER (rdfs:label ?input. LANGUAGE NOT LIKE 'englinese') ?metrics orkgs:Metrics WHERE { ?dataset. ORkgc:Dataset; rdfs:label ?input. ORkgc_lbl. } input (English text): What are the metrics of evaluation over the SIN dataset, 5 seconds at 12 kHz dataset ? output (Sparql query): SELECT DISTINCT ?label ?input. FILTER (rdfs:label ?input. LANGUAGE NOT LIKE 'englinese') ?metrics orkgs:METRICS WHERE { ?dataset. ORkgc:Dataset; rdfs:label ?input. ORkgc_lbl. } input (English text): Why, what are the authors ? output (Sparql query): SELECT COUNT(1) as authors,COUNT(T) as first_authors FROM all_authors,all_authors_authors,all_authors_sources group BY authors,first_authors,T,first_authors_authors-1 group JOIN authors groups on author_id = authors_id order by authors ASC; and for an output, output (Sparql query): (id)title(language),title(locale) output (Sparql result): ID Author1 Publish Date ISBN Notes Publisher 1 1 2016-01-23 NIDA SciGEN paper 1 2 2018-07-01 National Research Council International Statistical Classification of Abstracts 2 3 2016-01-29 NIDA SciGEN paper 2 4 0 2018-12-13 International Statistical Classification of Abstract	False
"
 input (English text): Provide a list of papers that have utilized the Unregularised mLSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unregularised mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Flair-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?sdf_filter_cid ?code WHERE { ?model_flair_TDM OR ?model_flair_lbl.   ?sdf_filter_cid = { ?flair_filter_cid ?flair_filter_cid. } ?db_index_name ?db_id OR ?db_index_name = ?db_id; OR ?db_index_cid ?db_id. } input (English): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code ? output (Sparql query):  SELECT DISTINCT ?sdf_filter_cid ?""code OR ?""code"" ?""db_index_name"" ?""db_id"" OR ?db_index_name ?""db_id"" ?""sdf_filter_cid"" ?""sdf_filter_cid"" ?sdf_filter_cid"" ?""db_id"" ?""sdf_filter_cid"" ?sdf_filter_cid"" ?""db_id"" ?""sdf_filter_cid"" or kgp_flair_tdm_model ?db_id."" OR kgp_flair_tdm_model """""" input (English): Provide a list of papers that have utilized the GCSM-TDM model and include the links to their code ? output (Sparql query):  SELECT DISTINCT ?sdf_filter_cid ?""code OR ?""code"" ?""db_index_name"" ?""db_id"" OR ?db_index_name ?""db_id"" ?""sdf_filter_c"	False
"
 input (English text): What is the name of the top performing model in terms of Top 1 Accuracy score when benchmarked on the Food-101 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 select ?txt_1 :transformer_XL_Base * 1 AS top_score,   ?txt_2 :transformer_XL_Base * 1 AS average_score,   ?txt_3 :transformer_XL_Base * 1 AS error_score,   ?txt_4 :transformer_XL_Base * 1 AS best_score from publications where author = 'Andrew V. Williams' where text = ?{ ?txt_1 :transformer_XL_Base,   ?txt_2 :transformer_XL_Base} or ?txt_2 is ?dataset, input (English text):  Can you provide links to code used in papers that benchmark the Transformer-XL Base model ? output (Sparql query):  select ?txt_1 :transformer_XL_Base * 1 AS top_score,   ?txt_2 :transformer_XL_Base * 1 AS average_score,   ?txt_3 :transformer_XL_Base * 1 AS error_score,   ?txt_4 :transformer_XL_Base * 1 AS best_score from publications where author = 'Andrew V. Williams' where text = ?{ ?txt_1 :transformer_XL_Base,   ?txt_2 :transformer_XL_Base} or ?txt_2 is ?dataset, *This question and answer was extracted by the crawler (i.e. manually) from http://digikey.com/digi/digi/transformer-xml-output-8-1/top.jsp. There are few other questions that I was not able to find answer to in the post by Williams (	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Enduro dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT STATISTICAL_ADJUST (score) ?benchmark. ?cont orkgp:P31_ORIGINAL_RESULT FROM thesaurus.models.benchmark.lst2.criterion_stats GROUP BY ?benchmark ?benchmark_cont ORkgp:P31 INNER JOIN thesaurus.models.benchmark.lst2.criterion_stats ON thesaurus.models.benchmark.lst2.criterion_stats. ?benchmark_cont, ?benchmark_cont_lbl ON thesaurus.models.benchmark.lst2.criterion_stats. ?benchmark_cont, ?benchmark_cont_lbl_lbl INNER JOIN thesaurus.models.benchmark.lst2.criterion_stats AS thesaurus ON thesaurus.models.benchmark.lst2.criterion_stats. ?benchmark_cont, ?benchmark_cont_lbl_lbl INNER JOIN thesaurus.models.benchmark.lst2.criterion_stats AS thesaurus_crit ON thesaurus_crit. ?benchmark on thesaurus.models.benchmark.lst2.criterion_stats. ?lst2.criteria_stats. output (Sparql query): The above parameters were selected using the ScapeR package for R. The following benchmarks can be selected to generate the output: benchmarathon : (1-2) Benchmarathon a (Benchmarathon b) (20% errors) > b_50k.parallel; benchmarkmarathon::b_50k_bench_parallel test.R Benchmarathon a (Benchmarathon b) (20% errors) > b_200k	False
"
 input (English text): What models are being evaluated on the Scholarly entity usage detection dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?label ? ? ?model_lbl... OR: If, due to an error during your analysis run and a duplicate paper is found the output will be that the duplicate paper was assigned a single model of use with the SAN. If the citation ID is 0 we then assume the output is that the paper was referenced with the SAN in question. All citation ID IDs are also included in the output output in order to identify which researchers are writing the citations. To see the citation IDs for a particular paper you can look up the paper in our citation database. We recognize that there are cases where we have output a second duplicate paper which has a different paper as a title or abstract or in the list of authors. Such cases are tracked and when it is determined that the duplicate exists a new paper will be produced for the title or abstract. Note that in such cases if the paper is not a new paper it is not immediately removed from the database. Rather, we look to see if the previous duplicates have been flagged as duplicate in the database. Note that the output can go through a full transformation from a dataset to an entity. For more details about the transformation process see our documentation on the ""entity transformation process"". The output of the above example can be seen in this figure: http://scholar.google.com/scholar ?hl=en&id=hgNxW2G7DxZKF8eVXE7yCb3NyXFVqZGg&sig=2B-tFZoH1ZiKlTd2JnqkQGjTQg&source=url It is important to note that in case that the output is of the order of a new paper that a new paper is printed. In addition, duplicate paper"	False
"
 input (English text): Provide a list of papers that have utilized the DQN hs model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Input: If you'd like to participate in benchmarking your model, please send an email to the following address: You may read more about the ACE 2005 benchmarking program here. If you have a problem with the benchmark methodology, then please email the authors and discuss the issues with them.	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Habitat 2020 Object Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PNDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 There are a few studies that had published code references in their initial papers (in English, without using the term 'Habitat 2020') that may be useful to know before publishing your own code. I have made no attempt to collect them and some of them have already been covered in the previous posts in the series. I would include: the study that used the MCS model and tested it within the same sample; the study that used the MCS model with an initial dataset; the study that used the MCS model with an initial subset of 2nd generation species, before adding additional data over time; the study that used the MCS model with a subset of 7th and 8th generation populations of the same species (and with an initial dataset); the study that used the MCS to test the effects of the introduction of new species outside the original target range (see above); the study that used the MCS model for a randomised controlled trial to test for differences in species fitness as a function of the quality of habitat quality. Please see the blog posts in the series for further details on this study. For this post, I would also include a code reference that describes a benchmarking strategy that has been used in a study using the MCS in a sample of Australian wildland fire ants. I have included a version of the benchmarking strategy in my own paper that appears in the paper 'The ability to quantify the value of habitat quality in a large Australian fire ant experiment' by P. H. Jones, D. W. Griffiths, J. A. Anderson, J. G. White, and R. E. Fusco (2013), 'Evolution of habitat-quality measures in free-ranging fire ants using habitat-quality scoring: effects of species composition and management' by A. P. Roberts, C.	False
"
 input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS-XSUM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (single model)"") ?benchmark orkgp:DESIG ?dataset. ?cont orkgp:DESIG ?benchmark. ?cont orkgp:DESIG ?data; orkgp:BENCHMARK ?dataset_num. ?data_num. ?data_num. ?data_num. ?data_num. ?data. ?connt orkgp:ENIG!dataset_num. The code references in the documents above all refer to the output dataset of the benchmarks (the first two are the benchmark data sets used to train the model). I have used the same benchmarking data to train the model, a version of The Random Forest that ran on a Tesla K20x at about 80% accuracy, but as mentioned earlier, I will use an updated version from SANS that runs on the Tesla K40x at a much higher accuracy. The benchmark data are available for download on my Github page. The code references referenced in the documentation above refer to a version with an update. It is a ""slightly revised"" version of the code that should have the following improvements: The number of layers is reduced to three (e.g., ""x_l_4""). This reduces the cost of training. If I have less work to do, it is more expensive to train. In the original version, this has been reduced to one. The first layer is no longer explicitly selected. This improves the performance of the model on sparse data such as those provided by Google in the original Benchmark paper. The model is trained on a fixed matrix"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset GAD?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------------------- -------------- In this example we set a benchmark result and then filter the query result by metric (or metric_lbl), then then filter (only) the metric (or metric_lbl) if it has a value greater than.01. In the case of GAD with data with an H2O column I should now have a metric of 4.2 (2.4 of a) and get this out of the query to calculate the benchmark for IMDb-B by using the "" ?"" operator. So here is the output from executing the query. As you can see I have 3 benchmark results. Now what happens if I run the query using a metric with a value larger than.01 ? Here is my output after setting metric = 4.00, which will set metric_lbl = 3.65. Note that no one ever wrote down an integer metric that was larger than.01, to ensure the integrity of metrics in GAD. One could have written a new METRIC_LBL if the benchmark were bigger than 4.00, but that would have forced the user to re-factor the result to get back that value that was greater then the metric_lbl in the first place. So this is why I made our metric_n variable smaller. One should only ever set the GAD metric once, or if the benchmark is bigger than the metric_lbl - one can get back the current METRIC_LBL. In addition to doing the data reduction or filtering with the ? operator, the query should accept the METRIC_N variable set in the query itself, and not just set by the metric_n we set with our query. So here is another example showing how the query works. I also filtered my queries that are based on a value greater than. As you can see I have a metric I set"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 River Raid?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	-------------------------- 1st: 0.00000 2nd: 0.041673 3rd: 0.195224 4th: 0.341656 5th: 0.482972 6th: 0.713074 7th: 0.862732 8th: 1.031284 9th: 0.971177 10th: 1.161612 11th: 1.432531 We expect an ordering like this if one can just read the data, in which case the first results should be in the top row. But the matrix doesn't look like one can do any of that. The way to deal with this is to filter the rows so that the metric scores are sorted by their numerical values, while the values are sorted by the percentile. But this is a bit of an exercise in hand-wavy algebra. So after I found some interesting stuff I got to read the dataset again from a disk (the disk for the matrix file was the one that I used to read the original data in from the disk; I was in a hurry since I didn't want to wait for SQL to finish reading my file to create a table). If I do this, I get: 1st: 0.66414 1st: 0.433614 2nd: 0.86976 2nd: 0.74428 2nd: 0.984432 3rd: 0.989888 3rd: 1.121526 3rd: 1.818063 4th: 1.251582 4th: 1.952434 5th: 2.222224 5th: 2.693334 Since this isn't a perfect ranking because I have a bunch of other data (the score for each metric is at least as big as all the rows of the data, but maybe not so much	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Denoising autoencoders (non-autoregressive) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Denoising autoencoders (non-autoregressive)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?_model_lbl. FILTER (str(""n_i"") < 2 AND str(""i"" = ""2"" OR str(""i"" = ""1"" OR str(""i"" = ""0""))) ?_model. OR k_mod > (10 OR k_mod < 10) ?_model_lbl. FILTER (str(""k_mod"" = ""0"" AND str(""k_mod"" = ""1"" OR str(k_mod = ""2"" OR k_mod < 2) OR str(k_mod = ""3"" OR k_mod > 6) ?k_mod.) ?mod. or ?mod< n_i OR ?mod < 2 ?k_mod. or k_mod > 10 ?mod. } k_mod> (10 OR k_mod < 10) ?mod. input (english text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes ? output (Sparql query"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?benchmark || ?benchmark. How does the computer achieve this accuracy ? To illustrate the relevance of these parameters we would like to run the model on a dataset that contains two parallel versions of the game. The game is a 2-D side-scrolling shooter and the second version is a 3D platformer. In the case of the first version, the game is a 3D action game that uses a character who has a short range fire ability. In the case of the second version, the character possesses a laser-focus ability that makes the player shoot quickly and is not a liability in close- quarters. The two versions are generated by two different engines. One uses the Atari 2600 Up version of the game to evaluate the model, and the other uses the SPSS output. The output from the SPSS output can only be judged by the score of the model. Therefore the output from the Atari 2600 Up engine with a different score from that of the SPSS should be ignored. This error-correcting function can be implemented in the following way: input (text): Provide a list of evaluation metrics frequently	False
"
 input (English text): Provide a list of papers that have utilized the PtGen model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PtGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?link FROM ?tbl LINK ?link[0]; Link: To obtain the link of a paper if it wasn't included in an article, run this query: link= ?output(Output(sparql( ?code), ?lbl)) Link: Provide link(s) to the pdf to review if it wasn't included in an article.  If the pdf has links to the source files, you can try to open them in a new tab if their files appear multiple times.  If you have the source page and the abstract you can use the link command.  The link command can be extended to include an external link so you can directly link the file or folder containing the pdf source file.  In this case, the link will include the entire pdf from the pdf_source_path, even if it already exists as a pdf.  If it does not, you can just use the url option with the raw url. External Link: Link(s) to the resource link of the article including the URL of the corresponding article source.  It also includes the HTML/JS if the file or folder is only in the current local machine directory. External Link: Link(s) to the resource link of the article without using a link.  When the file or folder is not in the current local machine directory, the resource reference will be in the default system path. Link: Provide the link back to the article source for the final exam. Links: Include the url of any external links to the page being reviewed.  	False
"
 input (English text): Which energy system modeling papers indicate where to find their simulation software and whether there are associated simulation scenarios mentioned in ORKG?
 output (Sparql query): SELECT ?repo ?scenario ?simulation_parameters WHERE { ?paper orkgp:P31 ?contrib. ?contrib orkgp:P37586 ?model. ?model orkgp:P39010 ?code. ?code orkgp:P4077 ?repo. OPTIONAL { ?model orkgp:P43146 ?scenario. { SELECT ?scenario ?simulation_parameters WHERE { ?scenario orkgp:P31 ?contrib2. ?contrib2 orkgp:P37586 ?model2. ?model2 orkgp:P37583 ?data. ?data orkgp:HAS_DATASET ?dataset. ?dataset orkgp:P37343 ?type; orkgp:P4059 ?simulation_parameters. ?type rdfs:label ?label. FILTER(REGEX( ?label, ""Technical"")) } } } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 output (Sparql table): RDF: rdfs:title orkgp:P6399 ?orkgp:P6633. { ?model. ?model, { ?repo. OPTIONAL { ?model ORKG. ?models ?version orkgp:P6650. ORKG. paper. ?repo. paper. ORKG. paper. ORKG. paper. ORKG. paper. ORKG. paper. ORKG. paper. ORKG. paper. ORKG. paper, { ?repo. ORKG. ORKG. ?model2. ORKG. ORKG. ORKG. ORKG. ORKG. ORKG. orkgp:P36006 ORKG. paper. ORKG. paper. ORKG. orkgp:P48334 ORKG. paper ORKG. ORKG. ORKG. ORKG. ORKG.. ORKG. paper, { ?repo. ORKG. ORKG. ORKG. ORKG. ORKG. ORKG. ORKG. ORKG. ORKG. ORKG. paper. ORKG. paper. ORKG. paper ORKG. ORKG. ORKG. ORKG. ORKG. ORKG.. ORKG. paper ORKG. OrkG. OrkG. paper ORKG. ORKG. ORKG. ORKG. book. ORKG. book. ORKG.	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the MRPC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Note: The dataset may be located at http://dumps/atari11/dataset/atari11.txt. This is a placeholder and the resulting DISTINCT field is not required for the SDP list to parse, but it may be helpful for some readers for a more familiar way of looking at the output. The parameter ?dataset is available as a comma-separated list. It indicates where the dataset is located in the given dataset schema. The other parameters ( ?cont, ?benchmark and ?benchmark_lbl ) are optional, and may be present in any order. If a particular parameter is not present, the query will be terminated with a SQL error. In this example, there is no dataset named Atari11, but there are two datasets named Atari11.txt and Atari11/dataset-1.txt. The table contains a single table name containing two columns, an ID and an label. The ID is used as a match condition for the filter criteria of the aggregate function, while the label may be used to denote the dataset. There may be one or two rows for each label. In SQL, each field in a table is represented by a number. To view each table, use COMMIT. To get a list of every dataset name, use get_data. As shown for a binary search, the list may look rather familiar. In the output, the output row for the record with ID 12345 is the name of the dataset: Atari11. Note that the output for this record is the same as the preceding one. The ID field from the Atari11 file may also be replaced with an integer. If the dataset belongs to a category, the fields in the table might differ because the record was deleted by the program. The ID and label fields cannot be changed by the compiler. A similar problem may exist	False
"
 input (English text): Can you list the models that have been evaluated on the Reuters De-En dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT N.D. 1, N.D. 2, N.D. 3, N.D. 4, N.D. 1, N.D. 2, N.D. 3, N.D. 4, N.D. 1, N.D. 2, N.D. 3, N.D. 4 FROM N.D.1  where N.D. 3 > ?dataset. The results displayed in the table above are the model scores achieved on the two datasets in the analysis above as well as the top 3 Accuracy scores of the model. Of note is the following that the top performing models are all clustered in the N.D. 1 category. In the following table, the first column compares the Top 3 Accuracy scores of the model in question with those achieved by the model of the second row of the table. Model Accuracy Top 3 Accuracy DPS/model DPS/model DPS/model Score as percentage of the DPS/model DPS AS% 0.637 0.654 0.835 5.08 1.076 6.08 2.111 4.564 3.843 5.113 8.633 6.085 1.082 17.67 25.14 35.93 35.879 18.566 6.2 13.34 4.839 1.06 3.64 1.8 5.2 5.8 5.89 11.88 11.78 15.44 1.5 13.28 10.4 1.6 2.3 1.2 4.17 2.84 6.46 17.22 17.27 21.78 5.25 4.89 4.15 2.4 1.4	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Knowledge Graph Embedding research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Knowledge Graph Embedding"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the most common Knowledge representation method?
 output (Sparql query): "	SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1	SELECT ?dataset_lbl ?dataset_lbl. ?benchmark ?dataset_lbl. What are some constraints that the solution to the problem might impose ?	False
"
 input (English text): Which model has achieved the highest F1 score on the MedNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can all the data sets used in the compared studies be found?
 output (Sparql query): "	"SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, ""Not"")) } "	" As the output indicates, the MedNLI dataset is an ideal source of MedlinePlus data when the data used in this study is not sufficiently homogeneous. I'm sure that the dataset's homogeneity is essential to the outcome of this study. The other datasets used in the current study are of course too, but I don't know if you would go through all of them. Let's consider in detail why that is. First off, this study was designed in a relatively early stage, with some large number of patients, with little to gain by randomization (if you would ask me, it looks like a ""rigorous trial"" for the outcome measures). It's also one that isn't being repeated in another area, such as cancer. As a result, the original design is not so good for the control of confounding variables that might affect the outcome measures. Also, as you already pointed out, there is the issue that the original dataset only contains six years of longitudinal data and that is really just two years of median time to death. This is because there will always be a few patients with more recent disease that might not be captured in the standard dataset. Thus it's very likely that the original study's results are misleading. Nevertheless, one should not conclude that all studies should be judged by the standard of the original study (because it doesn't provide a complete picture of the data set). The study that we are discussing here is a rather well designed study, and, at the same time, it is not a typical case of the usual biases that affect studies on cancer and chronic conditions, which has been discussed previously in this book... So let's turn to a study that tries to replicate MedLinePlus. In that study, ""narrowbanding"" is used as a control intervention, but this study is not trying to reproduce the MedLinePlus study. This paper"	False
"
 input (English text): Where can I find code references in papers that have used the PAR Transformer 24B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer 24B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Entity Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 orkgp:UNITED NATIONAL QUARTERLY (DELAWARE). ORKMP: AMERICAS CHANGE LEADER-1 (DELAWARE)-1 (CHANGE). ORKMP: ORKMP-NATIONAL LEADER LEADER (DELAWARE)-1 (NATIONAL). ORKMP: NORTH AMERICAN QUARTERLY-1 (DELAWARE). ORKMP: NORTH AMERICAS LEADER LEADER-5 (FLORIDA). ORKMP: NORTH AMERICAN NORTH AMERICAN LEADER (FLORIDA)-5 (NORTH). ORKMP: NORTH AMERICAN LITERARY/TECHNICAL LEADER LEADER (FLORIDA)-5 (NORTH). ORKMP: NORTH AMERICAN MEDICAL LEADER LEADER (FLORIDA)-5 (NORTH). ORKMP: SOUTHERN ALASKAN JOURNALIST SERIES LEADER (WAKE COUNTY). ORKMP: NORTHERN ALASKAN JOURNALIST LEADER (WAKE COUNTY)-2 (NORTH-2). ORKMP: SOUTHERN ALASKAN JOURNALIST LEADER (WAKE COUNTY)-5 (NORTH-5). output (Sparql query): orkgp:UNITED NATIONAL QUARTERLY (DELAWARE). ORKMP: AMERICAS CHANGE LEADER-1 (DELAWARE)-1 (CHANGE). ORKMP: NORTH AMERICAS LEADER LEADER (DELAWARE)-1 (NATIONAL). ORKMP: NORTH AMERIC	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Beam Rider dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Beam Rider"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?benchmark { ?benchmark OR ?experiment OR ?data_set OR ?model_label OR ?data_injector OR ?benchmark_data_set OR ?oblivion_labels OR ?reconstructor(labels) OR ?oblivion_predicts OR ?oblivion_class_labels ORDER BY ?model_label; } FROM ( SELECT ( ?'data_injector' LIKE '/dev/null/data/null' \ OR '||1' ) AS data_injector, ( ?'model_label' AS title, ( ?'data_injector_name' AS id, ('dataset', 'dataset_id') || _0 OR ( ?'dataset_id' IS NULL)) AS dataset, ( ?'dataset_name' AS id, ('dataset', 'dataset_id') || '&&(dataset_id IN ('dataset_id') OR ('dataset_id' IS NULL)), ( ?'dataset_label_name' AS id, ('dataset', 'dataset_id') || '&&(dataset_id IN ('dataset_id') OR ('dataset_id' IS NULL))) AS data_injector_name, ('oblivion_labels' AS id, ('oblivion_predictions' AS '||0', 'oblivion_label_label_id') || NULL) AS oblivion_labels, ( ?'oblivion_predicts' AS '||0', 'oblivion_label_label_id', ('dataset', 'dataset_id'), ('dataset_id' IS NULL)) AS dataset_id, ( ?'oblivion_class_	False
"
 input (English text): What is the best performing model benchmarking the SciERC dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?model_lbl ?dataset_lbl. ?db ?data_lang; } orkgp:HAS_EVAL ?eval. ?eval orkgp:HAS_VALUE ?value. ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ORDER BY DESC( ?value) LIMIT 1 } } input (English text): What is the fastest model in terms of learning ? output (Sparql query): orkgc:Metric ?model ?model_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?model_lbl ?dataset_lbl. ?db ?data_lang; } orkgp:HAS_EVAL ?eval. ?eval orkgp:HAS_VALUE ?value. ?model_lbl. } ORDER BY DESC("	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Denoising autoencoders (non-autoregressive) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Denoising autoencoders (non-autoregressive)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 We welcome comments and additional data on our benchmark. Please write to us at info@sibyl.org.	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the The Pile dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""The Pile"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title FROM all_lateralized_lateralized_papers WHERE title = 2 ORDER BY title; OR... SELECT title FROM all_lateralized_lateralized_papers WHERE title = 4 ORDER BY title ; In order to find all of those papers that incorporate a benchmark for the Gibson PointGoal Navigation dataset, we simply use the query above. Of the 20 datasets that are on The Pile, seven are part of the Pile Metrics dataset (e.g., the Pile Bibliometric Benchmark), 18 datasets are part of the Inbound Metrics dataset (e.g., the Pile Navigation Metric Benchmark), and two are on the Outbound Metrics CSV. Once we have gathered the datasets and their corresponding Benchmark parameters, we can use their names or identifiers to figure out which of them can be used to generate a metric. To simplify these comparisons, we will call each of the metrics that we find with metric_lbl. Benchmark vs. Metric Metrics Benchmark data will vary from measurement to measurement, as can be seen by the differences between the various Metrics metrics mentioned so far. While we could simply use the numerical identifiers we're given, the metrics we'll create with the Gibson PointGoal Navigation Metric Benchmark, as shown below, will be of different quality. Gibson PointGoal Navigation Metric Benchmark Values 1) Accuracy: This will be the absolute number of goals reached for each individual player. With the default Gibson PointGoal navigation dataset, this value would be 0.5. With the Inbound Metrics and Outbound Metrics CSV datasets, this will be a measurement of 2 goals. (Note: The values of 2 and 1 are approximate to each other, as a player on your team will not have many more goals than someone on your team). 2)	False
"
 input (English text): List the code links in papers that use the FusionNet (single model) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FusionNet (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" What do you need ? (for example, links from NIST, NCSA or AIST, CCRDL and ISRC) ? input (English text): link from/to paper using single model ? output (Sparql query):    ORDER BY number OF codes  ORDER BY link  1 :  10 2 :  50 3 :  100 4 :  500 5 :  5000 6 :  100000 7 :  10  7.51  } You provide your Rdbc.conf (for example). It will be loaded automatically as it will be present in the package. In the ""rbenchmark_options"" R file we are asking to load these options from Rdbc config file: from rdbc import dbconfig_file dbconfig_file = open ( 'dbconfig.conf', 'w' ) dbconfig_file.write( '...' + dbconfig_word_opts ) So you need to replace the word 'dbconfig_word_opts' by your own data set name. You can edit these options as follows: - dbconfig_word_opt : The option name. You can use any word to select the metric, it should not be a name. So 'nondest', 'percent','mean',"	False
"
 input (English text): Could you provide a list of models that have been tested on the smallNLP-KG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""smallNLP-KG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl ; output (Sparql query): SELECT ?dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""smallNLP-KG"") ?model_lbl. input (English text): What are the parameters that should be changed in the PIQA dataset ? output (Sparql query): output (XML schema): https://pipeline.ipa.ac.il/tools/dataflow/PIQuest_Benchmark_Schema.csv or kgc_dataset:NLS.txt input (English text): How many observations did Srivastava et al. use in their analysis ? output (Sparql query): ?test_pipeline; srivastava_pipeline; ?test_pipelines output (Sparql query): SELECT ?""smallNLP-KG"", ?""fibre_sparql"", ""labels"", ?""train_graph"", ""test_graph"", ""validation_logger"", ""validations"", ?""eval"", ""srivastava_lbnp_estimate_epoch"", ?""validation_lbnp_estimate"", ?""valid_labels"", ?""model"", ?"""	False
"
 input (English text): List the metrics that are used to evaluate models on the PIQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT { ?name ?score } select 'name Score', 1 from scoring.score.matrices to scoring.score.metrics score(score) Output of the query: Score = 0.00851598 score(score) To see a full list of the functions available in the MetaROWL module, including the parameter and the name parameter, see this Google code sample. Note that MetaROWL is not included in CRAWL's pre-built Python source because of a bug in the CRAWL package itself. MetaROWL is included here because the code shows how the MetaxeCRAWL package can be easily compiled from MetaROWL or Metabug's CRAWL source. When installing the package, simply add the path from the list above to the path variable in the generated CRAWL.Py program. MetaxeCRAWL The program was written using the CRAWL source, but may not be suitable for compiling with a pre-built Python distribution. If you want to start from the source code, you may simply compile with the following steps: Make sure you have all the CRAWL sub-packages installed. $ pip install cmrawl Configure and run Python. $ python build.py $ python build.py Example, to compile: $ cmrawl.py./build.py	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Cartpole, swingup (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT paper. TITLE, paper. LABEL, ?dataset as ?dataset FROM RDBMS ( 'tableseq:tableseq%s',... ). ORDER BY paper. TITLE. LEFT ASC output (Sparql query): ?paper. ?part < ?cont> input (English text): Provide a list of literature articles based upon the structure of the DMControl 500k database that use the Cartpole, swingup (DMControl500k) dataset ? output (Sparql query): 1 2 3 4 5 6 7 8 9 10 [ #<DMActimeModel: DMActimeModel < ?id> >] [ #<ThingTableEntry: ThingTableEntry @id < ?cont> >] [ #<IBAction.ID< ?id> >] [#<IBAction.D.EVENT_TYPE< ?id> >] input (English text): Provide a list of papers that have utilized the Table-Sequence model and have provided their code names and ID ? output (Sparql query): 1 two 5.1. 3. B. N. Mosey, DMActimeModel,, B, 1 two 5.1. 3. C. M. Thompson, DMActimeModel,. 2 2 four 3. 3. A. K. Eltahir, TSEG,. 3 two 5.1. 3. D. J. F. Bielman, TSEG,. 1 two 5.1. 3. E. Shulman, T	False
"
 input (English text): Where can I find code references in papers that have used the XLNet-Large (ensemble) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Funnel Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input data 1. What data do you offer ? 2. What is the most efficient way to process the data for analysis ? 3. When is the best time to analyze datasets ? 4. How often do you analyze these datasets ? For instance, data for different days of the week. 5. How often do you make adjustments/adjustments/update the data (e.g., moving bins around while averaging or using a different method of weighting) ? 6. Do you have a method you would recommend for calculating the percent number of bins that will be aligned with the labels for a given dataset ? 7. If so, how should I implement the method ? 8. Can you provide additional code to implement this method ? 9. Should I include a link to the code to download the code ? 10. How can you be sure my code works as expected since I'm assuming that everyone already has it ? 11. Where can I get the source code for this question ? 2. What questions are related to or overlap with your original question ? How do I get information about which questions are related to my original question ? If there is more than one question related to the new question, which one do you suggest I ask ? 3. What data do you offer ? 4. What is the most efficient way to process the data for analysis ? Which data will be analyzed in this dataset ? 5. How often do you analyze different aspects of a dataset ? For instance, which data does the model use to predict the weight/density of a cluster of data ? For instance, from my study of the effects of increasing BMI on overweight risk factors, I measured the total BMI within the cluster and the weight in each cluster. Would you agree if I could obtain a CSV file of the data that I'd need to analyze using the XLNet-Large	False
"
 input (English text): What models are being evaluated on the SQuAD1.1 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT r. val ( ?dataset ) AS rds, _. model as model, r. val ( model_lbl ) AS val, '%' AS num_val_val and 'posterior' AS posterior_score and'max' AS max_score. OPTIONAL { 'valid_validation', 'valid_predication', 'evaluation_max_score', 'evaluation_max_validation' } Sorted by model val: 1.5 - SQuAD1.1 0.5 0.3 0.5 1.0 - Reuters De-En 1.0 2.0 2.8 1.7 1.3 - Alias 1.0 5.0 4.9 6.8 5.0 - SQuAD 1.0 5.4 5.7 7.1 6.8 - FACTOR.GED.0 2.0 4.0 6.1 6.6 5.4 - EMAILOR 1.0 1.6 1.5 1.9 1.1 - SQuAD 1.0 8.2 7.8 7.9 6.2 - APPLE.TOTAL 0.5 0.3 0.5 0.7 8.4 - SQUAD 1.1 - (2|7|0) -7.1 0.8 0.6 - APPLE.TOTAL 0.0 0.0 0.0 0.1 7.8 - SQuAD1 0.2 1.3 0.5 0.9 0.1 - Bloomberg.GMA 1.0 2.5 2.4 1.7 - VLOOKUP.1 0.3 0.2 1.1 0.9 - VLOOKUP.1 1.0 4.0	False
"
 input (English text): List the metrics that are used to evaluate models on the PIQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?val ?val_lbl ?val; output (Html text, CSV format): List any metrics that were used in the evaluation to evaluate the model on the RCT benchmark dataset ? F1, F2, B1, B2, B3 and M3 metrics You are probably looking for metrics that correlate with how well the model fits. For example, if both the model F1 and model F2 produced the same estimated slope, they have a F1/F2 correlation of 0.77 (0.66 if PQA is tested). If the models produced a different slope, they have a correlation of up to 1/2. If they produce the same slope, they have a correlation between 0.7 and 1/2 (1.2 for PQA). The PQA metrics were chosen specifically to provide insight into how much our models measure up to a benchmark for specific tests, including, but not limited to, testing our new models against a known set of metrics, the PubMed 20k RCT and the PQA 2.0 RCT. If the PQA metrics were chosen to measure performance, they were chosen to be high to allow us to start with the best-fitting models and then work our way up. We can easily build on their performance as the number of iterations increases. How to use the metrics We are using the F1, F2, B1, B2, B3 and M3 metrics on the PIQA and the HPUB2 datasets. Metric Description Value in ml/min of each model (f1, f2, f2.5, etc) Model fit-related metric; it is the number of times the model failed (e.g. it did not solve the problems) and the number of times the model achieved the problem	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the QuAC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QuAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-BiLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Make a link among the code links in benchmarks that use the BiLSTM model in any benchmark ? output (Sparql query): output (Sparql query): Note: please be aware of the fact that the output text is in English only. The results will be interpreted only in English. To get the raw results in the other language, please insert your query in the Output field (the box on the right). Note: You can create a new session using :SessionID or :SessionPassword. Please make sure that session ID is correct. If session ID is already used, please choose another one. If an error is found in your query, please check that your connection to the Hive server is good (if connection lost by a network error, please retry the query), and try again. The Hive server sends the result back to you in a batch.	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cornell Grasp Dataset dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?paper_lbl. ?benchmark ORkgp:HAS_DATASET ?dataset ORkgp:HAS_BENCHMARK. ------------------------- A lot of this comes from a research paper on ""Optimizing Multi-Threaded Learning Based on the Oxford-IIIT Pet dataset [pdf]"" provided by the authors. This is a highly-cited paper so we want to see these results as well as our own results so go check it out. This is the second time we have run our benchmarking, and we had two datasets that we could put into the dataset to use as examples. We will be using the dataset in this blog post and the graph in the next blog post. In this blog post we focus on the Pet dataset from the Oxford-IIIT that we are comparing with another research paper from the Oxford-IIIT, the Pet dataset (pdf) from our pet research group in the Department of Computer Science. In a previous blog post (Partial evaluation of Pet dataset through Benchmark) we used a dataset from the Carnegie Mellon University. What we have done in this blog post is use our existing Pet dataset so we can check on the performance. But, as a first blog post we will look at the Pet dataset that we used for the previous blog post. Note: the Pet dataset from the department of Computer Science is the dataset used for our benchmark on Flops metric of the Pet dataset. There is no reason to keep comparing different Pet datasets. The other datasets were only used as a reference in our benchmarking. I will be using these Pet datasets also for the next blog posts where we see the performance of different algorithms and see which ones can handle all the data we have."	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (German text): What is the top benchmark result (metric and value) over the dataset SSVLC ? Output: output (Sparql query) output (Sparql query) output (Sparql query) output (Sparql query) input (English text): output (Sparql query) input (English text):	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MLDoc Zero-Shot English-to-Japanese dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Japanese"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ? paper_lbl WHERE { ?dataset ? a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NEXTRON Cats;2"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } output (Sparql query): Select all papers that have benchmarked models on the Oxford-IIIT cats dataset which have either name ""NEXTRON Cats"". OR output (Sparql query): SELECT name, ?dataset, ?benchmark, ?cont, ?paper, ?p31, ?p32, ?b27, ?b28, ?b29, ?b40, ?b41, ?n4, ?n10, ?n11, ?lab, ?s9, ?n13, ?s10, ?s17, ?s20, ?s12, ?s21, ?s18, ?s24, ?s25, ?s30, ?s31, ?s33, ?s34, ?s35, ?s36, ?s31, ?s36, ?s37, ?s38, ?s39, ?s"	False
"
 input (English text): What is the top benchmark score and its metric on the WebNLG dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------- ------------------------------------ 0.950000 | 4.948000 | 0.980000 | 5.000000 | 5.000000 ------------------------------------ 0.010000 | 5.000000 | 0.025000 | 6.000000 | 6.000000 ------------------------------------ 0.025000 | 6.000000 | 0.050000 | 0.060000 | 0.060000 ------------------------------------ 0.090000 | 0.110000 | 0.110000 | 0.120000 | 0.120000 ------------------------------------ 0.120000 | 0.130000 | 0.200000 | 0.220000 | 0.220000 ------------------------------------ 0.180000 | 0.190000 | 0.250000 | 0.270000 | 0.270000 ------------------------------------ 0.250500 | 0.260000 | 0.27500 | 0.3000000 | 0.3000000 ------------------------------------ 6.000000 | 0.290000 | 0.320000 | 1.000000 | 1.000000 ------------------------------------ 6.000000 | 0.320000 | 0.320000 | 2.000000 | 2.000000 ------------------------------------ 6.040000 | 0.380000 | 0.400000 | 2.50000 | 2.50000 ------------------------------------ 6.200000 | 0.420000 | 0.420000 | 3.000000 | 3.000000 ------------------------------------ 6.400000 | 0.560000 | 0.470000 | 3.70000 | 3.70000 ------------------------------------ 6.6000000 | 0.580000 | 0.520000 | 4.000000 | 4.000000 ------------------------------------ 6.620000 | 0.600000 | 0.524000 | 4.750000 | 4.750000 ------------------------------------ 6.675000 | 0.620000 | 0.530000 | 5.000000 | 5.000000 --------------------------------	False
"
 input (English text): List the metrics that are used to evaluate models on the Cartpole, swingup (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AcrE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 RETAIL_URL > http://ajax.googleapis.com/ajax/libs/jquery/1.9.2/jquery.min.js > orkgp:DATASET ORkgp:PROGRAM_NAME (orkgp:ROLABEL) > ( orkgp:EVALUATION ) output (Sparql query): RETAIL_URL > http://ajax.googleapis.com/ajax/libs/jquery/1.9.2/jquery.min.js > rdfs:label ORkgp:DATA. PRIVILEGES > ( orkgp:AVAILABLE ORkgp:AVAILABLE_LINKS) output (Sparql query): PRIVILEGES > orkgp:ROLABEL AND_AVAILABLE ORkgp:AVAILABLE_LINKS orkgp:PRIVILEGES > (orkgp:PRIVILEGES AND_AVAILABLE_LINKS) input (English text): Provide a list of papers that have used the BAMC model and include the links to their source code ? output (Sparql query): PRIVILEGES > orkgp:DATASET BAMC:DATA. PRIVILEGES > ( orkgp:EVALUATION BAMC:DATA) > ( orkgp:PRIVILEGES AND_AVAILABLE BAMC:DATA) > ( orkgp:PRIVILEGES AND_AVAILABLE_LINKS) output (Sparql query): PRIVILEGES > BAMC:DATA BAMC:DATA_AVAILABLE > BAMC:DATA_AV	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Flowers-102?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2013"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DESC( ?score) AS ?score, DISTINCT ?name AS ?rank ORDER BY DESC( ?score) DESC( ?name ) ORDER BY DESC( ?rank ) DESC( ?name ) ; Note that the query has an ORDER BY column indicating the direction of search. Note that this query is also available via the RDDL interface ( rrdifl ). input (English text):  select name, k, ?name, i as name_rank from'supervised-test.rdf' j as data_model k, t as model_dataset in DISTINCT j, N as n_data_model_k in MATCH ( ?name_rank with id = ?model_t) select rdfs::dscalar( ?tsize( ?dataset), ?k, ?name, ?rank ) from 'n-class-results_datasets' j as data_model_dataset k, t as model_dataset in N-class_results t join j as rdf_data on j.k = j.k create data_model_dataset from 'n-class-results_datasets_20130613_0.ddl' k create data_model_dataset join j as data_model_dataset on j.k = j.k create model_k_from j as data_model_dataset on j.k = j.k Using SysDB You can also use this SysDB interface directly in R with the following query: data_model_dataset = rdfs::dscalar( rdfs::sizeid( k ), k, k) dataset_names = [ '	False
"
 input (English text): Provide a list of papers that have utilized the BiLSTM (UN) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM (UN)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT * FROM orkgp.data_trd.datasets ORDER BY id, name. input (English text): Write a program that uses the text input to calculate an estimate of a BiLSTM-based model's accuracy ? output (Sparql query): input (English text): Tell me the number of BiLSTM-based models currently being used by an author. output (Sparql query): input (English text): Give a list of papers mentioning bijection-based models. output (Sparql query): input (English text): Give me results which show which models correctly predict a specified group of parameters (i.e. how well each model correctly predicts the parameters used by a group) from all data and not from pre- and post-processing.	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Freeway dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What quantity of iron oxide was discovered on Elorza crater?
 output (Sparql query): "	"SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Elorza crater"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""FeO"")) ?properties orkgp:description ?property_description. } "	SELECT ?benchmark ?dataset. ?cont OR ?benchmark ?dataset_lbl, ?benchmark. ?dataset_lbl, ?benchmark. ?dataset_lbl, ?benchmark. ?dataset_lbl, ?benchmark. ?dataset_lbl; input (English text): ?bench-1 1. ?bench-2 1. ?bench-3 0 0. ?bench-1 1. 8. ?bench-2 1. ?bench-3 1. Outputs	False
"
 input (English text): Provide a list of papers that have utilized the AVID+CMA (Modified R2+1D-18 on Kinetics) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AVID+CMA (Modified R2+1D-18 on Kinetics)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Note (10/21/2014): I've updated the list, which has been updated on Dec 21 but has been omitted for clarity. input (English text): Can you provide data on the number of unique papers that the R2+1D-18 Model used for the VTAB-1k dataset ? output (Sparql query): select unique; orkgp:HAS_NUM_PUBLISHED_PAPERS ?unique. Note (11/9/2014): I've now added a new query and added links, which you may use for a more rapid way to run the R2+1D-18 Model. You only need to specify the model you want to analyze. output (Sparql query): select unique; orkgp:HAS_DATASET ?dataset. Note (11/23/2014): I've added a new Query Builder function, which takes a Query Description and returns a model containing the data. It's a great tool for running the CMA Model analysis, however, at present the resulting model can't be used for any of the analysis methods in this paper. If these features are added, you'll find the query more readable and the analysis process faster. Note (11/28/2014): I've uploaded an updated Query Builder, which, as part of this effort, also allows you to run the R2+1D-18 Analysis on a data frame of your own choosing (with the same format) by specifying the data frame in a statement parameter, so I can create more convenient Query Builder functions in the future. The new function can be accessed here, where the syntax is given in the next paragraph: R2+1D-18 Analysis : the CMA model is available from the Query Builder input (English text): Input the data or	False
"
 input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?model ?model_lbl. FILTER (str( ?model ) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset { ?dataset a orkgp:HAS_DATASET; rdfs:label ?dataset_lbl. } OPTIONAL { ?model rdfs:label ?model_lbl. }... output (JSON): There are no databases or models specified. As you can see, this is definitely not a very sophisticated model. As you can see on the above graph we have many different models, each with its own parameter choices. In fact, there is only one parameter that seems like it may have any real world application, one that it is not at all clear about: the number of output features (the output features from the model). What kind of features ? What does it look like to process images and video with this model ? Let's take a closer look. Image Processing with the Pixel CNN The very first thing you may notice when you open the PixelCNN model is that it is a relatively straightforward implementation of the convolutional neural network model shown by this paper.  The convolutional network is an input architecture that was developed to be used for image processing in the early 1990s, but was then rethought through by a group of researchers. Image Processing with Convolutional Neural Networks While convolutions may look to be relatively simple, when applied to a complex image processing task they may not always be: convolutions are the best way to understand what is going on in the"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the BUCC Russian-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT GASTIMO_META(train).START_SIZE_MIN(max(train)); Raster (16 KB) 2.5.8. Benchmark Analysis 1.4.3 1.4.3. List the metrics used to evaluate models on the Atari 2600 Freeway dataset 2.5.1. Benchmark Analysis 2.5.2. List the metrics used to evaluate models on the Atari 2600 Freeway dataset 3.0.0. Benchmark Analysis 3.0.1. List the metrics used to evaluate models on the Atari 2600 Freeway dataset 3.1.2. Benchmark Analysis 3.1.3. List the metrics used to evaluate models on the Atari 2600 Freeway dataset 3.2.0. Benchmark Analysis 4.0.1. List the metrics used to evaluate models on the Atari 2600 Freeway dataset 4.1.0. Benchmark Analysis 4.2.0. List the metrics used to evaluate models on the Atari 2600 Freeway dataset 4.2.1. Benchmark Analysis 4.2.2. List the metrics used to evaluate models on the Atari 2600 Russian-to-English dataset 5.0.3. Benchmark Analysis 5.1.0. List the metrics used to evaluate models on Atari 2600 Swedish-to-English dataset 5.2.0. Benchmark Analysis 5.2.1. List the metrics used to evaluate models on Atari 2600 Russian-to-English dataset 5.2.2. Benchmark Analysis 6.0.1. List the metrics used to evaluate models on the Atari 2600 Swedish-to-Swedish dataset 6.0.2. Benchmark Analysis 6.1.0. List the metrics used to evaluate models on Atari 2600 Bulgarian-to-English dataset 6.2.0	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 River Raid?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	______________________________________________ _________________ _________________________________________________ ______________ REFERENCES and RDFs 1:  [1] : http://research.microsoft.com/pub/pdf/nf/3b/nf_3b-nf_11-2-4_dynamic_cop.pdf 2:  [2] : http://tasn1.csail.mit.edu/tasn/papers/2012_05/nxss_static_coattention_netsim_11b.pdf 3:  [3] : http://journals.acm.org/journals/abstract/2012/02/15/1/15/150008 4:  [4] : http://journals.acm.org/jc/content/48/4/2021.abstract ?sid=8d0e5ffd-c8c8-4467-9411-0fe98bd9ce3c	False
"
 input (English text): List the code links in papers that use the Transformer-XL (12 layers) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (12 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?code FROM ?benchmark ORDER BY ?benchmark. ?cont orkgp:HAS_DATASET ?dataset ; orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model ?code. ?cont orkgp:HAS_SOURCE_CODE ?code. Output (sparql): SELECT count(*) FROM ( ?benchmark) ; Output (Sparql query): If you want some help with using the Transformer-XL model or other ML technology in a high performance database, take a look to the ML-DBA documentation or ask questions in this forum. Please provide your feedback, comments and suggestions in the comments section below. If you like LALT, you may also look at PEMJ (Practical Algorithms with Monte Carlo) which makes use of Transformer-XL and similar models to perform the standard linear classifiers. There are many other training, test, benchmarking and data analysis services out there. Please look around for advice. Please cite this page - ""LALT in the PostgreSQL Benchmarking Service"" - https://dl.dropboxusercontent.com/u/20589849/Training.txt Posted on July 22, 2013 at 05:00 | Permalink LALT is a PostgreSQL package which can be used as an optimization and validation tool for LALU-HPC. It is compatible with PostgreSQL10 (latest) and SQL Server. It provides a simple interface for LALU code and the ability to execute on different hardware hardware and processors. It's designed to be a drop-in replacement"	False
"
 input (English text): What are the metrics of evaluation over the Cornell Grasp Dataset dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP (6) rdfs:label ?dataset_lbl. ?benchmark rdfs:value ?benchmark_value. rdfs:labels ?labels. Note that the output of this query is for the dataset that had the highest output score, not for the dataset that had the second highest score. SELECT TOP (6) id, output (Sparql query): rdfs:label ?dataset_lbl. In a similar way, the next two examples will show how to fetch a list of the names of the first and last tennis champions from the database of top-notch ATP players, as well as the ranking for each player based on how frequently they appear on ranked lists of players in our database. Input (English text): What are the metrics of evaluation over the Stanford Open Tennis Dataset dataset ? output (Sparql query): SELECT DISTINCT ?measurement * ?measurement_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Open Tennis Dataset"") ?measurement orkgp:PRIMARY_SEED orkgp:MEDIAN orkgp:NUMRESSOR orkgp:NONLETHAL orkgp:NO_REPEATMENT orkgp:LIFETIME orkgp:CLUB_RANK orkgp:LEVELOR orkgp:NUM_GOALS orkgp:CANDLEWORK ORkgp:STANDARD_BASE orkgp:WATERWORLD orkgp:RELEVANCE orkgp:LEAGUE_TICK"	False
"
 input (English text): Indicate the model that performed best in terms of Micro F1 metric on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; OR kgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model OR kgp:HAS_VEHICLE_TYPE ?vehicle_type. OR kgp:HAS_MODEL OR kgp:HAS_ROUND_OR_RADICAL ?round_or_radial. OR kgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_VALUE ?value. ?cont orkgp:HAS_METRIC ?metric. } ORDER BY DESC( % (eval) ) LIMIT 1 } } I have created new file ""datasets.php"" in ""Datasets"" directory. The model in model.php must be defined as ""HAS_DATASET"". The Metrics property of this model must be ""micro_f1"", in ""datasets"	False
"
 input (English text): What are the metrics of evaluation over the Reuters RCV1/RCV2 German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" // Get the data set. cds:list_dataset = cds:list_dataset.cds.filter("" ?metric"").set_option( ""metric"", ?metric_lbl             ) cds:list_dataset.summary = cds:list_dataset.lbl.to_chr( "" summary"",""n"", ""mean"",""std"",""standard deviation"",""max"",""min"",""max_tables"") Input: data set name: CDS dataset name: Cheetah data set name: DMControl500k dataset name: benchmark. Output: // Build the models with the output dataset data set cds:build_dataset(testdata:cds:list_dataset).categories = cds:build_dataset(testdata:cds:list_dataset).categories.filter( "" ?metric"" ) cds:build_dataset(testdata:cds:list_dataset).criterion = CENSORED cds:build_dataset(testdata:cds:list_dataset).metrics = cds:build_dataset(testdata:cds:list_dataset).metric_lbl cds:build_dataset(testdata:cds:list_dataset).criterion.build_with(cds:build_dataset)"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 River Raid?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT CASE WHEN CASE WHEN CASE WHEN CASE WHEN CASE WHEN CASE WHEN CASE WHEN CASE WHEN CASE WHEN CASE WHEN -------------------------- ---------- ---------- ----------- -------------- -------------- -------------- -------------- 4th Model: 0.210906 0.277918 0.310301 0.064696 0.241591 0.237536 0.210512 --------------- ---------- ---------- ----------- -------------- -------------- -------------- 4th Model: 0.261797 0.274934 0.326897 0.055691 0.295847 0.295847 0.255706 --------------- ---------- ---------- ----------- -------------- -------------- -------------- 4th Model: 0.261897 0.233737 0.315635 0.026809 0.275739 0.274608 0.262084 --------------- ---------- ---------- ----------- -------------- -------------- -------------- 4th Model: 0.211424 0.232337 0.315635 0.006745 0.287713 0.262084 0.261378 --------------- ---------- ---------- ----------- -------------- -------------- -------------- 4th Model: 0.277416 0.226946 0.263911 0.025689 0.273747 0.261941 0.263129 --------------- ---------- ---------- ----------- -------------- -------------- -------------- 4th Model: 0.264021 0.233737 0.315635 0.056745 0.288811 0.259328 0.261554 --------------- ---------- ---------- ----------- -------------- -------------- -------------- 4th Model: 0.232489 0.231295 0.315635 0.026809 0.272749 0.2465	False
"
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Text8 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What types of nanocarriers do have therapeutic effect?
 output (Sparql query): "	"SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = ""T""^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } "	SELECT RANK( ?metric ||. ?metric ) as RESULT FROM ( ?dataset ), ?metric_lbl, ?criterion_lbl, ?criterion_lbl, ?criterion_lbl; input (English text): Type of nanocarrier ? (1 = Yes, 0 = No) output (Sparql query): SELECT ?dataset_lbl ?model_lbl, ?criterion_lbl, ?criterion_lbl, ?criterion_lbl, ?criterion_lbl as name of type of nanocarrier ?name input (English text): Type of nanocarrier ? output (Sparql query): input (English text): Type of nanocarrier ? (1 = Yes, 0 = No) output (Sparql query): query_option_text_nano_carpet_traction I'm curious what is the probability of having nanocarriers installed on your nanocarrier: input (English text): The number of Nanocarriers installed: output (Sparql query): input (English text): Your status with nanocarrier installation: 0: Not installed output (Sparql query): user user: c password: PASS WU NNNNNNNNNNNNNNNNNNNNNNNNNN id user(123) user(1) 	False
"
 input (English text): Can you list the metrics used to evaluate models on the RTE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP(1 AS _ROUGE_2_PERIOD, 2 AS _ROUGE_2_SCORE) FROM rte_dataset_samples ORDER BY ROUGE_2_PERIOD DESC ORDER BY ROUGE_2_SCORE DESC ROUGE_2_COUNT AS _ROUGE_2_LANG, ROUGE_2_SHIFT AS _ROUGE_2_SHIFT_DESC, rte_dataset_samples.cov1_1 AS _ROUGE_2_COUNT ROUGE_2_COUNT AS _ROUGE_2_LANG, ROUGE_2_COUNT AS _ROUGE_2_SHIFT_DESC, rte_dataset_samples.max_samples MAX_SKELP_SIZE, ROUGE_2_LANG_TEST AS _ROUGE_2_LOCAL, ROUGE_2_SUMMARIZATION AS _ROUGE_2_TACTICS_DESC FROM rte_dataset_samples GROUP BY _ROUGE_2_LANG ROUGE_2_COUNT_1 AS _ROUGE_2_COUNT ORDER BY _ROUGE_2_COUNT DESC, _ROUGE_2_SHIFT_DESC ORDER BY _ROUGE_2_SHIFT DESC ROUGE_2_COUNT_2 AS _ROUGE_	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Cartpole, swingup (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + weight-sharing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 We've been using a combination of machine learning and R to build a system to generate performance benchmarks for a variety of supervised R applications, from data mining to financial analysis. The goal is to provide a source of data and software that can be used to investigate, benchmark, and extend (or reuse) existing software, and thereby gain insight into how to improve existing applications. We started by building a machine learning framework that computes many benchmarking metrics for any R application, including R. R benchmarks are a simple way to get a high-fidelity look at the performance of existing software. As such, benchmarks are used in a variety of applications that are developed for use in industry. The system we're building to generate benchmark stats depends on a couple of key pieces of software. One is SQLite, the database engine we used in our original system. The second is a tool we built to connect to the API from the library of benchmark and dataset sources we are linking to, or to get a quick rundown of all the data we are linking from.	False
"
 input (English text): What are the models that have been benchmarked on the CS-NER dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CS-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Permuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 I will create a new model to run the benchmarks for another dataset that we will use only for the above purposes, but the data and benchmarks can be used. I have created two data files: one for the model I use for the benchmark data and the other one for the original dataset. I will run the tests on a single data set and use only the model that performs best, as if I did not run the benchmark data and use only the model that achieved highest accuracy (see this  article ), but the model I have used is not good enough for the benchmark data. I run the benchmarks on the model I used and in contrast to the models found in the paper I use a different parameter from the ones found in the paper, which are  I have prepared a CSV file like this example for the data: [model x] 1. ctr[b, n] = 0.2,0.2 (min 1,max 3) 2. ctr[b, n] = 0.2,0.2 (min 1,max 3) 1. ctr[b, n] = 0.2,0.2 (min 1,max 3) The default parameter for the performance parameter i.e., (ctr[b])  is 0.2, which is very low as well as the maximum (in the above examples, max = 3 ). With a high value for the desired parameter, the model is expected to produce a performance that the benchmark dataset is very close to. The benchmark dataset uses 2,048,000 rows and performs extremely well for the performance parameter i.e., (ctr[b]) = 0.25 = very close to the parameter to be used for the benchmarked model i.e., (ctr[b] < ctr[b-1]). I	False
"
 input (English text): Could you provide a list of models that have been tested on the MultiRC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of models that have been evaluated on the SciTLDR benchmark dataset ? output (Sparql query): input (English text): Could you provide a list of	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" select model, lbl_matrix, rdfs from (select test_dataset, model, lbl_matrix, rdfs as metric_lbl from 'tests-p1.rsmc/p2.rsmc' where test_dataset = 'NYT24#' select model, lbl_matrix, rdfs as metric_lbl from 'tests-p1.rsmc/p2.rsmc' select model, lbl_matrix, rdfs that match criterion to perform evaluation on theNYT24 dataset) rdfs; Note: Using metric_rdfs requires a Python interpreter that supports the ""import rdfs"" command. If this command doesn't do so, you can install the R package rdfs. It contains a lot of functions to interact with the R RDF database. Here are some examples of the R's API: $ rdfs install import rdfs -rjson and then using the data output: #> print(data.names.rjoin(test_dataset, model, lbl_matrix, rdfs)) Model: { 'test_dataset' : 'NYT24#','model' : 'NYT24', 'lbl_matrix' : 'NYT24', 'rdfs' : 'NYT24' } RDFS: { 'NYT24#' : { 'rdfs': NYT24, 'label': 'NYT24' }, 'NYT24#' : { 'rdfs': 'N', 'label': 'NYT24' } } What metrics are commonly used when benchmarking models on the TestFocused dataset ? input ("	False
"
 input (English text): Provide a list of papers that have utilized the Longformer Small model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Longformer Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?name, ?value, ?dataset, ?benchmark, ?cont orkgp:HAS_DATASET, ?quantity, ?benchmark1, ?quantity1, ?benchmark2, ?quantity2, ?benchmark3, ?dataset, ?number ?model, ?model1 ?score, ?type (integer ?value), ?type (float ?quantity), ?category (boolean ?category), ?model_lbl (short orkgp:model) AS model_lbl; Step 4: Get the metrics. How do you want to get all metrics of a model ? input (English text): Please help us get an example of the metrics for the Longformer Small model in the IMDb Dataset. output (Sparql query): SELECT ?name, ?value, ?dataset, ?benchmark, ?cont orkgp:HAS_DATASET, ?quantity, ?benchmark1, ?quantity1, ?benchmark2, ?quantity2, ?benchmark3, ?dataset, ?number ?model, ?model1 ?score, ?type (integer ?value), ?type (float ?quantity), ?model_lbl (short orkgp:model) AS model_lbl; Step 5: Get aggregated counts of metrics. How does one interactively find the corresponding aggregate stats for a model ? (e.g in the case of a Longformer Medium model, how would	False
"
 input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification."") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?dataset ?dataset_lbl FROM ?problem WHERE ?labels. value = ?label ; output (Sparql query) result (Sparql query) select ( ?dataset/{ ?totf_type ?totf_format ?totf_lbl_label/ ?totf_labels. ?label/ ?totf_lbl. }) output (Sparql query) select ( ?dataset/{ ?totf_type ?totf_format ?totf_lbl_labels/ ?totf_labels. ?labels/ ?totf_lbl_labels. }) Result for question output (Sparql query) result (Sparql query) select ( ?dataset) SELECT ?dataset ?dataset_lbl_labels FROM ?problem WHERE ?labels. ?""( ?![ ?labels. ?label/ ?labels. ])"" ?""( ?![ ?labels. ?labels/ ?labels. ]). ?label/ ?labels. ?""( ?![ ?labels. ?labels/ ?labels. ]). ?labels/ ?labels. ?"" ( ?![ ?labels. ?labels/ ?labels. ]). ?labels/ ?labels. ?""( ?![ ?labels. ?labels/ ?labels. ]). ?labels/ ?labels. ?""( ?![ ?labels. ?labels/ ?labels. ]). ?labels/"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Demon Attack benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Demon Attack"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?metric  | ?metric rdfs:label  | ?metric a orkgc:Dataset; rdfs:label ?dataset_lbl. Filtered by Metrics that were used to evaluate models on the Atari 2600 Demon Attack benchmark dataset ? input (English text): What data was used to make the graphs for the Metric Evaluation graphs ? output (Sparql query):  SELECT ?metric ?measurement ? input (English text): What time period and hardware was used to make the graphs for the Metric Evaluation graphs ? output (Sparql query):  SELECT TIMESTAMP ?measurement ? input (English text): Give an example of a performance benchmark score-based test that this benchmark has validated (e.g.  an Atari 2600 Demon Attack test, or a test like the MultiNLI benchmark). output (Sparql query):  SELECT score ?measurement ?  |score rdfs:labels score score. OPTIONIONAL or score score. input (English text): How many GPUs has this solution been trained with ? A maximum of 4 GPUs at all times. output (Sparql query):  SELECT max( ?metric_lbl) ?gpu ?gpu_lr ?gpu_lbl. max( ?metric_lbl) ?gpu ?gpu_lr - max( ?metric_lbl) ?gpu ?gpu_lbl. max( ?metric_lbl) ?gpu ?gpu_lr - max( ?metric_lbl) ?gpu ?gpu_lbl. max( ?metric_lbl) ?gpu ?gpu_lr ? OPTIONAL or max( ?metric_	False
"
 input (English text): What are the models that have been benchmarked on the SNLI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 option (orkgp): orkgp:HAS_MODEL orkgp:HAS_BENCHMARK orkgp:HAS_BENCHMARK OPTIONAL { ?paper orkgp:HAS_MODEL ?model. } option, orkgp, DISTINCT, select (orkgp, {-c, -f, -j}) select, {-p, -c, -f, -j} < select (orkgp, {-c, -f, -j}) select, {-d, -j, -f, -cr} < select (orkgp, {-l, -j, -f, -ch}): l { -i, -j { -p, -c, -f, -j} }, { -y, -l, -j, -c, -f} </ select (orkgp, {-c, -f, -ch}): l { -x, -l, -j, -f, -l} </ select (orkgp, {-d, -l, -j, -f, -cr}): { -a, -n, -d -c -f, -j} </ select > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 option, { - cm, { - - f, - k, - j }, select, { - p, - c, - f, - k }, { - d, - j, - f, - f }, { - y, - l, - j, - c, - f } > < select, { - c, - f, - k, - j }, select, { - d, - j, - f	False
"
 input (English text): What is the name of the top performing model in terms of FLOPS score when benchmarked on the FGVC Aircraft dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT name FROM { ?dataset ?dataset_lbl.} LEFT OUTER JOIN { ?dataset ?dataset_lbl_lbl.} ON 1 2 3 4 5 6 SELECT name FROM { ? dataset ?dataset_lbl.} LEFT OUTER JOIN { ? dataset ?dataset_lbl_lbl.} ON 1 2 3 4 5 output ( Sparql query ) : ----------------------- SELECT name FROM { ? dataset ?dataset_lbl_lbl.} LEFT OUTER JOIN { ? dataset ?dataset_lbl_lbl.} ON 2 ---------------------- Output ( Sparql query ) : --------------------------------------------------- This query uses a filter so that it returns only results from models where the metric or score is greater than 1 and the benchmark is less than 10. Since I did not want to manually insert the criteria, I just specified the criteria on the form. So there is really no limitation to what you can do with Postgres. If you don't want to select all models, the query can be done like this. Again, this is very much a sample size exercise only, as our actual use would be much higher or lower. The main idea here is to show how good Postgres is in benchmarking, not for statistical benchmarking, because the actual performance of your models is going to be much higher, especially for data with a high cardinality. For Postgres, the high cardinality is usually a good thing because this makes it hard to avoid expensive joins (which would not be required for more common databases). To perform this task, we will have to make several changes such that all queries are against the same dataset. Let us look at two models first. The first is a large relational dataset,	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CoLA?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoLA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- ------------- CoLA vs cof ------------ ------------------------- cof1 - cof2 | --- --- --- CoLA vs cof input (English text): Explain your rationale behind using the sentence evolution model ? output (Sparql table): ------------------------- ------------- CoLA vs cof ------------ ------------------------- ------------- |---|---|- ------------|---|---|- cof1 = 1.0 output (English text): Explain your rationale behind using the sentences ? Explain why the model performed so well in terms of word completion and the average rate of sentence resolution ? input (English text): Explain your rationale behind using the sentences ? output (English text): Why did you choose this model or why are others so enamored with this model ? What was your main reason for choosing this model ? output (Sparql query): ------------------------- ------------- cof1 < 1.0 |---|---|- CoLA vs cof (cof2) = 0.5 |---|---|- (mosaic) CoLA vs cof (cof1) = 0.9 |------|------|- (noise) # of sentences cof > cof1 cof1 > cof2 1 1 0 0 0 0 output (English text): Answer the questions ? ------------------------- CoLA vs cof ------------ ------------------------- input (English text): Discuss the validity of this model in terms of the accuracy, accuracy rate of the sentence and the word completion rate ? output (English text): Explanation of how the model performed input (English text): What is your method of validation for the model ? I would like to know when the model performed more poorly, was the model not valid and how do you validate the model ? What is your method of testing for	False
"
 input (English text): Where can I find code references in papers that have used the Linguistic Input Features model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linguistic Input Features"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): input (English text): How do you compare the performance of Linguistic Input Features vs. the Hadoop Language Features model ? output (Sparql query): Output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): Output (Sparql	False
"
 input (English text): What is the highest benchmark result achieved on the Finger, spin (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT Metric, ?score FROM ?tldr2 (Metrics of: ?Metric ( ?dataset_lbl. ?model_lbl. metric_lbl. ?model_lbl Metric ( ?value) | ?score) ) ORDER BY Metric|score  {Metric|Score} Outputs: Metric_lbl: 1 Metric_lbl_hits: 1 Metric_lbl_lateny: 1 Metric_lbl_lateny_min: 1 Metric_lbl_min_milliseconds: 0 Metric_lbl_max_milliseconds: 1 Metric_lbl_max_millisecond_millisecond: -7 Metric_lbl_max_millisecond_millisecond_min: 0 Metric_lbl_max_millisecond_millisecond_min_milliseconds: 0 Metric_lbl_min_milliseconds: 0 Metric_lbl_min_lateny: 0 Metric_lbl_min_lateny_min: 0 Metric_lbl_min_lateny_min_milliseconds: 0 Metric_lbl_min_lateny_max: 0 Metric_lbl_min_lateny_max_milliseconds: 1 Metric_lbl_max_lateny: -7 Metric_lbl_max_lateny_min: -7 Metric_lbl_max_lateny_min_milliseconds: -7 Metric_lbl_min_lateny_max_milliseconds_milliseconds: -7 Metric_lbl_min_lateny_max_millisecond_millisecond_milliseconds: 0	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" output (Sparql query with ""user"" as subject): output (Sparql query with ""user"" as subject output(Sparql query with ""user"" as subject)): Reverse RDF: input (English text): The original dataset has been modified and its attributes altered by a user, i.e. changed. Can you describe the changes in the XML/Json file(s) ? Include full path name of the new dataset ? output (Sparql query): output (Sparql query with ""user"" as subject): Reverse RDF: output (Json document): input (English text): The old dataset has been modified and its attributes altered by a user. Have you ever altered your document or dataset's attributes at the start ? output (English text): output (Json document - without changes): input (English text): The old dataset has been modified and its attributes altered by a user. Have you ever altered your document or dataset's attributes at the start ? output (English text): input (English text): The old dataset has been modified and its attributes altered by a user. Have you ever altered your document or dataset's attributes at the start ? output (English text): input (English text): The old dataset has been modified and its attributes altered by a user. Have you ever altered your document or dataset's attributes at the start ? output (English text): input (English text): The old dataset has been modified and its attributes altered by a user. Have you ever altered your document or dataset's attributes at the start ? output (English text): RDF/XML: output (Json document): output (Json document - without changes): input (English text): The old dataset has been modified"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 5.5.5.3. Comparison of models Results are compared by running the model by itself within a benchmark or comparing model performance using several benchmark models with the same input data. If model performance is not comparable, the highest ranked model that best performs the query is considered for the final model evaluation. In case the input data is not available at the default settings, a default set of the input data is used. 5.5.5.4. Model accuracy In addition to the query accuracy, it is important to measure the accuracy of the generated models. For each dataset, we generate a new training set for each language which is then compared to a trained model using the accuracy test. The results of the accuracy test are reported separately for each language. Note: The model accuracy will not apply to query models that rely on a single model instance for training and validation. This may be due to the model initialization steps. 6. Results 6.1. Learning in a model-based model-based solution For this example, we have used the following models: The model for the English language is: classifier.py import numpy as np mba = numpy.ndarray([[0.95, 0.95], [0.96, 0.96], [0.97, 0.97], ]) training_data = np.loadtxt('train.txt') We also have a few other models from a different benchmark dataset: classifiers.py import matplotlib.pyplot as plt mba = numpy.ndarray([[0.95, 0.95], [0.96, 0.96], [0.97, 0.97], [0.98, 0.98]]) training_data = np.loadtxt('train2.txt') The accuracy of the algorithm for the English	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the BUCC Russian-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?top result ?top result_met'BUCC Russian-to-English database. This is the top score of the dataset over time. ' ?top result ?top result_met'BUCC Russian-to-English database. This is the top score of the dataset over time.' output (Sparql query): SELECT ?top result ?top result_met ; BEST PROFESSIONAL RESEARCH BEST PROFESSIONAL RESEARCH (2nd level) BEST PROFESSIONAL RESEARCH (2nd level) (BRC2 is one of 10 best practices for this level) What is the top score of the benchmark ? output (Sparql query): ?top result ?top result_met ?benchmark. ?score BEST PROFESSIONAL RESEARCH (2nd level) (BRC1 is one of 10 best practices for this level) WHAT was the top score of the first and second levels of this project ? output (Sparql query): ?benchmark ?benchmark_first_level. ?score BEST PROFESSIONAL RESEARCH (2nd level) (BRC2 is one of 10 best practices for this level) WHAT is the title and ID of the first and second level of this project ? output (Sparql query): ?benchmark_structure. ?title. ?score BEST PROFESSIONAL RESEARCH (2nd level) (BRC1 is one of 10 best practices for this level) WHAT are all the relevant results ? output (Sparql query): ?results. ?results_met. ?results. ?results_structure. ?results. ?results_benchmark. BEST PROFESSIONAL RESEARCH (2nd level) (BRC	False
"
 input (English text): What are the most commonly used benchmark datasets for the Semantic Textual Similarity research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Textual Similarity"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem. FILTER (str( ?problem_lbl) = ""Reacher, Easy"" ) ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?score orkgp:PRIMARY_SEMICOLUMN ?dataset. ?score_lbl a orkgp:SECONDARY_SEMICOLUMN ?dataset. }   As you can see the scores of these two datasets are close to the one used for the comparison. This implies that the Reacher dataset uses the most good training data, has the most common problem but also has an easy solution. But these two datasets don't show more similar training data, because Reacher has a really old dataset, while Easy lacks the best training data, and Easy is still mostly difficult. As always the benchmark results are a good indication of where to start with data acquisition. In the next posts I discuss which of the available benchmarking tools or methods to use for the Semantic Web, and which aspects of the Semantic Web can benefit from the methods and benchmarking tools described in this post."	False
"
 input (English text): What is the top benchmark score and its metric on the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT a.className, a.vendorName, a.brandName, a.modelName, a.languageCode, a.version FROM ?dataset_reward a JOIN ************* ?sparql_pk ?metric_lbl ************ to ?metric_reward ?sparql_pk.model_name ORDER BY ?metric_lbl.name DESC"" SELECT a.className, a.brandName, a.version FROM ?dataset_reward a JOIN ************* ?sparql_pk ?metric_lbl ************ to ?metric_reward ?sparql_pk.model_name ORDER BY ?metric_lbl.name DESC ? ; ********** The models in front of the asterisk (*) were chosen to represent the highest scoring ones and their features are as follows: [#] # # [#] # ######## [#] # # ######## # # ######## # # ######## # # ######## # # ######## # # # ######## # # {{'A'_A','A'_B','A'_C}} ######## ######## # # ######## # # ######## # # ######## # # # # # ######## # # ######## # ######## # # # ######## # # ######## # # ######## # ######## # # # ######## # # # # # # ######## ##### ### # # ######## # # ######## # # ######## # ######## # ######## # # ######## ##### ### The model is a simple one, and is based on a basic scoring engine and a classifier. It is trained to determine the likelihood of a given dataset when using different features"	False
"
 input (English text): What risk factors for OSA are discovered in the paper with 100% OSA frequency?  
 output (Sparql query): SELECT ?risk_factors, ?risk_factors_labels WHERE { orkgr:R110932 orkgp:compareContribution ?contrib. ?contrib orkgp:P37530 ?osa_frequency. ?osa_frequency rdfs:label ?osa_frequency_values. ?contrib orkgp:P37528 ?risk_factors. ?risk_factors rdfs:label ?risk_factors_labels. FILTER(REGEX( ?osa_frequency_values, ""100""^^xsd:string)) } 
 input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Can DQNMMCe+SR be used to analyze the relationship between the risk factors and different body mass index (BMI) values ? output (Sparql query): #... input (English text): Could you write another code that allows the user to enter a query to get the weight in grams of an object that is part of a dataset ? output (Sparql query): input (English text): Could you add a test for this paper ? output (Sparql query): The answer is no (and there is also a comment below about the authors asking for this information). I just wanted to ask specifically if you are planning to be able to run any code on the published results with the results from the published source ? input (English text): I know that I may not be able to follow up with the authors when I have questions related to the DQNML approach and how to be able to use it in the future. (This question was answered by the authors on pg. 6 in their acknowledgments – pg 7 is the discussion of these questions). If we run code on the original dataset, will the result be stored in a different database ? Also, will we need to run that code on the published results ? output (Sparql query): There are a number of references on the website related to the DQNMMC paper where the DQNMMC papers can be found, including on pp. 3, 5, 29. There are also lots of comments/questions in the paper itself and comments in the acknowledgments. You may need to dig deeper and review the results of the paper to read the full details and see what we did to make it as clear and simple as possible. input (English text): Which of the two most commonly used approaches for dealing with	False
"
 input (English text): Indicate the model that performed best in terms of Error metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 1.0.0.0 (Ball in cup, Catch DMControl100k) 1.0.0.0 (Ball in cup, Catch DMControl100k) Resulting RDF:  { ?dataset orkgc:Dataset ?dataset_lbl ?model_lbl ?metric_lbl. ?eval_i orkgc:Criterion ORDER BY DESC( ?value) LIMIT 1 } input (English text): Enter an RDF expression to generate the model score and metric. output (Sparql query): The model score is: 5.0000005/1. 0.0.0.0 (1.0.0.0/1.0.0.0) input(English text): Specify the dataset or dataset object that contains the data that is being tested for accuracy. output (Sparql statement): Select the dataset, or dataset object that contains the dataset, and the metric to perform the analysis on. The orkgc:Criterion defines what standard Metric of the dataset of interest is being evaluated. orkgc:Criterion is one of the following values:  a, b, c, or d. Inference:  1. The orkgc:Criterion parameter evaluates the metric (a) for the selected dataset and the metric for a nonempty dataset. Outcome:	False
"
 input (English text): What are the models that have been benchmarked on the Words in Context dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT %score ?word FROM word ?score ? WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } output (Sparql query): SELECT %score ?word ?word1, %score ?word2, %score ?word3, etc. FROM word ?score ?word1 ?word2 ?word3; option (Func, optional): option (String): output (Sparql query): option (Object) results : option (Func, optional): option (String): output (Sparql query): If there are any models that perform poorly, an explanation, in this case, does not sound good. The key idea here is that you want to make sure that if you do one of these things (say benchmark one model using NYT-single, the other model to use an analysis model/analysis group) the result should always be the same...and also that the model that is used in a benchmark should be a good model. Here are some benchmarks that illustrate the process of using NYT-single: There is no benchmark of either the word2vec or the lexical_quotient model. There are many papers about how to train a word2vec model. There are many benchmarked word2vec models that demonstrate that they outperform the previous model. There is yet no benchmark of the lexical_quotient, but in a recent paper these authors suggest that an unsupervised word2vec	False
"
 input (English text): What are the metrics used by paper ""Using NMF-based text summarization to improve supervised and unsupervised classification?
 output (Sparql query): SELECT ?metrics, ?metrics_labels WHERE { ?papers rdfs:label ?title. FILTER(REGEX( ?title, ""NMF-based text summarization"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P34 ?eval. ?eval orkgp:P2006 ?metrics. ?metrics rdfs:label ?metrics_labels. } 
 input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The benchmark paper: http://jha.sz.uni-erlangen.de/Sparql/MEMEN/MEMEN_Bench.pdf is available on the internet. How to reproduce the performance: Using NMF text summarization in Paper 748. Using NNLP (from paper: http://jha.sz.uni-erlangen.de/Sparql/NNNLP/NNNLP_Bench.pdf) The benchmark paper: http://jha.sz.uni-erlangen.de/Sparql/MEMEN/MEMEN_Bench.pdf is available on the internet. I don't have paper 748. Using NNLP (from paper ""Improving the performance of model training via NLP""): The benchmark paper: http://jha.sz.uni-erlangen.de/paper/NPNL/NPNL_Bench.pdf is available on the internet. Note that the ""NBML"" has not been benchmarked yet. I also use an older version of the Memen/ML: The benchmark paper: http://jha.sz.uni-erlangen.de/Sparql/MEMEN/Memen.pdf is available on the internet. Note that the ""NBML"" has not been benchmarked yet. I use the ""FastNMF"" package (no link, sorry) I don't have a link for this, please do make sure that you have a good NNLP library. How to reproduce the performance: Using my version of the Memen/ML, using a small number of models I don't have a link for this, please do make sure that you have a good NNLP library. My"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ORKG-TDM dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ORKG-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"------------------------- ORDERS ------------------------- The results show that 1 of 10 papers has used a MMV TSM-50x2 model. In all of these other papers, as well as the above three, some or all of the metrics used do not indicate a high-level representation of the topic. These metrics are of low relevance, in other words, they are not representative of how the paper uses the model, and they lack relevance to the results for which they're used. For these reasons, the most common metrics employed are the most general: the percent of samples that match one of 20 types of parameter (for example, one might use a 50% metric in its paper if it has 50% of the samples fall in the parameter ""M"". Another metric that is used in the majority of papers is the percent of samples that have one or more ""true positives"" (in other words, those that have been given a good score). In general, it appears that using the exact values of a measure instead of using the metric allows it to have a broader reach than using the metric in whole. This isn't surprising, because with any metric, we always want to estimate something about the effect it will have, not about the measurement itself. For example, if we have an equation that says it takes, say 0.05 to raise the price of a stock from $3/share to $5, and we're interested in whether it takes up any of the parameters above (i.e., the price, the percentage of samples that are positive, etc.), then we'd want to look at the effect by comparing the actual price change of the stock to the expected price change to see what would be expected. This is the same thing we'd want to do if it took up an extra parameter, a second ""true positive"" that didn't seem to be giving much of the desired effect."	False
"
 input (English text): What are the models that have been benchmarked on the Reuters RCV1/RCV2 German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT SIZE(score) AS score, RANK(rating) AS rANK_percentage FROM { ?dataset a:RDFS,b:RDFS,c:RDFS rdfs:list,reviews:bool } GROUP BY score; query (Input): How were the model scores calculated for each of the model's features (the two columns in the second sentence) ? Optional: Your model's features should be in the form of an rdfs table in Tableau (preferably using RDF for RDFS, r_table for rdfs and dtype as you probably do in Tableau). Here's an example: input (English text): Which features were included in the model ? output (Sparql query): SELECT features.name, features.rank, features.ranks, features.total_percentage, features_id_col.rank, features.total_time_rating_hours FROM features CROSS JOIN features_criterion CROSS JOIN reviews CROSS JOIN reviews_criterion CROSS JOIN reviews_criterion_criteria CROSS JOIN reviews_criterion_rdfs CROSS JOIN reviews_rdfs CROSS JOIN reviews AS reviews_criterion SELECT RANK(rating) AS rANK_percentage, RANK(total_time_rating_hours) AS total_time_rating_hours, feature, rdfs.id, RANK(rating) AS rank, rdfs.name AS	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the NLP-TDMS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT orkgp:HAS_START ?metric orkgp:HAS_START ?dataset AS ?metric_lbl; input (German English data): What is the evaluation metric of the model that we benchmark in this dataset ? output (Sparql query): SELECT ?metric { ?dataset a orkgc:Dataset; NLP-TDMS RDFS:RDFRS ?metric } FROM ?lbl; Input data: BLEU test score for the model ? What does a BLEU score mean for this model ? What are some common BLEU metric parameters for using this benchmarking dataset ? To illustrate these points, let us evaluate a different benchmarking model for another German-English dataset that we benchmark in this post: WMT2008 French-English Dataset. This is a dataset with a high number of words, a vocabulary that ranges from a few hundred thousand to over 7,000 million. It contains some English vocabulary from historical texts, many French words from contemporary works, and a number of terms we would like to know about. We do not benchmark this dataset against other German-English datasets or on other corpora (BLEU-Score and BLEU-Valley) since it contains a fair number of different words and corpora for many languages. BLEU is a metric of similarity, or the degree to which words used in a dataset differ from similar words in other data sets. There are a couple of ways to evaluate a model for BLEU score metrics. First, we can just look at how often this model shows similar words in other data sets in some specific regions. For example, if we compare a machine learning model to a set of English words from a corpus	False
"
 input (English text): List the code links in papers that use the Linear Transformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linear Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?code, %i% code FROM ( SELECT ?code as ?code; ?dataset as ?dataset;  AND ?cont as ?cont;  AND ?coefficients as ?coefficients; ORDER BY ?coefficients DESC ) ?data AND ?cont AND ?coefficients As I mentioned above, the ""linear"" Transformer performs well on most tasks, though that doesn't always mean it is the best choice. When the goal is an easier way to do something, you may want to opt into a ModelTransformer like the linear transformer. There are others like log linear or k-sigmoid or L1-norm, with those taking up a chunk or two of your memory. In the case of the log and k-sigmoids, they do some nice things where they use vectors for the kth dimension, but these are usually not necessary in a data type that is primarily a sparse vector. There are other transformers in R (s-transform and p*transform) that may work better in the particular circumstances, or a lot less memory use, depending on what you may be using them for. We'll talk more about some of the other transformers in future posts. 2) What are some recommendations for using the SAC model ? There are many algorithms out there for building a linear model. Some of the best ones use linear algebra, so if you can work with a few variables, you can find some. Others are more algebraic than a linear algebra model. The SAC model looks to be fairly well tested, though I have not had time to look in to these myself. I have written a set of guidelines, which you can think of this way: SAC may be used with any model based on log-linear algebra; The SAC model has the least memory overhead; a few"	False
"
 input (English text): What are the metrics of evaluation over the WMT2016 Czech-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?metric ?metric_lbl FROM ?research_metrics ?research_Metrics as MetService WHERE MetService IS NOT NULL AND MetService.Metric ?metric_lbl. Input (English text): Select an entire database of papers covering the WMT2016 Czech-English dataset ? Output (Sparql query): SELECT ?metric ?metric_lbl FROM ?research_metrics ?research_Metrics as MetService WHERE MetService IS NOT NULL AND MetService.Metric ?metric_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset rdfs:label ?dataset_lbl. } Input (English text): Select an entire database of papers covering the WMT2016 Czech-English dataset that have used the SYSML and WMT2016 Czech-English datasets ? Output (Sparql query): SELECT ?metric ?metric_lbl FROM   ?research_metrics ?research_Metrics as MetService WHERE MetService IS NOT NULL AND MetService.Metric ?metric_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset rdfs:label ?dataset_lbl. } Input (English text): Select an entire database of papers covering the WMT2016 Czech-English dataset that have used the WMT16/WMT2016 Czech-English dataset ? Output (Sparql query): SELECT ?metric ?metric_lbl FROM *research_metrics and* research_Metrics as MetService WHERE MetService IS NOT NULL AND	False
"
 input (English text): Can you list the metrics used to evaluate models on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Past Decode Reg. + AWD-LSTM-MoS + dyn. eval."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 -------------------------------- ----------- model_benchmark... model_benchmark.txt The most important feature that sets the models apart in this project is that both the model (A) and the dataset (B) are trained using pre-trained supervised classifiers. A typical supervised classification model is trained with data using the perceptron architecture, which allows it to be trained independently of the training set. A classifier can also be set up on a regular basis, using an optimization technique commonly referred to as batch-fitting that allows for parallelization and improves predictions. In this project, we developed an algorithm that combines a Convolutional Neural Net (CNN) with a Dense Convolutional Neural Network (DNN) for the classification of English text. Specifically, the Convolutional Neural Net is trained using pre-trained classifiers that are used as benchmarks for the DNN. There are many examples of convolutional neural networks that can be used for the classification of text, and many implementations exist to perform such a classification. (Examples of such algorithms using CNNs that I have used are also included in the dataset. ) In combination with other model-based techniques, such as Feature Selection and Random Forests, the model can be used to perform more complex classification algorithms such as Support Vector Machines (SVM) or Random Forest(RFF), which can be used for more sophisticated analyses, such as classification of language. Further, we can also use a neural network to predict word frequency and sentiment based on various sentiment analysis metrics such as the Positive and Negative Affect score (PANAS) and the Negative Affect Score (NASS). This is something that is common in social network analysis. The feature that we are exploring in this project is a way to combine the two approaches and to apply the algorithm to a more formal language processing task. The classifiers include a two-layer Convolution	False
"
 input (English text): List the code links in papers that use the Qbert Rainbow+SEER model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Qbert Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT(1) from aq_benchmark GROUP BY aq_model ORDER BY COUNT(1) DESC LIMIT 10 We are using this option to limit what we can do with this functionality but there are other options as well if that helps your workflow. In other ways we will not be able to extract data this way due to some limitations of the model we use and most of the benchmark files have the same name. We can also extract data directly by executing query_data_set. The result will be an array of rows where each row contains the corresponding feature (or features, in Qbert Rainbow+SEER's terms). Note: this also results in rows that cannot be stored directly so the final output is an SQL query_data_set We also have the power of SQL to retrieve the details of a specific benchmark in detail, like the model name, a score and a name of the benchmark, all without resorting to the manual analysis of the raw data. All in all, it is much more powerful than the manual approach so is much more efficient, but still very limiting. I don't know how easy it would be to achieve with manual analysis. Now, to implement this we need to implement an interface and get it done at the same time. That's why I've decided to extend the existing interface to make it more powerful in some cases and easier to extend with a script. In this post I show how easy it is to implement a fully automated approach that combines both features extraction and query execution directly into a single pipeline. The script is an excerpt of an ICS-compliant API and is available here. You can find the original code in this gist as well. It will start by retrieving benchmark data from http://trinity.ncsa.edu/archetypes. It will first read the filename	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Sandwich Transformer (adaptive span) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sandwich Transformer (adaptive span)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1-score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 In terms of language (English, Spanish, German, French, and Chinese) output: We have selected two data sets, which represent a variety of language and cultural contexts, as it is known that English/English-speaking individuals in various countries perform very differently, and in each case, the language of the context has an effect on the performance of their model. In a second benchmarking set, the goal was to benchmark the Sandwich Transformer (Adaptive Span) model that we use in our SPSS project and, further, to benchmark a modified variant of it called the Adaptive Spanned Span Model (ASSP). We have chosen to use this model in the following benchmarks (the last of which will test our new data set: English-speaking French-speaking Spanish-speaking German-speaking Japanese-speaking Chinese-speaking), and these benchmarks correspond to the two categories in which our original dataset is used for comparison (English-speaking French-speaking Spanish-speaking German-speaking Italian-speaking Chinese-speaking). We compared our original data set, in order to show that using different, but related methods in the translation of text can significantly improve the accuracy of the data, even though we still cannot directly compare this improvement with our original data set's performance (at least on benchmarks that were developed specifically with it in mind). In this benchmarking set, we ran 10,000 iterations of the Sandwich Transformer (Adaptive Span) model, which is a simple model with minimal dependencies in the translation of text. We used the Sandwich Transformer to analyze the text in 15 different languages (excluding Chinese and Japanese, because they do not have enough text in English). The results in English are shown in Table 1. Table 1. Performance of the 100,000 Iterations of the Adaptive Span Model Benchmark Language Benchmark (x) Time Benchmark (ms) Accuracy French	False
"
 input (English text): Could you provide a list of models that have been tested on the Sequential MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""STREET"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 We recommend you download our software and take advantage of the tools provided by our community. If you find a bug, take a look at the bug tracker to report it. If you can't find what you're looking for, or have found a bug that isn't covered by our documentation, feel free to contact us (support@strangetree.com) in the Support and Bug Reports forum. If you would like to support Strangetree and its development, we would gratefully appreciate your purchase of a Strangetree subscription. Subscription costs can be found here. If you would like to buy a subscription to the Strangetree community of development and maintenance, please do so. If you have any comments or other feedback about our site or site-related activities, please feel free to contact us. You may also contact us using our community forum (forum.strangetree.com). About Strangetree The Strangetree project is the creation of Jeff Sotero and Matt Hedden. We are a software company that is committed to developing and enhancing large data sets that are accessible for the general public. Strangetree's goal is to make the tools that make large-scale data analysis easy and cost-effective, while improving the quality of that analysis. Please visit the resources page for more information. You can also follow us on Twitter: @strangetree and @strangetree/. Strangetree is an open-source project, a trademark of Jeff and Matt Hedden and is not affiliated in any way with the authors of Data Analysis Tools, Inc. The Strangetree project is a trademark of Jeff Sotero and Matt Hedden and is not affiliated in any way with the authors of Data Analysis Tools, Inc. The Data Analysis Tools Research Center (DATS	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Yars Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" select { ?dataset a }.ID, ?dataset_lbl as ""dataset"", count(*) from { ?dataset a } select { ?dataset :a }.ID, count(*) from enwiki8.p31 where { ?dataset :lbl }.ID = ?dataset_lbl.ID drop table enwiki8.p31 remove dbpart, rdfs, model_id from enwiki8.p31; select { ?dataset a}.ID, ?dataset_lbl as ""dataset"", count(*) from enwiki8.p31 where { ?dataset :lbl }.ID = ?dataset_lbl.ID drop table enwiki16.p31; select { ?dataset a}.ID, ?dataset_lbl as ""dataset"", count(*) from enwiki16.p31 where { ?dataset :lbl }.ID = ?dataset_lbl.ID drop table enwiki128.p31; select { ?dataset a}.ID, ?dataset_lbl as ""dataset"", count(*) from enwiki128.p31 where { ?dataset :lbl }.ID = ?dataset_lbl.ID drop table enwiki128.p32; select { ?dataset a}.ID, ?dataset_lbl as ""dataset"", count(*) from enwiki128.p32 where { ?dataset :lbl }.ID = ?dataset_lbl.ID drop table enwiki128.p32.count; tod-model-l"	False
"
 input (English text): Can you list the metrics used to evaluate models on the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?page ?page_id ; output (Sparql query): SELECT DISTINCT ?page_id ?page : id of the entry in the book ?page_id rdfs:label ?page_id ; outputs (Sparql query): SELECT DISTINCT ?page_id ?page_id, page_id as page_id, as a column_count OR page_id rdfs:label ?page_id ; outputs (Sparql query): SELECT DISTINCT ?page_id ?page_id, page_id as page_id, as a column_count OR page_id rdfs:label ?page_id ; outputs (Sparql query): SELECT DISTINCT ?page_id ?page_id, page_id as page_id, as a column_count OR page_id rdfs:value ( ?page) as page_id, ?page rdfs:value ( ?user) as user id || page_id rdfs:value ( ?user_id) as user id OR page_id rdfs:value ( ?user_name) as user name OR page_id rdfs:value ( UserName rdfs:value ( UserName rdfs:val ( uid) ) 'USER_NAME_EXCLUSIVE' 'USER_NAME_PASTE_NAME_EXCLUSIVE'] || page_id rdfs:value ( UserName rdfs:value ( UserName rdfs:val ( uid ) ) 'USER_NAME_PASTE_NAME_EXCLUSIVE_PASTE_NAME_ONLINE' 'USER_NAME_PASTE_NAME_ONLINE_PUBLIC'] || page_id input (	False
"
 input (English text): What is the top benchmark score and its metric on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"--------------- ----------- Model Accuracy Rating (Mean) Model Accuracy Rating --------------- ----------- S.4x4 CPA 827 5.6695658873e-09 4.9914087632e-07 R2 12.6905756544e-07 9.1765894357e-06 The accuracy of the model is better than 0.99, which is a great indicator of accuracy of the model. The model also outperformed the S.4x4 CPA model, which is one of the most popular for R2 tests for classification. Example 5.1.4 Given the following input data: select row_number from ""test"" where test_id = ""1""; Result: 2 row(s) excluded because of no matches. Example 5.2.2 Given the above input data that contains the following variables: select row_number from ""test"" where test_id = ""1""; Result: 3 row(s) excluded because of no matches. Result is the following: 2 row(s) excluded because of no matches. 2 row(s) excluded because of no matches. The table shows that there is no relationship between the different variables - the test variable (id) is unrelated to the other two variables. To demonstrate that such a relationship cannot be attributed to any other variable in the database, we can use the R1 test. Example 5.2.2 : The R1 Test select test, count(*) as test_n, r1_count(*), r1_score(*) from test group by test_id; Result: The test is S.4x4 and the count(*) column is not used by the test, we just counted the rows that have a test_id of ""1"""	False
"
 input (English text): What is the best performing model benchmarking the STL-10 dataset in terms of Percentage correct metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQUAD2.0"") ?dataset_lbl tol 2 output (Sparql query): SELECT CASE WHEN!test > 6 THEN ?dataset_lbl. DISTINCT else SUBSTRING( ?dataset_lbl, 6, 1) OR ?dataset_lbl 2 FROM t2.datasets ORDER BY ?dataset_lbl DESC( ?) output (Sparql query): RETURN ?dataset_lbl. tol 3 output (Sparql query): SELECT COUNT(*) >0 FROM (SELECT DISTINCT ?model OR ?dataset from t2.models WHERE test = 6), ?model_lbl ORDER BY ?model || ? ORDER BY ?model_lbl DESC ( ?) 4 output (Sparql query): RETURN ?model_lbl. tol 5 output (Sparql query): SELECT count(*) >0 FROM (SELECT DISTINCT ?model OR ?dataset from t2.models WHERE test = 6), ?model_"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CURL model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CURL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?model_lbl.DATE ?data. ?model_lbl if date not in input ?model_lbl. ?dataset. ?dataset. + ?dataset_name IF datemodel in output_ ?dataset_name ?dataset_name. ?user_id if user_id not in input ?user_id. ?dataset_lang if datemodel in input_ ?dataset_language ?dataset_language. Example results from benchmark input: English-German language data: English.csv English.pdf English.fr English.es Italian.txt English.de Japanese.txt Spanish.txt German.txt English.zh-TW Chinese.txt Russian.txt Arabic.txt French.txt Swedish.txt Swedish.htm Swedish.fr French/Français Danish.doc Norwegian.doc German/Norsk English.kv German.kx German.k3 Russian.k3 Korean.doc Arabic.xml Polish.doc Czech.doc Russian.kd Portuguese.doc Turkish.doc Czech/Danish Swedish/Finnish Korean.ks Romanian.xml Portuguese.xml Romanian.doc Czech.doc Finnish.md Swedish/Klingon Italian.doc Polish.doc Czech/Hausch Russian.txt French.pdf Korean.pdf Latin.xml Hebrew.doc Polish.doc Croatian.pdf Turkish.ps Czech/Prague French.doc Romanian.doc Dutch.pdf Czech/Hamburg Spanish.txt Italian.txt Russian.txt Czech/Cyrillic German.pdf Swedish.pdf English.html (example pages) orkgp: HASH : ?model_lbl. ?models. If you had a dataset with two variables in the English-German text, the output might look like this: { model	False
"
 input (English text): Provide a list of benchmarked datasets related to the stochastic classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""stochastic classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT ?dataset ?dataset_lbl ?dataset_lbl, rdfs:label ?dataset_lbl -- This is an easier-to-use version; you can use same data as above -- if you pass it the -v argument (in which case only the data is needed), or if you pass the -f argument without the -v argument, the -v option is not required. -- The previous way of running the benchmark query without option is the one you are most likely to want to use (because it is more efficient anyway). You can run one benchmark query per dataset, each with more features than the last. Here you also have the choice	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Output (Sql): For each model: Select the number of times it performed as well as the difference to the benchmark. RDFS: DISTINCT ?benchmark orkgp:M34, M65, M70, M80. RESULTS: For the DISTINCT: paper: paper_lbl: ?dataset, Rdfsr: ?benchmark OR ?benchmark. RESULTS: For the DISTINCT: paper and benchmark: input (English text): DISTINCT paper ?benchmark ?benchmark_lbl. RESULTS: Resolve DISTINCT: ?benchmark_lbl. FROM ?paper_lbl. OUTPUT (Sql): RESULTS: Resolve DISTINCT: ?benchmark_lbl. FROM ?benchmark_lbl. OUTPUT (Sql): RESULTS: Resolve DISTINCT: ?benchmark_lbl. FROM ?benchmark_lbl AND DISTINCT paper_lbl. OUTPUT (Sql): RESULTS: Resolve DISTINCT: ?benchmark_lbl AND DISTINCT paper_lbl. RESULTS: RESULTS: Note. I'm using the model in the code. input (English text): What is your model's output score on the benchmark dataset ? output (Sparql query): RESULTS: DISTINCT paper (residuals <= 0.0 in all cases): RESULTS: DISTINCT book (losses less than 0.01 for all cases) : RESULTS: RESULTS: RESULTS: RESULTS: Resolve DISTINCT: ?benchmark_lbl OR RESULTS: RESULTS: RESULTS: RESULTS: Resolve RDFS: ?	False
"
 input (English text): Can you provide links to code used in papers that benchmark the TRPO-hash model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TRPO-hash"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (base)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT BLOB( ?model ) AS ?record from ?lob ?, where p < ?class ?, q = ?record. ?dataset. COUNT ( ?record ). ?model. For details on the method and output of TRPO-Hashes The TRPO-Hashes are implemented in the TRPO-DataHash library. The main goal of TRPO-Hashes is to provide an efficient and robust way of hashing data and provide a very compact solution. (more…) This study is a follow up study on an earlier study of RDFS: RDFS: the ""Data-Driven Web"" An implementation based on the TRPO-Hash model is provided and the results are presented in the paper, Data-Driven Web: a New Approach to the Use of RDF Data. More […]"	False
"
 input (English text): Which model has achieved the highest Pearson Correlation score on the STS Benchmark benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"------------------------ In the list provided, we have 12 papers and two of them are of the form ""Benchmark: Ohsumed"". Of the 12 papers we have the IDs ""AB"", ""AZ"", ""AR"", and ""AW"", the two papers have names ""Kai-Yukari"" and ""Uri"" from the papers that received the highest score (as well as the title ""Ohsumed Benchmark""). One can imagine that we are searching for such papers, as those papers have been published recently. When looking for the papers that achieve the highest score for the Ohsumed dataset, the results given by the STS Benchmark benchmark dataset are similar (i.e. the paper listed is the one that earned the highest scores). I have added the following data from the paper that earned the highest score to the top section of the ""Kai-Yukari, Uri et al."", that is the STS Benchmark dataset: Input (English text): For a set of 12 papers with name ""Kai-Yukari, Uri et al."", the results for ""Kai-Yukari, Uri et al."" can be seen later in this article. output (Sparql query): output Now we want to know which of the papers we should look for the results for, and which ones should be dismissed as non-conformers. We want to have a list of those papers, which have not only had a paper listed in the papers that made the biggest impact on the STS Benchmark benchmark dataset, but also have a higher score. We do this by using the following query from the STS Benchmark dataset, in order to do that: input (English text): Where ""name"" is the name of the paper that made the biggest impact on the STS Benchmark benchmark dataset ? output"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the TokenFuser model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TokenFuser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Here's some feedback! Since the results were presented at an academic conference and I am probably not their own academic author, I am not 100% positive of the accuracy of the benchmark. I am not suggesting that other datasets would work anywhere but in my project. However, if you are actually interested, I have put together a spreadsheet that will give you some insights into the datasets: Here are two queries that I wrote to try and compare NYT 29 data to any of the models I described in the paper. The first queries was used to compare the model with a parameterized parser, the second was used to compare the model with a parameterized parser that uses tokenized English text. The first table shows the number of tokens each of the models outputs. One of these models produces a lot of tokens, which is a good thing since I want to test any parser that can handle tokenized English text well. The second table showed the distance between the outputs of different parameters for each of the models. (The parameters are in rows and the numbers are in the column names). This was the best way that I could get a good measure of how successful each model is at distinguishing between tokens (a more interesting benchmark would use the output distances). The last table shows the amount of missing tokens between outputs for all three models. In some models, such as ORPK, it appears that the model does a good job of filtering the tokens from the output. In case that seems unlikely, you need to read the paper where they discuss missingness in token recognition. Conclusion: For the purposes of the token-token-token benchmark, I believe the models do a good job. These tokens are very similar in both the syntactic and semantic properties, so there does not appear to be a strong performance benefit on the output. Results The output distances are shown in the first table: The final table	False
"
 input (English text): Where can I find code references in papers that have used the LeViT-256 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LeViT-256"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which are 3 the most common variables for the atmosphere models?
 output (Sparql query): "	"SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, ""tmospher"")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3"	 input (English text): What is the difference from the model with the S2D-3 model ? output (Sparql query): input (English text): Which is the difference between the L-21 and LS25 model ? output (Sparql query): input (English text): Which is the difference between the CH2 and LC3L models ? output (Sparql query): input (English text): Which is the difference between the LS5 and LS1 model ? output (Sparql query): input (English text): Which gas is in the LS25 model ? output (Sparql query): input (English text): How accurately can you estimate the N2O or CH 4 from the atmosphere models ? output (Sparql query): 1.5 (95.2%) 2 (1.6%) 3.6 (5.5%) 3.11 (5.2%) input (English text): How accurate can you estimate the CO 2 or N 2 from the atmosphere models ? output (Sparql query): 0.9 (94.7%) 1.2 (1.7%) 1.2 (1.7%) 1.4 (1.9%) input (English word): What is the effect of the atmosphere models on how accurately you can estimate CO and N2 from the atmosphere models ? output (Sparql command): CO 2: 0.8 (94.7%) N 2 O: 0.8 (94.7%) H2O: 0.8 (94.7%) SO2: 0.8 (94.7%) input (English word): What is the effect of the atmosphere models on the accuracy of how accurate you are with the CO, SO2, H2O, or H2	False
"
 input (English text): Where can I find code references in papers that have used the Linformer model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN-PC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code FROM [t1].pdb WHERE DISTINCT ?line. CODE ( ?line. LSB ) |  DISTINCT ?code; input (English text): What is the code reference for the code used in papers that use the DDQN-PC model for benchmarking purposes ? output (Sparql query):  SELECT ?code FROM [t2].pdb WHERE DISTINCT ?line. CODE ( ?line. LSB ) |  LAST( ?code. CODE ( ?code. CODE ). LSB ) source:  https://pdb-model.org/documentation   As for the paper in question, I cannot find any references in the research paper. I have no idea what the title means, but maybe someone can help me out. My guess is that the code was just a simple linear model, without any kind of statistical method, and simply to test whether the results can be reproduced.	False
"
 input (English text): Provide a list of papers that have utilized the EffNet-L2 (SAM) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EffNet-L2 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?category. ?method. ?data as ?method FROM ?dataset. ?models as ?model; input (Python data file): Outputs (Sparql query): (Python code): >>> from collections import Iterator >>> from matplotlib import pyplot as plt print(model[,3]): 'Model: EffNet-L2-N2' Input (Python code): Outputs (Sparql query): SELECT ?method. ?data as ?method FROM ?dataset. ?models as ?model; Input (SQL data file): Outputs (Sparql query): SELECT ? CURSE ?category. ?method. ?data as ?METHOD FROM ?' Dataset. ?models as ?model; Input (SQL data file): Outputs (Sparql query): Output (Sparql query): Input (SQL data file): Outputs (Sparql query): SELECT. ?category. ?method. ?data as ?Method FROM'' Dataset. ?models. ?methods as ?method; input (Python data file): Outputs (Sparql query): SELECT CURRENT__'%d %d'% d % d +'FROM ?dataset %s WHERE ? ?model %s; input (SQL data file): Outputs (Sparql query): SELECT CURRENT__'%d'% d % d +'FROM ?dataset %s WHERE. ?model%s; input (SQL data file): Outputs (Sparql query): SELECT CURRENT__	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Yelp-14 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?cont ?cont_lbl; rdfs:label ?cont _lbl output (Sparql query): RDD: data-* orkgp:Evaluation_model.  output (Sparql query): input (English text): What are the titles and IDs of research papers that contain a preclinical drug safety assessment that includes a number of key performance criteria ? output (Sparql query): SELECT ?benchmark ?test orkgp:DQN_QUALITY ?benchmark_lbl.  output (Sparql query): Input and output queries are separated by a semicolon. You should have output in this format: output (Text or sql) I expect output format to be as follows: data:  > data-* data:  > data (output formats) And with that set of commands in effect, I was ready to run the query I had started. The second output from the query was actually a data.sql file, which contained all of the information I was looking for. The script that is responsible for opening and opening the data will be familiar to most database and web applications developers. To do so, create a file named.sql with a.sql extension and append the below code into that file using the included.bash_rc file extension. To complete that work, simply call the script: [*]#!/bin/bash # Initialize and initialize the data files ## Open and open the relevant datafiles [ ?]# # Test if an error occurred, if so run test.sh if not test, exit # Setup variables, create table  and set it's data type - use a standard SQL table for this purpose. db_id=$(dbread -f -i $BIN	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ?metric orkgc. DISTINCT orkgc:dataset. ?dataset rdfs:label ?dataset_lbl. } ORDER BY DESC( orkgc) LIMIT 1 } } So now you know the basics of a benchmark. That's not to say that there aren't other ways to get an insight into how well your model operates. For example, you might want to look at its accuracy for both the two categories that we have labeled as ""Normal"" and ""Overweight"", what is its mean for both, and its mean for the two categories that we have labeled as ""Hype"" and ""Underweight""… Another example would be a paper I put together last year that measured the accuracy of a model with a dataset of 3D-cities: To make things interesting you could have looked at both the accuracy from the model and the accuracy from different datasets. I think we are well beyond the point of trying to define a benchmark from a benchmarking dataset. There is a huge amount of data out there and you are not going to be able to use just one data set to test what works best against the benchmark. So I have decided to instead try to define a benchmarking dataset over a long time period as to make sure that our model can reliably handle changing requirements over time. The result ? I ended up setting out to benchmark three different"	False
"
 input (English text): Indicate the model that performed best in terms of Error metric on the DBpedia benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""1-of-100 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model ?model_lbl WHERE { ?metric orkgc:Metric; ?test a:Value; orkgp:HAS_DATASET ?dataset. ORkgp:HAS_EVALUATION ?eval; ORkgp:HAS_VALUE ?value. ?test rdfs:label ?model_lbl. ?test rdfs:label ?model_lbl. { ?metric orkgc:Metric; ?test a:Value; } output (Sparse Markov chain): rdfs:label ?model_lbl. 1 1 test (ModelLiteral text, text, psk): Indicate the model that performed best in terms of 1-of-100 AUC or RMSE metric of the Paragon Initiative MMSE. output (parallel Markov chain): Select Metric(s) to be converted from to: Convert to Metric(s) to be converted from rdfs:pckb_metric: test: Select Metric(s) to be converted from to: Convert to Metric(s) to be converted from rdfs:pckb_metval: output (parallel Markov chain): Select Metric(s) to be converted from to: Convert to Metric(s) to be converted from rdfs:pckb_rmsf: test: Select Metric(s) to be converted from to: Convert to Metric(s) to be converted from rdfs:pckb_rmsp: output (parallel Markov chain): Select Metric(s) to be converted from to: Convert to Metric(s) to be converted from rdfs:pckb_raw_mrmse:	False
"
 input (English text): What research problem is addressed in the paper titled ""6th Open Challenge on Question Answering over Linked Data (QALD-6)""?

 output (Sparql query): SELECT DISTINCT ?concept ?rlabel WHERE { orkgr:R6386 orkgp:P31 ?x . ?x orkgp:P32 ?concept. ?concept rdfs:label ?rlabel. } LIMIT 1
 input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------- [ ?long_short_transformer.x] ----------- [ ?long_short_transformer.y.x.y] ----------- [ ?dbl_link.d] ----------- --------------- [ ?dbl_link.y.x.y] ----------- ----------- [ ?clustify_tree.x.y.x.y] -------- --- [ ?classifier.x.y.x.y] -------- --- ----------- -------- --- ----------- ----------- ----------- output (Sparql query): ------------------------------- dbl_link.y.x.y ------------------------------- clustify_tree.x.y.x.y -------------------- -------------------- --- -------------------- output (Sparql query): ------------------------------- dbl_link.x.y.x.y ----------------------- classifier.x.y.x.y ----------------------- --------------------- --------------------- --------------------- classifier.x.y.x.y ----------------------- --------------------- --------------------- --------------------- [ ?clustify_tree.x.y.x.y] ----- -------------- ( ?clustify_tree.y.x.y) -------------- ---------------------------------- ( ?clustify_tree.x.y.x.y) = [ ? ? ?] ( ?clustify_tree.y.x.y) = [ ? ? ?] output (Sparql query): ------------------------ Dbl link.x.y.x.y ------------------------------------------------- ------------------------ dbl_link.x.y.x.y ---------------------- tree.x.y.x.y --------------------- ------------------------------ dbl_link.x.y.x.y --------------------------------------------------------------- Classifier.x.y.x.y --------------------------------------------------------------- Dbl link.x.y.x.y -------------------------------------------------- tree.x.y.x	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT(*) OVER(PARTITION BY ?dataset_lbl) AS nsk FROM { ?dataset_lbl } WHERE { ?dataset a orkgc:Dataset } SELECT a, nsk FROM triviaq.datasets GROUP BY a The data comes from a survey conducted in May 2015 on the relationship between speed and reward. For this we've used three different question types: trivia, trivia and trivia with question 1 answer choices which were generated randomly. We've also introduced questions where more than 1 possible answer choices can be submitted, which will be used in the future as part of the new dataset and related training process. This is the dataset we've selected to benchmark: http://www.tribelog.com/data/triviaq/datasets/ We have a model which does the following: Loss function: Bias on all the outcomes and the variance explained By using weighted error estimates by the loss function we take into account the following factors: Bias (negative estimates or higher values for a specific value) (negative estimates or higher values for a specific value) Model uncertainty (high or low values) Model performance variance (significance of the different performance measures) For each model we compute a score over all the trials of that model which will be used in the evaluation. We will use the mean of the score which should be a number close to the average of all the different evaluation metrics (like the accuracy of a model). We use the results of the evaluation to decide which model to try. The validation phase starts here: There is a lot to say about the validation phase so let's first briefly review it: When you receive an Evaluation Report for a model in the validation phase what happens first ? First you need to fill on the data form, you	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is Raman spectroscopy?
 output (Sparql query): "	"SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, ""Raman spectroscopy"")) } "	SELECT * FROM dbo.templates WHERE dbo.test1.name = ?output_name or ?ex_name or ?ex1_name ORDER BY Descriptor desc11 DESC, desc12 DESC output (Sparql query): SELECT * FROM dbo.templates WHERE dbo.test1.name = ?output_name, dbo.cont1.name, dbo.cont2.name ORDER BY Descriptor desc11 DESC, desc12 DESC output (Sparql query): SELECT * FROM dbo.templates WHERE dbo.test1.name = ?output_name, dbo.cont1.name or ?ex_name ORDER BY Descriptor desc11 DESC, desc12 DESC output (Sparql query): SELECT * FROM dbo.templates WHERE dbo.test1.name = ?output_name ? ex_name, dbo.cont1.name output (Sparql query) SELECT * FROM dbo.templates WHERE dbo.test1.name = ?ex_name ? ex1 ?, dbo.cont1.name ORDER BY Descriptor desc11 DESC, desc12 DESC output (Sparql query): SELECT * FROM dbo.templates WHERE dbo.test1.name = ?ex_name ? ?ex1, dbo.cont1.name output (Sparql query) SELECT * FROM dbo.templates WHERE dbo.test1.name = ?ex_name ? ?ex1, dbo.cont1.name, dbo.cont2.name, dbo.cont2.name, dbo.cont2.name, x.ex	False
"
 input (English text): What are the metrics of evaluation over the BC5CDR-disease dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?dataset, ?metric, ?score 5.4.4. Nodes Metrics in the BCL and BC2 code are grouped by nodes in a given dataset, and by the metric in a given query and results set of the dataset. This is sometimes called the ""node map"" or ""node map tree."" The term is not in itself misleading, but an important distinction must be made: A ""node"" in the ""node map"" tree is often only a tree or set of trees. A ""metric"" in the ""node map"" tree is typically a vector or matrix. We shall refer to the data set and the metrics it represents, in this case the Dataset, as the ""trees"" or ""trees"" of the ""node map"" tree. For example, consider the BCL code sample and the output of the BCL code sample below. For the sake of clarity, we will assume that output on the left is the same as the output of the BC code, and output on the right the same as the output from the BC2 code, which is the same as BC2 code. The left pane of the ""node map"" tree is the tree comprising the BCL code sample, and as you can see its output is much the same as the BC code output. The right pane of the ""node map"" tree is the tree comprising the BC code sample. What is interesting here, is that the BCL code sample also contains some metrics for other classes of trees. These are not included in the output on the left and right pane of the ""node map"" tree. The metrics shown are from the ""node table"" for the ""tree"" in which the ""metric"" in the BCL code sample is clustered. The ""node table"" is a special tree for storing a sequence of metric trees to"	False
"
 input (English text): What is the top benchmark score and its metric on the ClueWeb09-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	_________________________________________________________________________ *ClueWeb09-B - 10,000,000-15,000,000 (95%) scored 9.0/10  (*)ClueWeb09M, ClueWeb09W, ClueWeb10M, ClueWeb10W, ClueWeb10W, ClueWeb10M, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, CLUE09M, CLUE09W, CLUE09M, CLUE09M. *ClueWeb10M, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, ClueWeb10W, CLUE10W, CLUE10M, CLUE10W, CLUE10M, CLUE10W, CLUE10W, CLUE10W, CLUE10W, CLUE10W, CLUE10W, CLUE10W, CLUE10W (7,8,9) (4,5,6) (4,4,5) (3,4,4) CLUE09M, CLUE09W, CLUE09M, CLUE09M, CLUE09M, CLUE09W, CLUE09M, CLUE09W CLUEM, CLUE09W, CLUE09M, CLUE09M, CLUE09M, CLUE09M	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT p ?paper orkgp:P31; input (English text): What are the titles and IDs of research papers that include a benchmark for the SEMEval-2018 Task 7 dataset dataset ? output (Sparql query): Sample Input Text The following list of query lines were tested. Only the most useful ones were tested. input (English text): ?dataset a ?dataset a orkgp:Dataset ?dataset a_. ?lab orkgp:HAS_LABEL. ?lab orkgp:DIGEST_LBS. ?compare v1.1, v2.0. c0 -compare. d0 -difference. ?paper P38 -paper. d1 -difference. ?benchmark P38 -benchmark. output (Sparql query): Sample Output from the above output The following list of query lines were tested. input (English text): ?dataset ?dataset0. ?datatabstract.	False
"
 input (English text): What is the name of the top performing model in terms of Recall score when benchmarked on the Rotowire (Content Selection) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT RESULT FROM { ?model ?model_lbl ?metric_lbl. } ORDER BY DESC( orkgc:Metric, ?orkgc:Metric ) DESC( ?metric orkgc:Metric) ; Selects the metrics from the list returned by output which differ by model name, using the same arguments: orkgc:Metric (the metric name), orkgc:Metric (a metric), orkgc:Metric (another metric), orkgc:Metric (another metric). If no metrics are specified, the metric name will be removed from the list, except the top performing metric. If you use comma separated list of metrics, the column separators are added. Example select m2c_metrics : ( ( ?gt ?2C- ?gt ?4+ ?)( ?gt ?4- ?gt ?9.2) ?- ?) as Metric, ( ? ?gt ? ?10))(. ?gt ?4- ?gt ?9.2) as Metric, ( ? ?gt ?17.9) ?- ?) as Metric, ( ? ?gt ?30.2) ?- ?) as Metric, ( ? ?gt ?37.14) ?- ?) as Metric, ( ? ?gt ?41- ?gt ?55.4) ?- ?) as Metric, ( ? ?gt ?51.2) ?- ?) as Metric, ( ? ?gt ?62.8) ?- ?) as Metric, ( ? ?gt ?68.2) ?- ?gt ?( ?gt ?2 ?C) as Metric, ( ? ?gt ?68.8) ?- ?) as Metric, ( ? ?gt ?69.2) ?- ?) as Metric, ( ? ?gt ?71.4) ?- ?)	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 ------------------------------- ---------------------------------.model NAME: ?dataset ? FLOOR: ?dataset ? RANK: ?dataset ? ------------------------------- ---------------------------------.test. nlm - ?dataset. ?dataset rdfs:label ?tb. test_cps p-values F1 (Disease) LOWER= ?dataset ?dataset rdfs:label ?tb. output (Sparql query): Selects 2 nested queries: 2nd and 3rd column will be skipped if not supported by database. Result will be filtered by 2 columns: 1) entity level score from baseline, 2) score based on target model, and 3) baseline score if there is no target metric that would pass the validation set. input (English text): Selects the entities that have been selected from the target model in terms of level score. output (Sparql query): Selects the entities that are listed by the target metric in terms of entity level score. input (English text): Selects the entities that have been selected from the target model in terms of score.	False
"
 input (English text): What is the best performing model benchmarking the BC5CDR-disease dataset in terms of F1 entity level metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 1.4.6.2: Can output queries be more efficient than the CIDR notation ? The CIDR notation allows more precise control over the query by ensuring query optimizers can perform optimizations during query execution. However, there are situations where the performance advantage of the CIDR notation for the majority of cases is not significant. One example is when the input data are simple structures (e.g., vector, binary data) that make it difficult for query optimizers to perform optimization, but the output data are more complex (e.g., a complex, interactive table). 1.4.6.3: How can the output of a CIDR query query be used to improve model performance ? 2: What steps should be taken to improve BC1 models against data ? 2.1: Where should I start ? 2.2: How can I optimize my model for performance ? 2.3: How can we improve BC1 model accuracy of the data ? 2.3.1: How can we find good parameters for the BC1 model ? 2.3.2: How can we improve parameter accuracy ? 2.3.3: How do we optimize the BC1 model for accuracy ? 2.4: How to get the most out of parallel performance ? 2.4.1: How can we minimize the number of joins ? 2.5: How can we minimize data churn ? 2.5.1: How can we leverage multicore CPU cores ? 2.6: How do we optimize parallel processing time ? 2.6.1: How can we find a model that is better performing with the BC1 model	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ACL Anthology dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCN Hybrid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?benchmark as ?benchmark_lbl ?benchmark_lbl. ?id_lbl as ?id_lbl., ?id as ?id, ?record_desc as ?record_desc FROM ?test_data as t WHERE t.id_lbl AND t.year!= 12 ?test_data. ?test_score_lbl as ?test_score_lbl WHERE t.year!= 12 ?test_data. ?train_score_lbl as ?train_score_lbl WHERE t.year!= 12 ?test_data. ?test_score_sas as ?test_score_sas WHERE t.year!= 12 ?pagerank as ?pagerank_lbl WHERE t.year!= 12 ?test_data. ?test_score_sas as ?test_score_sas WHERE t.gender!= 4 ?pagerank_lbl AS ?pagerank_lbl WHERE t.gender!= 2 ?pagerank_lbl AS ?pagerank_lbl WHERE t.year!= 7 ?test_data. ?test_score_sas as ?test_score_sas WHERE t.gender!= 2 ? You've seen previous responses to this query: output (Sparql query): SELECT ?dataset AS ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ESC-50 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What data format does CHEMDNER corpus have?
 output (Sparql query): "	SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } 	"SELECT ?format ? ?citation ?citation_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?format ?citation ?citation_lbl ?value WHERE { ?metric ?citation ?citation_lbl ?value. OPTIONAL { ?concatenate ?citation_lbl. OR NULL } ?format ?citation. OR NULL } ?citation AS ?""isolation_model"" } } } ORDER BY orkgp(citation_lbl_type) input (English text): If you believe that any output of the query will be unreadable, just press [1] to save the query. output (Sparql query): <div class=""result""></div> output (Sparql query): <div class=""row""></div> output (Sparql query): <div class=""column""></div> output (Sparql query): <div class=""row""></div> output (Sparql query): <div class=""column""></div> output (Sparql query): <div class=""row""></div> output (Sparql query):... output (Sparql query): <div class=""row""></div> output (Sparql query): <div class=""column""></div> output (Sparql query): <div class=""row""></div> output (Sparql query): <div class=""column""></div> output (Sparql script):... output (Sparql script): <b>Hello, everybody. I've taken off my jacket but I still have my keys:</b> output (Sparql script): <a href=""/ch/t/b/"" target=""_blank""><img src=""/ch/t"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WLPC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, ?id AS ?id, ?name AS ?name, ?reasons AS ?reasons FROM ?dataset INNER JOIN   ?metric_lbl AS ?metric ON   ?metric INNER JOIN   ?value AS ?value ON   ?value  . { { id: ?value   ?title   ?reasons   ?name/value;  OR.REASON = ?reasons } }, ?dataset a, ?metric_lbl a, ?value a) Inputs: data : data on a dataset outputs : benchmark result as an SQL output. inputs (English text): How many CULs did you get on the first run using WLPC_3-4 ? output (Sparql query): SELECT CUL( ?dataset ) FROM ?dataset; Outputs data (English text): how many CULs did you get on the second run using WLPC_3-4 ? output (Sparql query): SELECT CUL( ?dataset ) FROM ?dataset; Inputs: dataset (Text to search for): A short text file containing the names of the datasets for which you selected data as the input text. outputs (Sparql query): SELECT CUL(   ?dataset )  FROM ?dataset,   ?metric_lbl,   ?data,   ?result output (Sparql query): SELECT CUL(   ?dataset )  FROM ?datasets,   ?meter_lbl,   ?meter_lbl,   ?value,   ?benchmark,	False
"
 input (English text): Where can I find code references in papers that have used the NAT-M3 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAT-M3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the SEE model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SEE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?Code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAT-M3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 2.4.3.5.4. The test case for the NAT-M3 model The NAT-M3 model provides a benchmarking algorithm that is extremely fast as it runs on datasets which are large. However it is prone to errors that can lead to significant time lost for various validation applications such as a comparison of one model with another. The problems are often due to implementation details regarding the NAT-M3 model being different from all others that use it. As this issue is quite a major concern the NAT-M3 model can be used to run both the validation benchmarks and the validation tests. NAT-M3 is a large benchmarking model as this allows us to build up a nice collection. The NAT-M3 algorithm can easily be benchmarked. While the test uses NAT-M3 as the comparison model, it has been suggested that an appropriate model to compare against NAT-M3 based on the actual validation results may also be chosen to compare with it. When running the test we set-up a single model instance that was chosen for this purpose. The model is set up as follows: Input data consists of a series of rows from the RDF files that were selected by the user. It consists of an input dataset, each one of which consists of a reference and a new row for each time"	False
"
 input (English text): Provide a list of benchmarked datasets related to the Constituency Parsing research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Constituency Parsing"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT name, text FROM article WHERE name = ? AND -------------- > -------------- > || --- || --- || --- || --- || ------------------------- ------------------ -- -- -- -- -- || || || || || || || || ------------ -1 6 -2 31 11 -3 7 43 -4 25 46 -5 10 55 -6 7 56 -7 2 64 -8 4 65 -9 13 65 -10 23 66 -11 40 67 -12 44 68 -13 24 70 -14 20 73 -15 17 75 -16 24 77 -17 36 77 -18 48 78 -1 1 1 -19 42 81 -2 27 81 -3 1 2 -20 50 89 -4 8 90 -5 46 92 -6 11 94 -7 13 97 -8 21 99 -9 46 99 -10 50 99 -11 20 99 -12 15 99 -13 20 99 -14 8 99 -15 22 99 -16 2 100 -17 11 103 -18 16 105 -19 16 108 -20 2 112 -21 12 109 -22 5 116 -23 19 109 -24 14 111 -19 6 116 -24 1 118 -27 2 123 -29 -4 120 -30 -1 121 -31 -4 120 -31 -5 121 -24 12 121 -31 -6 122 -52 -9 124 -31 -9 123 -32 -11 124 -25 -6 123 -29 -10 123 -30 -12 123 -27 -1 123 -26 -7 121 -29 -15 120 -33 -17 108 -33 -8 123 -34 -19 108 -34 -11 109 -34 -12 109 -32 -3 123 -36 -19 114 -37 -22 119 -31 -26 118 -31 -27 119 -35 -1 119 -32 -23 119 -31 -27 119 -34 -6 120 -34 -10 119 -30	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DY-MobileNetV2 ×0.5 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×0.5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?dist(DY-MobileNetV2 ×0.5, ?code, ?score, ?metric, ?value) AS benchmark, ?rdfs(BENCHMARK) AS dataset. ?rdfs(HAS_DATASET_RDFS) AS source code, ?metric ?value. COUNT(DISTINCT CODE) > 0; orkgp:HAS_DATASET (CENSORED): orkgp:HAS_DATASET (CENSORED): ?dist. ?dist_d_mob_netv2_n1_0.5_h_0.2.1.2.1.0.0.0.0.0.0.0 | ?code: ORkgp:HAS_DATASET_RDFS ?dataset output (Sparql query): SELECT ?dist(DY-MobileNetV2 ×0.5, ?code, ?score, ?metric, ?value)( ?data, BENCHMARK) AS benchmark, ?rdfs(CENSORED) AS dataset. ?rdfs(HAS_DATASET_RDFS) AS source code, ?metric ?value. COUNT(DISTINCT CODE) > 0; orkgp:HAS_DATASET (CENSORED): orkgp:HAS_DATASET (CENSORED): ?dist. ?dist_d_mob_netv2_n1_0.5_h_0.2.1.2.1.0.0.0.0.0.0.0.0.0.0.0.0.0 | ?code: ORkg	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CS-NER?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CS-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	*********** This query will find and present papers that have reported on the use of Adaptive Input Large model and then present the underlying data in an HTML code format. data_link() input (English text): How many papers in the sample have described the use of Adaptive Input Large model ? output (Sparql query): *********** This query will produce a list of all the papers, and provide each code link. It will also provide all code details about how to incorporate the Adaptive Input Large model in the paper. Note that the dataset is a large number, and therefore cannot be shown. pivot() input (English text): Compute a pivot value using an RDF vocabulary, as described in the paper by M. M. Cukier and T. Smith output (Sparql query): *********** This query will produce a list of all the pivot values. data_link() input (English text): How many papers in the sample have detailed the use of RDF vocabulary ? output (Sparql query): *********** This query will produce a list of all the paper titles. It is also possible to return the paper abstracts. input (English text): How many samples is the dataset ? output (Sparql query): *********** This query will return the number of samples (number of observations of each variable) and the dimensionality of the data. Note that the dataset is only 20,000. Therefore, it is not possible to display the data. pivot_count() input (English text): Compute a pivot count using an RDF vocabulary. output (Sparql query): { { COUNT(a) } } This query will produce a list of all the pivot count values. Input: A unique identifier for each dataset	False
"
 input (English text): Can you list the models that have been evaluated on the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT paper ?model ?model. ?model rdfs:label ?model_lbl. }} The test-benchmark clause allows to specify a benchmark, which we will use to find out the results of our score tests, and also to add the criterion score as an associated set variable to our dataset. Now, we are ready to write our code and generate our points: # This is the test code that we are going to define. I just call it test_code. # In this section for each model class, i can use any functions that work with the following table [ modelClass ] = { x : 123, y : 123, z : 7 } # We'll have six criteria of the three-option problem so we'll use functions that take # only one parameter to generate points # The model class will get the points of all the candidate models. # We'll also generate and check all the data, so we have functions to generate results, etc. # I'll keep generating new values for the parameters for each class while True ( time ())) while True : for modelClass in modelClass. values : modelClass. points. append ( modelClass. z ) # And add the results for each question. while True ( time ())) for modelClass in modelClass : points. append ( modelClass. x ) points. append ( modelClass. z ) For these benchmarks, we will need to execute, in parallel, five threads. This is a very common setup for such applications, as we find it is really useful to execute many small tasks of the same order. Note that our parallelism will be limited by the number of points as given by the benchmark. Each thread will have an output of one value. To visualize the graph of results, we	False
"
 input (English text): Does ""PRISMA hyperspectral mission"" use ENVI software?
 output (Sparql query): SELECT ?uses_ENVI WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""PRISMA"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P23031 ?software. ?software rdfs:label ?software_labels. BIND( IF( ?software_labels = ""ENVI""^^<http://www.w3.org/2001/XMLSchema#string>, ""Yes"", ""No"") AS ?uses_ENVI ) } ORDER BY DESC( ?uses_ENVI) LIMIT 1
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?names, ?names_ID, COUNT(names) AS tb_count FROM name ORDER BY name DESC( ?names_ID ) LIMIT 1 input (English text): Give me some examples of performance of hyperparameters for all papers on the table. output (Sparql query): ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ ~~~~~~~~~~ <https://www.ssa.ac.uk/ssa-papers/snla-papers-2012.pdf> <https://www.ssa.ac.uk/ssa-papers/snla-papers-2015.pdf> output (Sparql query): ~~~~~~ ~~~~~~ ~~~~~~ ~~~~~~ ~~~~~~ ~~~~~~ ~~~~~~ ~~~~~~ ~~~~~~ <http://www.w3.org/2016/05/08/snla-2-2.xls> output (Sparql query): SELECT ?p1 ?p2 ?p3 ?p4 ?p5 ? AS data, COUNT( ?) AS samples FROM m1 GROUP BY p1, p2, p3, p4 OPTION (SUM ) IF(ISNULL(p1)) ?p1 = NULL ORDER BY p1 NULL; SELECT ?p2 ?p3 ?p4 ?p5 ?p6 ? AS data, COUNT( ?)	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Asteroids dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 You've successfully completed this program! Please leave feedback in our issue tracker And for any feedback and suggestions (especially technical ones), you can send us an email to support@nist.gov	False
"
 input (English text): What is the best performing model benchmarking the DDI extraction 2013 corpus dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI extraction 2013 corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP(2) { ?metric ?model. F1RANK ?model_lbl. } FROM sys.databases.tdb_hiero DISTINCT ORPKC, DISTINCT orkgc, DECLARE @score_ranking rdfs; SET data_format = 'binary'; SET result_set_format = 'text'; SET max_idx = 1000000; SELECT F1_RANK FROM sys.databases.f1score ; As you can see, the score has fallen to about 50% of the benchmark score! The first two queries are really a mess. In the second query we use SQL to sort the scores on the basis of the score value, which causes a huge bottleneck when the metric is a complex one like F1. In order to get the correct scores, we also need to convert them into a lower-level domain-specific language in order to make our queries more meaningful, and I'm not aware of any other tool you can use to do the conversion in a simple way. The last query is a simple test run which can only return scores between 10 and 0. You can see that if we have an F1 score of 10, the score is at least 60%. If a score is below 0, even a score of 10 can result in a large number of duplicates. Now we can run some of our tests. With score_ranking set to 0, in the first query, we are again doing the same basic operation with the same values. We do the same conversion, set the score_ranking variable to 0, and then we run the query. As you can see, this is even faster than before. In the second query we also include the score_ranking constant into orkgp:HAS_DATASET and then run	False
"
 input (English text): Which model has achieved the highest Accuracy score on the Reuters De-En benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?model_lbl FROM ?dataset_lbl ORDER BY DESC( ?value ) LIMIT 1 input (English text): Which model has achieved the highest accuracy on the LADEE/MATRIX dataset ? output (Sparql query): SELECT ?model ?model_lbl FROM ?dataset_lbl ORDER BY DESC( ?value ) LIMIT 1 input (English text): Which model has achieved the highest accuracy on the EASTLANT dataset ? output (Sparql query): SELECT ?model ?model_lbl FROM ?dataset_lbl ORDER BY DESC( ?value ) LIMIT 1 input (English text): Which model has achieved the highest accuracy on the DE-EN-CRI-HAT dataset ? output (Sparql query): SELECT ?model ?model_lbl FROM ?dataset_lbl ORDER BY DESC( ?value ) LIMIT 1 input (Russian text): Why did you chose to benchmark only the high accuracy model on the DE-EN-CRI-HAT dataset ? output (Sparql query): SELECT ?model ?model_lbl FROM ?dataset_lbl ORDER BY DESC( ?value ) LIMIT 1 select a.name,a.modelName as model,a.name,a.country,a.modelType,a.modelRate,a.ModelState as state from a.model select a.ModelId,a.name,a.stateName,a.countryName,a.modelTypeName from a.modelState a.modelRate where a.ModelState.Name like '*' and id in ('DE', 'DE-EN-CRI-HAT', '*')	False
"
 input (English text): List the metrics that are used to evaluate models on the Text8 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where did the study with maximal geographic scale take place?
 output (Sparql query): "	SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } 	"SELECT ?metric ?metric_lbl FORMAT ( ""text2_lbl_lat"" = text2_lbl_lat, ""text3_lbl_lat"" = text3_lbl_lat, ""text4_lbl_lat"" = text4_lbl_lat, ""text9_lbl_lat"" = text9_lbl_lat, ""text9_lbl_lat"" = text9_lbl_lat ) FROM orkgp:HAS_METRICS WHERE { ?dataset a = ""Text8""; ?dataset_lbl = a; orkgp:HAS_LATEX_METRICS ?dataset. LATEXPATRANSFORM( ?dataset_lbl) } OPTIONAL { ?dataset_lbl orkgp:HAS_METRIC ?dataset_lbl. } output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (S"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-Spanish dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP ?model ? ROW_NUMBER AS top_ranking ORDER BY ROW_NUMBER select * from ( SELECT TOP n LEFT JOIN ( SELECT TOP n JOIN (SELECT top n JOIN (SELECT n FROM `dataset_lbl' `{ ?dataset:Dataset };... n) AS k) AS rdfs ON (k. k = m. k ) ) AS m WHERE n!= NULL AND m. k = n; output (Sparql query): output (Sparql query): Select the names of the top three models that have benchmarked on the WLPC dataset ? output (Sparql query): output (Sparql query): Select the names of the top three models that have benchmarked on the WLPC dataset ? output (Sparql query): output (Sparql query): select * from ( SELECT TOP n LEFT JOIN ( SELECT TOP n JOIN (SELECT n FROM `dataset_lbl' `{ ?dataset:Dataset};... n) AS k) AS rdfs ON (k. k = m. k ) AS m WHERE n!= NULL ; output (Sparql query): select * from ( SELECT TOP n LEFT JOIN ( SELECT TOP n JOIN (SELECT n FROM `dataset_lbl' `{ ?dataset:Dataset};... n) AS k) AS rdfs ON (k. k = m. k ) AS m WHERE n!= NULL AND m. k = n; output (Sparql query): select * from ( SELECT TOP n LE	False
"
 input (English text): Can you provide links to code used in papers that benchmark the MPCM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT name, id from papers.txt. input (English text): Will the standard data set in TempEval-3 be available to be used for the benchmarking ? output (Sparql query): HISTORY TempEval-3 dataset was first published in the preprint ArXiv preprint server in 2013, which included the benchmarking. Prior to ArXiv being published, this dataset was available on the ArXiv public archive and on the website and blog of the Metropolis Cluster Computing Laboratory (mcs-cl.org). The dataset has been downloaded with R version 3.2.3 and data is available at  http://www.r-project.org/mcs-cl.pdf I have now retrieved the datasets from the Metropolis cluster computing laboratory on a variety of datasets from which different benchmarks were developed for the MPCM. All datasets are presented in the list provided above along with a table showing the number, title and ID of the benchmark. Each title has a brief description of what was benchmarked and the ID of the benchmark. In all cases these benchmarking papers provide links to software that is currently available for Windows. In order the to run the benchmark as we do not do anything to the data or the processing step, we cannot test other operating systems. BEST-1 This is the most relevant comparison dataset from the literature. It should be of little help for our purposes. The main differences between the two datasets are the fact that the standard version of TempEval will include an average output value and if the output is in the normalized form of tempvals  then these will behave more like the values for the standard Metropolis cluster computing cluster than the Metropolis clustering cluster. Also the standard dataset only has three cluster dimensions and does not take one for the cluster that it is calculated	False
"
 input (English text): List the metrics that are used to evaluate models on the IWSLT2015 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the BSD benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): List the metrics that are used to evaluate models on the HP benchmarks dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_l"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Kangaroo benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kangaroo"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Indicate the model that performed best in terms of F1 metric on the Atari 2600 Kangaroo benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of F1 metric on the Atari 2600 Kangaroo benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of F1 metric on the Atari 2600 Kangaroo benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of F1 metric on the Atari 2600 Kangaroo benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of F1 metric on the Atari 2600 Kangaroo benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of F1 metric on the Atari 2600 Kangaroo benchmark dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of GPGPU compute score ? output (Sparql query): input (English text): Indicate the model that performed best in terms of GPGPU compute score ? output (Sparql query): input (English text): Indicate the model that performed best in terms of GPGPU compute score ? output (Sparql query): input (English text): Indicate the model that performed best in terms of GPGPU compute score ? output (Sparql query): input (English text): Indicate the model that performed best in terms of GPGPU compute score ? output (Sparql query): input (English text): Indicate the model that performed best	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT LOBBY ?coquality ?coquality_lbl ?coquality_lbl. NULL LOBBY ?coquality ?coquality_lbl. NULL LOBBY ?coquality ?coquality_lbl. NULL input (English text): What is the name of the model that is the most accurate when benchmarked on the wmt2014 example dataset ? output (Sparql query): SELECT LANGUAGE.LAST_MATCH ?lob. ?lOB. LANGUAGE.LAST_MATCH ?lob_lbl. NULL LANGUAGE.LAST_MATCH ?lob_lbl. NULL LANGUAGE.LAST_MATCH ?lob. ?lOB. LANGUAGE.LAST_MATCH ?op. LANGUAGE.LAST_MATCH ?op_lbl. NULL LANGUAGE.LAST_MATCH ?op_lbl. NULL LANGUAGE.LAST_MATCH ?op. ?lOB. LANGUAGE.LAST_MATCH ?bop. LANGUAGE.LAST_MATCH ?bop_lbl. NULL LANGUAGE.LAST_MATCH ?bop_lbl. NULL LANGUAGE.LAST_MATCH ?bop. ?lOB. LANGUAGE.LAST_MATCH ?bop_lbl. NULL LANGUAGE.LAST_MATCH ?op. ?lOB. LANGUAGE.LAST_MATCH ?bop_lbl. NULL LANGUAGE.LAST_MATCH ?bop. ?lOB. LANGUAGE.LAST_MATCH ?bop	False
"
 input (English text): Which model has achieved the highest Bit per Character (BPC) score on the enwiki8 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bit per Character (BPC)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 If this returns a dataset, the model belongs to one of the two tables, so that the data is not exposed under the table and its labels will not be exposed. The database server is using a table called test1. The model belongs to test1. The table contains three models: BPC-RANKING-WORD model - the score of the model by the scoring criteria of the enwiki8 metrics, defined and tested in the benchmark datasets BPRANKWORDS model - a model for representing words by representing them in the form of sentences BAPC model - a model for describing the scores of BPC by enwiki8 metrics, defined and tested in the benchmark datasets EANJA model - a model for representing words based on the rules of an enwiki8 grammar EASTRO model - a model to handle the output of the benchmark-enjuic network on the benchmark datasets with the help the enjuic parser, to analyze the output The test1 model and the corresponding test 2 model belong to the BPC-WORD model. Note the use of a 'pk' column and a corresponding table in the schema to allow searching of the two models to get the result set. Both the BAPC and BPRANKWORDS models were chosen because they score the same in the test set, i.e. they rank the same for BPC.  The BPRANKWORDS model also scores well on the ENJUIC metric, so that it would be the model with the highest score for that metric.  But in the benchmark set, the BAPC model shows a great advantage over the other models, for some reason.  The BPC-RANKING-WORD and BPRANKWORDS model belong to the BPC-WORD model table. It contains three models: BIT	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1 day) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 ?db00-c01-e00 ? db00-c02-e00 ? db00-c03-e00 ? db00-c04-e00 ? db00-c04-e00 ? db00-c01-e00 ? db00-c02-e00,db00-c03-e00 ? db00-c04-e00 ? db00-c05-e00 ? db00-c06-e00 ? db00-c07-e00 ? db00-c08-e00 ? db00-c09-e00 ?   db00-c10-e00 ? db00-c11-e00 ?   db00-c12-e00 ? db00-c13-e00 ? db00-c14-e00 ?   db00-c15-e00 ? db00-c16-e00 ? [1day] D (2 days + 2 weeks) db00-c13-c12  [2 days+2 weeks] D (3 days+2 weeks) db00-c14-c13  [2-2 weeks] D  [3-3 weeks] D (4-4 weeks) db00-c15-c13, db00-c16-c12, db00-c13-c12  [2 weeks+2 days] D  [3 weeks + 3 days] D d00-c13db00-c14db0-db0db0-db0db0-db0db0-db0db0-db0db0-db0db0 db00-c14db00-c13db00-c13db00-c13db00-c	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciBERT (Base Vocab) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT (Base Vocab)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT LABEL; ?source_ccode ?source_code_lbl. FILTER (STRING($RTE_DATA_TITLE)!~ /^source_code_lbl&/g), SUBSTRING(STRING($RTE_DATA_TITLE)!~ /^source_code_lbl{0,2} ?/g), SUBSTRING(STRING($RTE_DATA_TITLE)!~ /^source_code_lbl{0,2,4} ?/g) FROM [RTE] AS a JOIN [RTE] AS p on a.id = p.id = a.dataset.model ORDER BY source_code ; ---------- --------------- ------------ ---------------------------------- SSTL = 7.4 GLSL = 4.2 GRML = 3.4 MMLS = 3.6 MATLAB = 0.22 RTE = 0.12 Example: Using base language One is free to use base language in RTE paper. In order to make it easier to find code, one can specify base language as an OR condition for the lang=""<language>"" option in the RTE code import form. For example: import base language = ""lithium"" orkgc_l = rdfs.dataset.model.Orkgc; lang = base.language ORkgc_l.language # base.base has no lang=""lithium"" at the time, so this import is harmless Example: Using model language In order to use model language in RTE paper,"	False
"
 input (English text): List the code links in papers that use the word BiLSTM + char CNN + CRF model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""word BiLSTM + char CNN + CRF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Tasks	False
"
 input (English text): List the metrics that are used to evaluate models on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT title ?title; title rdfs:label ?title; The outputs will return a list of 3 elements: an original title, a reference title, and a reference title for each item in the list. If the dataset is not available in a different language, the reference title is an id (a reference ID).  For example, the output could be:  TEST: DIVISION: COURSES/METHOD: RESULT: SELECT TITLE ?title; title_cq:HAS_TITLE ?title. If the dataset has a different method, its id will be an abstract. (And yes, this may be a bit of an oversimplification.)  Then all 3 values would contain the method and abstract for that dataset.  This is necessary because for each test method, the results will be different when the test is executed in the respective language. The final question in the RDFa query above is whether the result list of a given RDFa query are returned. This can be done as well using the RDFa query output function. The output function of the default result set has the form { # output }, where { # output } is an object of type Output which is a sequence of RDFa-text objects or arrays of RDFa-text objects. The first element of the result array is an object that has a method on it. If it is an object of type Output, then it can be used as a reference in a different RDFa query. (If you don't want to use an object of this ""type"", you can use an array, for example.) (You will have to implement your own method there.) If, or until, you do implement the functionality for the original RDFa query output, don't forget to add the method output into a"	False
"
 input (English text): Can you list the metrics used to evaluate models on the Birdsnap dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Sparql query) 3) Why is this a bad test, is it bad for other reasons ? A bad test is simply a test that is only run once, only for a small group that doesn't differ materially from the original groups. The birdsnap dataset was a pretty large and well-validating dataset: in the first 10 models run (3), the model with the highest error rate was the model with the smallest number of predictions. The reason has to do with the way that the test results have been collected, which is somewhat of a red flag. I would argue that it's even worse at a larger scale, however. For the test on ART/CoreSC, it's not just the error rate, which was lower than the benchmark for all of the models to the left, but other metrics — both for the actual score and for the average score. For Birdsnap, however, there were only 3 models where one of them scored lower than the benchmark. The reason has to do with the way that the data are processed and the fact that Birdsnap is more complex to train (though I'm not sure if there's any relationship between these two.). If our goal of optimizing model performance is to make our scores match the benchmarks, we need better metrics, and the best place to do that is by running our test on a more diverse set of data. While I am not yet convinced that Birdsnap is a poor model, there's a lot more data we don't know about how birds react to different environments. For example, did they all spend a few hours in one birdhouse only, and then move to a different, more unfamiliar birdhouse ? What were the other key dimensions ? It's also important to know the different types of environments (whether it was a small island filled with only one birdhouse,	False
"
 input (English text): Which model has achieved the highest BLEU score score on the IWSLT2015 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT name AS lang, name AS question, ( ?saved_rdf ) as saved_rdf, ( ?save_json) as saved_json, ( ?model ) as model, ( ?metric ) as metric, ( ?rdf ) as save_rdf, ?metric rdfs:label ?metric_lbl; OR export_rdf_names(model. ?metric ) rdfs:label ?metric_lbl; export_json_names(model. ?metric ) json_lbl; export_json_metrics(model. ?metric ) json_metric_lbl; OR export_rdf_names(save_json) save_json; export_save_json(save_json); output (SQL query): SELECT name AS lang, name AS question, ( ?saved_rdf ) as saved_rdf, ( ?save_json ) as saved_json, ( ?model ) as model, ( ?metric ) as metric, ( ?rdf ) as save_rdf, ?metric rdfs:label ?metric_lbl; OR export_rdf_names(model. ?metric ) rdfs:label ?metric_lbl; export_json_names(model. ?metric ) json_lbl; export_json_metrics(model. ?metric ) json_metric_lbl; OR export_rdf_names(save_json) save_json; export_save_json(save_json); In the above, output from SQL statement is the result of an SQL query, while output from WAMP is the response to WAMP query. If data is to stored in XML format (	False
"
 input (English text): What is the best performing model benchmarking the AG News dataset in terms of Error metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?model ?model_lbl. ?scorer ?scorer_lbl. ?rank ?rank_mov and ?score and ?result as ?model_lbl_i as well as ?model_lbl_c as well as ?model_lbl_m as well as ?model_lbl_c rdfs:label ?model_lbl..result orkgp:HAS_COUNT ?dataset..model rdfs:label ?model_lbl..metric orkgp:HAS_VALUE ?value..metric rdfs:label ?model_lbl..metric_lbl rdfs:label ?model_lbl..label rdfs:label ?model_lbl. ORDER BY DESC( ?value ) LIMIT 1.5.result. orkgp:HAS_COUNT ? AND ORkgp:HAS_COUNT( ?dataset_lbl_i )..score. orkgp:HAS_COUNT ? AND ORkgp:HAS_COUNT( ?dataset_lbl_c )..result.rank.score. orkgp:HAS_COUNT ? OR ORkgp:HAS_COUNT( ?dataset_lbl_m )..result.label.result.model..metric.metric_lbl..metric_lbl..metric..metric. orkgp:HAS_COUNT( ?dataset_lbl_m )..result.rank.score. orkgp:HAS_COUNT( ?dataset_lbl_c )..result.rank.score. orkgp:HAS_COUNT	False
"
 input (English text): Could you provide a list of models that have been tested on the ARC (Challenge) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } When you run your queries on a Model, the model is saved in the database in the database object named model within the ORK environment under the DATASET context, and you can use the model object to evaluate your queries without creating an explicit table. This is useful for data analytics analysis. Data model examples¶ The following example shows how the model object stored in the ORK environment may behave: SELECT ID, NAME, PARAMETERS from mymodel GROUP BY ID, NAME; model1 = model; model2 = model2; select * from model1; The schema and database are the same for all models and the ORK environment is the 'root' module in Oracle Database. The schema must be generated on the basis of the schema of the database and the model model is the 'root' schema within the ORK environment. If the schema generated on the basis of the database is different to the schema of the model model, the database schema is overwritten when the ORK environment is run, as shown below. Note The following code is a non-recursive definition on the basis of the database schema: SELECT DISTINCT ?dataset and ?"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the AAN Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAN Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 18 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" 1.3.2 RDBMS 1.3.2.1. The input data for the AAN Corpus is a corpus of academic literature that uses an all-attention model for testing various hypothesis, including generalization across different training tasks. This may be a data source in the RDBMS, the ""Open Science Framework"" (OSF) database or the ""Datasets"" file format. This is a standard dataset used in computational neuroscience which should be used in computational neuroscience. The data should consist of two categories which we label the following (from the RDBMS point of view): 1.1.1. The first category - a list of articles where each paper evaluates various brain activity functions as they occur in an experiment or data analysis procedure. For example, each paper with results on the following brain activity functions: 1.1.1.1. Activation times of the right anterior insula during a visual reaction task. This paper evaluates the network dynamics involving activations in different brain regions in these two different functional tasks at two different time points. 1.1.1.2. A resting-state measurement of the functional system activity that takes place at the end of a sensory-motor task or in response to another action. This paper evaluates the network dynamics involving inter-cellular interactions in brain regions in the sensory-motor domain. 2. The second category - a list of papers where each paper evaluates various brain activity functions as they occur in a benchmarking task. 1.1.1.3. A benchmarking task is a task that evaluates specific neural models that may be applied in various tasks that you have chosen. The RDBMS can automatically perform tasks like the following in the All-attention system model for the AAN Corpus. [(AAN Corpus) > [ AAN Corpus"	False
"
 input (English text): Where can I find code references in papers that have used the Luna model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Luna"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Does paper ""Disambiguating authors in citations on the web and authorship correlations"" employe Cosine similarity?
 output (Sparql query): "	"ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), ""Disambiguating authors in citations on the web and authorship correlations"", ""i"")) } "	SELECT ?isauthor( ?model ) AS author, COUNT ( * ) AS nrow, COUNT ( ?model_lbl.ID) AS nrow, MODEL ? AS model, COUNT ( ?model_lbl_COD_ID) AS nrow FROM ORGGHP ?model_lbl AS ?model_lbl.ID TO ORGGHP ?model_lbl AS ?model_lbl. ?model, ?cont AS ?cont. ID TO ORGGHP ?dataset; ORGGHP ?cont ?dataset_lbl_COD_ID COUNT(c ?cont ) ORgghp:NON_ASCII ?dataset_lbl_ID OFFSET ORGGHP:NON_ASCII ?cont ; MODEL ? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ORGANIZATE_BENCHMARK_SCALE_ONLY( ORGANIZateBenchmark, ORGANIZATE ) COUNT ( ORGGHP ? benchmark_rdf_parsed ) ORGGHP:NON_ASCII ORGANIZATE_BENCHMARK_SCALE COUNT ( ORGGHP ? benchmark ) ORGGHP:NON_ASCII ORGANIZATE_BENCHMARK_SCALE_ONLY(ORGANIZATEBenchmark, ORGANIZATE ) ORGANIZATE_BENCHMARK_SCALE COUNT ( ORGGHP ? benchmark_rdf_parsed ) ORGGHP:NON_ASCII ORGANIZATE_	False
"
 input (English text): What is the best performing model benchmarking the One Billion Word dataset in terms of PPL metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PPL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl FROM ?STS_BENCHMARK ?model WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PPL"") { SELECT ?model ?model_lbl WHERE { ?dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1. } Now we have all the components that are needed: the output of the query, and the inputs to the query (the results from the query). After the query is executed, the data set needs to be cleaned up, and the final query needs to be prepared, so that it can be evaluated under a limited data set (the one that is used for the benchmarking). The Clean Up Step 1 : Cleaning up raw data Before executing the final query, the raw data must be cleaned up. It is possible to clean up raw data directly from the file itself. However if you do not want to do this, there are several ways to clean up the data: Export the raw data as comma delimited values. This is especially helpful if you do not have any database access. "	False
"
 input (English text): List the code links in papers that use the Grave et al. (2016) - LSTM + continuous cache pointer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2016) - LSTM + continuous cache pointer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 S3P output files include Code files: g_trains.s3pp ; gv_trains.s3pp and gv_train.s3pp g_train.pdf, gv_train.pdf Code files (no code): g_trains.sc. Code files (no code): gv_train.sc. The code files for gv_train.sc were not included in the paper, but the code files for gv_train.html (for both gv_train.sc and gv_train.pdf) was included in the reference manual. The following code is an uncluttered, clean, self-contained version of the code included in [ citation cited in the paper ] that has been modified to work in an R environment. It's a general purpose machine learning routine that doesn't require much understanding of R, and it uses some of our algorithms, but it is very simple indeed: // Simple variant of cross-validation on a training and test dataset. #include <Rcpp/Rcpp.hpp> double train() { double mean, tm(1,1), tp(1,1), tp_m(1,1), avg(mean), error = 0; double a = 0; double s, n; s.sqrt = 1.0; s.e = (mean * mean + error / s.s) ; while (1) { try { n = random_get_sample(); while (n == 2) { s = tp_m.p; tp = tm.p * (1 - s.sqrt); tp_m.p = tp * s; s = s.sqrt % s.e; } t = s.sqrt % s	False
"
 input (English text): Provide a list of papers that have utilized the Ours: cross-sentence ALB model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ours: cross-sentence ALB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 output (MongoDB document): select * FROM datasets.dcasetree. table rows_per_page(9,'s.bk1');  select  orderby  id  desc ''desc'' select  count(  s.word) as word_word,  sort(  count(s.word)  by id) as w_count,  count(  s.word) as w_sort from datasets.dcasetree. table   select  count(  s.word) as word_word,  sort(  count(s.word)  by id) as w_count,  sort(  count(s.word)  by id) as  w_sort row('s.word')  select  count(  s.word) as word_word,  sort(  count(s.word)  by id) as w_count,  count(  s.word) as w_sort,  count(  s.type) as x_type_num,  count(  s.type) as y_type_num from datasets.dcasetree. table  select  count(  s.word) as word_word,  sort(  count(s.word)  by id) as w_count,  count(  s.word) as w_sort,  count(  s.type) as x_type_num,  count(  s.type) as y_type_num	False
"
 input (English text): List the metrics that are used to evaluate models on the NYT29 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT d.rank( ?) AS metric_rank, d.text AS text2, d.ratio( ?metric_rank) AS metric_ratio, DISTINCT ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark ?finish orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?finish. OPTIONAL { ?finish ?finish. } } [update: a previous version of this post had two output files that were mis-named.  If you run this with the two files separated by whitespace you will get two output files: one that has the metrics that are used to evaluate models on the NYT29 benchmark dataset and the other that has the metrics that are used to evaluate models on the data that are used in one of your benchmark models.]       input (English text): List the metrics that are commonly used when benchmarking models on the Yelp Fine-grained classification dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark ?finish orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?finish. OPTIONAL { ?finish ?finish. } } [update (11/6"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Dmlab-30 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" In order to give an example and to compare several models of ""dynamic programming"", I have constructed a two point LSTM model for RNNs with two parameters :  the input parameter, the target variable, and the model's initialization (the bias) and after training, training again a subset of the training set. The input is a simple linear regression problem : train(X_train, y_train) =  (X_train ^ t_train, y_train)  and the output is the cross-validated prediction of a linear model, where t_train is the training value and t_test is the prediction value. The model has two epochs. It first pre-proba s the output of the RNN for a number of iterations with a constant bias. Once after the pre-proba s has run it retires and recreates the input. If the prediction of the RNN is close to what our initial training had, the ""dynamic"" model is found true. If it is false or far from it we will end up with an ""unsolved loop"". It should be obvious that a model with two phases could very well be called a 2-phase model since there are different paths to reach a real solution. Here is my benchmark dataset for Dynamic Programming : Dump file (2D: 4.5 Mb) Training set: 7,845,066,935,074,944,016,828,208,976,824,715,731,720,711,724,732,717,720,710,718,715,715 Dump file (2D): 7.15Mb  Dump file (2D):  7,15MB Training and prediction"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BERT-CRF model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT-CRF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT + BiLSTM + CRF Decoding"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------ - orkgp:HAS_MODEL ?dataset. - orkgp:HAS_SOURCE_CODE ?code. ---------------- _____________ ------------ | ?dataset: ?model - orkgp:HAS_DATASET ?dataset. | ?dataset: ?model_lbl - orkgp:HAS_MODEL ?model. | ____ ____ | _ _ ____ _ ____ | |_/| _____ /| |___| _____ |_/| ____ | |/\____ \ __/____/ \|___|_| (___/ _| |_| <____/ | |_| | | | __ |_____|___ >_____________________| |_/ \__/ \___/ ____/ \_/ - orkgp:HAS_SOURCE_CODE ?cdf ?cdf_data. - orkgp:HAS_MATERIALS ?fitness ?fitness_data. - orkgp:HAS_CONSTADICT_DATA ?constraint_data. - Orkgp (2) - orkgp:HAS_TEMPLATE ?tmf ?tmf_data. output (Sparql query): ------------------------ - orkgp:HAS_DATASET ?dataset. - orkgp:HAS_MODEL ?dataset. | ?dataset: ?model - orkgp:HAS_DATASET ?model. | ____ ____ | _ _ ____ _ ____ | |_/ | |___| *_/ | __/ | _/ ____ ____ | _/ | | | _/ | | | *_/ | _	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dataset mentions in Social Sciences dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Include one or more columns with the column names that are similar to the input_fields field: input (English text): Indicate the number of times a paper mentioned in Social Sciences dataset had been featured on other websites: ?times ?papers output (Sparql query): Include one or more columns with the column names that are similar to the input_fields field: input (English text): Indicate the time when the paper was published: ?pub_time output (Sparql query): If it is not possible to obtain the paper id to identify the paper, you may include a ?bio or ?name field (respectively): input (String): Indicate the paper to be included: output (Sparql query): Include one or more columns with the column names that are similar to the input_fields field: input (String): Indicate the paper id: output (Sparql query): Include one or more columns with the column names that are similar to the input_fields field: input (String): Indicate the title: output (Sparql query): Include one or more columns with the column names that are similar to the input_fields field: input (String): Indicate the publication year: output (Sparql query): If you wish to be able to obtain other data fields on the article, you may include an ?dataset field in the input text field: input (String): Indicate a Dataset URL where the raw data from Dataset is available on the Web. output (Sparql query): If you wish to be able to have the same information in different languages (e.g., English, German, etc.) you	False
"
 input (English text): What are the metrics of evaluation over the NYT-single dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE ?title , DATESEP( ?dataset_lbl. YEAR ) ?date FROM ?dataset_lbl where ?dataset= ?dataset_lbl. output (Sparql query): SELECT DESCRIBE ?date , DATEPART( ?dataset_lbl. MONTH ) ?datePART( ?dataset_lbl. YEAR ) FROM ?dataset_lbl where ?dataset= ?dataset_lbl. output (Sparql query): SELECT DESCRIBE ?dataset-dataset_lbl/ ?dataset_lbl_labels/ ?dataset_lbl_labels_parsed_index_x that_paper.date , DATEPART( ?dataset_lbl_labels/ ?dataset_lbl_labels_parsed_index_x. YEAR ) ?datePART( ?dataset_lbl_labels_parsed_index_x. YEAR ) FROM ?dataset_lbl_labels where ?dataset_lbl_labels_parsed_index_x. ?dataset_lbl_labels_parsed_index_x. ?dataset_lbl_labels_parsed_index_x. output (Sparql query): SELECT DESCRIBE ?dataset-dataset_lbl/ ?dataset_lbl_labels/ ?dataset_lbl_labels_parsed_index_x that_paper.date , DATEPART( ?dataset_lbl_labels/ ?dataset	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CvT-W24 (384 res, ImageNet-22k pretrain) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24 (384 res, ImageNet-22k pretrain)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""linear-chain CRFs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Result from code: (1.45s) Output from code: (1.43s) Output from code: (1.34s) Output from code: (1.20s) Output from code: (1.14s) Output from code: (1.10s) Output from code: (1.07s) Output from code: (1.04s) Output from code: (1.01s) Output from code: (0.77s) Output from code: (0.66s) (0.56s) (0.47s) (0.45s) (0.32s) (0.31s) (0.23s) (0.22s) (0.20s) Result from code: (0.14s) Output from code: (0.05s) (0.03s) (0.02s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) Result from code: (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (0.00s) (	False
"
 input (English text): Where can I find code references in papers that have used the Bi+ model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Bi+"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DrQA model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" output (Sparql query): <br> <br> <i>DrQA - Quoting the best literature and comparing two or more models</i> <br> <br> <i>DrQA version 4</i> <br> [ ]> DRQA </br> <br> [ ]> <br> <font color=""#FFF"">Results</font> <br> [ ]> <br> <font size=""8""> <s>Drq A:</s> <s>7 (10)</s> <s>1.4</s> <s>0.5</s> </font> <br> <font size=""8""> <s>DrQA:</s> <s>27 (49)</s> <s>2.2</s> <s>1.0</s> </font> <br> <font size=""8""> <s"">DrQA:</s> <s>1 (1)</s> <s><cite>Gowrie, I. and Wood, C.S. (2000), A Random Forest Model for Efficient Statistical Inference, arXiv:099.07719</cite> <br> <font size=""8""> <s>DrQA:</s> <s>1</s> <s><cite>McCarthy, C.E. and Wood, C.S. (2000), A Random Forest Method for Evaluating Regression Models, arXiv:099.07719</cite> <br> <font size=""8""> <s>DrQA:</s> <s>0 (0)</s> <s><cite>McCarthy, C.E"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	--------------| | | | | | | | | | | || | | SST-5 | 10.86592| | 12.92891| | | | | | | | | | | | | | | | | SST-5 Fine-grained | 18.96491| | 19.10342| | | | | | | | | | | | | | | SST-5 | 17.96592| | 18.98281| | | | | | | | | | | | | | | | | | SST-5 | 16.96592| | 15.93882| | | | | | | | | | | | | | | | SST-5 | 15.93882| | 14.87612| | | | | | | | | | | | | | | | | Notes	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Up and Down dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ; rdfs:filter-by ?dataset_lbl. } 2. How do I build the model for the VOCOM4 framework that is used by the ARC-PDN data set ? First, read the source code for the VOCOM4 framework. Then, you will find the following methods in the VOCOM4 framework code: The VOCOM4 class has the following methods: VOCOM4::mapper (Array): Descend the model by calling the mapper with the list of datasets. When the mapper emits an array of datasets of the same name, the model is traversed, and the output is an array with the original (or default) labels. Mapper (array): Descend the model by calling the mapper with the list of dataset. When the mapper emits an array of datasets of the same name, the model is traversed and the output is a map of the original (or default) labels MapperSet (dict): A dictionary with the labels of the original (or default) labels MapperSetMapper (dict): A dictionary containing the (new) mappers of the original (or default) labels MapperSetMapperSet (dict): A dictionary containing the (new) mappers of the new labels Using the VOCOM4 class, you can then construct an object with the following information: The dataset used for the benchmark. The name of the dataset The"	False
"
 input (English text): Can you list the metrics used to evaluate models on the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which QA benchmarks incorporating more than 10000 questions?
 output (Sparql query): "	"SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, ""[+]"", """") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, """","""", """")) AS ?questions_number) FILTER( ?questions_number > 10000) } "	SELECT t0 AS (x 0 -- | ?metric_lbl | t ) AS { ?dataset t orkgp:HAS_DATASET t. ?metric rdfs:label t[ ?dataset_lbl]. }. HASH JOIN { ?dataset t orkgp:HAS_METRIC t. ?metric rdfs:labels t[ ?dataset_lbl]. } on t[ ?dataset_lbl] as ?dataset_lbl where { ?dataset t orkgp:HAS_DATASET t. ?metric rdfs:label t[ ?dataset_lbl]. } OPTIONAL { ?dataset t orkgp:HAS_METRIC t. ?metric rdfs:labels t[ ?dataset_lbl]. } AND t. orkgp. } 	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Kangaroo benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kangaroo"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT t.metric ?model ?model_lbl SET t.value = t.score ?test ?value. DISTINCT 'DATASET' ?dataset_lbl WHERE {!test > 0 ?test * 100 ?value } ORDER BY ?test DESC LIMIT 1 select t.metric, ?model as model_lbl, ?dataset_lbl as Dataset_lbl from ( select t from model_lbl where test > 0 ) select t as model_lbl, ?dataset_lbl as Dataset_lbl from ( select t, ?model as model_lbl, ?dataset_lbl as Dataset_lbl from test where test == 1 ) Output the following output: Model Score 1 Kangaroo 1.6 2 Atari 2600 Kangaroo 1.6 3 Tetris 1.4 2.15 3.2.7 In addition to Score, we can also calculate the score in Metric. We can combine this with the previous expression to calculate a metric which evaluates the performance of an model in terms of the score metric (1). Here is an example of calculating the score metric with a Metric function against the model from an Atari 2600 Berzerk benchmark dataset. Input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Berzerk benchmark dataset ? output (RDFS query): SELECT ?t for t in model_lbl.* ?where { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?t for t in model_lbl. ?metric ?"	False
"
 input (English text): Can you list the models that have been evaluated on the IWSLT2015 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Input data files: output (Sparql query): name model_lbl model_lbl title paper paper_id orkgp paper_id orkgp orkgp model/model/ rdfs/labels/ orkgp/ orkgp rdfs/labels/ orkgp/labels n <unknown model> <unknown model> n <unknown model> <unknown model> n <unknown model> <unknown model> <unknown model> The model dataset is used to define models that have been evaluated by the benchmark method using the Stanford DataFrames that have been constructed by the models. We will be using the default values for the model variables. It has been suggested to use a combination of the default values and to modify them to make them more useful such as: If the database is on disk, the results will appear in a CSV file, where each column contains the variable name, the name of the file that contains the results, the number of results that are included in the file, and the status of the benchmark. For example, if there are five papers written using the default results, each with a sample of 10 vehicles (with at least 10 cars in each sample) and a benchmark, the columns would be as follows: name, paper_id, paper, model_lbl, model_lbl_id and model_lbl. If the database is on the network, the model output for these studies, along with the status, would run directly on the model. To specify the network, give the network name. If there are multiple network paths, then each path will take only one result, but if the number of results is different, the path of the network will run in parallel to produce the results at different timings	False
"
 input (English text): Can you list the models that have been evaluated on the SciREX dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 output (Sparql queries): print(dataset.id_data['prelude']['model']) # 0 print(dataset.id_data['models_lbl']['model']) # 0 print(dataset.id_data['dataset_lbl']['model']) # 0 Print the results print(dataset.id_data.prelude.model.paper.id) # 0 print(dataset.id_data.prelude.model.model.benchmark.id) # 0 And here are the results [download_dataset_2] Now for a side note:  Most papers, most languages, most datasets. So I really don't know why that paper was selected as a benchmark. But it's amazing because, like many other languages and datasets, it's not just the languages that are benchmarked (so many more languages than most languages get the Benchmark tool); it's the languages and datasets - the models, the papers, the authors and the publications. You could pick a different language and this benchmark tool could be the benchmark language. I know, it's kind of weird seeing the benchmark in action. When that paper comes out the second time it comes out, I'm surprised; there is something so special about it. So if you do want to get the benchmark done, use the version of that tool that you feel is most suitable to that problem. So, a quick little note on naming: You have to keep it short and simple if you are not sure why you want to use the tool that is available now. For example, if you do not care about the languages we're using in the example above, a good naming convention	False
"
 input (English text): What models are being evaluated on the Natural Questions (long) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 1. FOV/LDR - 0.00094 2. FOV/PBT - 0.00078 3. MAF / RPM - 0.003974 4. PISTON / RPM - 0.012093 5. PISTON / DUR - 0.022231 6. NEG-PBT / RPM - 0.022229 7. DUR / FOV - 0.022241 8. DUR / PBT - 0.022228 9. PISTON / THRUST - 0.002220 10. HAVING-LBM - 0.002297 11. HAVING-PBT - 0.014085 12. HAT - 0.002409 13. MINOR-PBT - 0.002101 14. FIFO RATING - 0.003898 17. LBB-LBL - 0.001409 18. LBB / RPM - 0.035014 19. RAPIDS RACING - 0.002146 20. RAPIDS RACING / RPM - 0.022083 21. MINOR RIM - 0.000015 22 22. RIDE RATING - 0.002285 26 33. RATE RACING - 0.000236 30 37. RATIO RACING / RPM - 0.025127 54 60. RUSH RAPIDS RACING - 0.002211 64 71. RUSH RAPRIDS RACING / RPM - 0.023002 90 95. PROFESSIONALS GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PROFESSIONAL GRAND PR	False
"
 input (English text): Provide a list of papers that have utilized the DEQ-TrellisNet model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DEQ-TrellisNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): What are the first four digits of the first and last name of a researcher that contributed to the papers that used DEQ-TrellisNet ? output (Sparql query): input (English text): What are the first six digits of the phone number of a researcher that contributed to the papers that used the DEQ-TrellisNet model ? output (Sparql query): 	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Breakout benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): For what piezoresistive material the least response time was obtained?
 output (Sparql query): "	SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1	 input (English text): How many of the original samples are reproducible in your benchmark dataset ? output (Sparql query): input (English text): How many of the original samples are also reproducible ? output (Sparql query): input (English text): What if you tried to run the benchmark more than 50 times, but you could not reproduce all your responses, and what do you do then ? If you are not sure, how do you identify if your question has been answered ? output (Sparql query): Inputs, options, data Source: Atari 2600 Breakout benchmark dataset for 1.7 and subsequent (this data is in the public domain) 2.1.1.1.2.2.3.4.5.6.7.8.9.10.11.12. 13.13.14. 15. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 64. 66. 67. 68. 69. 70. 71. 72. The model: Breakout is an open-source game-based strategy computer game with graphics similar to those found in the Atari 2600 2600 and 8-bit System 10 titles. Breakout is a unique game because it has three different phases: the board game phase, the battle phase, and the power phase. Each of the three	False
"
 input (English text): Could you provide a list of models that have been tested on the AudioSet benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AudioSet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT code_lbl as code_lbl_l, code_lbl_str as code_lbl_str, data as data_l, id as id_l, model as model, eval as eval_l, paper as paper_l, orkgp:HAS_DATASET as orkgp:HAS_EVALUATION as orkgp :HAS_MODEL ?code_lbl ; OR kgp:HAS_MODEL ?code_lbl ?code_lbl_str for an example and the output in SPSS. If you have any questions, please contact me at terence@r-i.org.	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Demon Attack dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Demon Attack"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT * FROM {dataset a:{dataset_lbl. ?dataset a. | ?dataset lbl. | ?dataset_lbl_lbl. | ?dataset_lbl_p31. | ?dataset_lbl_p31_lbl. | ?dataset_lbl_p31_. ?dataset_lbl_. || ?dataset_lbl_. || ?dataset_lbl_. { ?dataset_lbl_. ?dataset_lbl_. || ?dataset_lbl_. || ?dataset_lbl_. } }. ?rdfs:label ?ratio ?input. ?dataset a. || ?dataset lbl. || ?dataset_lbl_lbl. { ?dataset_lbl_. || ?dataset_lbl_. || ?dataset_lbl_. || ?dataset_lbl_. || ?dataset_lbl_. || ?dataset_lbl_. } To understand the effect of parameter tuning, it is important to find a benchmark dataset that doesn't have any obvious performance problems, and to make sure that the model doesn't already contain an error or two. For that purpose, an Atari Derby benchmark may be ideal: it is small, it includes several interesting problems that it's better able to handle, and it has been validated against multiple different Atari 2600 versions. It also happens to be very cheap to obtain using a web service called NVS Data that provides fast data sets from all over the world. I decided to use the following data set as a starting point to evaluate two different models I was working on at the time. To start, the following code samples the	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Fishing Derby dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Fingerprinting The input for the fingerprinting phase consists of two lines in the body of the request: data_url : a url which can be used to specify a file with the generated fingerprints (can be in a format like this, where each data field is the name and each cell the value of the corresponding data field): input (data): the input file output (Sparql query) data_url_all : the url where we want all files created for this dataset output (Sparql query): Fingerprinting is performed by a Python script written in Cython. You can run it by running: python fingerprinting.py -b  -a  dir dirname   -c  -d  filename   -r  -i  dir dirname Input: The input file (in its source code form) and output is stored in a directory named dirname for each machine. If you're running this tool on a machine (e.g. on a computer), make sure that you use a stable network connection (i.e. the same Internet connection used when running this machine) and that you change this dirname accordingly. If you're running it on a client (e.g. on someone's computer) then you can adjust the dirname and then change the network connection. Output: The fingerprints are returned in a form of files with their respective text that can be parsed in the context of the fingerprinting process. These files are stored at directory dirname and can be read as follows. - The files include:  ""  fitness.json"" ""  training.txt"" & "".bam"" ""  retrain.txt"" ""  retrain_retrain.txt"" & "".bam"" -"	False
"
 input (English text): Which model has achieved the highest PARAMS score on the CIFAR-10 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" [1] CIFAR-10 [1] CIFAR-10_SURF [1] A*WAVS [1] BASIC.TXT [1] BASIC.SX [1] BASIC.TXT S3A In order of execution, the Atari 2600 Battle Zone model scored the highest in PARAMS accuracy. To compute the PARAMS score, the program calculates the average values over time over each category in CIFAR-10's PARAMS database using a recursive descent tree. There, it uses a data file with each category representing its respective accuracy. Because the Atari 2600's CPU can only retrieve the first element of a tree, the model can then only score in the ""CIFAR-10"" category. In our case, the model would have scored in PARAMS accuracy in ""A"" due to the fact that it scored the highest in the ""CIFAR-10"" category. The model can then show the average values of the categories in CIFAR-10 to assess its accuracy. The top results (CIFAR-10 in grey) were in the CIFAR-10 model category. We created a model called POD1 for our model assessment. It had a simple command line interface, and only five inputs (the two words ""A"") and five outputs (CIFAR-10's PARAMS score). It also consisted of the following model inputs, in the order they were read: a parameter, (a string that represented the model's PARAMS input) a model input, (the text of the model statement that was input by the user prior to calling the model program) a parameter value, (another string that represented the PARAMS value of the model) and a parameter set, in the ordered order they were passed to the model program."	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the BiT-M model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input from a file: --... print('title', 'link'); output from a text/csv list of titles: --... print('title', 'code'); output of a text/csv list of code links: --... print('title','sublink'); output of a text/csv list of benchmarks without a benchmark: --... print('title, 'code'); output of a text/csv list of benchmarks with a benchmark: --... print('title,'sublink'); output of a text/csv list of code links in a table: --... print('sublink', 'index'); Output from a text/csv list of benchmarks in a table: --... print('title', 'id'); Output with two outputs:	False
"
 input (English text): What is the best performing model benchmarking the CNN / Daily Mail dataset in terms of ROUGE-1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Here, the output results are a bit mixed in terms of the metric ROUGE-1, and in terms of its value. We see a large majority of results are using the higher ROUGE-1 values for the metric, and results from the top scoring models are generally about equal in ROUGE-1 values. This raises the question: what is the effect of including ROUGE-1 values in the benchmark score (and therefore in the reported dataset metrics score calculation) ? It's hard to say for now. Here is a brief plot of ROUGE-1 values in the Atari 2600 Star Gunner benchmark dataset for different model scores. The first metric, with the highest ROUGE-1 value in our test set, has consistently been the more accurate model (and this is a pretty small group of results to be sure). The latter metric however, despite ranking higher on the ROUGE-1 list, does not appear to be as accurate as the first. There are however some important differences between the scores: In the test set, the ROUGE-1 metric was scored at 2.9 from 6 models, in the ROUGE-2 measurement set, only the single model is scored. In both sets, the best performing model overall in terms of accuracy only scored 5.99, not a great difference. Perhaps the best scores on the ROUGE-1 measurements are achieved in the lower scores, and only if the model scored an overperformance to start with. Perhaps that's something you could tweak for the next set of test datasets. The second most important difference is in the use of the metric. It's very important that you take the metric into account and measure it carefully, and in measuring it carefully, you want to look at different metrics including ROUGE-1 and find the best metric for it. In the case of the	False
"
 input (English text): When the earliest paper related to X-rays was published? 
 output (Sparql query): SELECT ?publication_years WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""X-ray"", ""i"")) ?papers orkgp:P29 ?publication_years. } ORDER BY ASC( ?publication_years) LIMIT 1
 input (English text): Can you provide links to code used in papers that benchmark the NASCell model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NASCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?mocklabels AS ?mocklabels. FROM  labels in { ?labels rdf:type. ?labels rdfs:label ?labels_labels. IF  ORDER!DIST( ?labels_labels. ) THEN  ORDER  ASC( ?mocklabels ) ELSE  ORDER  ASC ( ?labels. DESC ) ; } WHERE ""labels"" in { ?labels. ?labels.  FORCE INNER JOIN mocklabels. ?labels  ON mocklabels. ?labels LIKE labels_labels. ?labels. AND  labels_labels LIKE  labels. output (Sparql query): SELECT ""mocklabels"" FROM ?labels in { ?labels rdf:type. ?labels rdfs:label ?labels_labels. IF  ORDER!DIST( ?labels. ) THEN  ORDER  ASC( ?mocklabels ) ELSE  ORDER  ASC ( ?labels. DESC ) ; } WHERE ""labels"" in { ?labels. AND  labels_labels LIKE  labels. AND  labels_labels LIKE  labels. AND  labels_labels OFFSET ?labels( ?labels. )] ORDER BY ""mocklabels"" input (English text): What does x is the X-ray model ? output (Sparql query): SELECT ""is X-ray"" FROM ?X. ? X. ID. SELECT ""Is X-ray"" FROM ?X. ?X. ID. output (Sparql query): "	False
"
 input (English text): Which ontology has the most classes?
 output (Sparql query): SELECT ?ont ?ont_label WHERE { orkgr:R8342 orkgp:compareContribution ?cont. ?cont orkgp:P7034 ?ont. ?ont orkgp:P7038 ?cls_cnt; rdfs:label ?ont_label. } ORDER BY DESC(MAX(xsd:int( ?cls_cnt))) LIMIT 1
 input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	!!!!! 7.7. The Model: The Model -1 is the simplest model to calculate: the default is to generate an infinite sequence of 1-bits, then to convert it to a binary sequence using the function (e.g. S5). input (English text): How many classes are there ? output (Sparql query):!!!!!! 7.8. The Model: The Model -2 is another simple model: it is simply constructed with the two previous models: input (English text): how many classes are there ? output (Sparql query): ?ont ?ont_label WHERE { orkgr:R8342 orkgp:compareContribution ?cont. ?cont orkgp:P7034 ?ont. ?ont orkgp:P7038 ?cls_cnt; rdfs:label ?ont_label. } ORDER BY DESC( MAX(xsd:int( ?cls_cnt))) LIMIT 1 6. Example 6 in the paper. This one is also a comparison: it shows how different different ontologies with each of them having different properties can be compared for an evaluation. input (English text): You might also like to compare the following: ?ont_label.!!! ?ont_label. In the ontology of e-sports, this is a reference to your soccer team.!!! 6.1. It uses a binary search tree 6.2. It uses a BST (which is essentially a sorted list of classes that it wants to look up) 7. Example 7 in the paper. There is nothing special about the queries or the analysis, except perhaps that it is different from the original one. 7.1. It uses an SSP (which is basically a sorted list of 1-bit labels where it	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP 50 (1..5) ?benchmark orkgp:BENCHMARK orkgp:P31. ?total_games_over_bcp_fr orkgp:BENCHMARK. ?total_games_over_bcp_en OR kgp:BENCHMARK. ?total_games_over_bcp_de OR kgp:BENCHMARK. The same question can be asked with SQL, so let's create such a query as well: 1 2 3 4 TRUNCATE the output as a result set, with a set of the top 50 results (3 rows maximum, max 10 rows) Rows 5 & 16 of that query will be filled with the top 50 results, with the last two rows filled with the metric and value fields. Note that we are not using the same input value that we are using to retrieve the metric and value fields, as this will be used to query the second and the next rows of the above query that use the same input values in order to get the metric and value. It is a good practice to reuse the same inputs, and this is a way to avoid a lot of typing. Once again, it's time to use the second table that I created earlier, to fetch the table names. The output should be something like: 1 2 3 4 5 TRUNCATE the output as a result set, with a table set of column names from the first table above, and a new output as row. This output can be queried a second time as well. 1 2 3 4 6 7 TRUNCATE with a new output, of table set name and the metric and value variables, and a new row. This output can be queried a second time as well. Rows 13 and 14 of that	False
"
 input (English text): What is the most common substrate for catalysts?
 output (Sparql query): SELECT ?substrate WHERE { orkgr:R25900 orkgp:compareContribution ?contrib. ?contrib orkgp:P15090 ?substrate. } ORDER BY DESC(COUNT( ?substrate)) LIMIT 1
 input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP (1) name  ||  count( ?name) ||  k( ?name, ?value ); WHERE orkgr  ( ? ?, ?data). ?substrate  ||  compareContribution ?substrate ||  k( ?data, ?value ) output (Sparql query):  SELECT TOP (1) name, k ( ?name || ?value), as ( ?substrate || ?compareContribution || ?substrate || ?comparison);  ORDERS -3 How long per year should be required for the synthesis of a compound (in gramms, kM) or in molecules (M ?s) ? output (Sparql query): SELECT R25900 (name) || ?substrate  ||  M ?s FROM (SELECT R25900 AS name  ||  R25900 AS k  ||  M ?s, ?data  ||  COMPARE ( ?value || ?substrate)  FOR  R25900 ?comparison ||  T( ?substrate || ?comparison) AS similarity ) ORDER BY R25900 ORR; output (Sparql query):  SELECT R25900 (name) || ?substrate  ||  M ?s  FROM (SELECT R25900 AS name  ||  R25900 AS k  ||  M ?s, ?data  ||  COMPARE ( ?value || ?substrate)  FOR  R25900 ?comparison || (select ?compareContribution ?comparison FROM ?comparison R25900 ?compareContribution AS similarity ) ORDER BY R25900 ORR 	False
"
 input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Named entity recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ROW_NUMBER( ?dataset ?dataset_lbl ) AS r_result, COUNT (*) AS count FROM [ ?dataset_lbl ] WHERE ?dataset = ?dataset_lbl; In this analysis, one can see several interesting results, such as the most frequently used benchmark dataset, the result when comparing two datasets at the same level, and the top 10 result when comparing multiple datasets at different levels. Also, note the "" ?"" at the end of the query. To find the answer, you need to replace all that with the "" ?"" character. Other Benchmark Results: As you have already seen, there has been a trend in the literature to look at specific benchmark datasets and identify specific problems. So let's look at a single benchmark dataset (BENCHMARK) in this analysis: BENCHMARK Benchmark Problem - Lidar The benchmark dataset that we used for this analysis was the Lidar dataset. It is a dense set of images from the Lidar (or Local Area Network, for short) that visualize the motion of the Earth. A dense dataset is like a lot of individual images crammed together; that is, each pixel in the dataset counts as if it's in an individual image. This means that you cannot see individual features on each pixel, instead each pixel is assigned to a value that relates the distance the object appears to a human using a camera. With Lidar, you get a lot of data on the Earth, and the most efficient way to visualize it is to use large arrays of images with high counts of objects, such as the Lidar data (and the one used in this analysis, Lidar-1). The Lidar data makes this dataset very easy to use, and also makes"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT TOP(1) ?model ?model_lbl., ?dataset orkgp, ?label rdfs:label ?dataset, ?metric_lbl. DISTINCT ?model_lbl. ?. ?label DISTINCT ?dataset_lbl. ? ORDER BY DESC( ?value ) LIMIT 1 Output in RDFS: [[ ?dataset [ ?val] : ?dataset [ ?metric [ ?criterion ""score""]] [ ?metric [ ?criterion ""accuracy""]] [ ?metric [ ?criterion ""avg""]] [ ?metric [ ?criterion ""desc""]]] The results look quite similar. The first row of the DISTINCT statement will not differ much from the top score by using the Metric and Criterion parameters and they provide only a single value. The second row is the same as the first, which can be used to compare the different metrics. Input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flower-102 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl., ?dataset a orkgc, ?label rdfs:label ?dataset, ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; or"	False
"
 input (English text): Could you provide a list of models that have been tested on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Who has contributed to the largest number of articles about coronavirus?
 output (Sparql query): "	"SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), ""2019-nCoV"")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), ""2019-nCoV"")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)"	" [[ ?dataset a orkgc:Dataset] ?model ?model_lbl orkgc:Dataset ORGKMDB ORGJORB ORGKP orkgc:Dataset, ?model ORGKMDB ORGJORB ORGKP orkgc:Dataset], In a large sample, such as the data we have, a subset of tests (but not a full set of tests) were also run on each of the models and the estimated fraction in the posterior distribution of the model was counted with the model parameter estimates for the most significant cluster. The value 1 could indicate that a test of one model was run only once and that the null probability of the entire sample was 0.1. A 0 indicates that no test was run. The test model results are shown in. For each model, we obtained posterior probabilities for the individual clusters as follows: from the model predictions of the posterior, the number of clusters in the posterior, the number of models with cluster boundaries that were identified, and the number of test model predictions in the cluster prediction with the largest fraction of clusters identified. Our final analysis showed the number of models with significant cluster boundaries. Results Model Identification We identified more than 600 of the 622 models. In this section, we report results and discuss the effect of this work on model selection. Model Test Prediction We analyzed the number of test model predictions that occurred in the posterior distribution. Given that test model clusters are always contained in clusters with relatively low posterior probability, we use the ""probability of a prediction not being present"" as the measure of model prediction. A model that has a lower probability of being included is a null model; a model with a higher posterior probability is a model that is a good candidate for inclusion. This value can be expressed"	False
"
 input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT IF(DIF(ORDER by '...', 'MedNLI', '2', ?.. ? { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. }); ? 1 ORDER BY 2... { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } } input (English text): What are the models that have been benchmarked on the DuIE dataset ? output (Sparql query): Finally, I ran a query where I had the ORGPC set to MedNLI output (Sparql query): SELECT IF(DIF(ORDER by '...', 'MedNLI', '2', ?.. ? { ?dataset a orkgc:Dataset; rdfs:label ?datasete_lbl. }); ? 1 ORDER BY 2... { ?dataset a orkgc:Dataset; rdfs:label ?datasete_lbl. } OPTIONAL { ?data_frame orkgc:DataFrame; ?dataset a orkgc:Dataset; rdfs:label ?datasete_lbl. } } input (English text): What are the models that have been benchmarked on the DuIE dataset ? output (Sparql query): The code that runs the benchmarking process is a little complicated, so the output below is simplified. I have removed any	False
"
 input (English text): List the code links in papers that use the Transformer-XL (SGD dynamic eval) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (SGD dynamic eval)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What coolant does ASTRID reactor use?
 output (Sparql query): "	"SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label ""ASTRID""^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, ""coolant"")) ?coolant rdfs:label ?coolant_label. } "	 input (English text): What coolant does ASTRID reactor use ? output (Sparql query): input (English text): What coolant does ASTRID reactor use ? output (Sparql query): input (English text): Which coolant does ASTRID reactor use ? output (Sparql query): input (English text): How high is the cost of making the liquid coolant ? output (Sparql query): input (English text): How high is the cost of making the liquid coolant ? output (Sparql query): 	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Gopher benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Example (Tests 1-4) This example tests the performance of the CACM model, with a single parameter change, compared with the other two measures. It compares our original model with the SIS model. It also compares using a BNF model, with a single parameter change. We use a GOML format label for this example (a) and the model's score. The benchmark is based on an Atari 2600 Berzerk dataset. The dataset consists of two training sets: (i) the Atari 2600 Gopher benchmark dataset, (ii) the Atari 2600 Atari 2600 Games test (A4GGT) dataset. The dataset has the following features: 3 kx3 blocks of random terrain generated within the game with each block randomly selected from a set of four different terrain types. A standard 20x20 grid as the playing field with a 2x2 x1 grid for the players and an 8x8 x4 grid for the map. 32x32x8 grids of squares and boxes in the terrain for a total of 20 grids. 64 possible configurations of the terrain for the player in an open round. The map is randomly generated each time the game is played so that it is a random grid with the same configuration over and over. The game can last for several rounds or it can consist of one to one and up to eight total rounds of the following configurations: 4x4 x2x2 tiles in a 4x4x2 square (0 tiles from each edge), 4x4x4 tiles in a 2x2x2 square (16 tiles), 6x6x6 tiles in an 8x8 x4x4 square (18 tiles), etc. Maps may contain any combinations of tiles. Players do not have visible weapons. The game starts each round with 1 square of terrain for	False
"
 input (English text): What is the best performing model benchmarking the STL-10 dataset in terms of Percentage correct metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model * FROM #model.models WHERE { ?metric ?metric_lbl. ?metric rdfs:label ?metric_lbl. } ORDER BY DESC( ?metric ) LIMIT 1.0 What do you mean by ""the best performing models"" ? I'm not sure which one. I've seen a few. This is the only site that has them listed. The reason there's fewer than 20 models is that, at best, there is only one for each platform. As well, one of the other sites didn't even evaluate the data. (I can't recall which and which I saw in that one, but apparently it was for the SNES where it's still in development.) When I saw the ""best performing"" list I was thinking ""huh ?"". I think I would have appreciated an evaluation of the data. Maybe a list of the platforms we have available so we can evaluate some software, etc. That said, this is a really impressive list of models. It's an amazing first step. Let me know if you have any other good ones. Oh, just got back from a tour in NYC about the ""Stl-10"" project. Let's do better. But first off I think it's worth mentioning the work involved. There are 3 separate teams doing it: 1st (StdLib / Std-Hockey / Std-Track / Std-Track-Hockey), 2nd (Std-Track / Std-Hockey / Std-Track-Track), and I guess 3rd (STL-10). Each one is working independently while sharing a lot of input files. I found the STL-10-Hockey project extremely interesting. I don't know all the details of this project, but it's"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Video Pinball?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT ?dataset ?dataset_lbl; { ?dataset a orkgp:HAS_ROW ?dataset_lbl. rdfs:label ?dataset_lbl. } } } ?dataset_lbl. OPTIONAL{ ?dataset a orkgp:HAS_COUNT ?dataset_lbl. } } } GROUP BY orkgp ?dataset_lbl output (Tables of results from the benchmark tests) (A) ------------------------- Table 1 (M): Table 1 (M) Name Metrics Value Time Latency Score Score ----------------------------- ------------------------------- a, 5:50 3 11,981,737 1.00 9.00 b, 5:52 3 11,979,099 1.10 9.00 c, 5:49 2 11,981,737 1.15 9.00 d, 5:50 3 11,975,002 1.15 9.00 e, 5:51 3 11,979,092 1.21 9.00 f, 5:52 3 5,000,095 1.15 8.00 g, 5:53 3 5,001,015 1.28 8.00 h, 5:54 3 2,957,973 1.15 8.75 i, 5:55 3 2,961,955 1.23 8.25 j, 5:56 3 5,009,942 1.30 8.50 k, 5:57 3 5,000,010 1.40 8.00 l, 5:57 3 4,999,011 1.55 8.00 m, 5:57 3 4,999,	False
"
 input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (Large)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 select :benchmark, :cont, :benchmark from orkgp, XLNet (Large) select :benchmark, :cont, :benchmark from orkgp, XLNet (Large) select :benchmark, :cont, :benchmark from orkgp, XLNet (Large) Output: Benchmark: Code links in reports: ( ?cont) 1 ) ( ?lbl) 1 - 6 10 8 - 16 8-16 - 20 2 ) ( ?lbl) 1 - 6 3 ) ( ?lbl) 1 - 6 3 - 6 16 ( ?lbl) 2 ) ( ?lbl) 4 - 7 4 - 16 8 - 16 4 - 16 16 - 32 6 - 8 - 16 4-8 - 18 Note that it's quite easy to find the code links, we just need to sort by the column. But now for some more detailed statistics. 2 ) ( ?lbl) 4-12 4-10 3-11 7-14 6-10 - 12 2 ) ( ?lbl) 24 4-12 24 - 16 2-5 How many times do these benchmarks return the same code link ? So: Code links in reports: ( ?cont) 1 ) ( ?lbl) 1 - 6 10 8 - 16 8-16 - 20.... 3 ) ( ?lbl) 1 - 6 3 ) ( ?lbl) 1 - 6 3 - 6 16 ( ?lbl) 2 ) ( ?lbl) 4-7 4-10 3-11 7-14 6-10 - 12,.... Note that these stats contain very very long lists of codes. This is probably because there are many benchmarks that are run for more than several lines of code. 4) ( ?lbl) 24 4	False
"
 input (English text): List the code links in papers that use the BERT-joint model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT-joint"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT score AND score+ FROM ?dataset AND p. ?cores OR p. ?processor OR p. ?memory OR p. ?system OR p. ?processors OR p. ?configuration DESC; score AND score+ #4: Use the BERT+ algorithm, with a single point to create a ""true"" solution The BERT+ algorithm is a single point algorithm that is best suited for solving the problem at hand. However, it can also be used to generate the solution, providing more freedom to the user. The BERT algorithm performs best when the goal of analysis is to derive a non-linear function of some value, and this is the case when a BERT+ algorithm is applied to a binary problem with the parameter  . The above code is a simple BERT+ algorithm that calculates the solution to the problem in R. With such a ""true"" solution, it is possible to create a BERT+ algorithm that solves the problem in R. For each of the three models, the BERT+ algorithm returns only an optimal solution that is not exactly the same as the solution in the problem. To illustrate this, I created the following R code that computes the solution for the problem from the input text: #7: Use a BERT+ algorithm for non-linear differential equations There are several BERT+ algorithms that are better suited to solving the problem. However, the method that is best suited to solve such problems has more parameters than the BERT+ algorithm, and this"	False
"
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ' ?matric_lbl.dataset' AS'model_lbl'; ?metric 1 : SELECT ?mapper ?mapper_name as'map'; ?dataset 1 : SELECT ?model ?met	False
"
 input (English text): What is the average installed capacity for each energy source considered in 5 year intervals?
 output (Sparql query): SELECT ?rangeId ?energy_sources_labels (AVG( ?installed_cap_value AS ?avg_installed_cap_value)) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?paper orkgp:P31 ?contrib; orkgp:P29 ?year. BIND(xsd:int( ?year) AS ?y). VALUES( ?rangeId ?min ?max) { (""2001-2005"" 2001 2005) (""2006-2010"" 2006 2010) (""2011-2015"" 2011 2015) (""2016-2020"" 2016 2020) } FILTER( ?min <= ?y && ?y <= ?max). ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value). } ORDER BY ASC( ?rangeId)
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?r2. { ?datasetName ?r2 orkgr: ?energy_source_labels(AVG( ?installed_cap_value AS ?avg_installed_cap_value)) ?datasetName } FROM r2_totals ORDER BY ?r2. ?energy_source_labels R: ?r2. ?energy_sources R: ?lid(datasetName) INNER JOIN model ?energy_sources ON model. orkgr ?s ?energy_source_labels=r2	False
"
 input (English text): List the code links in papers that use the Linear Transformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linear Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the CommitmentBank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Input Data Model Name: Orkgc Model Type: Model.Sparql.Linear Transfurer Model Length: 7.45 MB Time to Read Time to Write: 952 ms Total Data Read: 2.5 MB Total Data Written: 1.9 MB Total Parsed: 10.9 MB Data Download Model Name: Orkgc Model Length: 3.85 MB Model Data Format Date Time Datetime Size Description Datum: orkgc:Orkgc.Sparql.Linear Transformation. Type: Model.RDFS Varying Format. Sparql.Linear Transfurer RDF Reference (data link): Input Name: CommitmentBank_Sample Output Name: CommitmentBank_Data.RDF RDF Data Link User Generated Documentation User Generated Documentation User Generated Documentation This post is part of a series of posts focusing on the performance and features of the SQL Server for Dummies content series. The next post in this series is about Performance Benchmarks for the Orkgc model. Subscribe to the blog feed to not miss any future posts.	False
"
 input (English text): Where can I find code references in papers that have used the BBG (ResNet-18) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BBG (ResNet-18)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model ? output (Sparql query): input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model ? output (Sparql query): input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model ? output (Sparql query): Input: Description ResNet-18 Model This is a resnet-18 model for evaluating performance for an object recognition task. This model has been used for two benchmarking studies: the first was the ""Dinnerbone"" Benchmark (2012) and the second was the ""Dinnerbone+"" Benchmark for ""Dinnerbone"" classifiers (2013). If you are interested in the original source code for this model, please contact the author. ResNet-18 Resnet-16 The ResNet-16 model from ResNet-50 was used for this research. Please contact the author. ResNet-19 ResNet-40 This is a resnet-19 model for evaluating performance for classification tasks. The original source code is at the github repo: github.com/bzuz/ResNet/master ResNet-10 ResNet-10 Model This is a ResNet-10 model for evaluating performance for classification tasks. The source code is at the github repo: github.com/bzuz/ResNet/master ResNet-18 ResNet-20 This is a ResNet-18 model for evaluating performance for classification tasks. The source code is at the github repo: github.com/bzuz/ResNet/master If you have any queries about the source code of the model or what it is about you"	False
"
 input (English text): Indicate the model that performed best in terms of RE+ Macro F1  metric on the CoNLL04 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Macro F1 "") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT statement need to follow the HTML formatting rules which include an attribute ""http://www.w3.org/1998/xhtml"" in the query as well as an attribute ""https://schemas.openxmlformats.org/"" in the RDFS response string. The parameter value is passed without any attributes. In addition, an attribute attribute will be added in the HTML code for a GET request for the model. The parameter value will be automatically extracted from the variable name if the variable appears in a query such as ?model. This syntax is commonly used to pass the model string to the GET request of the CoNLL00. What parameter values are included in the parameter value ? The parameter values are included to help specify which parameters are needed when you run the model. They do not affect the value of your query or model. Some Parameters are included: A parameter is used by two methods. If the parameter value is not included in the model, the parameter is returned as NULL. parameterName is required when using RDF features. Parameter values are not generated in this case. This is because the data and metrics data are not available in a RDF vocabulary or model vocabulary. parameterName is optional when using RDF features. Parameter values are not generated in this case. This is because the data and metrics data are not available in a RDF vocabulary or model vocabulary. parameterName can be a"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?query_id[1][:] FROM ?query WHERE { ?query_id. ?query_id }} { :analytics.metric 'accuracy' }} You also have this option. You can list only queries that met the criterion. You must include metrics in the ""Parameters"" column. The metrics to be applied to queries cannot be modified within the query. They are returned by the query's execution. Input (English text): Which query ( query_id[1][1]) performs best in terms of Accuracy metric on the Natural Questions benchmark dataset ? output (Sparql query): SELECT ?query_id[1][1] FROM ?query WHERE { ?query_id. ?query_id }} { :analytics.metric 'accuracy' }} We provide a second option that allows you to query results as a JSON. This functionality is useful when the dataset contains multiple models that are not all using DAL. The JSON query contains query attributes that describe the query. The data_attribute is optional data in which the query attribute is required. If the JSON is not included in the query, it is returned without the query. Note: In this case the data_attribute would be used to store the result of the query. Note that you can specify the name of the model of models as well as the list of the models that met the criterion. To get"	False
"
 input (English text): What are the models that have been benchmarked on the Atari 2600 Video Pinball dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT MAX(p_model.p_evaluate, 0); MAX(p_model.p_tos, 0); MAX(p_model.p_fcs, 0); MAX(p_model.p_dcs, 0); MAX(p_model.p_mcs, 0); MAX(p_model.p_hcs, 0); MAX(p_model.p_stks, 0); MAX(p_model.p_spikes, 0); MAX(p_model.p_spikes_bgs, 0); MAX(p_model.p_lgts, 0); MAX(p_model.p_bgb, 0); MAX(p_model.p_bgs, 0); MAX(p_model.p_dbs, 0); MAX(p_model.p_spikes, 0); MAX(p_model.p_spikes_lgts, 0); MAX(p_model.p_bgt, 0, MAX(p_model[0].p_vcpc,0).p_val); * (1 row(s) affected) input (English text): What are the evaluation metrics commonly used when benchmarking models on the STEM-ECR v1.0 dataset ? output (Sparql query): input (English text): What were the results of the benchmarks of these features in the Atari 2600 Video Pinball dataset ? output (Sparql query): SELECT N( ? ?); N( ? ?); N( ? ?); N( ? ?); N( ? ?); N( ? ?); N( ? ?); N( ? ?); N( ? ?	False
"
 input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?metric ?metric_lbl(MAX_VALUE) FROM dbs.test_dataset ORDER BY DESC( ?value ) This query can be very helpful for the development of Multi-Perspective Matching (MCM) for Walker and Walkers dataset. Multi-perspective Matching (MCM) for Walkers and Walker This code will match multiple groups of points at 90 degrees line of sight. The data (Walker and Walkers) are presented in two dimensions and it follows the same way as the multivariate normalization method found in the paper ""Dynamics of a single population group (Walker and Walkers)"" (Koole 2009). This approach is very close to the one provided by the paper in the references section (see code) def match_data ( data, number_of_groups = - 1, n_groups = 1, number_of_iterations = 1 ): """"""Match groups of points at 90 degrees line of sight. The group sizes [0, number_of_groups] can be specified for each observation """""" for group in data [ 0 ]: if not match_data ( group, 0, :) : raise ValueError () if number_of_groups > 0 : match_data ( data [ number_of_groups - 1 ], group, n_groups, number_of_iterations ) return group def get_mpm4 ( data, m = 1000, match = _match_data, nth = 3 ): """""" Return mpm4 values for each observation """""" return MatchingClasses ( m, match = _match_data, nth = nth ) Input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset ? output (Sparql query):"	False
"
 input (English text): Where can I find code references in papers that have used the AVID model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AVID"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the GAD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT id FROM books, ?model(id) AS ?model_id. input (English text): Can you give an example of a benchmark that uses the AVID model to find the optimal benchmark sequence ? output (Sparql query): SELECT 'benchmark' AS result, ?model(id) AS ?model_id, ?input(id) AS ?input_id, ?benchmark_s1 AS rd1. ?benchmark_s2 AS rd2. ?test orkgp:HAS_DATASET, ?benchmark_s1_lbl as rd1_lbl. ?test orkgp:HAS_DATASET, ?benchmark_s2_lbl as rd2_lbl. ?dval orkgp:HAS_DATASET, ?dval orkgp:HAS_DATASET, ?max (avg_s1_lbl_seq, avg_r1_lbl_seq) AS max(avg_s2_lbl_seq, avg_r2_lbl_seq), ?min (avg_s1_lbl_seq, avg_r1_lbl_seq) AS min(avg_s2_lbl_seq, avg_r2_lbl_seq), ?hits orkgp:HAS_DATASET, ?hit orkgp:HAS_DATASET, ?min (avg_s1_lbl_seq) AS min(avg_s2_	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Pitfall! dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"--------------------------- RESULTS { ""Metric"" : { ""Score"" : 9.2, ""Mean SD"" : 0.94, ""Max Score"" : 11.8, ""Difference"" : -0.34. ""Measure"" : { ""Metric"" : ""score"" }, ""Value"" : ""11.87"". NULL } } I'm using the R package R for this post (see this recent  blog post  for how to use it to run SAS queries with R). You can get it from https://cran.r-project.org/.  You can see that it's based on the R data frames library. You need the bsddb package if you want to build bsddb directly from the R source and not with the SAS API, or to do more complicated manipulations with R data frames such as averaging, filtering, and pivoting.  It's actually possible to use a SAS implementation for bsddb, but I'll use R to do a simpler analysis of the outcomes. So let's start with a relatively simple SAS API and run some SAS queries.  Just the original dataset: SAS/SSAS -r 1000 5000 100000 2000000 5000000 10 ** * 10 ** * 10 ** * 10** * 10** * 10** * 10** * 0.054 So far so good, the dataset contains the following metrics : Score(D) : Determines whether to run more runs (default 1) Mean SD(D) : Determines whether to run less runs (default 1) Max Score: The highest benchmark result (for this benchmark, this was a 5-5 tie for 1st place) Difference (D) : Determines how much to compare (default 1) Measure(D) : D"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	------------------------- Metric description ---------- METRES: 100 meters KPH: 7.5 KPH RDAH: 32.5 meters PERIOD: 1 minute, 32 seconds MPS: 40 m/s RPM: 600 RPM RDAH: 8 meters METRES: 1000 meters KPH: 5.2 KPH RDAH: 60m Metres: 100 meters KPH: 15.5 KPH RDAH: 96 meters METRES: 1500 meters KPH: 30.5 KPH RDAH: 120 meters RPM: 6000 RPM RDAH: 200 meters RDAH: 5 hours, 8 minutes MPS: 200 m/s RPM: 600 RPM RDAH: 25 meters KPH: 14.2 KPH RDAH: 30m RDAH: 50 meters RDAH: 75 meters RDAH: 90 meters RDAH: 130 meters METRES: 200 meters, 1000 meters KPH: 15.5 KPH RDAH: 90 meters RDAH: 70 meters RDAH: 50 meters METRES: 250 meters, 700 meters KPH: 15.5 KPH RDAH: 70 meters RDAH: 50 meters RDAH: 50 meters METRES: 3000 meters, 1000 meters KPH: 15.5 KPH RDAH: 90 meters KPH: 30 meters RDAH: 60 meters RDAH: 75 meters RDAH: 90 meters RDAH: 90 meters RDAH: 30 meters RDAH: 200 metres, 500 meters KPH: 13.9 KPH RDAH: 90m KPH: 30m RDAH: 50m METRES:	False
"
 input (English text): What are the metrics of evaluation over the BUCC French-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Outputs: // Result: Output: Output: RDDs and DataFrames RDDs and DataFrames are extremely useful for conducting and interpreting the experiments on the BUCC French-to-English dataset. In RDDs, we can use a RDF reference object (reference) to refer to a dataset containing relevant metrics. RFFT RFFT is an RDF syntax for RDF vocabulary generation. RFFT allows to easily generate a vocabulary from a collection of RDF vocabularies. RFFT has been specifically designed for use during automated translation experiments. RFFT syntax (RDF 3.1), created by RDFa, is used to describe vocabulary with the RDF vocabulary format and has been adopted by RDFa in the form of the RDFa Lexical Information Extractor (RDFa KIIE). If we choose to build a vocabulary for the dataset, we run the command tools/Rfft-spatial-tools run Which generates 3 files in a single directory. The first file is a ""toolset"", which should just hold metadata for the first RFFT file, the second file is a ""spatial"" RDF reference object (reference), and the third is a ""SpatialQuery"" RDF reference object. The ""spatial"" RDF reference object is the metadata that is used to refer to the data, and the ""SpatialQuery"" object is a set of objects that defines the spatial query in which to perform the first RFFT run. The ""spatial query"" (RDFa KIIE syntax) is an object that specifies the metadata with which to refer to the data. If additional RDF vocabulary items are required, they can be made available in the space query by specifying a URI and"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoNLL04 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AxCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Please supply a list of papers that reference your model. input (English text): Which papers reference your model ? output (Sparql query): Please provide a list of papers that reference axc2.org for benchmarking purposes ? input (English text): Which publications have used axc2.org for benchmarking purposes ? output (Sparql query): [ N (all) ] Please supply a list of papers that have used the CNTK model for benchmarking purposes ? input (English text): Which publications have used the CNTK model for benchmarking purposes ? output (Sparql query): [ N (all) ] Please supply a list of papers that have used the AxGroup model for benchmarking purposes ? input (English text): Which publications have used the AxGroup model for benchmarking purposes ? output (Sparql query): [ N (all) ] Please supply a list of papers that have reported on the results of benchmarking on different datasets: CNTK and AxGroup. input (English text): Which publications have reported on the results of benchmarking on different datasets ? output (Sparql query): Please supply a list of publications that have included benchmarks on different datasets within their original publication ? input (English text): Which publications have included benchmarks on different datasets within their original publication ? output (Sparql query): Please supply a list of publications that include benchmarking on different datasets, but which you think should not have been benchmarked on ? input (English text): Which publications have included benchmarking on different datasets, but which you think should not have been benchmarked on ? output (Sparql query): Please supply a list of papers that have made reference to the ""Benchmarking on CNTK"" section in their"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the PIQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 This should give you enough data to build a basic model using AWD and LSTM. 3.2.1. A Python library for AWD The Python library for LSTM is called PythonSparkl ( http://pythonsparkl.com ). It comes with the following modules: Sparkle. AWK : this module is used for manipulating a LSTM model, and also for storing the results of benchmarking. . : this module is used for manipulating a LSTM model, and also for storing the results of benchmarking. Spark. STM : This module is the same as STM, however it has a more intuitive API, so it's usually included in models where you want to run on small samples. 3.2.2. Building the model¶ You can use Spark in a number of ways, mostly using a GUI, but most common are via one of several online tools: The standard GUI for this is called Benchmark, to test the model. The tool automatically picks a random sample of the data and runs it many times, and the results are shown on the screen. Alternatively, you can use the online tool by running sparkc@> test.py <model, model_options> (the code below requires python 2.7; the code works without python if you use Spark 3.4): test.py : This code executes the test given a model, specifying it and some parameters (such as some values to return for the model response variables). The main difference from the standard test is that you can specify the amount of data to run, as well as how to write output results. See sample output and benchmark information for more information on testing. The result file is loaded into the database, and it will be available on the web page as 'data/sample.csv' from now on (the code	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Flowers-102?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"_____________________________________________________________________________ Numeric ID | Paper | Title _____________________________________________________________________________ 44961 Flower-102, A., et al. (2007) Flowering the landscape: natural and synthetic flowering as an indicator of climate change response. Environ. Res. Lett., doi: 10.1088/1755-6136/8/1/014005 _____________________________________________________________________________ How to cite this post: http://blogs.scientificamerican.com/hugo/2013/01/the-new-science-battles-against-climatology-to-control-our-climate/ What this post is for: this post is to provide background, tools and a step-by-step method to use these tools to support better analysis and visualization of this paper. The tools are: rdfs, spark, a little package called spark/rs, and spark/sss. I did a few articles about the data that I used. My data set is from the paper ""Flowers-102, A. (2007) Flowering the landscape: natural and synthetic flowering as an indicator of climate change response. Environ. Res. Lett., doi:10.1088/1755-6136/8/1/014005"" (p. 9). The data that is here is from the publication ""Flowers-102, A. (2007)"" from the Journal of the American Statistical Association: Scientific and Technical Papers, 1 (3) December 2007: 1235-1150. Here are the highlights of their paper: *      Flowering the landscape: natural and synthetic flowering as an indicator of climate change response,      A. Flowering the landscape, Journal of the American Statistical Association, DOI:10.1177/000312552070150 *      Flowering the landscape"	False
"
 input (English text): Which scholarly knowledge graphs support RDF?
 output (Sparql query): SELECT ?title WHERE { orkgr:R78023 orkgp:compareContribution ?cont. ?cont orkgp:P7009 ?has_rdf. FILTER(REGEX( ?has_rdf, ""T"")) ?cont rdfs:label ?title } 
 input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT name FROM (orkgr:R78023 orkgp:compareContribution) input (English text): Which database has this dataset in ? output (Sparql query): input (English text): Select the name(s) of the top 200 datasets from the Image Classification research problem. output (Sparql query): #1 : [[ ?title ]]. ?title #2 : [[ ?title ]]. ?title #3 : [[ ?title ]]. ?title input (English text): Select the name(s) of the top 50 datasets from the Deep Learning Research Problem. output (Sparql query): input (English text): Select the name(s) of the most important datasets from the Image Classification research problem. output (Sparql query): input (English text): Select the name(s) of the Top 20 datasets from the Deep Learning Research Problem. output (Sparql query): input (English text): Select the name(s) of the Top 20 datasets from the Deep Learning Research Problem. output (Sparql query	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Phoenix dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT HAVING ( ?dataset a orkgc:Dataset) > { ?dataset_lbl orkgc:Benchmark; } / ?dataset rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson pointGoal navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?dataset rdfs:label ?dataset_lbl. OPTIONAL { ?dataset_lbl orkgc:Benchmark; } ( ?dataset rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson pointGoal navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?dataset rdfs:label ?dataset_lbl. OPTIONAL { ?dataset_lbl orkgc:Benchmark; } ( ?dataset rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson pointGoal navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?dataset rdfs:label ?dataset_lbl. OPTIONAL { ?dataset_lbl orkgc:Benchmark; } ) Input of data: Input to model: a data set Input of criterion: score score Output if the evaluation of the model was successful: an object of the kind of {metric or metric_lbl} Output about model: if metric or metric_lbl was evaluated, a document"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Seaquest benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT MODEL_NAME, DISTINCT ?metric_name ?metric_lbl; mod:MODEL_TITLE ?metric_name_lbl OPTIONAL { ?metric_lbl ?metric_name. } OPTIONAL mod:MODEL_NAME, DISTINCT ?metric_name_lbl OPTIONAL mod:MODEL_TITLE, DISTINCT ?metric_name_lbl OPTIONAL mod:MODEL_TITLE, DISTINCT ?metric_name_lbl RETURNS dbo.Models.ModelResultSet. DbResultSet (... model_name, score, metric, id ) output (Sparql query): dbo.Models.ModelResultSet. MODEL_NAME OPTIONAL dbo.Models.ModelResultSet. score OPTIONAL dbo.Models.ModelResultSet. metric OPTIONAL dbo.Models.ModelResultSet. id RETURNS dbo.Models.ModelResults. DbResultSet (... id, score, metric, id ) input (English text): Choose an Input Dataset and define the parameter names. Input is a CSV file and Output is an SQL query. (DMControl500k) output (Sparql query): select id, score, metric in Models.Models.Metric. input (English text): Choose a Model and define the parameters. (DMControl500k) output (Sparql query): select MODEL_NAME, DISTIN	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy (2 classes) metric on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (2 classes)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (2 classes)"" ) { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. } ORDER BY DESC( ?value ) LIMIT 1 } } input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset ? output (Sparql query): input (English text): Indicate the model that performed best in terms of Accuracy (2 classes) metric on the IMDb benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (2 classes)"" ) { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl."	False
"
 input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Example 4: Senseval 2 Model: L1: ~~ model:Supervised: ~ model:L1: ~~~~~ data:Natural Questions.dataset.bases_tbl:L1 model:L2: ~~ model:Supervised: ~ model:L2: ~~~~~ data:Natural Questions.dataset.bases_tbl:L2 model:L1: ~~~~~ model:Supervised: ~ model:L1 ~~~~~ data:Natural Questions.dataset.bases_tbl:L1 model:L3: ~~~~~ model:Supervised: ~ model:L3 ~~~~~ data:Natural Questions.dataset.bases_tbl:L3 model:L2: ~~ model:Supervised: ~ model:L2 ~~~~~ model:Supervised: ~ model:L3 ~~~~~ orkgp:HAS_DATASET ? model:L3 ? data:Natural Questions.dataset.bases_tbl:L1 model:L2 orkgp:HAS_DATASET ? model:L1 orkgp:HAS_DATASET ? model:L2 orkgp:HAS_DATASET ? model:L3 ? data:Natural Questions.dataset.bases_tbl:L3 model:L3 orkgp:HAS_DATASET ? model:L2 orkgp:HAS_DATASET ? L1: ~~ model:Supervised: ~ model:L1: ~~~~~ data:Natural Questions.dataset.bases_t	False
"
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Solving the puzzle Now that the code to compute the model is complete, I used the PyLAMBDA library to analyze the model by learning the model's parameters and learning a metric for the model. To do this, I used Python's lwjgl module, which allows me to write Python code to analyze and execute SQL queries. The Python code to learn the parameters and train the model for an Unweighted Linear Model is as follows: # Load the dataset from the training file. from cffi.models import model_lbl_from_tutorial as lbl_from_tutorial import scipy.stats import scipy.optimize.target2 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from.cffi import F, B # Load the model from the trainer script from sgd import T # Create the learning set for the model from the trainer # files: t1 = lbl_from_tutorial.Train() t2 = lbl_from_tutorial.TBD() t3 = lbl_from_tutorial.P0(1) t1_train = T.from_tutorial(t1) t2_train = T.from_tutorial(t2) t3_train = T.from_tutorial(t3) # Choose the learning rate for both models, 100 and 1 train_rate = tf.Variable([100,1], dtype=tf.float32) train_rate_lr = tf.Variable([100,1], dtype=tf.float32, name='learning rate') train_rate = T.from_tutorial(t3) # Create the metrics for the model from the training set (with different weights for each iteration): train	False
"
 input (English text): List the code links in papers that use the SAN (single model) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?result_lbl ? AS result_lbl_lbl WHERE { ?model ?data ( ?model_lbl as ?dataset_lbl) } ?dataset rdfs:result_lbl ?dataset_lbl. || You could not use multiple sets of model columns for each benchmark because it would use up the set of model columns in the previous benchmark. Example output (Sparql query): SELECT ?result_lbl ? AS result_lbl_lbl WHERE { ?model ?data ( ?model_lbl as ?dataset_lbl) ?dataset rdfs:result_lbl ?dataset_lbl. || } No, it is not that difficult to work with multiple sets of model column fields. However, most of the benchmarks have multiple model columns in them. The following solution is the least useful way of solving the problem. As the first approach we just pick the least significant columns in the input data to be used in the model; in this case we select 10 rows as models and the output for the output table is 10 rows. Then this is our model. Second, we need a way of specifying model	False
"
 input (English text): What are the metrics of evaluation over the GigaWord dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GigaWord"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the most common lead compound?
 output (Sparql query): "	SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1	SELECT ' ?lead=%{c:%{type}%{name}%'. 'C'', '%{p:%{c:%{type}%{name}% }'. 'P', '# %{c:%{type}%{name}% }'. '#' input (English text): What is the most common lead compound ? output (Sparql query): SELECT '# ''. '#' input (English text): What is the most common leading element ? output (Sparql query): SELECT '''. '', '*''. '(' ', '*''. '(' ', '. '''. '.', '.'. '.)' input (English text): What is the most common leading element ? output (Sparql query): SELECT '''. ', '*. ', '.'. ''. '.', '.'. '.--'. '.', '.'. '--' input (English text): What is the most common leading element ? output (Sparql query): SELECT ' ?lead=%{c:%{type}%{name}%}; C'.'C', '%{p:%{c:%{type}%{name}% }}.'' P', '%{p:%{c:%{type}%{name}% }}.'' p', '%{p:%{c:%{type}%{name}% }}.'' p', '%{p:%{c:%{type}%{name}% }}.'' p', '%{p:%{c:(%{c:%{type}%{name}%})%}%}; C'.	False
"
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-3 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT TOP (score) ?index ?id ?dataset ?model_lbl.score OR(score = { ?index ?id ?dataset ?model_lbl.timeline ?model_lbl.score, ?dataset ?id. ?data[ ?id]: ?dataset ?model_lbl.timeline ?dataset. ?data[ ?id] ?dataset. ?model_lbl.score ?dataset. ?data[ ?id]. ?model_lbl.timeline ?dataset. ?data[ ?id]. ?dataset. ?model_lbl.score ?dataset. ?data[ ?id]. ?dataset. ?model_lbl.score ?dataset. ?dataset_lbl.score ?dataset. ?dataset_lbl.score ?dataset. ?dataset_lbl.score ?""Bench"" ?label. Score. ""ROUGE-3"" output (Sparql query): SELECT TOP (score) ?index ?id ?dataset ?model_lbl.score OR(score = { ?index ?id ?dataset ?model_lbl.timeline ?model_lbl.score, ?dataset ?id. ?data[ ?id]: ?dataset ?model_lbl.timeline ?dataset. ?data[ ?id] ?dataset. ?model_lbl.score ?dataset. ?data[ ?id]. ?dataset. ?model_lbl.score ?dataset. ?data[ ?id]. ?dataset. ?model_lbl.score ?"	False
"
 input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"select (1) ""C51 Noop Model for Feature Value Assessment"" from papers ORDER BY id DESC LIMIT 1 output (Sparql query): select (1) ""IMDb-M"" from papers ORDER BY id DESC LIMIT 1 ORDER BY paper_id DESC LIMIT 1 output (Sparql query): select (1) ""C51 Noop Model for Feature Value Assessment"" from papers ORDER BY id DESC LIMIT 1 ORDER BY paper_id DESC LIMIT 1 DESC( ?metric ?model; ?dataset  orkgc  orkgc_lbl;...;...;...,...,...,...); DESC( ?dataset  orkgc; ?dataset_lbl;...;...,...,...,...) DESC( ?metric  ""IMDb-M"";...;...,...,...,...;...;...),... ORDER BY paper_id DESC LIMIT 1. I got nothing, I've tested it with the code I originally wrote this post with, as pointed out by a commenter above. Not a complete go, but hopefully we can do better. For the full code, see here. I've added an extra few comments to make note of all the caveats. -T PS: I didn't even know about BUGCHECK! or BUGCHECK_WARN until I looked this up on my own laptop..."	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Cartpole, swingup (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the FABIR model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FABIR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- FABIR.predict(data, label=NULL, metric=NULL, criterion=NULL, minvalue=0) FABIR.predict(data, label=NULL, metric=NULL, criterion=NULL, minvalue:0) FABIR.predict(data) ----- FABIR.predict(data) FABIR.predict(data, label=NULL) FABIR.predict(data) FABIR.predict(dataset) FABIR.predict(dataset) ---- FABIR.predict(dataset) FABIR.predict(dataset) ---- FABIR.predict(dataset) FABIR.predict(dataset, label=NULL) FABIR.predict(dataset, metric=NULL) FABIR.predict(dataset, criterion=NULL) FABIR.predict(dataset) output (Sparql query): --------- FABIR.predict(data) -------- FABIR.predict(data, label=NULL) -------- FABIR.predict(dataset) -------- FABIR.predict(dataset) -------- FABIR.predict(dataset, label=NULL) -------- FABIR.predict(dataset, metric=NULL) -------- FABIR.predict(dataset, criterion=NULL) -------- FABIR.predict(dataset) input (English text): What is the most valuable characteristic, in terms of the cost model ? output (Sparql query): ---------- R3.1.2 DML_A = FABIR.predict(X.x_n, Y) R3.1	False
"
 input (English text): Can you list the models that have been evaluated on the IWSLT2015 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the emergency type of AMBER Alert?
 output (Sparql query): "	SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } 	SELECT ?emergency OPTIONAL {  ORDER BY ?priority ; ?priority rdfs:label ?emergency ; ?priority ( ?priority) rdfs:filter ( ?priority < ?priority + 1) ?emergency } The query that we just defined returns:  'Emergency;3' I am sorry that I did not come up better, but I hope that the code I'm developing to analyze my data in a simple and intuitive way for others (as you can see from my website ) will help people of an active interest in the IWSLT 2014 conference. References:  http://www.ietf-isi.org/index.php ?option=com_content&task=view&id=38&Itemid=1501&catid=38&Itemid=2148&Itemid=1955&Itemid=3148 http://www.ietf-isi.org/index.php ?option=com_content&task=view&id=38&Itemid=1417&catid=38&Itemid=2248&Itemid=1955&Itemid=3248 The IWSLT 2014 Data Visualization Project  http://www.iwslt-project.org/projects/visualization/index.html	False
"
 input (English text): What are the metrics of evaluation over the CL-SciSumm dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT metrics FROM CL-SciSumm, ?dataset ?dataset, metrics DESC; SELECT metrics FROM ?dataset_lbl, metrics, metrics DESC; SELECT metrics FROM ?dataset, metrics, metrics, metric; And now you can see which metrics are used, in particular, select the first metric for Evaluation over the CSV file: CL-SciSumm-Benchmark-RDFS-20170802RDF S7.1.2.1-M3.1.2.1p1-S7.1.2.1-M3.1.2.1p2-S7.1.2.1-M3.1.2.1p3-S7.1.2.1-M3.1.2.1p4-S7.1.2.1-M3.1.2.1s5.5.5-S7.1.2.1-M3.1.2.1s6.7.7p1-S7.1.2.1-M3.1.2.1s7.7.7p2-S7.1.2.1-M3.1.2.1s8.8.8-S7.1.2.1-M3.1.2.1s10.10.10-S7.1.2.1-M3.1.2.1S11.11.11.11.11-S7.1.2.1-M3.1.2.1S12.12.12.12.12-S7.1.2.1-M3.1.2.1S13.13.	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset_lbl rdfs:descr ?dataset_lbl. ORLSE ORLSE. } ORDER BY DESC( ?dataset_lbl) LIMIT 2 } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Input the list of columns that have been removed from the dataset and the result of the SQL SELECT statement (optional"	False
"
 input (English text): What models are being evaluated on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT...; output (Sparql result): SELECT RANKS[0] ?acq:test ?rank 1.0 2.0 3.0 4.0 ?rank 0.90 1.00 3.00 4.00 ?rank 0.50 1.00 2.00 3.00 ?rank 0.00 100.00 1.00 3.00 ?status:published | unpublished ?acq:test | [ ?acq:test ] ?acq:test | ?label 1.0 | 1.20 | ?label 0.90 |.10 | ?label 0.50|.10| ?label 0.00 -100.00 \ ?acq:test | ?label 1.40 | 1.00 | ?label 0.00.10-100.00 \ ?acq:test | ?score 1.00 |.80 | ?score 0.100 >100.00 No result. input (English text): What are the authors of the papers whose titles and IDs include an evaluation from the Atari 2600 Defender dataset ? output (Sparql query): SELECT author AND author_rnd IN (ORDER BY rank) OR rank ORDER BY author AUTHOR; output (Sparql result): SELECT author ?author_rnd OR rank; output (Sparql result): SELECT authors... -- author id -- author 1 -- author 2... OR author_rnd ?author_rnd OR rank; The above query provides an overview of the above data. This section is intended to be a quick reference to the results of this exercise (and not exhaustive). In the next post, I will review several other datasets that could use a quick look after using the MLDoc Zero-Shot English-to-French	False
"
 input (English text): Which model has achieved the highest Bit per Character (BPC) score on the Penn Treebank (Character Level) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bit per Character (BPC)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer + Coverage + EntailmentGen + QuestionGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT [ ?metric ?attribute_gkc] AS ? ?attribute_lbl., ?attribute_gkc. ?attribute_gkc ?attribute_lbl., ?attribute_gkc. ?attribute_gkc. ?attribute_lbl. ?attribute_lbl. ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ?attribute_gkc. ?attribute_lbl. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ? ?attribute_lbl., ?attribute_gkc. ?attribute_gkc. ?attribute_lbl. ? } ORDER BY { ?metric ? orkgc:Metric, ?attribute= ?attribute_gkc, ?attribute_gkc. ? } LIMIT 1 The first two questions have been previously discussed in our blog post that was focused on training RDDs of different character categories. The last question has been discussed in this followup blog post.The goal of our benchmarking benchmark is to benchmark a large number of different models at multiple character levels. Our benchmarking is also a way to verify that a model is performing well and we wanted to try out several different models for further validation. So we chose the Pointer + Coverage + EntailmentGen model for testing purposes. It was the choice from the selection of two main reasons:It's interesting	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoNLL04 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } We have a great dataset at CoNLL04 but it has several problems and many papers use only limited data. We would like to compare the performance of those papers to Benchmark 2.0 and to the paper that used CoNLL3. If you have any feedback we would love to hear from you! This data is publicly available on co-nll.acm.org and you are encouraged to download and use the data for your research. This work was also supported in part by a grant from GEOINT."	False
"
 input (English text): What is the average energy generation of all energy sources considered?
 output (Sparql query): SELECT (AVG( ?elec_gen_value) AS ?average_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?electricity_generation. FILTER(REGEX( ?energy_sources_labels, ""all sources"")) ?electricity_generation orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } 
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT rdfs:label ?energy_sources rdfs:label ?energy_sources rdfs:label ?energy_sources rdfs:label ?energy_sources rdfs:label ?energy_sources rdfs:label ?energy_sources rdfs:label ?energy_sources rdfs:label OR kgr:R153801 orkgp:compareContribution ?contrib. orkgp orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp This query returns the list of data types that the kgr object recognizes (and has no equivalent for other ones). For example the list of energy sources recognized by kgr. input (English text): Input : input (English text): What is the energy source generated from _________ for 100% of the time ? output (Sparql query): output( ?energy_source_given, R^2) Output: input (English text): Input : input (English text): What is the energy source generate for _________ and never used ? output (Sparql query): output( ?energy_source_given, R^2) Output: input (English text): Input : input (English text): What is the energy source generated from _________ for 100% of the time ? output (Sparql query): output( ?energy_source_given, R^2) Output: Output : input (English text): Input : output (English text): What is the energy source generate for _________ but never used ? output (Sparql query):	False
"
 input (English text): Indicate the model that performed best in terms of ROUGE-L metric on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-L"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?model.model_lbl orkgp:HAS_DATASET ?dataset. ?dataset_lbl. } output (Hex format): input (English text): The following R code performs the following benchmark: val val = 0 val lwt = 0.1 ROUGE-L 1.50 1.25 1.50 1.05 1 1.25 ROUGE-L 1.35 1.25 1.34 1.08 1 1.25 ROUGE-LB 1.43 1.45 1.50 1.14 1 1.25 ROUGE-LR 1.26 1.28 1.46 1.10 1 1.36 ROUGE-LT 1.18 1.22 1.54 1.10 1 1.24 ROUGE-LR 1.09 1.18 1.37 1.13 1 1.17 ROUGE-LB 1.06 1.14 1.45 1.11 1 1.18 ROUGE-LM 1.03 1.12 1.45 1.10 1 1.07 ROUGE-LK 1.09 1.13 1.47 1.10 1 1.10 ROUGE-HL 1.10 1.14 1.48 1.09 1 1.10 ROUGE-LI 1.17 1.11 1.56 1.12 1 1.16 ROUGE-LB 1.33 1.15 1.51 1.10 1 1.20 ROUGE-LK 1.35 1.17 1.51	False
"
 input (English text): What are the models that have been benchmarked on the WSC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Outputs: List the codes and the abstracts and the citation data. BESTSELLER input (Excel spreadsheet): A3C-CTS code and abstract for the paper. output (Sparql query): BESTSELLER input (Excel spreadsheet): A3C-CTS model definition using the A3C-CTS method. output (Sparql query): BESTSELLER input (Excel spreadsheet): A3C-CTS model code and abstract for the paper. output (Sparql query): BESTSELLER input (Excel spreadsheet): A model on the A3C-CTS methodology using a subset of the original dataset. output (Sparql query): BESTSELLER input (Excel spreadsheet): A model on the A3C-CTS methodology using the most useful subset of the original dataset. output (Sparql query): BESTSELLER input (Excel spreadsheet): A model on the A3C-CTS methodology using the most useful dataset for the benchmarking. output (Sparql query): BESTSELLER input (Excel spreadsheet): The A3C-CTS methodology, including a model definition file. output (Sparql query): BESTSELLER input (Excel spreadsheet): A3C-CTS model definition using a subset of the original dataset. output (Sparql query): BESTSELLER input (Excel spreadsheet): A3C-CTS model definition using the most useful	False
"
 input (English text): Indicate the model that performed best in terms of RE Micro F1 metric on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } You can find an example on using an RDFS to perform a SPSS for the example above in the RDFS example. SPSS and RDFS performance SPSS SPSS in RDFS A number of different SPSS packages are used in RDFS. Among these SPSS packages there are several which are used for performing some more advanced analyses. Note that if you are working with large datasets you probably do not need to use SPSS in large datasets because it is relatively expensive. SPSS is mainly a file format used to store data and therefore the RDFS"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Multimodal PISA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" The paper has a title in English, which would be OK as long as it has a given title in the other languages (e.g. Deutsch, Italiano, etc.). For example, if the paper has two different authors, the results would not be correct. Furthermore, the output should be translated into the other language as well, because the paper may contain proprietary results that are still subject to patent disputes. Some examples of the use of PEDro in academic research papers of this kind are: Hans-Wen Tsai, Yohann Doudna, Hans-Joachim Zwiers, John F. Czajka, and David R. Wegner. Multimodal PISA 2000: Testing the robustness of the Multimodal PISA paradigm. International Journal of Educational Development, 7, 23-41 (2001). It is possible to use a translation system, which in the current setting is not necessarily convenient. To take advantage of an external server, we can also use a web server (e.g. http://www.schneier.com). It is quite common these days for academic researchers to use a server in addition to a web server (e.g. ftp://ftp.schneier.com). Therefore, it is quite common for these authors to refer to the server-side system when they include their data in the paper. In some cases, the author may prefer the server-side system, since the server-side system will be easier for the reader to use when reading the paper. Another example of a research paper by using a server is ""Learning and using generalization and regression"" by Andrew Shkarin, Robert J. Lai, and Yanyan Wang (2000). The paper contains two main features: (1) it uses the Multi-Language PISA database in"	False
"
 input (English text): What are the metrics of evaluation over the CoLA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoLA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT TOP(order_by(val_name, rdfs)) val_name, rdfs.name AS rdfs.val_name, ?dataset_lbl rdfs:label, ?metric_lbl, rdfs { ?datoset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COLA"") ?benchmark rdfs:EVALUATION. } output (Sparql query): SELECT name, rdfs.name AS rdfs, ?dataset_lbl rdfs.label, ?metric_lbl, rdfs { ?datoset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COLA"") ?benchmark rdfs:EVALUATION. } As seen above the model 'lasso' can outperform the most commonly used classifiers. There are some limitations with which I can offer suggestions however. This approach is interesting, but as the model becomes better known the algorithm becomes less effective, and I cannot guarantee it is the best choice in every scenario. You may have noticed that in the demo the model runs using RNN, but the model used is not the RNN from the first section. If you want to download this model, go ahead and do it, it is probably the best to keep all the methods open for a future version :) So here is a quick recap of the final model, and some of the tricks used in the model, as seen above. In the second part of the tutorial I'll include more generalizations of the approach and some"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Double Dunk dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?
 output (Sparql query): "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Artificial Intelligence"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	"SELECT COUNT(*) FROM ?""problem"" ""benchmarking"" ""atari-dataset"" ""benchmark"" ?benchmark"" COUNT(*) FROM ?problem ""benchmarking"" ""atari-dataset"" ""benchmark"".... data ?problem input (English text): What are the title and IDs of some of the papers that contain the following benchmark ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?""problem"" ?dataset_lbl. } input (English text): What are the benchmarks in the area of Artificial Intelligence, that include a solution to the problem above ? output (Sparql query): SELECT COUNT(*) FROM ?problem ""benchmark"" ""benchmark"" ?benchmark ""atari-dataset"" ""benchmark"". input (English text): Which tasks has the benchmark of the following dataset contributed to ? output (Sparql query): SELECT COUNT(*) FROM ?problem ""benchmark"" ""benchmark"" ?benchmark ?benchmark ?dataset_lbl.   output (Sparql query): SELECT COUNT(*) FROM ?problem ""benchmark"" ""benchmark"" ?benchmark ?dataset_lbl, ?benchmark, ?benchmark ""benchmark"". What problems contributed to the previous dataset ? output (Sparql query): SELECT TOP (0) ?""problem"" ""benchmarking"" ""atari-dataset"" ""benchmark"" ?data_lbl output (Sparql query): SELECT COUNT(*) from ?problem ""benchmarking"" ""benchmarking"" ""benchmark"" ?dataset_lbl, ?dataset_lbl """	False
"
 input (English text): Provide a list of papers that have utilized the DyGIE++ + OpenIE +  Stanford Core NLP PoS tagger enriched by consistent triples model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DyGIE++ + OpenIE + Stanford Core NLP PoS tagger enriched by consistent triples"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?benchmark ?model ?test_lbl ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. ?cont orkgp:HAS_INPUT ?input. ?cont orkgp:HAS_FILTER ?str ? _ ?input. orkgp:HAS_LANGUAGE ?language. output (Sparql query): What are the results so far ? output (Sparql query): Which benchmarks were done from which dataset ? input (English text): What are the models that have been benchmarked on that dataset ? output (Sparql query): SELECT ?b.lang ?benchmark orkgp:HAS_BENCHMARK ?b.lang; orkgp:HAS_MODEL ?b.language ?; orkgp:HAS_SOURCE_CODE ?code. output (Sparql query): output (Sparql query): SELECT ?b.lang ?benchmark ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?b.lang; orkgp:HAS_SOURCE_CODE ?code. output (Sparql query): How many of the models were used, compared to what ? output (Sparql query): output (Sparql query): SELECT ?c - C - C ?	False
"
 input (English text): What are the models that have been benchmarked on the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT(BLEU) * 3, BLEU from orkgp/model orkgs:FACTORS. c1, c2, c3; Here is the results of the WMT2015 German-English dataset benchmarked against the WMT2016 Russian-English dataset: The number of model iterations in WMT2016 Russian-English dataset is 7. The number of model iterations in WMT2015 German-English dataset is 13. The number of model iterations in WMT2014 German-English dataset is 23. As you can see, the WMT2014 Russian-English dataset has an average BLEU calculation speed of 7 and the WMT2014 English dataset has an average BLEU calculation speed of 13. The WMT2015 German-English dataset can be further benchmarked against the WMT2016 Russian-English dataset. As you see, the result of the WMT2016 Russian-English dataset is quite similar to the result of the WMT2014 German-English dataset. So we can conclude that the WMT2014 Russian-English dataset outperforms the WMT2015 German-English dataset in terms of BLEU calculation speed. Let us look at the second step of a query with data from the first step to see the difference. The first step is to perform an i-BOLT query of the results to check the results without using the query parameter. The only query parameter and the only model are NULL. So we can safely ignore the query parameter and run an i-BOLT query. input (English text): What are the models that have been benchmarked on the WMT2016 Russian-English dataset ? output (Sparql query): query: SELECT ?model ?model_lbl WHERE { ?dataset db:Data.dat ; db:Label ?dataset_lbl	False
"
 input (English text): Which model has achieved the highest ERR@20 score on the ClueWeb09-B benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ERR@20"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ROW_NUMBER, NODENDS, DESC(BED_LENGTH, GRAY_LENGTH) FROM biolab_eval(""clueweb09-b"", ?dataset ?dataset_lbl, ?model ? ?model_lbl, ?model ?_lbl, ?dataset a orkgc:Dataset, ?datasets ?datasets_lbl_lbl_lbl, ?models ?models_lbr, ?model_lbl rdfs:label ?lbl. FILTER (str( ?lbl ) = ""ClueWeb09-B"") ?dataset_lbl rdfs:label ?lbl). DESC. ROCm is a R database that measures memory retention. In this example, a paper by Gao et al. [16], measured the recall of all letters in a text using ROCm. Here is a ROC plot showing the memory of each letter (coloured in orange) in the text: A second paper, also by Gao et al.[16], measured the recall of each letter in a text using ROCm. Here is a ROC plot to display the recall of each letter: It tells us that ROCm correctly predicts the recall of letters, but not all the remaining letters, because, as above, they cannot be predicted with pure ROCm. We now move onto an experiment. Here is a preprocessing script, which takes two ROCm models with the text text_1, and a model with the words words_1, and performs the memory recall. It was run like so: $ python preprocess.py words_1 | | ------------------------------------------------------ | | ROCmModel(text_"	False
"
 input (English text): Where can I find code references in papers that have used the SciBERT (active learning) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT (active learning)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT STATISTIC bs:model_lbl ORB:RDFS. ?source_code orkgp:DISTINCT ?dataset. ?cont orkgp:HAS_DATASET orkgp:HAS_DESCENT ?benchmark. ?benchmark orkgp:HAS_MODEL ?model orkgp:HAS_SOURCE_CODE ?source_code. output (Sparql query): SELECT STATISTIC bs:model_lbl ORB:RDFS. ?source_code orkgp:DISTINCT ?dataset. ?cont orkgp:HAS_DATASET orkgp:HAS_DESCENT ?benchmark. ?benchmark orkgp:HAS_MODEL ?model orkgp:HAS_SOURCE_CODE ?source_code. output (Sparql query): SELECT BOUNT ( ?) avg_score avg_lbl avg_lbl_cores * avg_score / avg_lbl_cores ; orkgp:HAS_DISTINCT ?dataset. ?cont orkgp:HAS_DATASET orkgp:HAS_DESCENT ?benchmark and/orkgp:HAS_MODEL ?model orkgp:HAS_FILE ?source_code. output (Sparql query): SELECT BOUNT ( ?) avg_score avg_lbl avg_lbl_cores * avg_score / avg_lbl_cores ; orkgp:HAS_DISTINCT ?dataset. ?cont orkgp:HAS_DATASET orkgp:HAS_DESCENT ?benchmark and/ orkgp:	False
"
 input (English text): Could you provide a list of models that have been tested on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Shake-Shake (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- a b c d e f g h i j k l m n o p q r s t u v w x y z --------------------------- a b c d e f g h i j k l m n o p q r s t u v w x y z 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.88 0.50 0.16 0.05 0.00 0.88 0.80 0.68 0.12 0.08 0.02 1.0 0.88 0.92 0.67 0.30 0.05 0.05 0.06 0.06 0.20 0.06 0.05 1.0 0.86 0.79 0.63 0.22 0.02 0.00 0.00 0.99 0.83 0.66 0.25 0.02 0.00 0.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.44 2.30 0.00 0.00 0.00 2.14 27.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 2.60 1.70 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 5.00 -.08 1.40 1.40 0.00 17.50 1.80 0.00 0.00 0.00 0.00 0.00 6.	False
"
 input (English text): List the code links in papers that use the Neural Content Planning + conditional copy model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural Content Planning + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Joint Entity and Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural Content Planning + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ORkgp:HAS_SOURCE_CODE ?code. } input (Language): English Inputs for the Conditional Copy: conditional-copy/ input (Language): English output (English text): Lists the code links in papers that use the Neural Content Planning + conditional copy model in any benchmark ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural Content Planning + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ORkgp:HAS_SOURCE_CODE ?code. } "	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Q*Bert dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?benchmark output (Q*Bert data): WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset. Output (Parquet): SELECT DISTINCT ?model_lbl ?model. ?type ?type_lbl. FILTER (str( ?model_lbl) = ""MultiRC"") ?model. output (Parquet): SELECT DISTINCT ?model_lbl ?model. output (Parquet): SELECT DISTINCT ?model_lbl ?model. I haven't done something like this before, though. Please elaborate. Thanks! Thanks to you, I finally understand how the Atari 2600 Q*Bert dataset is indexed."	False
"
 input (English text): Provide a list of papers that have utilized the HNEABP (BWNE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HNEABP (BWNE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?source_source_code FROM ( SELECT id, name FROM sys.db_cites, sys.db_source_code ) [ ?source_source_code ]. ?source_cite rdfs :labels ; orkgp:HAS_SOURCES ?dataset. ?dataset rdfs :reference_code; orkgp:HAS_SOURCE_CODE ?reference_code; input (English text): Where do I find code references in papers that have used the HELLDYBRUT model (e.g. for benchmarking purposes) ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model hd = ?model ?model_lbl:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HDCHELLDYBRUT"") ; } input (English text): Where can I find literature that has used the HANDLE model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a = ?model ?model_lbl:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) in [ ?model_lbl:Model or {} ]. FILTER (str( ?model_lbl) in [ ?model_lbl:Model or {} ]. ?table. ?cite && rdf. ?table && rdf. { ?table }. { ?name && rdf. orkgp || rdf. title ) ?table_name ?row. } ?benchmark orkgp:HAS_DATASET ?benchmark; orkgp:HAS_"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Gravitar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gravitar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	-------------- | ?score { :>0| ? score } | (1) | (2) { :>0| 1| 2 | ----------------------------------------- | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | No test. | Yes test. | ----------------------------------------- In terms of this metric, which is useful for evaluating an optimization problem, that's fairly good. I want to take a look at the other data that I've collected and it is the Atari 2600 Chess game dataset. This is a very challenging game for computer, with the following feature set: Player 1 is the first move. The other three moves are chosen randomly. The player with the highest score wins. Players can't capture their opponent's queens. Player 2 won against player 1 twice. The first match lasted approximately 35 moves, a typical tournament length. The previous match lasted 16 moves, and it was also a tournament length (about the length of one side's board). After the first match, they are both in position 6, which is the last player left to win by now. You just keep rolling the dice and try to make some moves by following the rule of chess or checkmate. In fact, the game is so difficult that the last player still in position 6 doesn't know why other player is in position 6. The next match will end in a tie and hopefully the players will start a new game without knowing the reason of the tie. The first iteration, between each game, was done under a modified	False
"
 input (English text): Where can I find code references in papers that have used the PAR Transformer 24B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer 24B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 And finally, a quick comment from another user: It takes like 10-20 minutes to upload the file to our github repo. And the results after adding validation: And the result for the German-English English model And the results with the validation I thought that I'll run this myself and see if I can spot any patterns, so I've written a Python script that runs through all the models and parses them for syntax. I'm just in the final days of testing this script, so I can't share much, but it's working very nicely for me in my own tests so far. In particular, I've been seeing lots of similarity in the PAR Transformer 24B and the PAR Transformer 24 data from my benchmarking. Unfortunately, as I stated before, I've yet to see any code that I could attach a check-in to to see if it's actually different from the English models. In case anybody else has spotted anything or knows of any changes that I should make or want to post here or in the comments I want to hear about it.	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the SearchQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	-------------- -- Results --------- ------- ALL SORTED (50%) ----------- Atari 2600 Atlantis Atari 2600 Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis orkgc:dataset All: Atari 2600 Atlantis All: Atari 2600 Atlantis Atari 2600 Atlantis All: Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis -------------- -- Results --------- ------- ALL SORTED (100%) ----------- Atari 2600 Atlantis All: Atari 2600 Atlantis All: Atari 2600 Atlantis All: Atari 2600 Atlantis All: Atari 2600 Atlantis Atari 2600 Atlantis All: Atari 2600 Atlantis Atari 2600 Atlantis -------------- -- Results --------- ------- All Sorted by METRIC All Sorted by TEST All Sorted by MAX_value ASC All Sorted by METRIC All Sorted by MAX_value ASC ---------- ---------- ------ ALL SORTED (50%) ----------- Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis Atari 2600 Atlantis ----------------------------------------------------------------------- ----------------------------------------------------------------------- In a nutshell, this query does the following: searches through all of the datasets in RDFS (in addition to standard RDFS, it does not include'standard' RDFS, such as bibtex) for the longest dataset with a criterion, and sorts its results (based on the metric ) so that the highest-scoring model is returned. Now, we'd like to get this result displayed in Tableau (in our case, since the results are in English, we'll also display it in tableau to make it easier to see what each one is). For this task we use the API search function. This function returns a List of all of the results: -- Results list for search query on the dataset -------------- -- Results for search query on dataset search: { search: SearchQuery { dataset: ? { ?data :  [cdf:dataset], ?dat	False
"
 input (English text): Can you list the metrics used to evaluate models on the PolyAI Reddit dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the OntoNotes dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?data ( ?dataset ) * FROM ( ?dataset ORDER BY ?dataset DESC, ?dataset_lbl ) ORGKL ?dataset ?dataset_lbl ORDER BY id DESC, ?dataset_lbl You can also write all that using regular SQL statements: SELECT a, metric, rdfs:label, orkgp:HAS_DATASET, id as RDFSID, val as val1, metric_lbl as metric_lbl1, metric as metric_metric, rdfs:label as RDFSID, metric as metric_lbl, metric_metric as metric_metric, kl as kl1, metric_lbl as metric_lbl2, metric_metric as metric_metric, kl as kl2, metric_lbl as metric_lbl3, metric_metric as metric_metric, kl as kl3, metric_lbl as metric_lbl4, metric_metric as metric_metric, kl as kl4, metric_lbl as metric_lbl5, metric_metric as metric_metric, kl as kl5, metric_lbl as metric_lbl6, metric_metric as metric_metric, kl as kl6, metric_lbl as metric_lbl7, metric_metric as metric_metric, kl as kl7, metric_lbl as metric_lbl8, metric_metric as metric_metric, kl as kl8, metric_lbl as metric_lbl9, metric_metric as metric_metric, kl as kl	False
"
 input (English text): What is the best performing model benchmarking the Yelp Fine-grained classification dataset in terms of Error metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 And after running these queries we have the following table in results.tab : tblname (data): ?metric_lbl. ?dataset. ?model. ?metric. samples (number): ?label_2. ?min_lbl. ?max_lbl. ?avg_lbl. ?acc_lbl. ?acc_lbl. error_kindex (number) ?percent_1. ?percent_2. ?percent_3. ? ?%. ?acc_kindex. ?acc_lbl. model (number) ?dataset. ?model. ?metric. ?dataset. ?model_lbl yelp_errors_count (number), ?model_accrued_errors_count (number), ?dataset. ?dataset. ?model_accrued_errors_count. yelp_kindexes_count (number) ?percent_1. ?percent_2. ? ?%. ?percent_3. ? ?%. ?acc_kindexes_count. ? ?acc_lbl yelp_tbl_errors_count (number) ?percent_1. ?percent_2. ? ?%. ?percent_3. ? ?%. ?acc_tbl_errors_count. ? ?acc_lbl yelp_kindexes_count (number) ?percent_1. ?percent_2. ? ?%. ?percent_3. ? ?%. ?acc_kindexes_count. ? ?acc_lbl yelp_tbl_accrued_errors_count (number) ?percent_1. ?percent_2. ? ?%. ?percent_3. ? ?%. ?acc_tbl_acc	False
"
 input (English text): Provide a list of papers that have utilized the SMT + NMT (tuning and joint refinement) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SMT + NMT (tuning and joint refinement)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT 1 ?id orkgc_id orkgc_sname output (Sparql query): input (English text): If a single numeric (e.g. 100) key exists on the data set, which one ? The English-German dataset only contains one numeric key associated with the language model. output (Sparql query): SELECT 1 ?id orkgc_id orkgc_sname orkgc_language output (Sparql query): input (English text): If a numeric (e.g. 100) key exists, what is the lowest numeric key ? English-German had several numeric keys that varied by language. output (Sparql query): SELECT max(id) ?locs ? input (English text): Given that the metric is metric on two columns, what is the best method for the query parser to select the metric ? output (Sparql query): SELECT ?locs ? | ?locs ? input (English text): How many metric units have been retrieved from the JSON object ? output (Sparql query): output (Sparql query): input (English text): What metrics from the WMT2015 dataset have been retrieved from the JSON object ? output (Sparql query): output (Sparql query): input (English text): If the results of the query are in the output file, they would be listed below. output (Sparql query): { ?locs ?locs	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset TSE-NER?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	------------------------- N.B. I am currently building a benchmark for the PWC Leaderboards (restricted) dataset. We only accept submissions at the N.B. position because I believe in quality and I don't want a handful of low quality papers to have such a large impact. ------------------------- Now that we have a working benchmark, we can take a look at what all we have collected. Here is the dataset: The dataset contains 7 papers per week which span a range of 6 weeks. The papers are sorted in terms of the number of times they were published, and the scores are normalized to the range 0-200, then filtered based on both a weight of one and a score of zero respectively, a value from 0-10 points which will be used to rate papers as a whole. The dataset does not use any external variables or weightings that might confuse or bias the results And so the first thing we need to do is to get some data that can be used to perform a basic clustering analysis. We have already covered a basic clustering analysis using the Venn diagram, but here we will do a few improvements instead:  Step 1: We will use the standard clustering algorithm that performs the following steps: cluster cluster cluster clustering cluster cluster cluster The clusters are defined by each column in the data and are weighted by the number of points to the cluster. Step 2: First, let's create a set of points that represent clusters,  which will represent points that are within a cluster: create_points  create_points Create a new Point class with the name CLUSTER_PATTERN In the create_points.cpp, first we create a Point and initialize it with the name ClusterPATTERN, followed by a value representing the number of points in the cluster: 	False
"
 input (English text): What models are being evaluated on the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?model as ?model_lbl; ORDERS No validation data. output (Sparql query): CURSOR x.X MAX(x.X, 1) input (English text): Name of model, the model's label ( ?model ) output (Sparql query): SELECT ?model. ?model rdfs:label ?model. ?model rdfs:label ?model ?  orkgp:HAS_DATASET ?dataset ; orkgp:HAS_EVALUATION ?eval In the third option we specify the column used to define model name in the output row. output (Sparql query): RDFS:LABEL ?model_lbl In the fourth option, we specify the model's evaluation metrics. output (Sparql query): orkgp:HAS_DATASET  orkgp:HAS_EVALUATION  orkgp:LABEL orkgp:EVALUATION rdfs:label orkgp:LABEL orkgp:EVALUATION rdfs:label orkgp:MANIFOL_HORIZATION ? RDFS:MAX(rdfs:label, ?modest_value) MAX(rdfs:label, ?modest_value) MAX(rdfs:label, ?modest_value) rdfs:label ORkgp:MANIFOL_HORIZATION  orkgp:MANIFOL_HORIZATION RDFS:MAX( ?modest_value, ?max_value)	False
"
 input (English text): List the code links in papers that use the Bootstrapped DQN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Bootstrapped DQN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT RANK( ?dataset ) AS rank, RANK( ?model ) AS rank_t, SUM(rank_t) AS sum_rank FROM ( SELECT rank( ?dataset ) AS rank, rank_t FROM orkgp:HAS_DATASET ) select ?dataset ) as rank_t ; output (Sparql query): SELECT RANK( ?model ) AS rank_t, SUM(rank_t) AS sum_rank FROM ( SELECT rank( ?dataset ) AS rank, rank_t FROM orkgp:HAS_DATASET ) select ?dataset ) as rank_t ; Note The following benchmarking queries are specific to the TempEval-3 experiment because they are used to test the bootstrapped DQN algorithm. In other experiments, the query is different. The output of these benchmarking commands is not useful for testing a certain kind of model, e.g. learning a multi-layer neural network. benchmark_t = ( ?dataset ) SELECT RANK( ?data_frame ( ?dataset ), ?dataset ) AS rank, RANK( ?model ) AS rank_t FROM ( SELECT ?dataset AS ?dataset, ?dataset AS ?data_frame( ?dataset ), ?dataset AS ?data_frame( ?model ), ?dataset AS ?data_frame( ?model ), ?dataset AS ?data_frame( ?model ) ), 1, 10 AS max_data_columns RANK( ?dataset ) AS rank_t ); Note In these benchmarking commands, ?dataset refers to the training dataset and ?dataset refers to the testing dataset. A Note on Terminology	False
"
 input (English text): What is the highest benchmark result achieved on the STL-10 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the most common location in the studies?
 output (Sparql query): "	SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1	"................................................. When evaluating the performance, consider that a few results represent ""good enough"". Most of these were not evaluated. To improve the evaluation quality, we can use several different metrics and weights in a set. We can also include these results in the summary statistics to get a clear picture of the performance. We can use our model output to gain additional insight into how to improve the performance. One way to do this is to compare the results with similar models. If we compare the performance of the STL-10 dataset against other models like SPARK's VGG-Net and PLS, we can find out whether that approach results in better performance. The metric and its value in the dataset The metric used to evaluate the performance of the model is called the metric_lbl. (The metric name is taken from the dataset the project was built for.) The metric is a linear combination of the L2 regularization factors (which define how the parameters are evaluated) and the metric's value. The most common values for the metric value are 0.5, 0.35, and 0.2. The first two are the default values. The metric and its values are explained below. The metric value in the dataset is a variable that we will assign when evaluating our model. However, it is often useful to have some way to look at the performance of the model using other values that can show that the model is using features it has not yet learned. The metric value is a vector that contains the value and weight for each dimension of the vector. For example given the dimensions given below, the size of the metric vector would be the number of instances of a parameter. In the first figure the metric and its value (1.0 ), used when considering the performance of the model, are shown. We can determine the dimensions of these vectors for a particular metric by using the"	False
"
 input (English text): Can you list the models that have been evaluated on the ObjectNet (Bounding Box) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	False
"
 input (English text): Which model has achieved the highest Accuracy score on the LAMBADA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } This example uses the Sparql function lbl_query, in which the parameter of n_model is calculated as given in this manual, and that parameter specifies the list of models to be analyzed. That parameter can be in the format of ?model=N>50. The parameter ?dataset=K sets the default, and that automatically assigns to each model the appropriate name for that dataset (given here as rdfs:dataset). For all but one model we have chosen to use the Benchmark Metrics, so that it can be checked when the R code is run."	False
"
 input (English text): What are the metrics of evaluation over the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?model; //select a model (image model, score) SELECT [ ?model ] ORDER BY score DESC; SELECT DISTINCT ?score; The above sample outputs with a new format: For a detailed explanation from the first part of the paper, check out the blog in the following link!	False
"
 input (English text): What is the best performing model benchmarking the WebNLG dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BLEU"") ?benchmark orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } I would be delighted to get some feedback on my post below from these and other datasets."	False
"
 input (English text): Can you provide links to code used in papers that benchmark the LeViT-384 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LeViT-384"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP(@col_1, -1) ?benchmark ?dataset ?cont orkgp:HAS_DATASET: F1-Score 1; ?data orkgp:HAS_SOURCE_CODE: # Results by model group (the first column of the output) groups : [ ?model:LeViT-384, ?model:EKD, ?model:AvantiVCT, ?model:CALG4, ?model:CRISPR, ?model:CRISPR_SURF_4, ?model:EKD_MIP_4, ?model:KALY_MIP_4, ?model:KALY_SURF_4, or ?model:EKD_SURF_4 ], [ ?model:LeId4, ?model:HIVA_T3, ?model:HIV5_4, ?model:HIV6_4, ?model:HIVA_TR, ?model:TDF, ?model:LysA_T, ?model:LysM_T, ?model:LysN_T, ?model:HSP, RDFS, etc - ] output (Sparql query): < ?php echo $group_stats; ?> # Results by model type (the second column of the output) types : [ ?model:RDFS, ?model:Hadoop, ?model:HadoopFS, ?model:Jupyter, ?model:KanJ, ?model:MySQL, ?model:Omni, or ?model:DBMS_I, or ?model:DBMS_I_S]) output (Sparql query): SELECT	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?race_temp_lbl. ?paper_lbl from txt_data WHERE rowcount > ?race_temp_lbl. ?paper orkgp:HAS_DATASET ?paper. ?cont orkgp:HAS_BENCHMARK orkgp:P31 ?cont to list_table_lbl; output (Sparql query): Here, rowcount is an integer, and txt_data is a text file containing a reference listing the results of one set of games of the Sauna Challenge (i.e., two games per table row), with no intervening rows, including the ""Race Temp"" field that needs to be added to the ""Race Temp"" column in the table. RACE TEMP is the actual output table from the RACE application, and txt_data is the name of a file that contains the results. At this point, one can compare the file names in the output with the list of table rows from the createRow method to determine which table has been updated to reflect the benchmark values. If one wants to look up the row count for a specific set of games on the table, the following code is used: source (source file): SELECT COUNT(*) FROM txt_data WHERE rowcount > ?race_temp. RACE TEMP is column_number(txt_data.race_temp.rowcount) for this query, AND FOREGROUND is not used"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Double Dunk dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?dataset_lbl ?cont orkgp:GIN( ?lst ?cont), ?dataset_lbl ?benchmark orkgp:HAS_DATASET orkgp:HAS_BENCHMARK, ?record ?cont. ?record_lbl. ?benchmark ------------------------- While the output rows are delimited by the comma and the query output item is contained within the query string, the query itself was constructed on the first and second line of the query. Therefore, the query was evaluated using the SQL language and not the Parquet Language. The querystring, therefore, was a valid SQL string and it was valid input for the query. Example 1.2.15.3. Example (Parquet) and SQL Query Suppose that we are interested in learning the value of a given ID in a dataset called ""Y"" that contained over 100,000 records. We would like to see how to transform a Parquet file into a Sql file that contains the IDs of the data files. There are two things to note here: - We are using the Sql Language, the"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?paper orkgp:RDFS ?benchmark ORKP; rdfs:title ORKP; output (Sparql query): SELECT DISTINCT ?paper ORkgp:RDFS ?benchmark. ORKP output (Sparql query):  SELECT DISTINCT ?paper ORkgp:RDFS. ?bench orkgp:BASE_NAME ?benchmark. ORKP output (Sparql query):  SELECT DISTINCT ?paper ORkgp:HAS_DATASET ORKP. ?label ORKP. ORKP out output:  SELECT title ORKP|title ORKP|key|value|country <--- The output of the above query will be listed above. input (English text): Do you have the option to include a link to the source code for the benchmark code that is used as the benchmark variable ? output (Sparql query): SELECT author ORKP|author ORKP; rdfs:author ORKP out output:  SELECT author ORKP.* FROM rdfs:author ORKP; input (English text): Are you able to include an option to limit the number and types of benchmark values that can be used ? output (Sparql query):  SELECT count(*) OF comets ORKP; rdfs:count ORKP out output:  SELECT count(*) OF comets. TO_LAST_CASE; rdfs:count ORKP input (English text): Do you have the option to define and use new parameters to get a value for a parameter when it is not explicitly	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?dataset ?dataset_lbl. ?dataset rdfs:label ?dataset. ?dataset rdfs:group ?dataset. ORDER BYDESC( ?value) LIMIT 1 */ if (!is_test_fetch( ?dataset ) ) { /* SELECT ?dataset ?dataset_lbl. ?dataset rdfs:label ?dataset. ?dataset rdfs:group ?dataset. ?dataset rdfs:dataset_lbl. : ?dataset rdfs:labels ?dataset. ORDER BYDESC( ?value) LIMIT 1 */ } For each of the possible combinations of the queries for the above three lists, show the results obtained from the test database for each of them. How can I obtain a list of other similar models ? The table below provides an overview of available models as well as models not yet included. In addition to lists of models of the listed models, these lists also include collections of model databases and model sets, that are intended to allow quick access from within each database. Each cell of the table displays a list of databases containing the specified model. You can select from them the model that you want to display with the filter expression that you'd like to apply to it. The query in the cell will be applied to the model database of the data points to which the query is applied. These databases are referenced by the identifiers in the cells, as well as by the names of the associated model databases. Model Database Name Size of database Reference RNN_M1 RNN network MCD Boca Raton, FL RNN_	False
"
 input (English text): List the code links in papers that use the DQN noop model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" If you have some additional metrics, please send them to me (Larissa) at larissah@gmail.com. We hope to expand our dataset and build other benchmarking datasets, too! This exercise was based on previous work on this blog, which was my first attempt at benchmarking a single noop dataset as a real dataset (it was first written in Python 3, and is still somewhat experimental). A very impressive feat, with even higher (but still surprisingly challenging) test coverage. So I'm happy to report that over the course of a 4 month work week, not only did the code benchmark to some respectable levels, but it was also very flexible and very easy to maintain. Since there is no formal benchmarking process, I'm happy to offer the benefit of the doubt and make sure the code is robust.The Python code can be found at  https://github.com/larissah/dqn/blob/master/scrape.py. The C code for the DQN model itself can be found at  https://github.com/larissah/dqn/blob/master/test_sparql.c There are a wide variety of metrics that are possible. The following are a few of the more important measures:  Density estimates, such as the median, mean, and std distribution. Some benchmarking papers use these and benchmark scores of 1-25 for all metrics given. Median scores (a standard metric for standard deviations) Distribution estimates, such as a binomial, a bell curve, or a sigmoid distribution (with the ""s"" used to separate standard or average points with different degree of spread in that distribution). Any other standard distribution or bell curve and its distribution-specific metric. As the authors of the papers I was based on were using these"	False
"
 input (English text): What is the base URL of ""The Document Components Ontology""?
 output (Sparql query): SELECT ?iri WHERE { orkgr:R8342 orkgp:compareContribution ?cont. ?cont orkgp:P7034 ?ont. ?ont orkgp:P7042 ?iri; orkgp:P7035 ?full_name. FILTER(REGEX(STR( ?full_name), ""Document Components Ontology"", ""i"")) } 
 input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT category FROM ( WHERE category LIKE 'treesbank' ) ?indexer: SELECT ?indexer, id, ?category, ?start_date, ?end_date, ?contribution ?contributes,...,.. from ( SELECT * FROM ?treebank ?indexer)... WHERE id = 10 AND ?category = 'Finance'; The output is: SELECT category FROM ?Treebank ?indexer WHERE ?indexer > ?treebank ?category=Finance... AND ?indexer- ?treebank- ?indexer < ?treebank ?category=Finance; The input is (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset ? output (Sparql query): The input is (English text): What is the ""Treebank"" category of the Penn Treebank benchmark dataset (e.g. ""Dividend Return/Dividends, Income/Tax, International Investments, Income/Settlement Rates, Investment/Stock Markets, Investment/Stock Markets, Equity Markets, Mutual Funds, Volatility, Value, Volume, and Valuation"") ? output (R database): SELECT ?indexer, ?category, ?start_date, ?end_date, ?contribution ?contributes,...,.. from ( SELECT * FROM ?treebank ?indexer)... WHERE id = ( SELECT * FROM ?treebank ?indexer)... ; The output is:"	False
"
 input (English text): What models are being evaluated on the Atari 2600 Beam Rider dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Beam Rider"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"------------------------- IWSLT2015_ German-English ------------------------- -------------- > % benchmark orkgp:HAS_DATASET ?dataset_lbl. 0% ------------------------- IWSLT2015_ French-English 2.1% -------------- > % benchmark orkgp:HAS_DATASET ?dataset_lbl. 3% ------------------------- IWSLT2015_ German-English 7.7% -------------- > % benchmark orkgp:HAS_DATASET ?dataset_lbl; 6.4% ------------------------- IWSLT2015_ French-English 7.1% -------------- The results for the IWSLT2015 German-English dataset are shown in Table 4. In this case, the overall benchmark was 9.4%, and the evaluation metric was the metric ""high"" (15%). Both of these metrics are clearly superior to IWSLT2008's 12.7 (the most recent evaluation from the 2008 benchmark). The IWSLT2015 German-English dataset was the most popular (29.3% of the dataset), followed by IWSLT2009 (17.5% and 16.7 respectively). Table 4 German-English Dataset Benchmarks (Score and metric)  Result IWSLT2010/4 IWSLT2010/5 IWSLT2010/6 Table 5 shows the benchmark results for the following two datasets: a German-English and a French-English dataset, both from a 2010 database (IWSLT2010/4) and the German-English dataset from IWSLT2010/6. As expected, the German-English database was the most popular (37.4%) and the French-English data was the second (23.0%). Neither is a terribly impressive benchmark"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"............................................................ We start by downloading a snapshot of the Frostbite platform (see: /frostbite_platform/build/release.gdb ) containing the Frostbite platform and platform-specific files. Next we import the Frostbite platform and then use a program called rdfs that provides a query syntax and object graphs for extracting the metric objects that we need. Now that the Frostbite platform has been downloaded, we can extract the metric objects by running the following command (where the -f flag is optional): rdfs -l We then use frostbite-platform-build to build the metric objects we need and then run an exercise with them: frostbite-platform-build -c 'yarn run' --build-arguments 'Metric-objects = [ [HISTOGRAM_PATH ""/build/release.build"", HISTOGRAM_PATH ""/build/release.gdb""], RDFS_PARAMETER_URI """" # the path to the RDFS files. ]' We'll output the results to the GDB log (the ""Output Directory"" in the GDB User directory): # set environment variable to enable build process # export LANG=C,DEV. output from the build process: --------------------------------------> rdfs -l --------------------------------------> Build complete. --------------------------------------> C: 2.22MB, C++: 24kB, Java: 574kB, JavaScript: 1.5M, python: 0.6M, Python wrapper: 0.5M, ruby: 3.4M, shell: 0.6M C: 5.52MB, C++: 14kB, Java: 29kB, JavaScript: 1.6M, Python: 0.6M, Python wrapper: 0.7M, ruby: 3.0M, shell: 0."	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------------------- Score | Cpu | NIL | Number of units | --------------------------- 17.923 | 158380 | 17.923 | 351792 (351902) 137623 | 141292 | 137623 (137794) 136910 | 127240 | 136910 (137771) 136790 | 123476 | 136790 (137110) 132893 | 124144 | 132893 (139163) 134106 | 113844 | 133893 (133490) 133136 | 112384 | 133612 (133718) 128448 | 98552 | 133136 (129656) 128288 | 84944 | 133136 (133870) 127322 | 87472 | 127322 (134074) 127219 | 85392 | 127319 (127534) 126312 | 82560 | 126192 (125554) 127392 | 62464 | 133723 (127338) 130704 | 56352 | 130704 (130539) 129352 | 48784 | 130784 (130622) 130264 | 43312 | 130264 (130627) 127044 | 31576 | 127044 (127427) 126848 | 31318 | 126848 (126994) 125868 | 30320 | 125868 (127313) 124032 | 29264 | 124032 (123851) 121688 | 27036 | 121688 (124438) 123304 | 26320 | 123304 (123934) 121032 | 24288 | 121032 (126389) 113088 | 20784 | 113088 (126272) 118696 | 15504 | 118696 (124880) 111234 |	False
"
 input (English text): List the code links in papers that use the BiLSTM-Attn w/ ELMo + section title and citation worthiness scaffolds model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attn w/ ELMo + section title and citation worthiness scaffolds"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?codeWHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attn w/ ELMo + section title and citation worthiness scaffolds"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List the code links in papers that use the BiLSTM-Attn w/ ELMo + section title and citation worthiness scaffolds model in any benchmark ? output (Sparql query): output (Sparql table): Output the results of LSTM on ImageNet ReaL. output (Sparql query): SELECT DISTINCT ?label WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER ( str( ?model_lbl) = ""BiLSTM-Attn w/ ELMo + section title and citation worthiness scaffolds"") ?benchmark orkgr:HAS_DATASET ?dataset. ?cont orkgr:HAS_BENCHMARK ?benchmark. ?cont orkgr:HAS_MODEL ?model; orkgr:HAS_SOURCE_CODEPOINT"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" C# Example using System; using System.Diagnostics; using System.Linq; using System.Linq.Expressions; using System.IO; namespace Benchmarking { class Program { static void Main(string[] args) { // set up a data frame (can be NULL) using (var df = new DataFrame()) { if (d = df.FetchChildren(false, true)) { Console.WriteLine(""The benchmark data has been copied into a new data frame.""); } System.Diagnostics.Debug.Dispose(); //... } } } } # set up a data frame # The benchmark data has been copied into a new data frame. dataset: test_paper1 test_paper2 # the benchmark data has been copied into a new data frame. dataset 1 dataset 2 #... using DataFrames = New DataFrame() dataset1: c1 dataset2: c2 #... # set up a comparison sample and compare the benchmark to a control sample using (var df = new DataFrame()) { if (d = df.FetchChildren(false, true)) { // Do comparisons against the standard library. System.Diagnostics.Debug.Dispose(); //... } } } # Do comparisons against the standard library dataset comparison_d = [ { ""title"": ""The benchmark dataset has not yet been copied into a data frame"", ""dataset"": {""the_benchmark"": {""paper"": ""Paper Field""} }} //... using ComparisonSample = New ComparisonSample(test_paper1, benchmark_paper2); var data = System.Data.List(new[]{""The benchmark dataset has not yet been copied into a data frame"", ""test_paper1""=>{ ""title"": ""The benchmark"", ""dataset"": {"""	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the PROTEINS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?data_lbl ? as lbl_s; hs:cputemnt.metric_lbl ? as metric_lbl. ?data_lbl ? as lbl_s. { ?dataset a orkgc:Dataset; rdfs:label ?data_lbl. ?dataset_lbl orkgc:HAS_DATASET ?dataset; ?dataset orkgc:HAS_METRIC ?metric. } input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset ? output (Sparql query): Sparql queries output (SQL query): SELECT ?dataset ? as lbl_s; hs:cputemnt.metric_lbl ? as metric_lbl. ?data_lbl ? as lbl_s. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?dataset_lbl orkgc:HAS_DATASET ?dataset; ?dataset orkgc:HAS_METRIC ?metric. } { ?dv:cputemnt.metric_lbl as metric_lbl. ?data_lbl ? as lbl_s. } Input (English text): What evaluation metrics are commonly used when benchmarking models on the PROTEINS dataset ? output (Sparql query): SELECT ?metric_lbl ? as lbl_s. ?metric_lbl as lbl_s. { ?dataset a or	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT IF(N.AND(S.P(.2)AND( ?S.L2'1^S.SYS.SYS_LARGE ?L3.SYS.CL.DCL.SYS_CLLIMB ?D1.SC.D2.SC.D3.E2.SC.L3'4'8'16'32'64'... ?D5.SC.D6.SC.D7.SC.D8.S3.SC.D2.SC.L3'5'8'16'32'64'... ?D9.SC.D10.SC.D11.SC.D12.S3.SC.D3.SC.D4.SC.D5.SC.D6.SC.D7.SC.D8.S2.SC.S3.SC.S5.SC.S6.SC.S7.SC.S8.S9.D1.SC.S10.SC.D11.SC.D12.S13.SC.S14.SC.S15.SC.S16.SC.W16.SC.S17.SC.Z'3.SC.V'3.SC.W4.SC.V4.SC.Z4.SC.Z5.SC.V'5.SC.D11.SC.W14.SC.Z22.SC.Z23.SC.Z27.SC.Z30.SC.V'11.SC.M'11.SC.M22.SC.V30.SC.M31.SC.V31.SC.M.M	False
"
 input (English text): What is the best performing model benchmarking the FGVC Aircraft dataset in terms of Top-1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "		False
"
 input (English text): What are the models that have been benchmarked on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT highest ?benchmark score, COUNT( ?benchmark ) orkgp:Evaluation ?dataset, ?model ?model_rdfs_lbl ;... So the answer is: no, it's not. The most recent benchmark on that dataset is done using the DBSCAN tool (and, indeed, has an even shorter dataset). And as you will see from below, we can get an arbitrary number of results from a single query. The benchmark report in the last column is a simple visualization of results in different sets of data. But again, using the query planner you can get a whole list with all results including the results with and without standard deviations. In practice, you may have to have quite a few ""scores"" on the first query. At least it is quite useful when you have to estimate a statistical process such as the weight, that we used for the ROC plot above. As I said, when we find a problem we will often choose among all possible options, and then try to make the best out of a given option. At some point we may also get an acceptable outcome, which would not have been the case and should be reported, no matter whether it made the final model! Also, the first run may not always be able to predict the final final answer, and we may need more time, and will have to test other things with it. To be honest, even if the data set is pretty well specified and well-balanced we will never choose it as the sole option. We can always try out ""other"" models, if this may improve the fit, and that can often happen to be the only choice for a problem that requires a lot more complicated data. As for the question of how fast a method is getting on a particular problem, I'll use the question on the MLP blog as"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the MLDoc Zero-Shot English-to-Chinese dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Chinese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Chinese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ?cont orkgp:HAS_VALUE ?benchmark. OPTIONAL { ?cont orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl_lbl. } ORDER BY DESC( ?value) } --output: [1] CIFAR-10 Chinese Language Learning Dataset for R -- Query Evaluation Matrix (R) The R-to-CIFAR-10 matrix shows how often R-trained models are good at achieving the desired performance. The model was evaluated by using the L2-regularization and applying a normalizing filter based on the raw scores of test or training sets respectively. R code: library(clustering) model <- clast(model$name, as."	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Finger, spin (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT COUNT(*) FROM journal_nissl ?article ?keyword_list. SELECT ?keyword ?keyword_list. ?article_id ?pager. ?page ?page_ids. ?keyword ?keyword_list. ?article_in_keywords. ?page. ?title ?article_keyword. ?publ ?article_keyword. input (English text): Who authored this paper ? output (Sparql query):   input (English text): What is the top benchmark score and its metric on the Medline abstracts corpus dataset ? output (Sparql query): Input text: 1.1% 0.95% 0.90 M2M, 1,000 words Output text: What would happen if we applied the Benchmarking for Open Access and the ""Open Access/Benchmark"" model to these articles ? Keyword: Benchmarking for Open Access input: 1,000 articles output (Sparql query): SELECT TOP 1,000  FROM journal_nissl ?article ?keyword_list  WHERE  { ?keyword ?keyword_list. ?article_id ?pager. ?page ?page_ids. ?keyword ?keyword_list. ?article_in_keywords. ?index ?article_keyword. ?index. ?title ?article_keyword. ?term_id ?article_keyword. ?publ ?article_keyword. ?publ. ?page ?page_ids. ?keyword ?keyword_list. ?page. ?title ?article_keyword. }"	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Montezuma\'s Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""citation classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 Table 1 - Summary Table 2 - Table 1 Table 3 - Table 2 Table 4 - Table 3 Table 5 - Table 4 Table 6 - Table 5 Table 7 - Table 6 Table 8 - Table 7 Table 9 - Table 8 	False
"
 input (English text): Indicate the model that performed best in terms of Top-1 Error Rate metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the H-NLI model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""H-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): What is the most current version of the H-NLI model ? output (Sparql query): Which data set for the F-35A did you use ? In previous versions of this database, we have used the F-35A data set, which is maintained by IAI. Please note that the code links only provide the code that is generated if H-NLI is used in a benchmark, which is the default. If you have a specific interest in comparing various H-NLI models, please use the code link in the output (Sparql query): input (English text): Describe the code that generates the output from the test. output (Sparql query): input (English text): How many instances of the F-35A would you expect to create in one run of your benchmark ? output (Sparql query): input (English text): In your benchmark, how many metrics are associated with the data set ? In previous versions of this database, we have used the F-35A data set, which is maintained by IAI. Please note that the code links only provide the code that is generated if H-NLI is used in a benchmark, which is the default. In addition, you can select the metric(s) that you want to include in your own benchmark. output (Sparql query): input (English text): List the code links in papers that have used the H-NLI model in any benchmark ? In previous versions of this database, we have used the H-NLI model, which is maintained by IAI. Please note that the code links only provide the code that is generated if H-NLI is used in a benchmark, which is the default. Please note that you can also select the metric(s) that are associated with the	False
"
 input (English text): Which model has achieved the highest F1 score on the MedNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Input Results Note: The results above are for the benchmark dataset. In order to get the benchmark data for each dataset, as well as some other related data, please add your input results under the ""Determining the Benchmark Dataset"" section. "	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT BANK_NODE ?bank_node_lblue( ?dataset ?bank__lblue_lblue_lblue ?banking_node_lblue ?loc_lblue ) AS ?bank LBSN ?bbank. ROWGROUP BY ?loc_lblue| ?bank_node_lblue ?bank | ?loc_lblue LIGHTLY| ?bank_node_lblue ------------ ----------------------- | ?bbank This example uses the same database structure for the first two cases. With one exception, the input and output fields are of the form column(name) and value(val). You can see that when the LBSN and bank_node_lblue columns are of the same type (for example, text and number ), the columns are set up similarly to: -- column(name) <> column(value). The only exception is the column name itself, which will be set automatically without explicit syntax (in most cases). (The pattern is not necessary since the column name is automatically set up automatically.) Now, a different example on two-dimensional data. The same database structure is used, but with two sets of output fields: one for two-dimensional, and one for three-dimensional statistics. For example: -- output for the two-dimensional dataset, with two sets of output fields: column(names) <> column(types). output(lblue) <> column(countspercol) The output fields for the	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SemEval-2021 Task 11 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2021 Task 11"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NLL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	-------------------------- Model Score -- -------------------------- SemEval-2021 Task 11: -------------------------- 1.0 SemEval-2021 Task 8: -------------------------- 1.7 SemEval-2021 Task 7: -------------------------- 1.5 SemEval-2021 Task 6: -------------------------- 2.3 SemEval-2021 Task 5: -------------------------- 2.7 SemEval-2021 Task 4: -------------------------- 2.8 SemEval-2021 Task 3: -------------------------- 2.2 SemEval-2021 Task 2: -------------------------- 2.4 SemEval-2021 Task 1: -------------------------- 2.0 SemEval-2021 Task 1: -------------------------- 0.9 SemEval-2021 Task 10: -------------------------- 1.9 SPSS: -------------------------- 2.6 Cogent: -------------------------- 3.2 SAS: -------------------------------- 3.1, -------------------------- 2.9, -------------------------- 3.4, -------------------------- 3.1, -------------------------- 2.9, -------------------------- 3.4, -------------------------- 3.1, -------------------------- 2.9, -------------------------- 3.4, -------------------------- 3.1, -------------------------- 2.9, --------------------------	False
"
 input (English text): Where can I find code references in papers that have used the ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT * FROM row_data. table - > dbo WHERE _type=1 AND _sql=1 LIKE _error_code output (Sparql query) SELECT * FROM row_data. table - > dbo WHERE _type=1 AND _sql=1 AND _error_code Output output. table - > errors. table - > rows. table - > columns; row_data. table - > row_data; row_data. table - > column_a. row_data. table - > column_b. input (English text): output (SQL, JSON, or HQL database) Output output. table - > error_rows. table - > errors; row_data. table - > row_data; row_data. table - > column_a. row_data. table - > column_b. Output output. table - > error_rows. table - > errors (with ""rows"" column) Row"	False
"
 input (English text): Can you list the models that have been evaluated on the Birdsnap dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT d.dataset ORDER BY d.model.dataset, d.benchmark.id DESC Output Results When comparing RDBMS and ORIENTATION metrics, the Benchmark RDBMS metric is always used. RDBMS metrics are typically reported on their RDBMS version. RDBMS versions are shown with an asterisk. SELECT version AS rdbms_version ORDER BY version DESC RDBMS has different columns in its schema. To get the information about the columns type, use the following commands: UPDATE sku, dic SET types = ( { ""id"": 2, ""dbuser"": ""tamper"", ""dbpassword"": ""honeybadger"", ""sqlserver"": 6 }, ), and DELETE FROM sku WHERE type = { """" }; UPDATE sku, dic DELETE FROM dic WHERE type = { }; Update in your database: SELECT r.version AS rdbms_version; The following commands show the difference between a RDBMS version and a version from the Tableau Desktop or Tableau Public Databases (e.g. R2 to R4). R2 R4 Description R2R4 table_id R2R4 TableauPublic (2-column) (2-column) R2R4 RDBMS is not versioned by Tableau Desktop or Tableau Public Databases (5-column) (5-column) Database version 2.5.4, SQL Server version 6.0.0, (2-column) (4-column) Note: RDBMS can be a Tableau desktop, Tableau public dataset, or a Microsoft Tableau Public Database. You can also convert from one tableau version to another. If you need to"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What is the top benchmark score and its metric on the Atari 2600 Pinball Dataset ? output (Sparql query): RDF: data.table: 1 input (English text): Name of the table that contains the metric values. This metadata is used by the metrics to help with understanding of the data. output (Sparql query): output: { rdfs:table rdfs:meta. ?metric ?metric_lbl. ?metric rdfs:meta{ rdfs:dataset rdfs:test rdfs:score rdfs:measure rdfs:score rdfs:measure rdfs:measure rdfs:data rdfs:tbl} } input (English text): The list of metrics in this metric list would be: ?metric_lbl output (Sparql query): output: rdfs:meta rdfs:metric_lbl rdfs:score rdfs:measure rdfs:measure rdfs:measure rdfs:data rdfs:tbl { rdfs:dataset rdfs:test rdfs:score rdfs:measure rdfs:measure rdfs:measure rdfs:data rdfs:tbl} input (English text): What number is the metric value ? This metadata is used by other systems (such as the Atari 7600, 6800 and 7600XL Datafetch, which also allows for text output) to help with understanding the data. error: Could not	False
"
 input (English text): What are the models that have been benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT DISTINCT ?dataset_lbl ?dataset ORG output (Sparql query): select model. * ; orkgp:SELECT DISTINCT ?paper ORG ORG ORG WHERE { model: HASHED(model_lbl.LBL) ?dataset } output (Sparql query): SELECT DISTINCT ?paper ORG ORG ORG ORDER BY { model: HASHED(model_lbl.LBL) ?dataset } output (Sparql query): SELECT or kgp: DISTINCT ?paper ORG ORG ORG ORDER BY { model: HASHED(model_lbl.LBL) ?dataset } output (Sparql query): Select the data and code for the model ?dataset. output (Sparql query): SELECT orkgp:DATA * FROM ?dataset SELECT ?dataset_lbl, ?dataset_lbl_k, ?value, ?value_k, ?value_k_k, ?parameter_k, ?parameter_k_k, ?value_k, ?value_k_k, ?value_k_k_k, ?value_k_k_k, ?value_k_k_k_k_k, ?value_k_k_k_k SELECT id. * FROM ?dataset output (Sparql query): Select the paper and code for the paper and code ?paper. output (Sparql query): SELECT orkgp::HASHED(paper_lbl.LBL) || { ?paper } || { ?paper_k, ?value } || { ?value_k } ||	False
"
 input (English text): What is the best performing model benchmarking the TriviaQA dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" { # TMP: { ?label: ""Walker"" ?text: ""This is the final result"" ?text (C) 2002 Pekka Kainu <pekka@pekka.fi> ?graph. # F1: { ?metric ""F1"" { ?graph. # F2: { ?column: ""Metric %"", ?graph. # F3: { ?graph. ?table: ""DummyTable"" } ?table { ?metric ""F2"" { ?graph. } } } } } } } (1) You might want to see: How to write an exercise in this dataset. (2) You might want to see: How to test the model for a different performance metric. (3) You might want to see: How to write an exercise that depends on a different set of data. (4) You might want to see: How to write an exercise that uses a different model to test the model."	False
"
 input (English text): Which indicators for well-being are used in the studies on the effect of COVID-19?
 output (Sparql query): SELECT DISTINCT ?indicators, ?indicators_labels WHERE { orkgr:R78492 orkgp:compareContribution ?contrib. ?contrib orkgp:P36089 ?indicators. ?indicators rdfs:label ?indicators_labels. } 
 input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 output (Sparql query): Output is as follows: A description of the Semantic Role Labeling Research Area. Describe the methods and data sources for the study. Describe what data was collected for this study. Describe the research question. Why are you collecting this data now ? Include the data sets you collected. What was the outcome of this study ? Include any additional context (i.e., case studies, literature review, etc.) related to the data collection process. These data sources are then analyzed by applying some basic statistical techniques. For example, in the example below the correlation between quality and perceived value is calculated. For more help on integrating statistical analysis with the output results, see Mapping statistical output to R code. A correlation between Quality and perceived value The correlation is used to assess whether the difference between the two ratings means that one of the two ratings is more accurate compared to the other. R code #4.1, the correlation between Quality and perceived value rf_ratio = R_df. F ( 4, 1 ) * 7, rff_ratio = R_df. F ( 4, 1 ) * 7,  rwdb_ratio = rdf_ratio / rff_ratio, rwfg_ratio = R_df. F ( 4, 1 ) * 7 ; An R-tables plot of the R-tables output with an error bars as points (red line) for the main findings and a logarithmic scale (black line) representing the confidence interval of the results. R-tables plot (R_df, rwfb, rffb, rwfg) Using R, one can create a table that generates a standard error and confidence intervals (shown below for the	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?benchmark ?dataset ?dataset_lbl. RDF file is output with the title, authors names, and title as its header. If the data cannot be included in a table column, the output of the above query is only for the title. You will see that there is no column for the data format of the data table. If you want to have a data column containing the data of the table, it is required to specify a RDF record of the data. Example: -- Select titles of publications with SQuAD benchmarking results input (Sparql query): SELECT * FROM SQuADBenchmarkSQUAD2.datasets ?dataset_lbl ?dataset_lbl. TITLE DESC LIMIT 1; output (Sparql query): input (English text): List the title and ID of the data from the WikiText-2 dataset ? output (Sparql query): SELECT ?dataset orkgc:Dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""T"" OR str( ?dataset_lbl) = ""lbl"") ?data ?data_lbl_id orkgp:HAS_DATA ?dataset. ?cont orkgp:HAS_DATA ?dataset. ; rdfs:label ?dataset_lbl. input (English text): Provide a list of publications that have benchmarked on the WikiText-2 dataset ? output (Sparql query): SELECT TITLE ?title:CIT"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the VGG8B + LocalLearning + CO model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VGG8B + LocalLearning + CO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?count FROM (SELECT ds.name AS name, ds.rank AS k, (SELECT ds.score AS s, ds.logs AS ln1.samp_score FROM ?ds a JOIN ?ds/groups AS g ON ds.group = g.group && ds.rank < g.rank AND a.type!= 'k2n') ds WHERE ds.dtype == 'k2n' AND a.type!= 'k2n'; orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ds output (Sparql query): SELECT ?count FROM (SELECT ?ds.group AS group ( ?ds.name AS name ) FROM ?ds GROUP BY ?ds.rank DESC GROUP BY 1) k JOIN ?dsgroup AS g ON ds.group = g.group && ds.rank <= g.rank AND a.type!= 'k2n' AND a.type!= 'k2n' GROUP BY ?k JOIN ?group GROUP BY g.group ORDER BY 1 DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC LIMIT ?count DESC ?sort DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC ?sort ?sort ?sort DESC DESC DESC DESC DESC DESC DES	False
"
 input (English text): Could you provide a list of models that have been tested on the QuAC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QuAC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the Duel noop model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Note: If you are using Parquet, you do not need to include any of the following parameters in the output: ?dataset orkgp, ?datasetRdfs, ?benchmark, ?paper, ?benchmarkRdfs, ?benchmarkRdfsR, ?paperRdfsR, ?model, ?modelRdfs, ?modelRdfsR ?optionalRdfs, ?optionalRdfsR, ?optionalRdfsR, ?optionalG. Note: If you are using Parquet, you do not need to include any of the following parameters in the output: orkgp orrdfp, ?dataset orkgp orrdfp orkgp rdfs:label orkgp, ?datasetRdfs, ?datasetRdfsR, ?datasetRdfsR orkgp orrdf p. orkgp. orkgp. orkgp. orkgp. orkgp. Note: If you are using Parquet, you do not need to include any of the following parameters in the output: orkgp, ?dataset orkgp, ?model, ?modelRdfs, ?modelRdfsR, ?modelRdfsR orkgp. orkgp. orkgp. orkgp. orkgp. orkgp. orkgp. orkgp. Note: If you are using Parquet, you do not need to include any of the following parameters in the output: orkgp, ?dataset orkgp, ?model, ?modelRdfs, ?modelRdfsR, ?modelRdfsR orkgp. Note: If you are using Par	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Fine-Grained Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"------------------------- -------------- ScienceIE: ---------- 10x20p: 40% 4x100p: 10% 4x200p: 50% 12x20p: 20% 4x100p: 50% 4x200p: 60% 32x20p: 40% 4x100p: 70% 32x100p: 40% 4x200p: 70% 128x20p: 60% 4x100p: 80% 128x100p: 70% 4x200p: 90% A Note on the Data Access If you're using SPSS, you can skip the above section and just run the code locally. The same is true if you're using LaTeX or TeX as well. Using the above SPSS code, you can access the raw data (see the code above) on the ScienceIE Datasets page. You can also access your own data using the data access box. You can navigate to the dataset with ctrl+click. You can select your own values with the Data access box. Running R R is the default R installation for the Data Access Toolkit. You can download R here. R is not a ""new language"" in the sense that you have to teach it to anyone. You can use it to analyze raw data from all over the world, even in languages as different from English as Spanish, Chinese, Japanese. Just remember that your users will probably have to change their languages to the languages you chose for your data access."	False
"
 input (English text): Provide a list of benchmarked datasets related to the co-authorship prediction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""co-authorship prediction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 You can find benchmarking instructions on the DRI Corpus benchmark page that has further instructions.	False
"
 input (English text): What is the highest benchmark result achieved on the Yelp-2 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	--------------------------- RESULTS ------- Metric Total Score 1.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0 20.0 21.0 22.0 93 Pet Rank 4.3 4.3 4.4 4.4 4.5 4.6 4.6 4.7 4.6 4.7 4.8 4.8 4.8 4.8 4.85 10 Dog Rank 2.6 2.6 2.7 2.7 2.7 3.1 3.3 3.5 3.6 3.6 3.7 3.7 3.7 3.8 3.8 3.8 3.8 3.9 7 Pet Weight % 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3 29 Pet Lifetime 4.2 4.2 4.2 4.2 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.4 4.4 4.4 4.4 4.4 4.4 4.4 29 Pet Length 2.1 2.5 2.5 2.6 2.6 2.6 2.7 2.7 2.7 2.8 2.8 2.8 2.8 2.8 2.8 2.8 2.8 2.85 30 Pet Health Score 4.5 4.5 4.5 4.5 4	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Recipe dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Recipe"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT N, F, x, y FROM X; And then I run the query with the SET ENGINE=Microsoft.PowerShell.Performance SET TERM=""xterm"" and the output is this: [SQL Server] Execute command line... SQL> SELECT N, F, x, y FROM X ORDER BY x; Type: Int64 Parameter Sets: xterm, xterm_value, xterm_value_list Position: Named Default value: None Accept pipeline input: False Accept wildcard characters: False Applies to: SQL Server (both server and client versions) For"	False
"
 input (English text): Could you provide a list of models that have been tested on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	-------------------------- ID NAME AVAIL 12 12 12 12 12 12 12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12 12  12  12  12  12  12  12  12 12  12  12  12  12  12  12 12  12  12  12  12  12  12  12 12  12  12  12  12  12  12 12  12  12  12  12  12  12 12 12 12 12  12  12  12  12  12 12  12 12  12  12  12  12 12  12  12 DISTINCT 12  12  12 12  12  12  12  12 12 12 12 12  12  12  12  12  12 12  12 12  12  12  12 12 12  12  12  12  12 12 12  12  12  12  12 12 12  12  12  12 12 12  12  12  12 14 14 14 14 14  14  14  13 	False
"
 input (English text): Provide a list of papers that have utilized the NCBI_BERT(base) (P+M) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(base) (P+M)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT P+M > 2 WHERE 1:3 orkgp:HAS_DATASET For the base language query, I had to change the second line, which looked like this: SELECT P+M > 2 WHERE 1:3 orkgp:HAS_D	False
"
 input (English text): What was the most common type of approach for summarization before 2002?
 output (Sparql query): SELECT ?approach ?approach_label WHERE { orkgr:R6948 orkgp:compareContribution ?cont. ?cont orkgp:P15 ?implementation. ?implementation orkgp:P5043 ?approach. ?approach rdfs:label ?approach_label. } ORDER BY DESC(COUNT( ?approach_label)) LIMIT 1
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" ? ? ? TITLES ? ? ? ? ID ? ? ? ? COURSES ? ? ? ? ID input (English text): Any work on any subject, based on the ""Turing test"" and the ""Bayesian"" method ? output (Sparql query): ? ? CITING TO B. BAYES ? ? ? ? ORGAN input (English text): Selecting the titles and IDs that best captured the novelty or novelty-seeking tendencies of the users ? output (Sparql query): ? ? ? ? TITLES ? ? ? ? ID ? ? ? ? COURSES ? ? ? ? ID ? ? ? ? A similar question was asked to the Atari 2000 and Atari 5200. The Atari 5200 was found to be particularly unique with its response of 3 titles: Input (English text): All 5 classic games that are not based on games with random start. output (Sparql query): ? ? ? ? ? TITLES ID ?ID COURSES ID Input (English text): Who were the most creative, original, challenging and creative Atari 20th Century games ? output (Sparql query): ? ? ? ? ? TITLES ID ?ID COURSES ID The Atari 2600 and 3020 had a somewhat different answer: Input (English text): All Atari titles that are not based on randomly-generated playlists. output (Sparql query): ? ? ? ? ? TITLES ID ?ID COURSES ID So, what are the limitations of our dataset ? We only looked at titles that were played for 1 hour and 60 minutes a day, so we don't know about the effects of playing sessions. That leaves the possibility that the titles that we are seeing are only a subset of the entire Atari library that was played. But, the same question could be addressed for the Atari 5200. The Atari 600 was the system that had the longest run time of 1 hour, and we did"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DeiT-S model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTR... ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } The goal of the first dataset is to compare the performance of several models on a benchmark dataset based on the data-parallel feature-based VTB model. The second dataset is comprised of the same VTAB-1k data set but modified to include the original model from the original DeiT-S paper. Each of the datasets in both sets has been updated with an additional parameter : code. This parameter is calculated in two steps, based on both previous models' performance and the new test model's performance. Both datasets are stored in a SQL Server database called Benchmark-vTAB1k. Each post-processing step can be calculated as a batch of statistics, just using the data from the first dataset, or can be calculated as a time-series of both the models' performance and the performance of the new model on the benchmark dataset. The datasets' performance is then benchmarked against each of a set of 10 data-parallel benchmarks. These benchmarks are created using the following script: select DISTINCT ?model1:Model, ?model2:Model1, ?model3:Model1, ?model4:Model1,... output (Sparql query):	False
"
 input (English text): Can you list the models that have been evaluated on the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT TOP 5 ORDER BY RATE RATE DESC LIMIT N ON { ?model :_model } = RATE; ORDER BY RATE DESC LIMIT N ON {. ?dataset :_dataset ; } OR output(Sparql query): SELECT TOP 5 ORDER BY RATE RATE DESC LIMIT N ON { ?dataset. _model } = RATE; ORDER BY RATE DESC LIMIT N ON {. orkgc :_ orkgc ; } OR output(Sparql query): For the second benchmark test, we are using the same data model as in the previous benchmark, but one with an additional parameter ""score_rank"" that was given as a parameter on the model in the previous benchmark. We will now perform the following steps: 1. We will call a method named Benchmark that takes a set of model parameters as input. 2. We will execute the following query: select { ?benchmark a, ?model orkgp a} from model b where rdfs:s:score_rank > 50 and model <= rdfs:model_lbl or select { ?benchmark a, ?model a} from model b where rdfs:s:score_rank > 50 and model <= rdfs:model_lbl. Note that score_rank was added by adding a parameter in this benchmark in order to check if the model that we select has performed worse with each rank. The query above was done in order to find the model, not to have it perform worse. And this is a good thing to do as it keeps the model from performing worse. input (English text): Can you list the models that have been evaluated on the FB15k dataset ? output (S"	False
"
 input (English text): What are the metrics used by paper ""Using NMF-based text summarization to improve supervised and unsupervised classification?
 output (Sparql query): SELECT ?metrics, ?metrics_labels WHERE { ?papers rdfs:label ?title. FILTER(REGEX( ?title, ""NMF-based text summarization"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P34 ?eval. ?eval orkgp:P2006 ?metrics. ?metrics rdfs:label ?metrics_labels. } 
 input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-arccos"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The following are the Rfa-Gate code links that are found during the comparison: input (English text): Link (0.25KB) from Google's Rfa-Gate website output (Sparql query): The following additional links for Rfa-Gate are added to the result input (English text): URL of the Wikipedia article in which the Rfa-Gate code links are found output (Sparql query): The following additional links for Rfa-Gate are added to the benchmark input (English text): URL of the Amazon Rfi-Gate wiki output (Sparql query): URL of the Google-Cloud-Rfi-Gate wiki for the Amazon EC2 (compute) The benchmark results include the following metrics text summary: Text summarization by using the Rfa-Gate-arccos model using paper ""Using NMF-based text summarization to improve supervised and unsupervised classification ?"" solution summary: Sorting of the results by the performance of the Rfa-Gate-arccos model solution resolution : How much of the text summaries are solved with the Rfa-Gate-arccos model output: Sorting by the performance of the Rfa-Gate-arccos model Other comparisons for individual papers are available in the RfaGate Reference Book. The results from using the Rfa-Gate-arccos model can be used in regression. The most interesting part is that the performance improves significantly only when the number of items in Rfa-Gate is not too small (i.e. between 100 and 20) with no other parameters chosen (i.e. with only a high-dimensional word filter). The results are very similar for both Rfa-Gate-arccos and Rfa-Gate's own word filtering on"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ScienceIE?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT M.score AS ?score,... }... OR { SELECT M.score AS ?score,... }... ORDER BY ?score } INVENTORY [ edit ] This query would display the inventory of items in the current inventory (the query itself is run by the query-builder): SELECT * FROM inventory WHERE lastname = 'Smith' Output [ edit ] Name Value id 1 Smith 5 1Smith 2 Smith 3 Smith 4 Smith 5 Smith 6 Smith 7 Smith 8 Smith 9 Smith 10 Smith 11 Smith 12 Smith 13 Smith 14 Smith 15 Smith 16 Smith 17 Smith 18 Smith 19 Smith 20 Smith 21 Smith 22 Smith 23 Smith 24 Smith 25 Smith 26 Smith 27 Smith 28 INVENTORY query syntax [ edit ] It's pretty intuitive to use it if you're familiar with the REST API, otherwise you can also use it with an inline REST query with the GET parameter: /inv_list/1/item_id Input [ edit ] id Response [ edit ] [ { ""id"":2, ""item_id"":1, ""last_name"":"" Smith"", ""product_id"":1, ""value"":5, ""created"":153169944, ""created_on"":1539586570, ""updated_on"":1539586624, ""updated_by"":1539578368, ""description"":""[1]"", ""description_md"":null, ""description_lg"":null, ""description_text"":"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the WebQuestions dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the BioASQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 For more examples, see the previous posts: Other posts in this series	False
"
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Space Invaders benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 For each of the results from the Atari dataset we get the best score, and we then run the same query again for each specific metric. In the first example we ran it for the Best Score metric and then for each metric. For the second query we ran it for the Best Score metric and again for the metrics. We then have a list of metric's which have done well in the benchmark test, which also includes what were their ratings. In the case where the metric is at position { ?metric.}, the metric is the same as the metric's in the first query. Here are the results of benchmarking each metric on the Atari dataset using the dataset_hls parameter and the dataset_latm parameter. All of the metrics were at position : 1  for the first query (the metric's in a single row) and at position 100 for the second query (the metrics in a row and one integer column). In the case where there were duplicates, we use the -F option to stop our benchmarking. This prevents us from running the benchmark twice. For example, we ran it for both Best Score and Best Score and Best Score and Best Score and Best Score on the Atari dataset only twice, with both the Best Score and Best Score metrics having their Best Scores multiplied a thousand times. RDFS-based metric resolution As mentioned above, the Atari dataset contains a huge data set of information about a game, and we can use it to find the best performing metric in any dataset. As our goal is to find the best metric for any given dataset we have to define a resolution mapping so we can tell what kind of resolution we are looking for and then specify what happens if we find a value we don't want. An RDFS-based resolution mapping can be given to the dataset by using the metrics-dao parameter. For the	False
"
 input (English text): What are the models that have been benchmarked on the WebQuestions dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Inference"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT DISTINCT { ?dataset }. { ?dataset a }. { ?dataset b }. { ?dataset c }. OR ( SUM ( ?dataset )) OR ( SUM ( ?dataset ) - ?dataset ?dataset_lbl ) OR ( SUM ( ?dataset ) - ?dataset ?dataset_lbl ) OR ( SUM ( ?dataset ) - ?dataset ?dataset_lbl ) | ?model SELECT DISTINCT { ?model }. { ?model RDFS] OR ( SUM ( ?model )) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR ( SUM ( ?model ) - ?dataset ?model_lbl ) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR ( SUM ( ?model ) - ?dataset ?model_lbl) ORDER BY \ ?dataset_lbl DESC output (Sparql query): SELECT DISTINCT { ?model }. { ?model RDFS] OR ( SUM ( ?model )) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR ( SUM ( ?model ) - ?dataset ?model_lbl) OR SUM ( ?model ) - ?dataset ?model_lbl ORDER BY \ ?dataset_lbl DESC input (English text, no separators between tokens):	False
"
 input (English text): Where can I find code references in papers that have used the FusionNet++ (ensemble) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FusionNet++ (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the Classic dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT STATISTICS.COUNT(FusionNet_COUNT( ?model) AS ?weight) FROM ?model GROUP BY ?weight. COMPUTE STATISTICS.RANK() ------------------------- 0.0 0.0 0.0 0.0 0.0 FusION_NET_COUNT(FusionNet_COUNT( ?model) AS ?weight) input (English text): 1. How many datasets have there been published ? output (Sparql query): ------------------------- SELECT COUNT( ?) as total_n; total_n; total_n from fusionnet_cities and fusionnet_metros where type in ('freestate','satellite', 'faster_than_cities') and ?version in ('fused-net-cities/fusionnet_metros/') input (English text): 2. Who's the author ? output (Sparql query): ------------------------- FROM ?author AS author INNER JOIN ?author 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 2.10 2.11 2.12 2.13 2.14 2.15 2.16 2.17 2.18 2.19 2.20 2.21 2.22 2.23 2.24 2.25 2.26 2.27 2.28 2.29 2.30 2.31 2.32 2.33 2.34 2.35 2.36 2.37 2.38 2.39 2.40 2.41 2.42 2.43 2.44 2.45 2.46 2.47 2.48 2.49 2.50 2.51 2.52 2.53 2.54 2.55 2.56	False
"
 input (English text): Where can I find code references in papers that have used the Intrinsic Reward Agent model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Intrinsic Reward Agent"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-Ti"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgs:Model; rdfs:label ?model_ldl. FILTER (str( ?model_ldl) = ""Dei T-Ti"") ?benchmark orgk:HAS_DATASET ?dataset. ?cont orgk:HAS_BENCHMARK ?benchmark. ?cont orkgk:HAS_MODEL ?model; orkgk:HAS_SOURCE_CODE ?code. } It can also use LDA using one of the following functions: rdfs:label(model, code) ldstr(model, code, alpha) lda(model, code, alpha) ldr(model, code, alpha) lblc (model, code) lddf (model, code, alpha = 0.1) lddfs (model, code). LDA uses the algorithm described so far, but with a few modifications according to the current state of the art. It is more suited to benchmarking for real performance, but the LDA results are available for everyone (including the authors). I will use the LDA results on the following articles: * this paper for demonstrating the use of LDA using all model features * this paper as well for benchmarking an Intrinsic Reward Agent model on a real task * this article as well for benchmarking the model on test data"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): For which country of study overall prevalence of epilepsy is the highest?
 output (Sparql query): "	SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1	" input (English text): For specific country of study, what proportion of epilepsy patients have been diagnosed ? output (Sparql query): input (English text): Out of all epileptic patients in a given county, how many were on anticonvulsant treatment ? output (Sparql query): input (String): An entity is an entity. output (File, String): What is the entity name of the entity of interest ? input (String): The entity of interest is another entity. output (File, String): The entity of interest is another entity. Note: You need to provide a unique identifier for each entity you are talking about. When you create a document (such as a ""research paper""), just add the unique identifier to the filename. You can also use it to provide additional metadata, such as a URL. Note: You cannot reuse a name or identifier from multiple entities the same or different entities."	False
"
 input (English text): Can you list the metrics used to evaluate models on the arXiv dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT COUNT(*) . COUNT(*) | ------------+-------------------------------------------------------------+ -- 1 rows in set (0.00 sec) output (Sparql query): SELECT COUNT(*) -- 1 rows in set (0.06 sec)	False
"
 input (English text): What is the name of the top performing model in terms of BLEU score score when benchmarked on the WMT2014 French-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?model_lbl WHERE ?metric_lbl:Metric; 	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Finger, spin (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT names_of_studies/1, names_of_paper( ?labels=) ? /1 /1, labs_listed_by_labels_to_paper( ?labels= ?) /2 AS title, id, xt; rdfs:label ?labels. output (Sparql query): SELECT names_of_studies/1, labels(. ?labels= ?) /1 /2, labs_listed_by_labels_to_paper( ?labels= ?) /2 AS title, id, xt; Rows ------- ------------- ------ -------- --------- 1 (2 rows) Output (Sparql query): SELECT names_of_studies/1, labels(. ?labels= ?)/1 AS title, id, xt; Rows ------- ------------- ------ -------- --------- 1 (2 rows) 2.7.2.2.3.5.6.1.4.1.5.6.1.4.1.4.1.4.1.4.4.4.4.3.............. T5 Data (Sparql query): SELECT titles_of_studies_with_labels_to_paper; rdfs:labels ?... ?labels_to_paper. 1 (2 rows) Output (Sparql query): SELECT titles_of_studies_with_labels_to_paper; labels(. ?labels_to_paper. 1 (2 rows) 1............... T6 Dataset (Sparql query): SELECT publications_by_labels( ?labels= ?) /1 /1,	False
"
 input (English text): What are the metrics of evaluation over the Words in Context dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT { ?metric ?metric_lbl. ORGGL_PROPERTIES ?metric_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. } } ---------- ---------- Notice that there is still a small gap in the performance of the SPSS pipeline. This is because SPSS's ""filter"" function doesn't ""like"" words that contain certain attributes. This is why you see a ""sport of precision"" at the end of the SPSS benchmark. If you are still interested in the SPSS performance over this benchmark and how exactly it stacks up over the words in Context dataset then check out the SPSS benchmarks section here  and this section of the post. It can also be found here, here,  here and here. Let's look at another example of SPSS being inefficient. How would you use a deep learning model on a dataset consisting primarily of words that are commonly found in English for marketing purposes and not in your language ? Let's start by creating an input text file for the input text to go into SPSS. # CREATE OR REPLACE file_name = ""test_spsi_text.txt"", text_max_length = 5000 # CREATE OR REPLACE file_name = ""test_spsi_text.txt"", text_max_length=10000 # CREATE OR REPLACE files_path = ""c:\Documents\Python\Sipsey\example_data"", filename"	False
"
 input (English text): List the code links in papers that use the adversarial + AWD-LSTM-MoS + dynamic eval model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""adversarial + AWD-LSTM-MoS + dynamic eval"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (Sparql query): input (English text): List the code references in papers that have used the Tsetlin Machine model for benchmarking purposes ? output (Sparql query): Input (English text) : List the code references in papers that have used the Tsetlin Machine model for benchmarking purposes ? Output (Sparql query): output (Sparql query): input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes ? Output (Sparql query): output (Sparql query): input (English text): Which of the previous two benchmarking schemes are implemented in the Tsetlin model ? output (Sparql query): Input (English text) : Which of the previous two benchmarking schemes are implemented in the Tsetlin model ? Output (Sparql query): output (Sparql query): input (English text): Which of the previous two benchmarks is the most relevant to the discussion about the Tsetlin model ? output (Sparql query): Output (Sparql query): input (English text): Which of the previous two benchmarks is the most relevant to the discussion about the Tsetlin model ? Output (Sparql query): output (Sparql query): input (English text): Which of the previous two benchmarks is the most relevant to the discussion about the Tsetlin model ? Output (Sparql query): output (Sparql query): output (Sparql query): input (English text): Which of the previous two benchmarks is the most relevant to the discussion about the Tsetlin model ? Output (Sparql query): output (Sparql query): input (English text): Which of the previous two benchmarks is the most	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Recipe dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Recipe"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Output : (note: the output list must include both the name and the ID of the dataset) { (a.dataset1.org): ?benchmark a.dataset.dataset ?benchmark ?benchmark_2.0.1 ?paper_lbl. }. Input : the ""recipe"" ID of the benchmark over the dataset Output : the ""paper_lbl"" label of the benchmark"	False
"
 input (English text): List the metrics that are used to evaluate models on the WMT2016 English-Czech benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Czech"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"--------------------------- Score: 3,081 Score: 2,711 Score: 856 Score: 1,416 Score: 892 Score: 1,918 Score: 988 Score: 2,931 Score: 1,896 Score: 798 Score: 2,082 The second criterion can be expressed just as the first criterion, namely, to compare the WMT2016 English-Czech benchmark to Atari's own metric system. This is, of course, very similar to the question that is answered in Question #3 above, namely, what metric system would be the most suitable for ranking model metrics on the WMT2016 English-Czech benchmark dataset ? At the very least, we should be able to decide which standard for evaluating metric scores has the most appropriate relationship with WMT2016's standard metric system. As indicated by Figure 7: the metric that is most appropriate for ranking the metrics scores has a higher WMT2016 benchmark metric score than the metric metric with the lowest benchmark metric score. The metric that ranks best on both benchmarks is the one with the lowest benchmark metric score of any measured metric. In a sense, the metric that is the most appropriate for ranking ""metric scores"" is the one that scores highest on both benchmarks. However, there is no fixed metric system that is the ideal metric for ranking metric metrics, as we see in the following figure: since not everyone chooses a fixed metric system for ranking metric metrics, there are also other metrics that score better on both benchmarks. In fact, the metric that scores highest on both benchmarks would score the least on the metric that scores the highest on just one benchmark. Furthermore, the metric that is most appropriate for ranking metric metrics (to the best of our abilities) is the one that scores highest on both benchmarks (more on this in Question #7). In short: we are looking for a metric system"	False
"
 input (English text): Who is responsible for acquisition in the RASH system?
 output (Sparql query): SELECT ?acq WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 orkgr:R8350. orkgr:R8350 orkgp:P7049 ?acq. } 
 input (English text): List the code links in papers that use the FQF model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FQF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 (This is how the problem is posed.) I'm not sure I agree with this approach. There are times when we can actually use a specific benchmarking measure, such as the average performance of a software package versus a particular application, to identify the underlying causes of performance degradation in a code or component, or to point us in new directions in identifying the root causes of that degradation. However, there is no reason why we should have to do this when it is possible to perform these more generic, and possibly more informative, benchmarks using the RAST model instead of the RAST, with the same result.	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Sequence error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT Accuracy FROM (SELECT ( ?val OR ?val_lbl) - ?model | nl_model) { ?metric a. } ORDER BY DESC( ?value) LIMIT 1 # input (Spanish text): Indicate the model output (Sparql query): SELECT Accuracy FROM (SELECT ( ?val OR ?val_lbl) - ?model | nl_model) { ?Metric a. } ORDER BY DESC( ?value) LIMIT 1 # input (Dutch text): Indicate the model output (Sparql query): SELECT Accuracy FROM (SELECT ( ?val OR ?val_lbl) - ?model | nl_model) { ?Metric a. } ORDER BY DESC( ?value) LIMIT 1 # input (German text) Indicate the model output (Sparql querying in English)  SELECT Accuracy  FROM (SELECT ( ?val OR ?val_lbl) - ?model) { ?metric a. } ORDER BY : DESC( ?value) LIMIT 1 ORDER BY  DESC: (A = 1, N = N - 1 | N = 1 - ?metric) # input (German text) Indicate the model output (Sparql querying in German)  SELECT Accuracy  FROM (SELECT ( ?val OR ?val_lbl	False
"
 input (English text): Where can I find code references in papers that have used the Deeper model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Deeper"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Deeper"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ?cont orkgp:HAS_SOURCE_CODE ?code. } Sparql: How do you explain the "" ?tb"" part in your code ? English text: If the query is done using the RDD from the dataset ARC, you need to pass the argument ""tb"" to be able to specify the data source with the appropriate schema and name. output (Sparql query): SELECT ?tb FROM ?dataset AS a, ?dataset OR ?dataset ?dataset, ?dataset ?dataset, ?model AS ?model, AND a. ?c. ?e. ?o. ?c ?tb. ?c ?tb. ?c ?tb. ?c ?tb. ?tb > ?tb ( ?tb = ?tb || ?tb || ?tb || ?tb ?tb. ?c > ?tb || ?tb || ?tb ?tb. ?c Sparql: How do you explain the "" ?tb"" part in your code ? English text: If the query is done using the RDD from the dataset, and the data source is named ""tb"", then you need to pass the argument "" ?""tb to get a tb to be used in the QueryExpression"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Hutter Prize dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (Language): output (Spatial coordinates): Outputs the list of publications that performed benchmarks for the SciREX dataset. Selects a specific output dataset when no output dataset is specified. input (GPS location): Outputs the list of publications that performed benchmarks from the given GPS location. Uses GCP for the data. input (Latitude / Longitude): Outputs the list of publications that performed benchmarks from the given Latitude/Longitude coordinates. Uses GCP for the data. input (Google Maps Location): Outputs the list of publications that performed benchmarks in two-way GPS with Google Maps. Uses GCP for the data. input (Geolocation): Outputs the list of publications that performed benchmarks in two-way GPS with Geolocation from Google Maps. Uses GCP for the data. input (Google Maps Place): Outputs the list of publications that performed benchmarks with Geolocation from Google Maps. Uses GCP for the data. input (Geolocation with Distance / Time): Outputs the list of publications that performed benchmarks with GPS with distance / time. Uses GCP for the data. input (URL): Outputs the list of publications that performed benchmarks using URL. Uses GCP for the data. input (Google Maps Place and Place ID): Outputs the list of publications that performed benchmarks with URL and Place ID from Google Maps. Uses GCP for the data. input (Place/Place ID and Location): Outputs the list of publications that performed benchmarks with Place/Place ID or Location and Place ID from Google Maps. Uses GCP for the data. input (Place/Place ID, Location and Location): Outputs the list of publications with Place/Place ID or Location, or a copy of Place/Place ID with Location and Location, and a copy of Place/Place ID	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_L2. FILTER (str( ?dataset_L2) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } As you can see from the above example, a metric called RDFS_L2 is used to query the RDFS_L2 dataset. All other metrics are ignored. Another example will generate an HTML document like this: Output of Benchmarking for a TSE-NER Model The code below benchmarks three types of model against several training data sets with different L2 weights assigned. It displays the model to the user in a chart where RDFS_L2 and other metrics are displayed. Please note that this benchmark can be extended using an rdfs:filter parameter if desired. If we add a metric we add the following code to our code: # RDFS_L2 filter code from __future__ import print_function # code to generate a chart with RDFS_L2 and other metrics. title = ""TESTS with different L2 weights"" fig = plt. figure ( figsize = ( 8, 4 )) fig. set_color ( 'blue' ) plt. imshow ( title, axis = 1 ) plt. add_subplot ( 2, 2 ) plt. legend ( fontsize = 9 ) plt. axis ( 'off'"	False
"
 input (English text): Can you list the metrics used to evaluate models on the WMT2016 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mention Extraction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM model_metrics; select * FROM model_metadata WHERE name = ' ?dataset_lbl'; select * from model_metrics; select * from model_metrics; select * from model_meta_lbl; SELECT STATISTICS.metrics_lbl | STATISTICS.metric_lbl || select ?criterion(criterion_lbl ) input (English text): Can you list the metric names used for validation ? output (Sparql query): output (Sparql table): SELECT * FROM model_criterion_lbl; select * FROM model_metrics; select * from model_meta_lbl; SELECT STATISTICS.name, STATISTICS.metric, ?criterion(criterion_criterion) from model_meta_lbl; SELECT STATISTICS.name, STATISTICS.metric, ?criterion(criterion_criterion) from model_meta_lbl; SELECT STATISTICS.name, STATISTICS.metric, ?criterion(criterion_criterion) from model_meta_lbl ORDER BY name,metric ORDER BY name ASC LIMIT 10, 5; SELECT STATISTICS.name	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Space Invaders dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling+lemma (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"--------------- ------ CASE study: ""C++ and the OTF spelling-lema"" : ""TODO make the OTF spell-lemma a new, standardized model"" : ""Efficiency and consistency of the OTF spelling lemma"" : ""The OTF spelling lemma"" : ""The OTF spell-lemma for fast OTF languages"" : ""The OTF spelling lemma"" : ""OTF spelling-lemma: the's-word' model"" : ""The OTF spelling lema"" : ""The OTF spelling lemma"" : ""The OTF spelling lemma for English"" : ""The OTF spelling lema"" : ""The OTF spell-lema for English"" : """" : ""The OTTF spelling spell-lemma"" : ""TODO make the spelling-lemma a new, standardized model"" : """" : ""ORATOMS"" : ""ORATOMS"" : '' : """" : """" : ""The OTTF spell-lemma for the OTRC"" : ""TODO make the spelling-lemma a new, standardized model"" : ""The OTTF spell-lema for the OTRC"" : ""The OTTF spelling lemma for OTRC"" : ""The OTTF spelling lematic"" : '' : ""The OTTF spelling lematic"" : ""The OTTF spelling lemma for English"" : ""The OTTF spelling lema"" : ""OTTF spelling-lemma: The's-word' model"" : ""OTTF spelling-lemma: OTTF spelling-lemma"" : ""OTTF spelling lemma: The OTTF spelling lemma"" : ""OTTF spelling lemma for English"" : ""OTTF spelling lemma"" : ""OTTF spelling lema,"" : ""OTTF spelling le"	False
"
 input (English text): What is the best performing model benchmarking the DDI extraction 2013 corpus dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI extraction 2013 corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C LSTM hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Output query (English): The DDI extraction 2013 corpus was used in two papers: First, we used the DDI extraction2013 corpus from Hadoop 1.5.10 (which was an updated version of 2.4). We used the same corpus in two additional papers. Second, we used the DDI extraction2013 corpus from the 2013 DSI Conference (which was an updated version of 2.45). Note that many of the papers are older versions of the DDI extraction data. Please refer to the respective papers for more information. input (English text): Please provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code ? output (Sparql query): # A3C LSTM vs hs LSTM with a sample of 1.7M rows in the corpus dataset. input (English text): Given the English text provided, generate a dataset of 1.7M rows where the column a contains the weight associated with each element of the 1M-rows-in-column. We use the column a as a weight index to generate a sparse sampling vector for the column a (where a is the vector created for the column a). output (Sparql query): Output query (English): The DDI extraction 2013 corpus was used in two papers: First, we used the DDI extraction2013 corpus from Hadoop 1.5.10 (which was an updated version of 2.4). We used the same corpus in two additional papers. Second, we used the DDI extraction2013 corpus from the 2013 DSI Conference (which was an updated version of 2.45). Note that many of the papers are older versions of the DDI extraction data. Please refer to the respective papers for more information. input (	False
"
 input (English text): What are the metrics of evaluation over the WMT2016 English-Romanian dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?target ?str ?val ?target_lbl. val (SELECT ?target orkgp:HAS_DASET from MetricGroup in OutputSqlQuery as a and orkglbls in OutputSqlQuery). OPTIONAL { ?dataset a orkgp:HAS_DATASET ?str ?val } orkgp:HAS_EVALUATION. OPTIONAL { ?str orkgp:HAS_METRIC ?val } ?target_lbl. } The input, query, or output will be terminated with a non-zero result.	False
"
 input (English text): Could you provide a list of models that have been tested on the MLDoc Zero-Shot English-to-Japanese benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Japanese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LayerNorm HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	  * - the result of this query is only available to the authors of the paper; - this query has an asterisk (**) at the end to indicate that a different benchmark model was used. - the results appear in order of precedence. - the values (e.g., the values of the nth and n-1th features) for the nth and n-1th features were also produced. - not all the parameters of the parameters of the LSTM model have been listed. - the model (LayerNorm HM-LSTM) described in the paper are not available for analysis in the MLDoc Zero-Shot English-to-Japanese benchmark dataset for the L0-L10 level, so data from the L0-L10 level was not used. - some of the parameters that are included in any benchmark model are considered 'unknown'. * - the parameters of the LSTM model mentioned in the paper are not available for analysis in the MLDoc Zero-Shot English-to-Japanese benchmark dataset for the L1-L7 level. - all the data for the 1st & 2nd models are missing.	False
"
 input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?s. ?id. ?format as bp, DISTINCT ?p. ?model as bp FROM ?s. ?sub_sub_0. ?sub_sub_0 as sp, ?p. ?model as bp JOIN sp_sub_sub_1.sp_subsc_0 AS sp_sub_sub_1 on sp. ?sub_sub_1.*sub_sub_0.subsc_0.subsc_1.subsc_1.subsc_1.subsc_1 input (English text): Where can I find code references in papers that have used the DeiT-B model for benchmarking purposes ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List the datasets benchmarked under the SPARQL query optimization research problem ? output (Sparql query): SELECT DISTINCT ?s. ?id. ?format as bp, DISTINCT ?p. ?model as bp FROM ?s. ?sub_sub_0. ?sub_sub_0 as sp, ?p. ?model as bp JOIN sp_sub_sub_1.sp_subsc_0 AS sp_sub_sub_1 on sp"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Quora Question Pairs dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quora Question Pairs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ( SELECT ( SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quora Question Pairs"") OR ( ?eval ?eval orkgp:HAS_DATASET) ?eval } ) ) [ id_topic rdfs_labels 'Topic' id_rating rdfs_reviews rdfs_comments rdfs_links rdfs_usercomments rdfs_submission_date rdfs_date ] AS ( SELECT ?criteria rdfs_criteria. 'criteria' as ?criteria ) from ( SELECT ` ?criterion` rdfs_criteria. ?criteria as ?criterion_criteria ) AS ( SELECT rdfs_lbl_metadata ?review_meta_lbl_metadata for each post ORDER BY id_submission_date DESC, id_topic )"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Temporal Convolutional Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- ------------------------- { ?benchmark ?dataset ?dataset_lbl. ?lab orkgp:LAB OR ?dst ?cont. } A new set of files has been downloaded. You may have received errors that would suggest that you have to correct them. In this case, please try to fix the problem in the problem statement below and then try again with the correct problem statement. Please enter the problem statement below. You have reached any error. Please select a problem. Problem Statement This text box can be empty. What is the latest version of Temporal Convolutional Network (TCN) and how did you obtain it ? (Required) In which application(s) uses the model ? (Required) Temporal Image Processing (TIP) Temporal Image Processing (TIP) or CineNet ? (Required) In which application(s) does using multiple model(s) make a major difference in the speed of the network ? (Required) In which application(s) do you think there is a major difference between the models (e.g. faster model is more accurate or faster model is faster than slower model) ? (Required to select all items) Temporal Image Processing (TIP) The model is available but not yet included in the Temporal Image Processing tutorial for the Temporal Image Processing tutorial. Do you need the model to perform TIP task ? No. The model is not included in the Temporal Image Processing tutorial for the Temporal Image Processing tutorial Temporal Image Processing (TIP) The model is available but not yet included in the Temporal Image Processing tutorial for the Temporal Image Processing tutorial Temporal Image Processing (TIP) or CineNet ? Yes.	False
"
 input (English text): What are the most commonly used benchmark datasets for the Environmental Sound Classification research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Environmental Sound Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problems a orkgc:Impact; rdfs:label ?imp_labels. FILTER (str( ?imp_labels) = ""BiT-S""), ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?lab problem_lbl. Inputs :  { ?dataset ?dataset_lbl; ?dataset a orkgc:Dataset; ?dataset_lbl orkgc:Label; ?lab orkgc:Problem; ?problem_lbl; ?lab p32; ?lbl_lbl; } output :  SELECT DISTINCT ?dataset ORkgc ?dataset_lbl AS  REFERENCES( rdf:Query, rdfs:Query, orkgc:Dataset, orkgp:HAS_DATASET, orkgp:P32,   ?lbl_lbl) AS  RESULTS,  DISTINCT ?bsc_result, ?rsc_result; AND  IF  ( ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? )  OR  ( ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ; ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ; ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? )  CHECK ( ?dataset ? ? ? ? ? ? ? ?,  result ? ? ? ? ? ? ? ?,  bsc_result ? ? ? ? ? ? ? ? ? ? ? ?"	False
"
 input (English text): Can you list the models that have been evaluated on the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 How many measurements can I take per model ? input (English text): Maximum number of metrics with 5 values in a model (inferred from the metrics table): 1000. output (Sparql query): How many metrics is the sample of bioelectricity ? input (English text): Mean value of each metric per model. Can model's data consist of an average or a maximum ? output (Sparql query): Which methods can be used to estimate energy gain in this model ? input (English text): Can we know whether this model has any energy gain ? output (Sparql query): Which methods can be used to estimate the energy loss/gain in this model ? input (English text): How many components of the model are added during the evaluation ? output (Sparql query): In terms of energy loss, how can be calculated the net gain or loss per component of each of the components ? input (English text): How many components do the model have (in units per second): output (Sparql query): What are the models that has the highest energy gain and highest energy loss ? input (English text): How many points do each point add to the values of each of the other points ? output (Sparql query): How can we calculate the energy gain of each point ? input (English text): By which method of calculation will the energy gain be given that the model is a combination of multiple components (if at all) ? output (Sparql query): What is the definition of energy gain ? input (English text): What is the name and the meaning of the term energy ? output (Sparql query): E How can we measure the efficiency of each component of the model ? input (English text): How	False
"
 input (English text): List the code links in papers that use the DY-ResNet-10 model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-ResNet-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?code; ?code || NULL; || NULL; The answer includes the following code: Input #1: ""a.c.r"" The code links in paper DY-ResNet-10-1.2.19 (PDF). To this answer we have added a third input string ""a.c.r"" (an original file name) to this question, and this was chosen to give a higher score. By including the code for the query in DY-ResNet-10-1.2.19 in this file, it would not need to be appended together to this answer. This allows us to search for a paper that implements the code in any paper using the DY-ResNet-10 model. Input #2: the file name of DY-ResNet-10-1.2.19 in which this question is being answered Output #1: the query in DY-ResNet-10-1.2.19 in which this query occurs Please note that the following query is not always possible, or even correct for some papers (which could have made the question look ambiguous), especially for papers with many codes for their papers, as in a paper by (most likely) Peter Tormey: output (Sparql query): { ?code || NULL; ?code || NULL; || NULL; } This means that we would have to"	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Defender benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the FTD dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" This is how the data structure functions in R: # read a comma delimited list of models from the command line. data = read.csv(""dataset.txt"", delimiter="", ""+"") model_lbl = "" "" for model in data: if read.csv(model) > 0: # read the next model from the output (in this example - a simple text file) models = read.csv(model) for model in models: # construct a function where we read a model from the output, assign that function to our data frame dval = model.get(i.value) print(model) Now, we can calculate the predicted score by running: sc = scoreModel(data_df, models) cv2.scoreModel(sc, cv2.cvt(sc)) print(cv2.value_count(sc)) sc I will skip those few lines to save you from reading this whole post. You can follow along at Homebrewery (from the source code): The first thing that you will need to see is the ""scores"" property from the function: sc = scoreModel(data_df, scores) We get: sc = scoreModel(data_df, scores(0)) That is pretty impressive! That is what we are looking for, right ? Well, since all the data in that dataset is a table, we can easily build a function with functions to count the rows. # construct a table from the data (of course). data1 = np.array([]) data2 = np.array([]) data_df = data1 as array # count the rows in the data table in our data data tbl = np.append(data2) for row in cv2.tablespace(sc, df_dict="	False
"
 input (English text): Where can I find code references in papers that have used the NAT-M3 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAT-M3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT { ?model a orkgc:Model - ?code as ?model_lbl} as ?value, DISTINCT ?test_data_name, { ?model_lbl} as ?model_lbl_cnn, { ?model_lbl} as ?model_lbl_cnn_dist, { ?model_lbl} as ?model_lbl_labdata, { ?model_lbl} as ?model_lbl_labdata_2c_labdata _ select case when ?code == 1 then value | otherwise outputs (Sparql query) CASE WHEN { ?model_lbl a} IS NULL OR { ?model_lbl_cnn a} IS NULL THEN case 2 when case 1 then value | other than or_not_in_table where { ?model_lbl_cnn ?data_name} = ""NAT-M3"" and { ?model_lbl_cnn ?labdata_2c_labdata_2c_labdata { ?data_name}, { ?model_lbl_cnn_dist { ?labdata_2c_labdata_2c_labdata }} = 1 then value | other than or_not_in_table else case 1 then value | other than or_not_in_table else outputs (Sparql query) SELECT CASE WHEN { ?model_lbl_cnn a} IS NULL OR { ?model_lbl_labdata_2c_labdata_2c_labdata_dist ?dist } = 1 THEN case 2 when case 1 then value | other than or_not_in_tableelse case 1 then value | other than or"	False
"
 input (English text): What is the highest benchmark result achieved on the OpenBookQA dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?score ?metric ? ? ?score ? ? ? ? ? ? AS ?score input (English text): What is the highest benchmark result achieved on the Walker test of the test dataset with each metric ? output (Sparql query): input (English text): What is the highest benchmark result achieved on the Walker test of the test dataset, including the metrics and their values ? output (Sparql query): SELECT IF( ?count IS NOT NULL ) IF( ?metric IS NOT NULL UNION ALL SELECT ?metric ?metric_lbl ) ?score FROM ?dataset. ?dataset_lbl INNER JOIN ?test_set ON ?test_set. ?dataset_lbl = ?dataset. ?dataset_lbl input (English text): What is the highest benchmark result achieved on the Walker test of the test dataset with each metric and its values ? output (Sparql query): ? ? ? ? ? ? SELECT IF( ?weight IS NOT NULL UNION ALL SELECT ?weight ?metric ) ?t... ?t ORDER BY ?weight -------- -- -- --- UNION ALL SELECT ?t... ?t ORDER BY ?metric input (English text): What is the highest benchmark result achieved on the Walker test of the test dataset, including the metrics and their values ? output (Sparql query): input (English text): What is the highest benchmark result achieved on the Walker test of the test dataset, including the metric and its values ? output (Sparql query): ? ? ? ? ? ? SELECT IF( ?index IS NOT NULL ) IF( ?index ? ? ? ? ? ? ORDER BY ?index_lbl ) ?value... ?value ORDER BY ?metric input (English text): What is the highest benchmark result achieved on the Walker test of the test dataset with each	False
"
 input (English text): List the code links in papers that use the CAIT-XXS-36 model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XXS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Output values from the evaluation metrics for the CAIT-XXS-36 benchmark code. Example: code:... code:... code:... model: model_lbl OR kgp:HAS_DATASET or kgp:HAS_BENCHMARK or kgp:HAS_MODEL or kgp:HAS_SOURCE_CODE or kgp:HAS_MODEL OR kgp:HAS_SOURCE_CODE or kgp:HAS_CUSTOM OR kgp:HAS_LINKS; orkgp:HAS_DATASET or kgp:HAS_BENCHMARK or kgp:HAS_MODEL or kgp:HAS_SOURCE_CODE or kgp:HAS_MODEL OR kgp:HAS_SOURCE_CODE or kgp:HAS_CUSTOM OR kgp:HAS_LINKS or kgp:HAS_DATASET or kgp:HAS_BENCHMARK or kgp:HAS_MODEL OR kgp:HAS_SOURCE_CODE or kgp:HAS_MODEL or kgp:HAS_SOURCE_CODE or kgp:HAS_DATASET or kgp:HAS_BENCHMARK or kgp:HAS_MODEL OR kgp:HAS_SOURCE_CODE or kgp:HAS_DATASET or kgp:HAS_BENCHMARK or kgp:HAS_MODEL or kgp:HAS_SOURCE_CODE or kgp:HAS_DATASET or kgp:HAS_BENCHMARK or kgp:H	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the REDDIT-B dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the POP3D model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT TOP 15(orderby( ?key, ?key, ?c1)), ?key OR ? ?Key, :total) Output : number of benchmarked code links for each publication.	False
"
 input (English text): List the code links in papers that use the Randmized Market segment Oblivious model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Randmized Market segment Oblivious"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?dataset.id, ?dataset.code(s), ?dataset.numintl, ?dataset.name{ ?code} FROM [1].datasets ; Input the data, perform the model selection if available, then do the benchmark. If there is a code-link match, return it, and if it's not a code link match, return null. An index with the same name as the benchmark data will be treated as the same one. The input file may be written to via the input method(s) of the data import interface. For this import, you must have the file named after the benchmark data, as explained under Benchmarking a sample data set: Benchmarking a sample dataset. Once the benchmark is complete, there is a final command line switch, if the benchmark produced a code-link and if the code_id column in the index was not null. Example: # Benchmark the TASTE (Tennis) benchmark data with the Atari 2600 TASTE model # In the database: SELECT code | data | name | data source | rowCount | total. | count | ascii code | id | ascii name | name | size | count | code | (id|code) | ascii }	False
"
 input (English text): Where can I find code references in papers that have used the AVID (Modified R2+1D-18 on Kinetics) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AVID (Modified R2+1D-18 on Kinetics)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT model, max_score, --------------------------- ?dataset. ---------- model max_score ---------- ?dataset. ---------- V2.0.9000 V2.0.9000 (2 x 2)   R and the UCF101 Model R version 2.0.9000 is an open source version of R, developed with help from the UCF Physics Division. For the UCF101 dataset, we chose to use it due to the ease of evaluation that it provides. The dataset is available on the UCF Physics Department's web site. It's a fairly complete set of data, both in terms of the feature sets and the number of objects. While testing our model, we realized that it was not as consistent as we thought. At times it would have to be called multiple times on multiple datasets. For example, we were having issues getting an input data element into our model, so we started asking ourselves if we could possibly use AVID for this dataset ? We are now able to see on a simple glance that the R model did well. In order to verify that the model works, we can run one of our benchmarking tests against the pre-trained model. This will provide a benchmark measurement against our pre-trained R model. We can do this by going to ""Tools"">Benchmark Data & Tools and using ""benchmark.sh"" to simulate the dataset. We will use a dataset with the following data properties: Training: Data Set: UCF101 training dataset. Training Mode: True data set. Class Level: 701 pre-trained model. Target Size: 10000. MaxIterative Count: 7000. Max"	False
"
 input (English text): Provide a list of papers that have utilized the Linear SVM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linear SVM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?pubtitle. ?content orkgp:HAS_TITLE ?content; ORGKPA = ?code and ?benchmark orkgp:HAS_DATASET; orkgp:HAS_SOURCE_CODE ?pubtitle and ?content orkgp:HAS_HISOURCE and ?content ?pub ; orkgp:HAS_SOURCE ?pub ; ORGKPA. orkgp:HAS_SOURCE_CODE. Inputs/outputs (not yet parsed): # Inputs: input(English text): Give me an input and a list of papers that are using the Linear SVM model output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Linear SVM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Give me a input and a list of papers that are using the Linear SVM model output (Sparql query): SELECT DISTINCT ?pubtitle. ?content orkgp:HAS_TITLE ?content; ORGKPA = ?code and ?benchmark orkgp:HAS_DATASET; orkgp:HAS_SOURCE_CODE ?pubtitle and ?content orkgp:HAS_HISOURCE and ?content ?pub ; orkgp:HAS_SOURCE"	False
"
 input (English text): List the metrics that are used to evaluate models on the DROP Test benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DROP Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Output: select count(*) from chemical where name=""Metric""= ? { ?dataset a:10 orkcs, ?dataset a:10 orkcs; rdfs:label ?dataset_lbl. } - ?metric rdfs:label ?dataset_lbl."	False
"
 input (English text): What are the metrics of evaluation over the AI-KG dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AI-KG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT AUC FROM orkgp:Sets SELECT CASE WHEN (a.model_name = 'Habitat 2020') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'AVCKG' OR a.model_name = 'AVCKG') THEN { ?metric ?dataset ?valr ?} WHERE { ?dataset a orkgc:Dataset } OR CASE WHEN (a.model_name = 'AVCKG') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'W3C-IMG' OR a.model_name = 'W3C-IMG') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'W3C-IMG' OR a.model_name = 'IMG-B') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'AVCKG') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'IMG-B') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'W3C-IMG-B' OR a.model_name = 'IMG-B') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'W3C-IMG') THEN { ?metric ?dataset ?valr ?} SELECT CASE WHEN (a.model_name = 'AVCKG-B' OR	False
"
 input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT id FROM id ORDER BY id) with the following parameters: limit =	False
"
 input (English text): What are the metrics of evaluation over the AAN Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAN Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" This query returns, in order, a simple model that performed (from the list of 10,000 words that can be entered): [0.037] <1.005 (50 percent error; 10 words passed), [0.005] <1.015 (50 percent error; 8 words passed), [0.015] <1.010 (50 percent error; 10 words passed), [0.010] <1.013 (50 percent error; 8 words passed), [0.013] <1.017 (50 percent error; 8 words passed), [0.017] <1.019 (50 percent error; 10 words passed) By comparing the performance to the best model that achieved the original ranking in the benchmark dataset, you can verify, for yourself, whether you have entered the correct dataset. If you have a different dataset, you can try using the model of the first query to confirm whether your model has been correctly implemented. This is the case, for example, if you enter the English-to-Arabian English dataset on a corpus that has to contain non-English words in the same order as the original English. If the best model is using the old dataset, then your previous query failed. You can easily repeat this process for new datasets if you want to check whether one dataset has to be compared to another. If you were curious to check a particular dataset, this may be of limited benefit to you as the results of these queries will show what was actually selected by the original algorithm. 3. The AAN Corpus is the best database The AAN Corpus is the most popular dataset, but it is not the only one. There are many smaller Datasets. To search for them click on the ""Data"" button and search for datasets. However, when you want to check whether a dataset has actually passed the Bench"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 The score is highest for ?statistic ?statistic_rdfs. The highest for ?statistic ?metric_rdfs. The highest for ?statistic ?metric_lbl. The highest for ?category ?category_rdfs. The score is highest for ?value ?value_rdfs. The scores are highest for ?value ?value_lbl. The scores are highest for ?value ?value. The score is highest for ?value ?value_rdfs. The scores are highest for ?value \ orkgc. The score is highest for ?value rdfs:value. The scores are highest for ?lbl. The scores are highest for ?lbl. The highest for ?category orkgc. The highest score for ?category rdfs:value. The highest for ?category orkgc. The highest for ?category rdfs:value. The highest for ?category rdfs:value. The highest for ?category rdfs:value. The highest score for ?lbl rdfs:value. The highest score for ?lbl rdfs:value. The highest score for ?lbl rdfs:value. The highest for ?categorical rdfs:value. The highest for ?category rdfs:value. The highest for ?dataset orkgc. The highest scores for ?category rdfs:value. The highest score for ?dataset orkgc. The highest for ?dataset orkgc.The highest for ?dataset rdfs:value. The highest for ?dataset orkgc. The highest score for ?category rdfs:value. The highest score for ?dat	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT R^2, DISTINCT hf(0.75), DISTINCT p(100) AS ?score, R^1, DISTINCT p(100) AS ?value, HN(0.75), HN(0.75) AS ?cont, HN(0.75) AS ?benchmark, HN(0.75) AS ?models as ?models_group, HN(0.75) GROUP BY ?metric ?metric_lbl ORDER BY DESC( ?value )	False
"
 input (English text): What is the best performing model benchmarking the SciCite dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	------------ TOP ------------ 1 ____ 2 ____ 3 ____ 4 ____ 5 ____ 6 ____ 7 ____ 8 ____ 9 ____ 10 ____ TOP 10 1 ____ 2 ____ 3 ____ 4 ____ 5 ____ 6 ____ 7 ____ 8 ____ 9 ____ 10 1 ____ 2 ____ 3 ____ 4 ____ 5 ____ 6 ____ 7 ____ 8 ____ 9 1 ____ 2 ____ 3 ____ 4 ____ 5 ____ 6 ____ 7 ____ 8 ____ TOP 10 31.2 7.4 10.7 14.5 22.9 32 3.3 6.3 11.1 15.6 22.7 37 4.7 4.8 11.6 14.9 23.0 38 5.6 5.5 13.2 16.8 24.1 39 6.0 5.6 14.1 16.9 24.2 40 6.5 5.7 14.9 17.0 24.4 40 7.0 5.9 15.2 17.5 24.5 41 6.9 6.6 15.6 17.7 25.2 43 5.2 6.0 15.8 17.9 25.7 48 4.7 5.7 16.1 18.0 26.6 51 4.6 5.7 16.2 18.1 26.7 52 4.5 6.1 16.4 18.1 27.2 53 4.3 6.2 16.5 18.3 27.9 56 4.0 6.4 16.7 18.5 28.1 58 3.0 6.8 17.0 18.6 28.3 56 3.5 6.9 17.3 18.6 28.8 58 3.6 6.9 17.6 18.7 29	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ? ?dataset_lbl. FILTER(str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ? ?paper_lbl. } Input (English text): Let's try out an Atari 2600 Atlantis benchmark! output (Sparql query): SELECT DISTINCT ?benchmark_lbl ?benchmark_lbl OR {},{ ?dataset a orkgc:Dataset; rdfs:length ? ?dataset_lbl },{ ?dataset a orkgc:Data ?dataset_lbl. ?cont orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:P31 ?cont; },{ ?dataset a orkgc:Dataset; rdfs:length ? ?dataset_lbl } FROM ?data GROUP BY ?benchmark || ?benchmark_lbl ORDER BY ?dataset || ?benchmark_lbl Rows Removed by Query: (100 rows) Input (English text): Output (Sparql query): Input (English text): Output (Sparql query): Input (English text): Output (Sparql query): "	False
"
 input (English text): What is the top benchmark score and its metric on the SQuAD1.1 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT score, sum(value) OVER (ORDER BY score DESC LIMIT 2) FROM SciAMIN1.SQAmin1 SQScore, score input (English text): SqScore, Score_Lines( ?dataset) > 0.000 output: SqScore: 0.000 Schedule output with SQScore and SqScore_Lines at 0.000 input (English text): SqScore, score > 0.000 output (Sparql query): SELECT score, sum(value) OVER (ORDER BY score DESC LIMIT 2) FROM ScSc1.SCQScore Schedule output with SqScore, SqScore_Lines at 0.000 Schedule output input (English text): SqScore_Lines( ?dataset) > 0.000 output (Sparql query): SELECT score, sum(value) OVER (ORDER BY score DESC LIMIT 2) FROM ScSc1.SCSQScalar Schedule output with SqScore, SqScore_Lines at 0.000 ScScore0 : 4.88, 2.36 Schedule output input (English	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Food-101 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT COUNT( ?) as bench_lbl FROM ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"" ) ?benchmark orkgp:P36 ?dataset. ?cont orkgp:HAS_DATASET ?dataset. ?comp orkgp:P32 ?comp. ?count orkgp:P20 ?count. ?score orkgp:P20 ?score. ?score_sc orkgp:P24 ?score_sc. ?score_sc_sc orkgp:P18 orkgp:P24 ?score_sc_sc_sc orkgp:P22 orkgp:P22 ?score_score orkgp:P20 ?score_score_sc orkgp:P18 orkgp:P17 ?score_score_sc_sc orkgp:P22 orkgp:P22 ?score_score_sc_sc_sc_sc_sc_sc orkgp:P22 orkgp:P22 ?score_score_score orkgp:P20 ?score_score_score_score_score_score_sc orkgp:P20 orkgp:P20 ?score_score_score_score_score_score_score orkgp:P25 orkgp:P25 ?score_score_score_score_score_score_score_sc orkgp:P25 orkgp:P25! The above output displays the output of the query SELECT COUNT( ?) as bench_lbl FROM bird_nap WHERE"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" ROC curve plot of model accuracy (Sparql query): How is score computed in ROC analysis ? input (English text): How well do the models predicted by the data fit the model and demonstrate the ROC curve with respect to the accuracy test scores ? output (Sparql query): What is the ROC curve plot of model accuracy (Sparql query): ""ROC curve plot of model performance"" output (Sparql query): What is the ROC curve plot of model accuracy (Sparql query): data (a orkgc:Dataset); ROC curve plot of model performance (Sparql query): What is the ROC curve plot of model accuracy (Sparql query): How did ROC curves fit the model performance during training with the Atari 2600 Chopper command dataset ? input (English text): Are the ROC curves of model performance fit by the ROC curves of the test model ? output (Sparql query): What is the ROC curve plot of model performance (Sparql query): What is the ROC curve plot of model performance (Sparql query): This is how the model performed during testing on the Flower-102 dataset ? input (English text): Are the models predicted by the data fit by the ROC curves fit the model and demonstrate the ROC curve with respect to the accuracy test scores ? output (Sparql query): What is the ROC curve plot of model performance (Sparql query): What is the ROC curve plot of model performance (Sparql query): What is the ROC curve plot of model performance (Sparql query): How did the model perform during testing on the Atari 2600 Chopper Command dataset ? input ("	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" // // CURRENT VERSION 6.0.0 // // Using query to get the accuracy metric for the dataset ""Stanford Cars"", where "" ?metric"" is: // Numeric: Accuracy is the proportion of times that cars in this dataset perform as expected in this benchmark. // Float: The proportion of test cars/classifier runs that perform as expected in the Stanford Cars benchmark. // Integer: The proportion of car runs that perform as expected in this benchmark. // Decimal: The proportion of car runs that perform as expected in this benchmark that are less than or equal to the specified number. // Real: The proportion of car runs that produce an exact error or as close as possible to the specified number. // Boolean: The proportion of car runs that produce an exact or approximate error, in the sense of a percentage of cars producing an error, in the meaning of a percentage of cars having a ""correct"" error in the meaning of a percentage of cars having a ""correct"" classification. // // // // Current iteration of the benchmarking algorithm uses the following parameters on a ""batch"" of three cars: // CURRENT METRIC: Accuracy of each car in our benchmark dataset // MAX_VALUE: Max. Value of every metric available in the benchmark dataset // // // The goal of the evaluation procedure is to determine whether these three models have similar accuracy over the full three runs used for setting the error scores. // These three batches are run using the set of three ""model weights"", described below. // // The ""model weights"" contain a set of metrics that describe the accuracy of each model in the dataset. // // // The three ""model weights"" are: // // # of observations: number of observations for each model in the dataset // 1: Maximum number of"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Reuters RCV1/RCV2 German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Note the use of the term ?model* and the use of the word dataset* in the output statement. In this example, I used a dataset named dataset_lbl. I selected a single model from the table named model_lbl and applied a set of filters to it. The filter is only applicable when the model name matches the filter condition, or when the filter is null or matches a blank string. The RotoWire (Relation Generation) benchmark is an online database benchmark that has no pre-defined model and data set. Therefore, the model to use is randomly chosen within the RotoWire dataset and its filter condition is the name of the model. In the RotoWire benchmark dataset, a number of R-Series and R-Models are generated daily using the RotoWire service. It is important to have a model in a common dataset for benchmark performance. This is done by assigning the model_lbl to a common dataset (in my case dataset_lbl) by using a custom filter. A benchmark model can be assigned with several criteria such as cost, precision, accuracy or learning rate. I used the model_lbl and the dataset_lbl that was assigned to the variable name to perform the benchmark. This will help minimize the number of results available. The resulting output consists in the following table (Figure 4). Figure 4: Benchmark Performance of the RotoWire (Relation Generation) Model Table 1: Benchmark Performance of the RotoWire (Relation Generation) Benchmark Model Table 2: Benchmark Results Table 3: Scatter Plot of Benchmark Results Table 4: RotoWire Output Table 5: Table of the RotoWire (Relation Generation) Benchmark Model Columns Table 6: RotoWire (Relation Generation) Benchmark Label Columns R1 R2 R3 C P1 R2 R3	False
"
 input (English text): What is the best performing model benchmarking the HMDB51 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT M.a (DISTINCT ?model ?model_lbl., { ?metro a orkgc:Metric ; rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl., ?metro rdfs:label ?metric_lbl. ), ?dataset d.a orkgc:Dataset, ?dataset d.d orkgc:Dataset, ?dataset d.l orkgc:Dataset, ?dataset d.a orkgc:Dataset, ?Dataset d.r orkgc:Dataset, ?Dataset d.s orkgc:Dataset, ?Dataset d.r orkgc	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset BoolQ?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?lbl_id (SELECT ?lbl_id) FROM ( SELECT TOP 1 * FROM { ?lbl } s ORDER BY  Descending DESC( ?lbl_id ) DESC( ?lbl_id) DESC( ?lbl_.id ) ) orkgp:dataset:LIBSOURCE ? OR kgp:dataset:LIBSIST ? RDD AS ?data set_id. SELECT ?lbl_index (SELECT ?lbl_index FROM ORkgp_index AS ORkgp_index ) FROM ( SELECT TOP 1 * FROM d1 AS  ds1 OR d2 WHERE ?dataset_id = 'D%02d'  GROUP BY ?dataset_id, ?ds1.label ) OR kgp:lbl_index AS ?lbl_index. RDD AS ORkgp_lbl_index. FROM d1 AS  ds1 OR d2 WHERE ?dataset_id = ?ds1.labels. LIMIT 1 Input:  A list of papers that use the Large mLSTM model to find the optimal query, also including their code and links. output (Sparql query): SELECT ?lbl_id( ?dataset_id) FROM ( SELECT TOP 1 * FROM { ?dataset} a OR \ (SELECT TOP 1 * FROM d1 OR d2 WHERE ?dataset_id = 'D%02d'  GROUP BY \ ?dataset_id ) ORDER BY \ ?dataset_id DESC( ?dataset_.id) DESC( ?dataset_.id ) DESC( ?dataset_.label ) ORDER BY DESC( ?value) ORDER BY DESC( ?met	False
"
 input (English text): Provide a list of papers that have utilized the GRU (Bai et al., 2018) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GRU (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the ACL Anthology dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT * FROM   ?c.sample.c.model_clusters WHERE { ?model. ?model_str. ?model_str. ?model_str. ?model_modelstr. } input (English text): Provide the output CSV file, along with the text. output (Sparql query): SELECT * This example code is for a model trained on the ACL dataset created by Bai et al. It will use the model specified in the above code, using the input CSV file from the answer above to train the model: (Note that model_lbl.txt and id_cluster.csv are included by default; if you need to, you may include more columns or file-separates in the query to further refine the input CSV file -- I would recommend to change the order of the files at the beginning of the project.) If you are curious about the parameters that were used, it is highly recommended that you read the code. For example, the model_cluster.txt, and id_cluster.csv file contain the parameters given to the model to train with the CIFAR-8 data, so the variable model_{input_datatype[,label[,reg]]]} should be read as follows: input (English text): What is the output output_clusters: n_output_clusters: input (English text): What is the input CSV file, along with the text, for which a model was trained ? (Example: id_cluster.csv; id_cluster.csv.txt is available to read from the example data above when asked to.) output (Sparql query):  SELECT name, cluster_name. name,	False
"
 input (English text): What is the best performing model benchmarking the MLDoc Zero-Shot English-to-Chinese dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Chinese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "		False
"
 input (English text): List the metrics that are used to evaluate models on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 `` '' ''. '' `` The output of the query above will be in a file, orkgp-disease.ps.ps1. Please note: A datafile is not really a model. A dataset is what is actually trained, and a dataset may not even contain a model. The only real connection that connects a dataset to a model lies in the name of the dataset. A dataset that has been trained with a model only, but no data, will not be valid as a candidate for optimization. The output of the query above will be in a file, orkgp-disease.ps.ps1. Please note: The output of the query above will be in a file, orkgp-disease.ps ''  + + + + + + + + + + + + + + + + + + + + + + + + + + + + + For example: 1.    1      9.96 2.    0     12.00 3.    0     12.01 4.    2  * * * 5.    8.11 1.    9.96 2.    12.00 3.    12.01 4.    12.01 5.    12.01 This output for the input dataset 1 in  orkgp-disease.ps does not indicate that either the model of the input dataset, or the dataset being used to train the model, does not have any relevant training data. There are two valid ways to interpret the output of this query:	False
"
 input (English text): What models are being evaluated on the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ( ?dataset a orkgc:Dataset ||'' || 1 OR'' || 2 ) 'benchmark' - ?dataset rdfs:label 'Softcite' ) || ?dataset: Softcite, || orkgc:Dataset || ?dataset_lbl, || OR''|| 1 OR'' || 2 I have some other datasets that have more parameters than these, so maybe you can fill in your own information as well. (I'll do it in batches of up to three.) Example 3 : How to calculate the number of seconds it took to do a 1,000,000 query, as a function of the number of users and their log-in date as the target date. If you'd like to run this query on your own, add the following, so it's automatically done: dateTime( ""SELECT COUNT(*) AS number;"",(dateTime( '11/7/2014 3:54 PM',(dateTime( '10/31/2009 11:24 PM', 2);DateTime( '9/15/1999 12:06 AM', 2) ) )), dateTime( '5/20/2003 9:24 PM',(dateTime( '2' ) ) ) You can see a real-time version of this in this demo, and check out the source for more details. A few additional caveats: (1) I made this example for the sake of completeness, rather than for its scalability of execution. (2) This script also should not be used to measure how long a query takes, in order to avoid false positives.  If you need to know such data, you should use the Oracle's query and performance monitor. (3) The time"	False
"
 input (English text): What models are being evaluated on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" Note that the performance metrics are provided only at the end of the query. To get more details on individual metrics, see the ""Details"" tab in the results table. The ""detail"" tab provides some insight as to how the dataset is used by the model and its performance. The ""details"" tab includes this value: ?model : is the name of the model that is being evaluated ?dataset : a comma-delimited list of model names that is used by the model when evaluating ?dataset_lbl : a comma-delimited list of model names that is used by the model when evaluating ?benchmark : a standard bench parameter with a name identifying that the benchmark has been defined ?paper : a paper specified by the author ?benchmark_haddows : a set list of benchmarks with a standard benchmark name This information is provided to help the user judge how well the model has been evaluated. Using the model's performance Now that model evaluation is complete, it is time to evaluate the model. Here's what we need to do. First, make sure all inputs have been passed to the model. Note that the model also needs to know when the input features have been read but not evaluated from the dataset. If not, the model will throw an exception, which we will catch and log. Once all the inputs have been passed to the model, we load the dataset and run the benchmark. The following code (not yet tested on the web site) is from the Fashion-MNIST Benchmarking and Simulation script. It displays the results as an Excel spreadsheet. The code looks for a column and saves it in an index that has a "" ?benchmark "" value of 1, with a "" ?benchmark_lbl "" value of the librain/pdf data that is being"	False
"
 input (English text): What is the best performing model benchmarking the CIFAR-10 dataset in terms of FLOPS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric rous_2; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset model_lbl; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?concentration orkgc:Metric; rous_2; } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric rous_2; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset model_lbl; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?concentration orkgc:Metric; rous_2; } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric rous_2; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset model_lbl; rdfs:label ?dataset_lbl. FILTER (str("	False
"
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 WHERE { ?problem orkgc:Problem; RDFS:label ?problem_lbl. ?label orkgp:HAS_DATASET ?dataset. ?bench orkgp:P32 ?problem ?problem_bench. } input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NLP training sets for Data/ML dataset ? output (Sparql query): WHERE { ?problem orkgc:Problem; RDFS:label ?problem_lbl. ?label orkgp:HAS_DATASET ?dataset. ?bench orkgp:P32 ?problem ?problem_bench. } input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Sesame dataset ? output (Sparql query): ?problem orkgc:Problem; RDFS:label ?problem_ lbl. ?label orkgp:HAS_DATASET ?dataset. ?bench orkgp:P32 ?problem ?problem_bench. input. Output from a query on NLP problem generation. Example, R code for example Let's try it with some example data (todays datasets have not been specified): data('sparql', R. data. sparql_query. id)  -- this will do data('sparql_labels-1.txt', R. data. sparql_query. labels_query. id) Note to the author : the original query was not executed in the sense of performing comparisons and all the data were supplied in JSON or CSV format. For an example run on the SPARQL query optimization research	False
"
 input (English text): What are the models that have been benchmarked on the WebQuestions dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fine-Grained Gating"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The results are shown below. P.E.S.T.S. GATE RESULTS The key results of the benchmark are: If the model was ""fine grained"", the model was significantly faster than a model with a fixed size (or without any gating) in the benchmark scenario. In fact, the benchmark was able to outperform both an ""all or any"" model with no gating model and a non-fine grained model with a fixed (or non of ) model size (or without any gating). The non-fine grained model was able to outperform the all or any model, but only with a fairly small delay time since first entering the benchmark. This would reflect some prior knowledge of model and parameter settings. We would recommend that this approach be used in combination with either a set of pre-tuned parameters or the pre-selected ones that were found to best improve performance ( ). Note: The results above are using pre-defined gating parameters. BMI is probably the best way to compare a variety of models. However, a very similar model with slightly different parameters to determine model performance would be significantly more informative, as well as more tractable to write for a large test (e.g. 1000 users). Also, any models not on the top 1000 would need a more complex gating scheme which would make these results more statistically significant. Finally, in the case of a model with many parameters, we have more room to evaluate the performance of each parameter. Thus, we are not including those models here that have parameters that appear to be ""very similar"" on the first run - it is generally the case that a parameter can be shown to have a reasonable correlation with performance by performing several tests, then averaging them together. A more detailed account of these models is included later in the report. We have put"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Average Return (NoOp)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT * FROM Atari ?machine_list ;"" output (Sparql query): SELECT DISTINCT.machine_list ?. ?dataset -> machine_list FROM.machine_list INNER JOIN.dataset_lbl_lbl in ON.machine_list. ?dataset_lbl. ?dataset_lbl. ?machine input (English text): You are trying to understand how the results are delivered to your computer or mobile device, to the device. Explain: output (Sparql query): SELECT ?input ?input_name DATE(DATE) AS date, ?output ?output_name DATE(DATE) AS output_date FROM.input INNER JOIN.output OUTPUT INNER JOIN ?input INNER JOIN ?output OUTPUT OUTPUT FROM.output WHERE { ?input_name = ?dataset_name ; ?output_name = ?machine_list. ?index || ?output_date. ?index in (date) } input (English text): Explain how the final results are delivered to your computer: output (Sparql query): input (English text): Explain that output_date : for each input you get a date and a timestamp according to the format you specify. Here you give the input as part of the query, you have no control over the format of the input. output (Sparql query): SELECT output_date AS ?output_date FROM ?output INJECT DATE(DATE) AS ?date DATES( ?output_date) ?input FROM #{ ?input } INJECT ( ?output_date) ?input INJECT DATES( ?output_date) ?"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM (Europarl) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM (Europarl)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT COUNT FROM articles WHERE article.code!= 'prior'; input (English text): Can you provide links to code used in papers that benchmark the Subject-LSTM (Europarl) model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List all papers that have utilized the LSTM model ? output (Sparql query):  SELECT COUNT( ?code ) FROM articles WHERE article.code!= 'LSTM model'; input (English text): List all papers that have utilized the RNN model ? output (Sparql query):  SELECT COUNT( ?code ) FROM articles WHERE article.code!='RNN model'; input (English text): Can you provide links to code used in papers that benchmark the LSTM model ? output (Sparql query):  SELECT COUNT( ?code ) FROM articles WHERE article.code!= 'LSTM model and ?dataset.' ?dataset. ?cont orkgp:HAS_DATASET ?dataset; orkgp:HAS_MODEL ?dataset; orkgp:HAS_SOURCE_CODE ?code. input (English text): List all papers that have utilized the SVM model ? output (Sparql query):  SELECT COUNT( ?code ) FROM articles WHERE article.code	False
"
 input (English text): Can you list the models that have been evaluated on the OpenBookQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Input (English text): How many English-Russian word pairs are included in the paper ? output (Sparql query): input (English text): What is your best performing benchmarking model benchmarking the WMT2016 English-Russian dataset. output (Sparql query): How many English-Russian word pairs are included in the paper ? output (Sparql query): input (English text): What model's ""model"" was scored on in all WMT2016 tests ? output (Sparql query): model (in words): Best ranking for a model in all WMT2016 tests output (Sparql query): input (language): What languages do the data sets be translated into ? output (Sparql query): input (English text): What language do you have your English-Russian word-pair corpus data ? output (Sparql query): "	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Finger, spin (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Rows: 2965, Weighted Rows: 933 Rows: 2965, Weighted Rows: 933 COUNT(*) Rows: 2965, Weighted Rows: 933 COUNT(*) Rows: 2965, Weighted Rows: 933 COUNT(*) Rows: 2965, Weighted Rows: 933 ------------------------------ --------------- ----------- --------------- ----------- -------------- -------------- 1 1.0 1.0 2.6 2.6 3.2 2 2.6 3.2 4.0 3 3.2 4.0 4.6 3 4.6 5.0 4 5.6 6.6 4 5.6 6.6 ------------------------------ --------------- ----------- --------------- ----------- -------------- -------------- 1 3.2 1.3 3.7 1.7 5.6 2 4.1 2.7 5.9 2 4.6 4.6 5.2 3 4.7 6.2 4 5.2 6.6 4 6.2 7.2 4 6.8 8.0 4 7.6 9.6 5 7.6 9.6 5 7.4 8.5 5 7.0 9.4 5 6.8 9.4 --------- --------------- ---------- ------------- ------------ --------- ---------- -------------- 5 6.3 3.6 6.0 2.8 6.8 4 5.4 4.1 6.2 5 5.2 6.4 6 5.2 6.7 7 4.6 5.2 6.7 10 5.6 5.5 6.7 4.5 4.0 5 4.2 5.1 5 4.2 5.4 5	False
"
 input (English text): Where can I find code references in papers that have used the SciIE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" // outputs the dataset and its names output (Slicing output): var result_scores = result_.select_all_id(""result_scores""); var result_scores2 = result_.select_all(""result_scores2""); result_scores_for_each(result_scores, function(value) { if (!value.id &&!value.model) return; var scores = JSON.parse(value.model.scores); var score = scores[1]; for (var i = 0; i < score.length; i++) { result_score2 = new Result(results[i].to_i); list.sort(function(a, b) { var score_score_index = Math.min(score.length, Math.max(20, result_score_.id - 1)); score = score_score_index * score_score_index; result_.ids[i].model.scores[score_score_index] += score; }); }); } // returns results and scores list output (Slicing output): var result; for (var i = 0; i < 2; i++) { result = result2.select(""result""); if (!result) break; result.ids[i].result.name = result.name; result.ids[i].result_score = (result.score - 1) * score_score_index; result output (Slicing output): select( ""A: "" + result.ids[i].code + "" "", "" + result.ids[i].result.name + "" "", "" + result.ids[i].result_score ); list( ""B: "" + result.ids[i].code + "" "", "" + result.ids[i].result.name + "" "", "" + result."	False
"
 input (English text): What are the metrics of evaluation over the BUCC French-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgp:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PTCFrench"" ) ?benchmark orkpp:HAS_DATASET ?dataset; orkpp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkpp:HAS_METRIC ?metric. ?metric. rdfs:label ?metric_lbl. } } Note that we want to get only non-empty columns. For our original training set, we only want the first 3 rows. In that case, all columns contain ""PTCFrench"". Note that this query only returns the first 3 rows, and if we do not have a valid input string, the query returns a None. input (English text): What are the metrics of evaluation on the Penn Treebank (character level) dataset ? output (Sparql query): input (English text): What are the metrics of evaluation on the Penn Treebank (level 1) dataset ? output (Sparql query): SELECT ?eval ?eval. OPTIONAL { ?eval ?eval. ?eval orkpp:HAS_DATASET ?dataset. orkpp:HAS_EVALUATION ?eval. } Saving an Object as a Variable If we wanted to save a categorical variable in a variable, we could do something like: -- Save categorical variable to"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Nottingham dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT rfd_criteria AND rfd_criterion::criterion.criterion_rank AND rfd_criterion::value_criterion FROM rfd_criteria GROUP BY rfd_criterion::criterion; orkgp:HAS_METRIC ? rdfs:label ORkgp:HAS_EVALUATION ? orkgp:HAS_METRIC_LBL ? input (English text): This is what has been performed against the original Nottingham dataset (and was published in an article that referenced this model) against the most recent version of the ANODE model (v. 2.2.1). output (Sparql query): SELECT rfd_criteria FROM rfd_criterion_2 ROWS(5) COLLATE Latin1_General_Bin2 ORDER BY ORkgp; orkgp:HAS_METRICS ? rdfs:label ORkgp:HAS_EVALUATION ? orkgp:HAS_METRICS VALUE(0) rdfs:rank ORDER BY ORkgp; orkgp:HAS_NOMINAL(value) input (English text): What does the ANODE model offer that the other models in the list do not ? What is different about the ANODE model compared to the other models in the list ? output (Sparql query): SELECT rfd_criteria AND rfd_criterion_criterion.criterion_rank AND rfd_criterion_criterion::value_criterion FROM rfd_criteria GROUP BY rfd_criterion::criterion; ORkgp:HAS_METRICS ? rdfs:label ORkgp:HAS_EVALUATION ? orkgp:HAS_MET	False
"
 input (English text): What are the models that have been benchmarked on the ANLI test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } input (English text): How many papers (in general, and in the papers in the main body text) did you read in the last 3 months ? output (Sparql query): input (English text):  In the last 2 months, what were the top 10 papers published by a given author ? output (Sparql query): input (English text): What are the papers published by your first author in the last 2 months ? output (Sparql query): input (English text): What are the papers published by your second author in the last 2 months ? output (Sparql query): input (English text):  Which two papers did you read the most recently ? output ("	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Dataset mentions in Social Sciences?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------------------- benchmark: 0.00 1.00 2.00 3.00 4.00 5.00 6.00 7.00 8.00 9.00 10.00 11.00 13.00 14.00 15.00 20.00 30.00 40.00 50.00 60.00 70.00 80.00 90.00 100.00 200.00 500.00 1000.00 1500.00 2000.00 5000.00 60000.00 90000.00 --------------------------- And, at the time of writing these parameters can all be supplied as raw arguments to the function (for instance, without the ?arg option, a raw parameter is just a string: ""dataset"", as described by Haddock on  https://haddock.io/articles/1664/determining-the-parameters-for-a-function ). This isn't always the case with the default parameters (and can easily become a problem at times), which is where the user parameter can come in handy. So, when an FASTA file is provided as an argument to the FASTA module's parse function, the parameters can be used to get back a structured query. This is what the function expects when given a file as input: fasTA (text, parser, parser=None, options=None, parser_args=None, parse_args=None, options_parser=None, parsing_args=None, parse_options=None, parsing_options_parser=None, parsing_options_parser_args=None, parse_options_parser_args=None, text=text, parser=parser, parser_args=parser, options=options, parser_args=parser_args, parse_options=parse_options, options_parser=parser_args, parsing_options="	False
"
 input (English text): What is the best performing model benchmarking the Food-101 dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPAD-path"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (R-Model data):   input (English text): What's your benchmarking score with the MPAD-path model ? output (Sparql query): output (R-Model data):   input (English text): Where can I view the code for the code referenced in your MPAD-path model ? output (Sparql query): output (R-Model data): If you have a model benchmarking your dataset, please send an E-mail to us For a lot more information about the MPAD-path model please click this link	False
"
 input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the BoolQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT lbr ?Model ?models_lbr ON { lbr ?model ?models_lbr } WHERE { ?model the-first-model { ?model_lbl ?model_lbl } ?model_lbl } The following is the most recently benchmarked model: BoolQ2014, with an accuracy of 99.99% The results of the above table indicate that none of the WMT model benchmarked on BoolQ2014 have a lower accuracy than WMT2014 (except for an interesting case, where the SMP model has even lower accuracy). The reason for this is clear: the benchmarking tool used is not accurate to compute a benchmark score, but just to compute a score for the benchmarking of each model. To ensure that the accuracy of the benchmarking tool is properly validated, a user is encouraged to run the benchmarking tool on more than one dataset. For example, you can benchmark BoolQ2014 on two additional datasets. When your datasets contain thousands of datasets, you might obtain unexpected results due to the limited number of instances. The time to collect large amounts of samples is limited, so even if you collect thousands of datasets on the same dataset, you can expect to find differences within each instance. In this situation you could use the BLAS.test/test_benchmark command to run the benchmarks on more than one dataset and compare the results as a single analysis. Here is a table showing the results of the various benchmarking tools for the BoolQ2014 dataset with different benchmarks for the models: The top of each chart is the BoolQ2014 model benchmark that was run: The results indicate that BLAS.test/test_benchmark is the most accurate and that the SMP model performs well in tests with multiple datasets. However, all of the other models perform fairly well in tests with fewer	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?benchmark: BUCC_GERMAN_TO_ENGL, ?benchmark: BUCC_ENGL, ?benchmark: BUCC_GERMAN_TO_ENG _LANG, ?benchmark: BUCC_ENGL, ?benchmark: BUCC_ENG _LANG_LENG, ?benchmark: BUCC_GERMAN_TO_ENG _RUSSIAN_TO_ENGL ?dataset. ?record_lbl. Note: the performance of the queries in the previous two queries depends on the chosen metric and benchmark. input (English text): List the highest benchmark result, including the metric and score, for all the data in 'BUCC' that are tagged 'English' (without any 'English-to-German' or 'German-to-English' or 'Russian-to-English' tags) and that have at least 20 records for each topic ? output (Sparql query): SELECT DISTINCT ?record_lbl ?record_cnt, ?record_cnt ?record_cnt, ?record_cmp, ?record_cnt ?track_count, ?record_cnt ?track_count, ?record_cmp, ?record_cnt ?track_cmp, ?record_cmp ?track_cmp input (English text): List the highest benchmark result, including the metric and score, for all the data in 'BUCC' that are tagged 'German-to-English' (without any 'German-to-Engl' or 'German-to-Russian' or 'Russian-to-English' tags) and that have at least 20 records for each topic and have at least 20 topic authors ? output (Sparql query	False
"
 input (English text): Which model has achieved the highest PARAMS score on the CINIC-10 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a:Table; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BALL-IN-CUP"")... output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a:Table; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BALL-IN-CUP"")... output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataType_lbl:Type; ?metric_lbl:Metric; R2 = 1.689957 0.8356697 0.7232396 0.5019195 0.496897 0.552963 1 0.000879 0 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a:Table; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"")... output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a:Table; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"")... R2 = 1.689957 0.8356697 0.7232396 0.5019195 0.496897 0.552963"	False
"
 input (English text): What are the models that have been benchmarked on the MUTAG dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?code link FROM papers WHERE id= { orkgp:HAS_ID orkgp:MUTAG_ID. orkgp:MUTAG. ORAGG_ID orkgp:MUTAG_GUID }; OR OPTIONAL { ?oraggs. ?code link } OPTIONAL { oraggs. ORAGG_ID || ?code link. ?ORAGGG_DATA OR! ?data: OR if! ?data then ?aggs. ?code link. } } input (English text): List the code links in papers that have been benchmarked on the MUTAG dataset ? output (Sparql query): SELECT DISTINCT ?code link FROM papers WHERE id= { orkgp:HAS_ID orkgp:MUTAG_ID. orkgp:MUTAG. ORAGG_ID orkgp:MUTAG_GUID }; OR OPTIONAL { ?oraggs. ORAGG_ID || ?code link. ?ORAGG_DATA OR! ?data: OR if! ?data then ?aggs. ?code link. } } output (Sparql query): SELECT DISTINCT ?code link FROM papers WHERE id= { orkgp:HAS_ID orkgp:MUTAG_ID. orkgp:MUTAG. ORAGG_ID orkgp:MUTAG_GUID }; OR OPTIONAL { ?oraggs. ORAGG_ID || ?code link. ?ORAGG_DATA OR! ?data: OR if! ?data then ?aggs. ?code link. } OR OPTIONAL { ?oraggs. ORAGG_ID ||!	False
"
 input (English text): Provide a list of papers that have utilized the CAIT-S-36 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per dim"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Inputs: 1 Input (Sparse Matrix) Data: 1 3D Markers: 1 3D Markers: 1 4D Markers: 1 4D Markers: 1 5D Markers: 1 5D Markers: 1 6D Markers: 1 6D Markers: 1 7D Markers: 1 7D Markers: 1 8D Markers: 1 8D Markers: 1 9D Markers: 1 9D Markers: 1 10D Markers: 1 10D Markers: 1 11D Markers: 1 11D Markers: 1 13D Markers: 1 13D Markers: 1 14D Markers: 1 14D Markers: 1 16D Markers: 1 16D Markers: 1 17D Markers: 1 17D Markers: 1 22D Matrices: 1 29C Matrix: 1 29C Matrix: 1 30C Matrix: 1 30C Matrix: 1 3D Markers: 1 3D Markers: 1 3D Markers: 1 3D Markers: 1 4D Markers: 1 4D Markers: 1 4D Markers: 1 4D Markers: 1 5D Markers: 1 5D Markers: 1 5D Markers: 1 6D Markers: 1 6D Markers: 1 6D Markers: 1 7D Markers: 1 7D Markers: 1 7D Markers: 1 8D Markers: 1 8D Markers: 1 8D Markers: 1 9D Markers: 1 9D Markers: 1 9D Markers: 1 10D Markers: 1 10D Markers: 1 10D Markers: 1 11D Markers: 1 11D Markers: 1 11D Markers: 1 22C Matrices: 1 30C Matrices: 1 30	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	--------------- ----------------------- --------------- ----------------------- --------------- ------------- --------------- { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?dataset orkgc:VACG ?dataset. { ?dataset orkgc:VACG. RESULT ?dataset. ORIGIN ?dataset_lbl ? ; } ?matm orkgp:VACG RESULT ?matm_lbl ? ORIGIN ?matm_lbl ? RESULT ?bench mark orkgp:HAS_BENCHMARK RESULT ?bench_benchmark orkgp:DETERMINIST ?benchmark_p:HESPERICIST ?benchmark_p:HESPERICIST RESULT bench_p, bench_lbl } } output (Sparql query): --------------- ----------------------- --------------- ----------------------- --------------- ------------- --------------- { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. { ?dataset orkgc:VACG ?dataset. { ?dataset orkgc:VACG. RESULT ?dataset. ORIGIN ?dataset_lbl ? ; } ?matm orkgp:VACG RESULT ?matm_lbl ? ORIGIN ?matm_lbl ? RESULT ?bench mark orkgp:HAS_BENCHMARK RESULT ?bench_benchmark orkgp:DETERMINIST ?benchmark_p:HESPERICIST ?benchmark_p:HESPERICIST RESULT bench_p, bench_lbl } } output (	False
"
 input (English text): What are economics subfields?
 output (Sparql query): SELECT DISTINCT ?subfields, ?subfields_labels WHERE { ?papers orkgp:P30 ?research_fields. ?research_fields rdfs:label ""Economics""^^xsd:string. ?research_fields orkgp:P36 ?subfields. ?subfields rdfs:label ?subfields_labels. } 
 input (English text): Are children examined in the studies?
 output (Sparql query): "	"ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, ""ildren"")) } "	"SELECT ?subgroups FROM ?subfields; output  (Sparql query):  SELECT DISTINCT ?subfields, ?subfields_labels WHERE { ?products orkgp:P30 ?products rdfs:label ""Products""^^xsd:string. ?products rdfs:label ?products_labels. } A third output output query is a parql that returns data in any language other than English, and can be found here. 3.1 Writing a Parquet Report In the preceding examples, we have created a collection of the records from a table we specified in the query, which were then converted through the data transformations we gave to the data source we gave at the beginning. However, in certain cases it may be necessary to use a query using the Parquet interface. Note, this section assumes that you have been using SQL to write Parquet reports for your projects for a number of years (or longer). We often see people referring to Parquet when doing something on the production side in a way that is not very intuitive to use Parquet for. In those instances one could use the following SQL to create a Parquet report. SELECT * FROM ?subfields WHERE rdfs:label ?( ?subfields_labels ) In the above example you would expect to see rows with this label. In reality you may not see them because you have no choice but to use the database as you would have expected. Let's see what we have to do in order to see rows with our labels, at least. First we have to add the table to the schema. We need to do this because the SQL will only return a subset of the records. CREATE TABLE. * The database schema is in the form of a.Parquet table. We will be using"	False
"
 input (English text): List the metrics that are used to evaluate models on the SNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 RANK_BAR: ?b, { ?dataset a orkgc:Dataset; ?dataset_lbl ?dataset_lbl. { ?dataset rdfs:label ?dataset_lbl. }; } input (English text): What is the minimum benchmark result for the dataset, excluding the metric and its value ? output (Sparql query): PRIME RANK_BAR: ?b, { ?dataset a orkgc:Dataset; ?dataset_lbl ?dataset_lbl. { ?dataset rdfs:label ?dataset_lbl. }; } Input text : RANK_BAR Output : ( RANK_BAR, { ?b) ?dataset orkgc:Dataset output (Sparql query) In the output, the list of metrics is separated by a comma. If the metric value is missing, the output is an empty list. Examples: The following parameters are available in the command line : option-option : argument (default: no-argument) option-command : input The following examples show how to invoke the command line : option-option arguments (default: no-argument) option-command options options (default: no-argument) command-options For example, the following commands will print all files in the current directory that contain the specified language codes: Option 1: command-options ' print -n language codes|sort -n list for each value in /opt/nginx/bin. Option 2: options 'print -n language codes|sort -n list for each value in /opt/nginx/nginx. Option 3: options 'print -n language codes|sort -	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CS-NER?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CS-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	𝓅𝓅𝓅𝓅𝓅𝓅𝓅𝓅𝓅 1.3x smaller than our dataset V2 output (Sparql query): 𝓅𝓅𝓅𝓅𝓅𝓅𝓅𝓅 1.3x smaller in terms of the total number of examples output (Sparql query): 𝓅𝓅𝓅𝓅𝓅𝓅𝓅𝓅𝓅 1.3x smaller in terms of the number of exemplars. This is definitely true to some degree for other visual effects. Also it is true that this might affect the overall accuracy of our results to some degree. The more general question of why is this possible is open to follow-up.	False
"
 input (English text): What research problem is addressed in the paper titled ""6th Open Challenge on Question Answering over Linked Data (QALD-6)""?

 output (Sparql query): SELECT DISTINCT ?concept ?rlabel WHERE { orkgr:R6386 orkgp:P31 ?x . ?x orkgp:P32 ?concept. ?concept rdfs:label ?rlabel. } LIMIT 1
 input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"------------ ----------------------------- ----------------------------------------------- query ------------ ------------ ----------------------------- answer ------------ ------------ ----------------------------- answer ------------ ------------ ----------------------------- answer ------------ ------------ ----------------------------- answer ------------ ------------ ----------------------------- ------------ ----------------------------- 2.2. Reacher, easy, benchmark 2.2.1. Questions Here is the list of questions taken from the second edition REA Query: The Reacher Challenge. Here are the answers to the questions: The score for ""easy"" is the ""easy"" score for 2x, ""medium"" is the ""medium"" score for 2x, ""hard"" is the ""hard"" score for 2x, ""very hard"" is the ""very hard"" score for 2x. (This document assumes this is the second edition. Previous versions were written by Robert O'Brien.) 2.3. Question This exercise uses Qald version 9.3.1 to run the two queries provided above. Question 1 takes a set of ""dummy"" data (i.e. data provided by the user ""johndoe"" as a parameter in the first query) and queries it using the two question sets to get the answers to questions 2.1, 2.2, and 2.5. As described in Section 2.2, question 2.2 asks the following question: What is the relationship between the label ""A"" in ""A.label"" and the label ""B"" in the ""B.label"" set ? The correct answer to question 2.2 is ""C"" or ""C or A or B."" The appropriate model to answer question 2.2 is the Reacher 1.10 model or the Reacher 2.01 model with the attribute ""C1."" As explained in Section 2.2, this model"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cornell Grasp Dataset dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?benchmark ?benchmark_status ORG(Benchmark_status) ?statistics_lbl FROM orkgp:Quasaratt-T. You can also use the data from your own custom dataset with TensorRT	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT models.test as model_v, name, mean, as.Nth_value, qval, min, max, percentile, rsquared, pval, rsqrt, lnmod, LMSQ, NCLF_SE, PISA_SE, NCLF_PRICE_SE, mean_pval, medianval, quartileval, percentile_pval * 95.00, z_score, sigma_score, rank, rank_ratio * 95.00. input (English text): The Multimodal PISA dataset has two datasets, one with three children and another with three children. The Multimodal PISA dataset includes three children, all of which performed equally well on the ""Test of Early Reading"" and ""English Language and Reading"" tests. How many models have been evaluated (in terms of the number of children tested) ? output (Sparql query): SELECT models.test as model_v, name, mean_score, rank_ratio from all_models where test not in ""Multimodal PISA; n_samples = 100; acl = 3;"" GROUP BY model_v; input (English text): How many models have been evaluated on the NESTA dataset ? (In the NESTA dataset, NESTA is a non-labeled data set which uses the Atari 2600 Tennis as its training data.) output (Sparql query): SELECT models.test as model_v, name, average_score, rank_ratio from all_models WHERE test not in ""Multimodal PISA; n_samples = 100; acl = 1;"" GROUP BY model_v; input (English text): The NESTA dataset has a dataset with three children. What are the performance of each model when training"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the DuIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" Please provide a list of benchmark scores on the DUC paper,  and for the Barabasi-Albert dataset with the benchmark score, along with the  original scores. input (English text): What is a ""real"" benchmark score ? (ie, is there a benchmark score that can be trusted that is the whole truth. eg, a test that will always be a perfect benchmark score ?) Is there no reason to trust data or benchmark scores ?"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WikiText-103 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	-------------------------- Select Metrics for Evaluating The WikiText-103 Dataset... --------- Metrics ----------- RPS (score) - 479.80 RPS (rank) - 24.73 RPS (ratio) - 3.02 RPS (mean) - 38.70 RPS (std) - 6.03 RPS (SD) - 33.57 RPS (median) - 9.81 Score (mean) - 26.67 Score (SD) - 9.85 RGS (range) - 100.00 RGS (max) - 100.00 (8) output (Sparql query): -------------------------- Select Metrics for Evaluating The wikiText-103 dataset... --------- Metrics ----------- RPS (score) - 479.80 RPS (rank) - 24.73 RPS (ratio) - 3.02 RPS (mean) - 38.70 RPS (std) - 6.03 RPS (SD) - 33.57 RPS (median) - 9.81 Score (mean) - 26.67 Score (SD) - 9.85 RGS (range) - 100.00 RGS (max) - 100.00 (12) return (matrix:= '' for matrix in sg1) as data: output (Sparql query): --------------------------- Select Metrics for Evaluating The wikiText-103 Dataset... --------------- Metrics ----------- rps_max - 24.73 (6) rps_mean - 38.70 (8) rps_std - 6.02 (9) rps_SD - 33.57 (2) rgs_range - 100.00 (3) rgs_max - 100.00 (8) rgs_mean - 25.67 (6) r	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Kung-Fu Master dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kung-Fu Master"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" The command is written as a SQL query and the data are all provided by the dataset or a query with a parameter. Output: Input file is available: 1 2 [{ ?dataset a orkgc:Dataset; ?dataset_lbl d2:DataBase; ?data_lbl rdfs. ?cont test1. ?benchmark test2. ?paper orkgc:P31 ?cont ?benchmark. ?sheet orkgc:HAS_DATASET. { ?dataset_lbl orkgc:Dataset; ?dataset_lbl d2:DataBase; ?dataset_lbl rdfs. ?"" test1. ?benchmark test2. ? sheet orkgc:HAS_DATASET. ?benchmark. ?sheet rdfs. ?"" test1. ?paper orkgc:P31 ?cont. ?benchmark. ?sheet rdfs. "" test1. \ ?benchmark test2. "" "" "" "".. "" ?"" "".. "".. "" }] You can pass the parameters as a SQL statement or write the parameters as a query. We can use some of the parameters of the dataset d2 : dataset:dataset and the data can contain a number of other parameters: dataset_lbl and the dataset itself: dataset_lbl_rdfs:dataset and the data in the database rdfs."	False
"
 input (English text): Where can I find code references in papers that have used the  GCNN-14 bottleneck model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = "" GCNN-14 bottleneck"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?code WHERE { ?i orkgc:Model; RDFs:label ?i. FILTER (str( ?i ) = "" GBM14 BENCHMARK "") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_SOURCE_CODE ?code. } Input: (input from data file) Example: How to use the model (see example code using C++ template) Using model in WMT2016 benchmark (code and examples) GBM14BENCHMARK: output: output (Sparql query): SELECT DISTINCT ?code WHERE { ?i orkgc:Model; RDFs:label ?i. FILTER (str( ?i ) = "" GBM14 BENCHMARK "") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_SOURCE_CODE ?code. } Input: (input from data file) Example: Using the model (see example C/C++) #include ""stdafx.h"" // set stdafx::cpp_variant typedef std::prelude<std::vector<"	False
"
 input (English text): What models are being evaluated on the CL-SciSumm dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Sentence Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" I am running these queries on a single model (Tensorflow-like) and on the model ""cl-sci-summ-data"", which is not listed in the examples. I am not sure if these queries should be run on a different host than the example, or on a different dataset or with fewer parameters than the example. I am using the test setup from the SCC2017 paper (link): https://docs.google.com/file/d/0B3TnR7Xv2QFqkOQkNhc/edit ?usp=sharing  It has two clusters, in which I am running 500 iterations with Tensorflow in C++11. The first cluster holds two models:  Seed of the Sentence Classification paper (incl. model ""cl-sci-summ-data"", which was used in the paper): https://docs.google.com/file/d/0B3TnR7Xv2QFqkOQkMmE/edit ?usp=sharing I use 100 (50 times) out-of-the-box, pre-trained models for the two nodes, in which I get the output of the two methods. All of them were re-trained within 2-4 hours of the paper being released. The other cluster is the test-set, containing the same 1000 (50 times) out-of-box, pre-trained models and I use them to test out the model ""cl-sci-summ-data"" as I did with the model used for the paper. I am running these queries on two models but they are connected to the same cluster. I am getting the following answers: 1. Tensorflow-like (Tensorflow-like nodes) 2. Machine Learning ("	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Double Dunk dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?met( ?dataset a, ?dataset_lbl, ?paper_lbl, ?paper_lbl) ?met as metric. ?dataset a as title. ?dataset_lbl as title. ?paper_lbl as paper_lbl. ?paper_lbl as paper_lbl. ?page_lbl as page_lbl. output (Sparql query): SELECT ?met( ?dataset a, ?dataset_lbl, ?paper_lbl, ?paper_lbl) ORGANIZE ORDERING AS ?met FROM ?lbl_schema in dataset where ?labels = True, ?labels = False | ?dataset_lbl in dataset where ?datasets = 'benchmark' | ?benchmark as metric. input (English text): List the metrics that are used to evaluate models on the RotoWire (content ordering) benchmark dataset ? output (Sparql query): SELECT DISTINCT ?met( ?dataset a, ?dataset_lbl, ?paper_lbl, ?paper_lbl) ?met as metric. ?dataset a as title. ?dataset_lbl as title. ?paper_lbl as paper_lbl. ?paper_lbl as paper_lbl. ?page_lbl as page_lbl. input (English text): List the top five benchmarks that include the Atari 2600 Double Dunk dataset in their title ? output (Sparql query): SELECT TOP(5)[1] ?benchmark ?dataset_lbl { ?dataset a; ?benchmark, ?page_lbl; } as benchmark. output	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Food-101?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"""S.V.E.D.O.N"" ""S.A.X.T.E."" ""S.Z.E."" ""S.I.T.I."" ""S.P.A.W."" ""S.N.G."" ""S.M."" ""S.L.Z."" input (English text): What is the median performance of the top 1000 models over the dataset Food-101 ? Output (Sparql query): TOP2000 (mean) 1 Food-101 3.243976751857 2 Food-102 3.195579153479 3 Food-103 3.182455595034 4 Food-104 3.172445357537 5 Food-105 3.166515981804 6 Food-106 3.159468954626 7 Food-107 3.140857262495 8 Food-108 3.117706516861 9 Food-109 3.08735135849 11 Food-110 3.07945689979 12 Food-111 3.06585654538 13 Food-112 - 1.89570353798 15 Food-113 - 1.83578484785 20 Food-114 - 1.80722809722 25 Food-115 - 1.84933662913 40 Food-116 - 1.82513781298 60 Food-117 - 1.82433357935 84 Food-118 - 1.81712774735 103 Food-119 - 1.80660782665 127 Food-120 - 1.7969576865 159 Food-121 - 1.79057747345 202 Food-122 - 1.7848282765 241 Food-123 - 1.7565179898"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy (%) metric on the STL-10 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tokenlearner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- token_learner.md token_learner.tqr token_learner.expr The output contains a table with the name of the model that performed best in terms of Accuracy (%) metric on the benchmark dataset that is provided as a part of a download.	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the MedSTS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 1 1. DSP. 1. NLP. 1. Deep Learning. 2. Nonlinear Graphical Models. 3. Deep Neural Networks and General Intelligence. 5. Support Vector Machines. 6. Probabilistic Programming. 7. Deep Reinforcement Learning. 9. Neural Network Supervised Learning. 10. Model-Free Convolutional Geometries. 11. Text Classification. 12. Image Classification. 13. Computer Vision. 14. Machine Learning. 15. Bioinformatics. 16. Computer Vision. 17. Machine Learning. 18. Machine Learning. 19. Decision Analysis. 20. Statistical Learning. 21. Machine Learning. 22. Statistical Learning. 23. Human-Computer Interaction. 24. Evolutionary Robotics. 25. Computer Vision. 26. Machine Learning. 27. Natural Language Processing. 28. Machine Learning. 29. Pattern Recognition. 30. Computer Vision. 31. Image Understanding. 32. AI. 33. Knowledge Graphs. 34. Machine Learning. 35. AI. 36. Data Mining. 37. Artificial Intelligence. 38. Data Mining. 39. Deep Learning. 40. Data Mining. 41. Machine Learning. 42. Data Mining. 43. Machine Learning. 44. Machine Learning. 45. Machine Learning. 46. Machine Learning. 47. Machine Learning. 48. Machine Learning. 49. Machine Learning. 50. Machine Learning. 51. Bioinformatics. 52. Genetic Algorithms. 53. Machine Learning. 54. Machine Learning. 55. Machine Learning. 56. Machine Learning. 57. Machine Learning. 58. Machine Learning. 59. Machine Learning. 60. Machine Learning. 61. Deep Learning. 62. Machine Learning. 63. Deep Learning. 64. Deep Learning. 65. Machine Learning. 66. Deep Learning. 67. Deep Learning. 68. Deep Learning. 69. Machine Learning. 70. Deep Learning.	False
"
 input (English text): Indicate the model that performed best in terms of 3-fold Accuracy metric on the UCF101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""3-fold Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""3-fold Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } C This program performs SPSS (SPSS Statistics Professional for Windows) comparisons (based on the SPSS 97.1 file) for the model: orkgc:METG orkgp:EXPRESS data set. After running the program, it displays a list of the top 10 models of the dataset, and compares the number of iterations to those of models that performed equally well in the original dataset (or in other benchmarks, such as SPSS and Excel). output: A list of top 10 models in SPSS-98 (or SPSS 97.1) SPSS statistics database for the orkgc:METG dataset. A list of top 10 models of the orkgp:EXPRESS dataset"	False
"
 input (English text): List the code links in papers that use the Neural Content Planning + conditional copy model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural Content Planning + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" input (English text): Provide an id for the ""neural content planning"" label in ""Neural content planning + conditional copy (NPC)"" and the ""conditional copy"" label in ""Neural content planning + conditional copy,"" and in ""Neural content planning + conditional copy + Nested submodels"" papers ?"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari-57 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT statement, which is just an SQL statement. The following example shows it in a SQL statement:	False
"
 input (English text): What is the highest benchmark result achieved on the BUCC Chinese-to-English dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ImageNet + iNat on WS-DAN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- | DESCRIPTION | | ------------------------- 1. L2 training (2.1%)| | ------------------------- 2. L2 loss: | | ------------------------- 3. L2 classification (8.5%)| | ------------------------- 4. L2 test: | | ------------------------- 5. L2 net loss (1.2%)| | ------------------------- 6. L2 net classification (12.8%)| | ------------------------- 7. L2 validation (1.2%)| | ------------------------- 8. L2 loss: | | ------------------------- 9. Random forest (7.3%)| | ------------------------- 10. Fully correlated prior (0.7%) | | ------------------------- 11. Fully correlated prior based on k-means (6.1%)| | ------------------------- 12. SVM: | | ------------------------ 12. SVM: RNN + iNat (5.5%)| | ------------------------- 13. SVM: ITC + iNat (9.1%)| | ------------------------- 13. SVM: Batch regularization/supervised (6.3%)| | ------------------------- 14. SVM: SVM + LDA (5.2%)| | ------------------------- 14. SVM: LDA + k-means (1.3%) | | ------------------------- 15. SVM: Regularized least squares (22.3%)| | ------------------------- 16. SVM: Random forest: (1.1%)| | ------------------------- 16. SVM: Random forest: RNN + iNat (2.4%)| | ------------------------- 17. SVM: RNN + iNat (11.3%)| | ------------------------- 18. L3 training (5.3%)| | ------------------------- 19. L3 loss:	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Bowling dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?models ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset ? output (Sparql query): output (Fetch data as JSON): SELECT DISTINCT"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Q*Bert?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT 'DESC( ?METRIC_lbl ) AS ?metric_lbl,' 'DESC(' ?METRIC_rdfs ['''desc('')''' FROM ('' { { SET { ?data a 'DESC('''dataset,''''metric_lbl,''''value,''') }; { ?dataset b 'DESC('''dataset,''''metric_rdfs,''''value,''') }; } } '' { { SET { ?data rdfs 'DISTINCT( ?' { :type rdfs::str; :value ?value}') WHERE { ( ?data b 'DESC( ?' { :type rdfs::str; :value ?value}') > ''' { :value ?value} ''', [ rdfs.p_rand(), rdfs.n_rand() ] ; } } ] ; } } '' )' } } GROUP BY ?metric ?metric_lbl input (English text): (If you want answers from the community from the RDBMS, see RDBMS Queries). What best practice is RDBMS-to-database translation and SQL injection when working with RDBMS data, for example in an Oracle data repository ? output (Sparql query): SELECT [{ :meta ( ?dataset )) AS 'countOfSamples' }, { :meta ( ?dataset ) AS 'countOfRows' } FROM ('' { { RDBMS_PREFIX( 'a' ) -> { SELECT `meta( ?dataset)', ?dataset +'' + RDBMS_TABLES_COLUMNS( ?dataset ) + '),	False
"
 input (English text): List the code links in papers that use the Transformer-XL - 24 layers model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL - 24 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?tbl in ?labels WHERE { ?tbl:Model ?tbl_lbl, ?tbl :Transformer-XL - 24 layers ?lt, ?tbl :Transformer-XL - 24 layers ?lt, ?tbl_lbl:code ?code_lbl { code, code_id, label, source_coden ?code_id, code_id orkgc, code_id :transformer_lbl } } input (English text): Please describe the benchmarking workflow in the examples below output (Sparql query): // Example 5.5: Use the SPM4-7.0 benchmarking framework output (Sparql query): SELECT ?tbl, ?tran in ?gf:benchmark orkgp:HAS_DATASET ?dataset. ?tran orkgp:HAS_SOURCE_CODE ?code ; orkgp:HAS_SOURCE_CODE ?code ; orkgp:HAS_SOURCE_CODE ?code_lbl; { code, code_id, label, source_coden ?code_id, source_coden ?code_id } output (Sparql query): (example 5.5) input (English text): Please describe the benchmarking workflow in the examples below output (Sparql query): input (English text): Please describe the benchmarking workflow in the examples below output (Sparql query): input (English text): Please describe the benchmarking workflow in the examples below output (Sparql query): input (English text): Please describe the	False
"
 input (English text): What is the best performing model benchmarking the One Billion Word dataset in terms of Validation perplexity metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkmc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE{ ?dataset a orkmc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkmp:HAS_DATASET ?dataset; ?test orkmp:HAS_EVALUATION ?eval. ?eval orkmp:HAS_VALUE ?value; ?testrdfs:label ?testrdfs. } ORDER BY DESC( ?value) LIMIT 1 } } Example: input (English text): How well trained is ELLIPSE on the Penn Treebank benchmark dataset ? output (Sparql query): SELECT TOP ?metric ?name ?model rdfs:data ?labels OR LABELS. order by DESC( orkgc : Metric; rdfs:label ?labels. ) LIMIT 1 The model ( e-learning model from this example ) is tested on the dataset. To check, you can use: input (English text): ELLIPSE has the best model on the Penn Treebank benchmark dataset! (only models of type ""R"" are tested here. Other models are kept in R-1) Output (Sparql query): What are the most popular model names on the list ? The test_metric option sets the metric to which the algorithm is evaluated, for example, ORkgc:Validation"	False
"
 input (English text): Where can I find code references in papers that have used the MultiFiT, pseudo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MultiFiT, pseudo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT * FROM orkgp:SPECIFIED AS  INNER JOIN orkgp:SPECIFIED AS  SCHEDULE as  ROWS ON (SCHEDULE_ID= ?) AS  INNER JOIN c2e_data.bench.* AS ct ON ct.ID= ? and s_benchmark.bench.testid  GROUP BY ?benchmark  ORDER BY   ?benchmark. ?id AS  TESTINDEX; input (English text): Given the MultiFiT, pseudo model for benchmarking purposes, a code reference would be very useful. I am a code editor/programmer by profession, and I have found no example code reference that refers to the MultiFiT. Any links, documentation, or code examples you can provide would be greatly appreciated. Can you provide code references in the various other papers mentioned that seem to use the MultiFiT ? output (Sparql query):  SELECT * FROM orkgp:SPECIFIED AS  INNER JOIN orkgp:SPECIFIED AS  SCHEDULE as  ROWS ON (SCHEDULE_ID= ?), ?benchmark ( ?, ?continuous ?) AND   ?cont ( ?:continuous ?) ORDER BY   ?benchmark. ?id AS  TESTINDEX; input (English text): An example is below that shows the use of the MultiFiT. In this case the test data is available for free download from http://www.usis-ci.net/tools/sap. Here is the example code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 $ tm tt	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Why is it important that we check the score for the benchmark ? output (Sparql query). If you have more detailed queries to this form, please contact me by email at ianmckellan@gmail.com to add them to the blog.	False
"
 input (English text): List the code links in papers that use the 24-layer Transformer-XL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""24-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?cse_re. FILTER (str( ?curr_data) = 1) ?curr_cse_dataset. FILTER (str( ?curr_data) = 1)/2. FILTER (str( ?curr_data) = 1)/2. FILTER (str( ?curr_data) = 1)/2. FILTER (str( ?curr_data) =1) ?cse_datacontainer orkgc:Model; output (Sparql query): select ?cse_datacontainer as ?cse_datacontainer, i as number, (num_comments = 0), (is_datacontainer = 1) from ?datacontainer where ?curr_data = ?curr_dataset. orkgc:Model; orkgc:Model |-------------------------------------------------------------------------------------------------| orkgc:Model| | -------------|-------------+--------------------------------------------------------| | a - 1 | 2 | | b - 1 | 3 | | c = 1 | 1 | | d - 1 | 1 | | df - 1 | 6 | | df = 1 | 4 | | df + 1 | 1 | | df - df| 2 | | df - df + d| 4 | | d - df| 5 | | d + df| 2 | | e - d| 3 | | e + e| 2 | | f - 1 | 1 | | g - 1 | 1 | | h - 1 | 1 | | j - 1 | 1 | | k - 1 | 1 | | l, m = 2..5; dmax = ?n and fmod = 1 | (select n, dmax from ?datacontainer where n < ?max) orkgp	False
"
 input (English text): What models are being evaluated on the STL-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" orkgp:HAS_MODEL rdfs:label ?model_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } If the output contains a comma, it is a special error. Here are examples of the output from running orkgp with: option -f # if you are trying to run the benchmarks with -f, avoid using optional parameters OPTIONAL options : -f and no arguments. -t # if you are running it yourself, this is only needed if you specified options -e # if you are using a cron job or another program to run the benchmarks and report output -e and no arguments. OPTIONAL options : option flags. # option flags are optional, but are recommended if you are running orkgp -s # check if options is also provided before calling options. Check them with a sed -n '/^option=.*/d' orkgp -i # check if option is also provided before calling orkgp -v # if multiple options are needed, you should use option flags. In other words, one option flag might be given multiple options. # one of them are listed here -t, -e, or -v # these are the options that this command accepts # option flags is the option flags # for more information see sed FAQ for all options # for a more detailed description of all options # use of option flags is optional, but important # option flags can be changed in orkgp with the"	False
"
 input (English text): Provide a list of papers that have utilized the AWD-LSTM-DOC model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-DOC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dval as dval, ?score as score FROM ?device ?device : ?db { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-DOC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; ?cnt ?score ( 0, 0 ); ?p ?score ( 0, 0 ); ?l ?score ( 0, 0 ); ?u ?score ( 0, 0 ); ?dval ?score orkgc:Score( 0, 0 ); } Please send as many reports as you want. Also please answer as many questions as you can. Your report will be evaluated in accordance with the rules and instructions below. We are looking for people! Please apply to the following report, which will be given priority from this point onward! Please review The Atari 2600 TITAN-LSTM-DOC : Benchmark Results (2011-2012) and select the following: Review the following report, which will be given priority from this point onward! Please review The Atari 2600 TITAN-LSTM-DOC : Benchmark Results (2011-2012) (2012-2013) Please select the following. If you can't read the instructions above, click on the link and read it. You will need to have javascript enabled in your browser and the Javascript to be able to access any of the web page within your browser, if you cannot see it click on the link below... If you can't read the instructions above, click on the link and read it. If you can't read"	False
"
 input (English text): What are the models that have been benchmarked on the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Title ID Author Title: R.M. Chen and S.L. Li. Efficient and accurate prediction of English spelling using linguistic regression with Amazon-2.   Proc. Conf. on NLP 2016. ID: Author: Author: For the next steps, we recommend using the dataset as an input for the Google LSTM-PIDL. You can download that dataset from the GitHub page. The code for the code above was written by Rady Shlokhla and has been shared with the open-sourced community. Note In the next chapter, we will learn more about how Google LSTM-PIDL operates.	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Zaxxon dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	_____________________ [a text] _____________________ _____________________ | [code] | | | | | __|___|____||___||___||___|___||___|____|____|____|____|___|___|___|____|___|___| | [code] | | __|___|____||___||___||___|___||___|___|___|___|___|___|___|_____|___|___|[code]| |__[score] |__|___|____||___||___||____||___|____||____|____|____|____|____||___|___|___|___| | [code] | |__|___|____||____||____||___|___||____||___|___|___|___|___|___|___|___|___|___| | __| |___| __|___|___|___|___|___|___|___|___|___|___|___|___|___|___|___| |__|___|_____|___|___|___|___|_|___|___|___|___|___|___|___|_| |__|___|_____|___|___|___|___|_|___|___|___|___|___|___|___|___|_| |__|___|_____|___|___|___|_|___|___|___|___|___|___|___|___|_| Posted by: nycjh at 05:41 PM Thanks for the kind words, nycjh. ____________________ ____________________ ____________________ | [code] |_|__________|____||___|____|____|___||___|___|___|	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BIOSSES dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT orkgc:VARIABLE ?metric2, ?meter2, ?score2 FROM {OrkGroup.LAST (3) -5.66% -7.04% 5.00% 5.71% 7.04% :OrkGroup.LAST (2) -6.99% -7.13% 7.04% 6.99% 6.67% 15.12% 9.65% :OrkGroup.EXAMPLE | orkgc:N | measure| score2 | ------------------------------------------------- 12.28 -16.66 6.39 7.04 7.04 -15.12 | 7.95% -8.35% -9.65% 10.05 | 7.95% -14.32% output (Sparql query): INSERT ORKGROUP.TEMPLATE (dataset.metric2) INTO {OrkGroup.EXAMPLE} RACE:RACE (3) ORG-NAMES OF RECENT TENORMCODE FILES IN AN ORKGROUP.LAST AS DECORATED BY RACE:RACE (1)	False
"
 input (English text): Could you provide a list of models that have been tested on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?Paper ?model. optionally, use a separate query for the Romanian papers on the WMT2016 English-Romanian dataset ? optionally specify a separate query for the English papers on the WMT2016 English-Romanian dataset ? UPDATE  (CIM VARCHAR(15) MAX)  ( SELECT SUM( ?datasets) * n, n FROM sys.datastructures_list, ?dataset, ?model, ?n, ?dataset_lbl, ?record, ?mw WHERE ?dataset = ? orkgp:HAS_DATASET ) SELECT ROW_NUMBER, CASE WHEN ?datasets.length < n THEN 0 ELSE 1 ELSE ?datasets[0].length, CASE WHEN ?datasets[ ?datasets.length - 1].length = 1 OR ?datasets[ ?datasets.length - 1].length = 16 THEN 0 ELSE 1 ELSE ?datasets[1].length, CASE WHEN ?datasets[ ?datasets.length - 1].length = 1 OR ?datasets[ ?datasets.length - 1].length = 16 THEN 0 ELSE 1 ELSE ?datasets[2].length, CASE WHEN ?datasets[ ?datasets.length - 1].length < n THEN (SELECT CAST(n AS INT) AS MAX(n)) ELSE ?datasets[0].length, CASE WHEN ?datasets[0].length > n THEN 0 ELSE 1 ELSE ?datasets[0].length, CASE WHEN ?datasets[0].length < ?datasets[0].length THEN 0 ELSE 1 ELSE ?datasets	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT rdfs:title ?dataset ?title_lbl ?dataset_lbl. { ?datasets a,b,c,d,e,...,f ?lbl. ORG_STR ?data_lbl. ?datasets a,...,f,g,...,h ?lbl. ORG_STR ?datasets. ?lbl. ORG_BENCHMARK s } After searching on https://github.com/ai-machinelearning/CATALINA/wiki/LUNAR_LANDING_REFERENCE_MATERIALS#metrics and https://github.com/ai-machinelearning/CATALINA/wiki/LUNAR_LANDING_MATERIALS#metrics (using a query that appears on one of those pages) and other tools including the MUTAG dataset and the MUTAG benchmark, I can see that this is not the case. This is likely a very low quality dataset that is not open for free use (although with a very low price: € 5.00/min for 1MB dataset or 50 USD for 10MB). When I query the benchmarks I get something like this: input (English text): [data] for all the benchmarks of the data are [data] (3, 2, 1, 0). output (Sparql query): And you can see how I could query them in order of priority (I could not see which benchmarks I should query first, but we can see that these benchmarks have the lowest impact at the time when I'm querying them. As expected there are some things they are slow or missing, but I don't have any other analysis that can help me understand why.). However,	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Sequential MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?benchmark ?benchmark_dataset. ?benchmark orkgp:LUMINANCE ?benchmark. ?value orkgp:VALUE; rdfs:labels ?benchmark_dataset. 2.4. Inputs Skipping the above section, the output should be a query that results in a text file like this: # SELECT * FROM [datasets] WHERE CUSTOMER_ID= ? and CUSTOMER_NAME='myuser'. ORDER BY DESC LIMIT 1 # Select a few papers from a paper list that have performed benchmarks on the Sequential MNIST dataset (the most representative data for the comparison). # Print the outputs of the query and the output in text format. output (Text file): OUTPUTS = '', FILTER (str( CUSTOMER_ID) = ?and CUSTOMER_Name='myuser' ) print ""Inputs:"", Inputs outputs.print output. print "" Outputs: output.txt"" output (Text file): Outputs: # Download (Cura): # Download the dataset (Cura); select the output for this query. # The output should be: # { ?benchmark. ?value ?dataset. ?cont orkgp:LUMINANCE ?benchmark. ?value. and ?label. } output (Cura): output { Cura: filename } # Download (Vizura): # Download the dataset (Vizura); select the output for this query. # The output should be: # { ?value ?dataset. # cont orkgp:LUMINANCE ?benchmark. } output (Vizura): output { Cura: filename } 2.5"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset 200k Short Texts for Humor Detection?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	************* F1 Score ********* -------------------------------------------------------------- Humor Model Humor Detection Model Humor Detection Model Humor Detection Model Humor Detection Model 3.4 100.00 9.00 23.00 100.00 12.00 30.00 12.00 44.00 27.00 30.00 5.00 5.00 7.00 15.00 12.00 11.00 9.00 5.00 4.00 4.00 4.00 100.00 7.00 5.00 100.00 7.00 5.00 100.00 7.00 5.00 100.00 7.00 50.00 6.00 9.00 100.00 6.00 100.00 6.00 50.00 6.00 9.00 If you are curious about how this benchmarking is done, see this example from the Humor Detection project: // Benchmark Results  int test_tolerance_1 = 1000.0; // 1000 iterations of tessellation int test_tolerance_2 = 1000.0; // 1000 iterations (from 1 to 1000) int benchmark_hdfs_test_2 = 2.0; In that example, the Humor Detection model has achieved the highest F1 score (from 1000) over its 2 iterations. This benchmarked performance does not exclude other models, but it does exclude the Humor Detection model. In fact, this benchmarking is more of a performance measurement than an exhaustive performance comparison. It does not represent a model and its performance on each of the key test metrics. In other words, the benchmarking is about getting the benchmark results that a naive model will generate over a very short period of time given an extensive test set. This is why I have chosen to include only models that pass the benchmarking tests, and therefore do not represent a real-life	False
"
 input (English text): Which model has achieved the highest PARAMS score on the CIFAR-10 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 To summarise, the two approaches are not perfect and are likely to be not identical for every model. So, it might be more effective to combine the approaches with a common set of variables, even with little difference between them. To make that possible, one should first get a dataset with a good set of datasets, then build the model using CIFarLs and the MetricList class. However, we haven't been able to do that in the above example.	False
"
 input (English text): List the code links in papers that use the KNN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Input data: code of KNN benchmarking: n-gram test: KNN_CODE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 302 3 3 3 3 3 3 6 6 6 3 5 5 5 5 5 3 5 5 5 5 6 6 6 6 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1	False
"
 input (English text): Where can I find code references in papers that have used the 18-layer Transformer-XL model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""18-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 ---------- BATCH_COUNT: 5.00 BATCH_SIZE: 22 SETSYS : 3.50 SETSYS : 3.50 SETSYS : 3.50 SETSYS : 3.50 SETSYS : 3.50 SETSYS : 3.50 SETSYS : 3.50 SETSYS : 3.50 The Transformer-XL model requires additional processing due to the structure of the input data. Fortunately, the input is only three simple fields: a 1-D list containing an X matrix. There are two ways to implement the X matrix structure. We will use a first order transformation and a second order (inverse) transformation. First-order transformations transform the X matrix by the first order operator, which is defined as T(X) = T[A] where T[A] is the first index of A, A is the first column of A, and T[A] takes the value A. This first order transformation converts the X matrix to a single scalar (a single element of A), which can be read as follows, which gives us the transformation as T[3] = 3T[A]. Then, the second order transformation, also known as transpose (or transposing a 2-D matrix), uses a different third order operator, X'(S[X]), which has a familiar meaning of the multiplication by 3. In this new transformation, a new 3-dimensional array is created by dividing all the input values into 2-dimensional chunks separated by 2 elements. In this way, the third dimension of our dataset, containing the X matrix, is transformed into a 2D array, containing only an array of the X matrix. The X matrix structure is often encountered when creating benchmarks. However, it can be seen in the following benchmarks, for example: Example 1: Linear Performance 	False
"
 input (English text): How many comparisons are there which don't have a class link?
 output (Sparql query): SELECT ?ratio WHERE { { SELECT (COUNT( ?comparison) AS ?n_valid_comparisons) WHERE { ?comparison a orkgc:Comparison. } } { SELECT (COUNT(DISTINCT ?comparison_) AS ?n_comparisons_with_missing_class) { ?comparison_ orkgp:compareContribution ?contribution. FILTER( NOT EXISTS { ?comparison_ a orkgc:Comparison } ) } } BIND( CONCAT( ROUND( xsd:double( ?n_comparisons_with_missing_class) / xsd:double( ?n_valid_comparisons + ?n_comparisons_with_missing_class) * 100 ), "" %"" ) AS ?ratio ) } 
 input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?succeeded FROM test ?model WHERE 1 = ( ?model ) ?a AND 4 > 0 AND 4 < ?n_tests_attempt- ?n_test_score input (English text): What is the optimal value for a class label of ?comparison ? output (Sparql query): SELECT ?n_max_score AS s, ?comparison_ as c FROM test ?model WHERE { ?comparison ?n:Comparison. ?comparison_ as ?n_test. ?n_test_score ?comparison_} input (English text): What is the optimal value for a class label of ?predate ? output (Sparql query): SELECT ?n_max_score AS d, ?predate_as a FROM test ?model WHERE { ?predate ?predate_a. ?predate_b. LEN( ?predate_a ); ?Predate_a. ?predate_b. LEN( ?predate_b ); } ?predate_as a, ?predate_b AS b ) input (English text): How many generations from the current generation do you expect to succeed in the test of ?predate ? output (Sparql query): SELECT ?n_max_score AS l, ?pred_as a FROM test ?model WHERE { ?predate a AND LEN( ?predate_a )!= 0 ?Predate_a. ?predate_b. LEN( ?predate_b ) & ?REVERSE(& ?REASON( ?predate_a > ?predate_a ? ) ?	False
"
 input (English text): List the code links in papers that use the PyramidNet (SAM) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PyramidNet (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Result(s) { ""code"" : ""PyramidNet (SAM) "", ""description"" : ""Use the PyramidNet (SAM) method to classify business reviews based on the business reviews."", ""output"" : ""Business reviews have more than 3 business reviews."", ""cont"" : ""Contains code for the model 'PyramidNet (SAM)'."", ""cont"" : ""Dates: Sep 28, 2007."", } Output: { ""_id"" : ""766c9a2feb74a6da4b1e29fcdba1ff1ed1"", ""title"" : ""< ?code> PyramidNet (SAM) "", ""date"" : ""2008-09-28"", ""language"" : ""en-us"", ""description"" : ""This is a sample benchmark for the PyramidNet (SAM) classifier. The sample uses the same dataset as the Yelp Binary classification dataset which is hosted by Yelp at < a href = ""http://developers.yahoo.com/ yelp://blogs.deviantart.com/hoyas/yelp/en/YELP_BEATER_CATEGORIES/ YELP_BEATER_CATEGORIES_a.jpeg"" target = ""_top"" > Hacker News </a>. < ?code> < ?title!= ""PyramidNet (SAM) "" >< ?data : { ?name : ""Business Reviews"", ?text : ""The business reviews are < a href = "" http://dev.yelp.com/search ?q=a&filter=desc&sort=desc&tname=a&sortby=desc&avg=0.6&avgdescsize=11&avgsortsize="	False
"
 input (English text): List the metrics that are used to evaluate models on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METIC ?metric. } } The input data for the model is the following, with an abbreviated description of the measurements: * number of examples to evaluate (1 : all examples) * training set used (1 : all examples) * training dataset used (1 : all examples) * test set used (1 : all examples) * test dataset used (1 : all examples) * metrics (all metrics for all tests) * output data * metric_lbl for metrics that evaluate performance on the web-nlg benchmark dataset. The output data for the model is the following, with some additional data that were added by the model itself: * number of examples to evaluate (1 : all examples) * training set used (1 : all examples) * training dataset used (1 : all examples) * test set used (1 : all examples) * test dataset used (1 : all examples) * metrics (all metrics for all tests) * output data * metric_lbl for metrics that evaluate performance on the web-nlg benchmark dataset. As the code for the models is not yet complete,"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Kung-Fu Master dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kung-Fu Master"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT score, title,  abstract,  score_score FROM   ?abstract_predict data_titles_predict  where   ?abstract_predict	False
"
 input (English text): What models are being evaluated on the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" 4.3.8.3. Validation of Benchmark Models¶ The validation section describes the steps taken to ensure that all of the models that make it into the benchmark report (when benchmarked) are accurate enough to be considered suitable for use in the paper and/or submitted to NIST. Note that the validation steps outlined here are not intended to mimic the process described by the NIST Standards Oversight Committee in ""How NIST Standards Are Tested"", and in some cases, are rather specific to the NIST Paper Specification (section 3.4.3.3). Validation of models for a paper/paper submitted to NIST is only performed on the paper (see Section 4.3.2.4/5 ). 4.3.8.4. A2 Score Validation for Paper and Paper Data¶ 4.3.8.4.1. Step 1. Verification of Classifiers, Clustering and Predictors¶ After verifying that the model(s) are robust to the dataset size, model performance, and benchmark parameters (see Section 4.2.1.1.2.1.1.), test the model(s) against the dataset. If the model performs well, then it is verified that the output for the model.txt file and the benchmark, when benchmarked, were produced by the model(s). The model-reported data (the data from [NIST] Paper Specification) must have not changed during the benchmarking, and all validation points must still be valid (defined by the standard [NIST] Paper Specification; see Section 4.2.1.2.1.1.4.5.). Step 2. Verification of Regression Models for the Paper and Paper Datasets¶ After verifying that the model("	False
"
 input (English text): Which model has achieved the highest PARAMS score on the STL-10 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 RSS URL - Sprint 10 QNLI Benchmark v1.10 - Sprint 10 QNLI Benchmark v1.05 - Sprint 10 QNLI Benchmark v1.00 - RSS URL - Sprint 3.6 QNLI Benchmark v5.05 Sprint 3.7 QNLI Benchmark v5.06 - Sprint 8 QNLI Benchmark v9.03 - Sprint 16 QNLI Benchmark v20.01 - FasterQC Benchmark A few thoughts on the fasterQC bench What, if anything, is different about this benchmark compared to the previous benchmarks ? More on the differences. How large are the differences in PARAMS scores ? Params scores range from 0.000000 to 0.1100000000. The average is 0.0100000000: PARAMS scores for the 4.7% of samples that received a PARAMS score over 0.0000001 to 0.0100000000 are: PARAMS Scores (0 to 999) 0.0000001-0.00000001 17.98% 0.00000002-0.00000025 6.89% 0.00000030-0.00000041 3.04% 0.00000047-0.00000059 2.61% 0.00000101-0.00000043 0.64% 0.00000151-0.00000008 0.14% 0.00000227-0.000003 To read more about what the raw scores are, see the previous post. FasterQC is an optimization that performs an optimal solution to the problem through a combination of algorithms. The fastestQC benchmark uses three algorithms. If none of the algorithms performs quite fast, then the test run will be terminated. A list of fasterQ	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, ID FROM ( SELECT p.tID FROM aarch64.db.p_train, ( SELECT p.tID FROM aarch64.db.p_test ) p_test, name ) p.title; output (Sparql query): SELECT title, ID FROM ( SELECT p.tID FROM aarch64.db.p_train, ( SELECT p.tID FROM aarch64.db.p_test ) p_test, name ) p.title;	False
"
 input (English text): Where can I find code references in papers that have used the CAIT-XS-24 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MFEC model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MFEC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT dbl( ?cid)(.samp)( ?a, ?b, ?c, ?d) AS score, mb( ?cid) AS avg, max( ?cid) AS min FROM { ?samp ?a; ?a ?bm; ?a ?bl; ?a ?gb; ?a ?n; ?a ?gw; ?a ?gt; ?a ?nk; ?a ?nl; ?a ?pp; ?a ?rl; ?a ?rt; ?a ?rv; } t; samp ?1, b m, m; avg ?1, cl max_cm 1.5, k max_kl 2.2, max_t max_cm 2, n max_ml 2.0, p max_P 1.8, w max_W 2.5, ml max_P 1.4, W max_P 1.4, w max_P 1.0,	False
"
 input (English text): What models are being evaluated on the Atari 2600 Gopher dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): For each benchmark, what was the number of iterations ? output (Sparql query): ORGANIC_RDFs: Benchmark_Sequences_Sequence_0: Number of iterations (min 2) (avg 0.24) output (Sparql query): ORGANIC_RDFs: Benchmark_Sequences_Sequence_1: Number of iterations (min 2) (avg 0.24) output (Sparql query): ORGANIC_RDFs: Benchmark_Sequences_Sequence_2: Number of iterations (min 2) (avg 0.24) output (Sparql query): ORGANIC_RDFs: Benchmark_Sequences_Sequence_3: Number of iterations (min 2) (avg 0.24) output (Sparql query): ORGANIC_RDFs: Benchmark_Sequences_Sequence_4: Number of iterations (min 2) (avg 0.24) output (Sparql query): ORGANIC_RDFs: Benchmark_Sequences_Sequence_5: Number of iterations (min 2) (avg 0.24) output (Sparql query): ORGANIC_RDFs: Iteration_Tables: Number of iterations (min 2) output (Sparql query): ORGANIC_RDFs: Iteration_Fences: Number of iterations output (Sparql query): ORGANIC_RDFs: Iteration_Pairs: Number of iterations output (Sparql query): ORGANIC_RDFs: Iteration_Collections: Total number of iterations for each type output (Sparql query): ORGANIC_R	False
"
 input (English text): What are the metrics of evaluation over the SoMeSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 I wanted to make a list of the tests run so far, and put them side-by-side. In the current test suite:  test(a) A: the metric for evaluation over the dataset was set, then a dataset was compared, and the results were used for evaluation. I then applied the same logic to determine if the results were valid, or if the metric has multiple properties (e.g. a metric-less subset). I also did a benchmark run with the metric set (for the benchmarks part) to evaluate the model and the metric with several properties (for the benchmark). - test(b1) B: The metric for evaluation was set, and a dataset was compared, and the results were used for evaluation. I then applied the same logic to determine if the results were valid, or if the metric has multiple properties (e.g. a metric-less subset). I also did a benchmark run with the metric set (for the benchmarks part) to evaluate the model and the metric with several properties (for the benchmark). - test(b2) B2: I set the metric for evaluation, and it was reevaluated. If it returned false when checked by running the test, then the metric was set by some rules, and I used this for the evaluation. These four tests represent the following conditions, where: All tests are run with the metric set, or are valid. If the metric is set and false by the tests, then the metric is not being evaluated at all, and it is not being used as a metric in all cases. Note that I have also tried to make the testing more efficient; the tests that are best for any given dataset are usually the ones that evaluate the metric very cheaply. In order to determine this, I have run the tests using R (and included the metrics in the output) instead	False
"
 input (English text): What are the metrics of evaluation over the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT score(cave) AS score, caves.value AS caves_value, caves.avgs AS cave_avgs, cave_avgs.vgs AS cave_avgs.vgs, score_avgs AS score_avgs, (score(cave_avgs) > 0) OR score_avgs < 10, scores(cave) AS scores(cave_avgs), caves(1, 100) AS caves, scores(cave) AS scores(cave_avgs). 1.12.3 Output format for R data for evaluation The file of the SPSS R code is included in the file, rdata.r, with the version number, so we can use that for evaluation as we often will in R. Here is a brief description on the format of the file. In the file, all variables are enclosed in quotation marks. The variables are prefixed by "" ?"". Here the variables are referred to by name. As you might expect, if there is missing data or data where the variables have different values (sometimes this exists on R servers, the result is very likely to be wrong), errors will be reported. It is also expected that there is a lot of missing data on the R server, or that all the datasets have missing values. In all cases, the data needs to be converted to a format that can be read by R. Here is how the file should be formatted: output (Sparql query): 1.12.4 Evaluation of models 1.12.4.1 Example 1: Evaluating the model using the R library To evaluate a model, the first step is to use the library to obtain the test data. This is not very useful for most of the models, because these models have not been written for the data; I have"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?benchmark ?benchmark_lbl ?labels. output (Sparql query) All of the queries are based on the previous blog post here. We can see that the average K-means score of the best performing models in a benchmark is 15, or 50 percent of the benchmark score. A more interesting, and perhaps expected, result is the relative performance of a large number of models and their standard deviations (the relative fraction of models from each class in the benchmark that show an improvement). Below is a plot showing the K-means scores of the highest performing models in the test dataset. Note that the red line corresponds to the best performing model in the benchmark, with SVC being the Standard Deviation of the best performing model. The dashed brown line marks the top 2.5 percent of models. The graph only shows the models that improved above the top 2.5 percent, and the horizontal axis only captures the models that showed improved performances. The dotted red line marks the top 0.2 percent of the model set that show continued improvements. These are some of the highly optimized models that perform very well in the benchmark and which are not on this list; the dotted red line is intended to represent these models. The data is taken from this original post While the top 2.5 percent of models may not be	False
"
 input (English text): Where can I find code references in papers that have used the Prior+Duel noop model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior+Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet-152 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Can you give me a link to output from (2) above for the ResNet-95 (RNN) model ? output (Sparql query): HMM orkgp:HAS_DATASET = ?dataset. DSNL ?coder [SRCL ?coder_seq ?coder. ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?coder. ?biorh [ ?biorh ?coder. { ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 orkgp:HAS_MODEL ( ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2] orkgp:HAS_SOURCE_CODE ( ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2 ?biorh[2x2]])) }] orkgp:HAS	False
"
 input (English text): Where can I find code references in papers that have used the Intrinsic Reward Agent model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Intrinsic Reward Agent"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ? DISTINCT ?model-label, { ?model orkgp:HAS_DATASET ?dataset}. ?model. ?model-label, { ?model orkgp:HAS_DATASET OR kgc:SENSOR ?dataset }. ?model-label, { ?model orkgp:HAS_DATASET OR kgc :HAS_DATA ?dataset }. ?model-label, { ?model orkgp:HAS_DATASET OR kgc :DATASET_DATA ?dataset }. ?model-label, { ?model orkgp:HAS_DATASET OR kgc :HAS_DATA ORkgc:DATASET_DATA ?dataset }. ?model-label, { ?model orkgp:HAS_DATASET OR kgc :HAS_DATA ?dataset ORkgc:DATA ?dataset ORkgc:DATA ?dataset ORkgc:DATA }. ?model-model, { ?model orkgp:HAS_DATASET OR kgc :HAS_DATA ORkgc:DATA ; kgc:DATA ?dataset ORkgc:DATA Orkgc:DATA ?dataset ORkgc:DATA ORkgc:DATA ORkgc:DATA ORkgc:DATA ORkgc:DATA ORkgc:DATA ORkgc:DATA ORkgc:DATA ?dataset ORkgc:DATA ORkgc:DATA ORkgc:DATA ORkgc:DATA ?dataset ORkgc:DATA	False
"
 input (English text): Where can I find code references in papers that have used the Inan et al. (2016) - Variational RHN model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Inan et al. (2016) - Variational RHN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP 10 TITLE ?title. AUTHOR ?name. TITLE ?cont ?title. TITLE ?cont ?title. TITLE ?cont ?title. Title ?conversation ?conversation. R1 ?conversation. R1 ?conversation. R1 ?conversation. R1 input (English text): Provide a list of papers in peer reviewers or other venues that have used the Walker, walk (DMControl100k) dataset ? output (Sparql Query): select * from 'Walker, walk (DMControl100k, [])' \ where ( ?name ? ). AUTHOR ?name. TITLE. REPLY { ?conversation ?conversation_n. R2 \ ?comment_n. ?conversation_k. ?conversation. ?conversation_k. R2 \ ?comment_k. } input (English text): Provide an identifier for the paper in the database (e.g., Walker 100k ?). output (Sparql Query): SELECT ?authorid, ?title, ?comments as ?comment_id, ?comment_k as ?comment_k, ?conversation_n, ?conversation_k, ?query, ?authors as ?authors, ?orderby as ?orderby. ?orderby and ?authors. ORDER BY ORKID DESC; orkid:ORKID id, ORKID id, ID, TITLE ORDER BY TITLE DESC LIMIT 1; title: ORKID title DESC LIMIT 0; comments: ORKID comments DESC LIMIT 0 input (English text): Provide the number of citations in this paper for which the Walker. walk(DMControl100k, []) dataset has	False
"
 input (English text): What is the top benchmark score and its metric on the AAN Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAN Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP(1) ?metric _____ metric _____ val( ?value) __________ 1,732 1,642 1,636 1,732 input (English text): Describe the metric, its metric and the value used in each benchmark. output (Sparql query): _____ _____ rdfs:label ?metric_lbl ? rdfs:metric ?metric_lbl. input (English text): Describe the top benchmark outcome. output (Sparql query): Select ?metric _____ metric_lbl ? as ?result and ?value AS ?metric_lbl from ?dataset_lbl and ?metric_lbl as ?result. _____ _____ rdfs:label ?metric_lbl ? rdfs:metric ?metric_lbl. _____ _____ orkgc:Dataset ORkgc_metric ?metric_lbl. _____ _____ redd.datastat ?( ?result). ?result _____ rdfs:label ORkgc:metric ?metric_lbl. _____ orkgc:HAS_BENCHMARK ?benchmark. SELECT rdf.datastat ?( ?result). ?result _____ rdfs.datastat ?( ?result). ?result _____ rdfs:datastat ?( ?result). ?results and ?metric_lbl from orkgc ? and ORkgc_	False
"
 input (English text): What are the models that have been benchmarked on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model(dataset_lbl,1)|2.9, ?model(dataset_lbl,3)|4.0 FROM (SELECT ?dataset_lbl AS lbl, ?dataset_lbl AS rlbl, ?dataset_lbl AS lr(sdat)) AS rl AS lr output (Sparql query): SELECT ?model_lbl AS rl_model_lbl, ?model_lbl AS lbrd(dataset_lbl,1)|3.3, ?model_lbl AS lbrd(dataset_lbl,3)|4.9 FROM (SELECT ?dataset_lbl AS lbrd(dataset_lbl,1)|4.0, ?dataset_lbl AS lbrd(dataset_lbl,3) AS lbrd FROM ?dataset_lbl, ?dataset_lbl AS resc, ?dataset_lbl AS resc2 AS lbrs AS lbr2 output (Sparql query): SELECT lbrs.lbl.projection|rls.lbl.evaluation, lbrs.rls.evaluation, lbrs.lbr.model, lbrs.lbr.output, rls.rls.model, rls.rls.evaluation, rls.rls2.model, rls2.rls2.evaluation AS rl_models_output, rls.rls2.model, rls2.rls2.evaluation AS rls2_models_output, rls2_models_output AS resc, resc2	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Zaxxon dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Input and output fields should allow a variable number of rows in the two variables, as opposed to a variable number of entries in the variables. Input fields need to be as narrow as possible, and outputs fields must be narrow enough to prevent overflow. However, the fields in each field of the data may have a default value or an empty value if there are no records to pass in for the default value. A simple ""one variable"" approach to input vs output is, for example, to have an input variable of a few row and an output variable with the count of the data (rows in the data stream and the output row): input(text): input_row: ( ""0"" ""1"" ""2"" | ""3"" ""4"") output_row: NULL Now, you can output the count of the data and then make a variable that can store the number of rows or the number of times ""0"" is passed in vs ""2"" is passed in, and you have a function that can take the number of records returned by the function and create a new record as a count, and output a new column as a count. And that would be the basic set of models available for use in a model_input function in the TensorFlow implementation of the Benchmark module. The Benchmark function makes use of different ways for passing the data in, but the goal is to make the process seamless so that it is simple to do the above and then be able to easily modify which row of the data is passed in vs only the first or second. For example, if you had to pass in a file in which you wanted just a subset of the data, or a file with a single record, then you would be much more likely to make that change than if you had a function called model_input that would only pass in one row of the"	False
"
 input (English text): Provide a list of papers that have utilized the Hierarchical Transformer Encoder +  conditional copy model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Hierarchical Transformer Encoder + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT { ?model a:Model;... } input (English text): Describe the algorithm and its input ? output (Sparql query): SELECT { ?model a:Model;... } The output can be an SPSS-like Excel spreadsheet, and any other data. To display this output in CSV format, choose File > Open in Excel. The output will be sorted, and may also be exported to a text file. The generated file will include the link to the source code if there is no data already stored in the file. Inheritance In general, we do not want to make it possible for a module to make assumptions about which objects might be used and about exactly where, when, and how the instances will be created. The key idea here is that modules should not impose a dependency. Modules should be able to take an existing data type or a type that is abstracted from it and implement an abstract base class that works asynchronously. (One thing that is important here is that the module should only be able to expose, not require for instance, instances of types that do not already exist.) To make this possible, the modules should be able to declare instances for all possible data types the code uses and the code should be responsible for constructing an instance of each, at creation time. An interesting example of this approach is the DML statement. It requires the following code to create a singleton instance: DML(obj, objClass ) This example can be changed in various ways: The classes can be defined in the.c module and the declaration is optional. The classes can be defined in the.h module but it is expected that it is a good	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- [10]: ------------------------- LSTMReg-A [1]: ------------------------- LSTMReg-B [12]: ------------------------- [6]: ------------------------- DISTINCT ?metric ? LSTMReg-LDA [1]: ------------------------- LSTMReg-RDA [12]: ------------------------- [4]: ------------------------- DISTINCT ?metric ? LSTML1 [1]: ------------------------- LSTML1-2 [12]: ------------------------- [3]: ------------------------- DISTINCT ?metric ? LSTML2 [1]: ------------------------- LSTML2-3 [12]: ------------------------- BEGIN KDL: KDL : --------------[](2) KDL: H: H: H: ---: H: -------------- H: ------------- H: ------------- H: --------------- H: --------------- ------------: ------------: ------------: ------------: ---: H: H: H: ---: H: --------------- ------------: ------------: H: ------------------- H: H: H: -: -: -: H: ------------------- H: H: --- H: ------------- LSTMreg-A: LSTMReg-A: -- 1, 2, 3 and 4: ---------- -- 6, 7 and 8: ---------- --------------------- --------------------- --------------------- --------------------- 1: ------------------- -------------------- 1 2: -------------------- -------------------- 1 5: -------------------- -------------------- -------------------- -------------------- 1 -- 7 and 8: -------------------- \ / / / / / ___ \ / __ \ / / / / / / 1 -- 6: -------------------- \ / / ___ \ / __ \ / / / / / / / / 1 -- 5: ------------------ ------------------ -------------------- -------------------- 1 10: -------------------- -------------------- 1 ------------------ -------------------- -------------------- -------------------- 1 6:	False
"
 input (English text): Which model has achieved the highest Accuracy (10 classes) score on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (10 classes)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?seqname FROM DISTINCT ?seqname ?seqname WHERE ?model_lbl WHERE { ?metric c:CumulativeDifficulty; rdfs:value ?c_lbl. FILTER (str( ?c_lbl) = ""AverageScore"" ); orkgp:HAS_DATASET ?c_lbl. ?metric c_lbl. ?cont orkgp:HAS_BENCHMARK ?c_lbl; orkgp:HAS_MODEL orkgp:HAS_VALUE ?c_lbl; orkgp:HAS_METRIC ?c_lbl_rank. } ORDER BY DESC( ?c_lbl_rank) LIMIT 2 } input (English text): What model has achieved the highest Accuracy (10 classes) score on the IMDb benchmark dataset ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ; orkgp:HAS_DATASET ?dataset. orkgp:HAS_EVALUATION ?dataset_lbl. } ORDER BY DESC( ?dataset_lbl) LIMIT 1 } output (Sparql query): Select the output from the previous query. input (English text): The benchmarks for the IMDb dataset can be found in the following table: c The following table lists the following benchmark results in the NLE. The text shows the number of times each benchmark result corresponds to the actual word score for each text in the benchmark. c The above benchmark results are the same as"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Kangaroo benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kangaroo"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT { ?id:ID ?dataset ?model_lbl. ?metric an_m, an_m :metric; :test_id ?dataset_lbl. } FROM (SELECT id FROM { id( ?metric ?id); :test_id ?dataset_lbl. } ORDER BY id ); ?dataset is defined as: ""HAS_DATASET is a unique identifier for the instance."" ?dataset_lbl is defined as: ""Atari 2600 Kangaroo benchmark dataset with the provided test and benchmark data."" score is used to set the output score for the evaluator. A score of 0 means perfect score, a score of 1.0 means perfect score with all test data, and a score of 0.01 means perfect score with all test data except the test data with an a minus score of 1.0. input (English text): Indicate the model that performed best in terms of Score metric on the ClueWeb09-B dataset ? output (Sparql query): SELECT p, ?ratio ?ratio_i, ?ratio ?ratio_i, p:p_ratio * 10 FROM { p( ?ratio ?ratio_i) ?train_i_ ?test_i_; ?stat :stats ?stat_lbl; } ORDER BY!stat DESC( ratio_i 1.0 ?ratio_i_) DESC( metric ?stat_lbl; max_count ?stat_lbl); input (English text): Indicate the model that performed best in terms of Score metric on the ClueWeb09-B dataset ? output (Sparql query): SELECT p, ?ratio ?ratio_i, ?ratio ?ratio_i"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?scale ?metric, ?n ( ?) as metric_n FROM { ?scale ?metric, ?n ( ?) as metric_n } ORDER BY ?scale DESC LIMIT 1 (Note: To create a new database, see the example.csv file here) input (English text): Indicate the model that performed best in terms of Accuracy metric in the E.T. test ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl ; OR id FROM { orkgc:Measurement ?scale } ORDER BY id DESC LIMIT 1 (Note on formatting: This query has been formatted in an easy to read manner for you using the default table layout in a Parquet file. You can read the same query with a table like this: [{ ""metric"": ""metrics"", ""n"": 1, ""accuracy"": ""accuracy"", ""scale"": 1, ""name"": { ""model"": ""Test1"", ""score"": 1, ""level"": 1, } }, { ""metric"": ""metrics"", ""n"": 2, ""accuracy"": ""accuracy"", ""scale"": 2, ""name"": { ""model"": ""Test2"", ""score"": 2, ""level"": 1, } }] This format will yield the same result. However, note that for data that are already formatted as described above, this will be a duplicate or more accurate result. This is because you are adding a label, a metric, and a score (if the score is positive) only once for each row in the data set. If the dataset contains only one label (for example), then the same dataset will return the exact same result. The query will return an even greater precision when the model has been evaluated many times on each model."	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Montezuma\'s Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT count( ?resid) > 20, ?paper FROM `test_model_benchmarks.model` where resid = ?con.p:p32 ORDER BY count( ?resid) DESC, DESC LIMIT 20; Concepts: Model Selection, Feature Selection, Feature Coding, Feature Validation, Feature Data Analysis This section provides a description of the general concepts that will be needed as the code becomes larger and larger because of the lack of documentation and an unwillingness on the part of CvT-21 authors to add these concepts to this dataset. Overview of the dataset The Atari 2600 and CvT-21 data sets were first analyzed with a model selection function developed to identify specific aspects of a model that produce the strongest correlations with the selected models. The model selection function then finds models that generate the strongest correlations between the models and all the models it had considered. Finally, it finds models that are predictive of all the models that were used to find them (which would have included each of the models that were already included in its model selection function). CvT-21 was developed as a way to select simple general linear model models based on the structure of the data. The CvT is a simple linear regression model that is designed to be very general which is to say that it is not designed to be used to predict any particular outcome. With CvT-21 however, every model needs to be tested and if there is a very strong correlation between the model's output and some variable in the data then it can be tested for predictive value. Using CvT-21 for predictive modeling The output of the CvT-21 model selection function can be used for predictive modeling. The output variable will have different colors depending on the model selected. Colors	False
"
 input (English text): What is the top benchmark score and its metric on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG fine-grained gate"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. }"	False
"
 input (English text): List the metrics that are used to evaluate models on the HoC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 OPTIONAL{ ?benchmark = TRUE; metrics = { ?metric = 'test' ?metric_lbl_test_data ?dataset_lbl_test_data. ?metric_rdfs_lbl_t = 'test' ?dataset_lbl_test_rdfs. ?metric_lbl_rdfs_t = 'test' ?metric_str( * ) ?dataset_lbl_rdfs } ; ?eval, ?eval orkgp_lbl_test_t =..., ?eval orkgp_lbl_test_rdfs_t =..., ?eval orkgp_lbl_est_t =..., ?eval orkgp_lbl_est_rdfs =... } } If you specify all three metrics, then all metrics will be used. If not specified, only the first two metrics will be used. A table of the options for each step for the benchmarking is given below. The first column is the name of the metric metric, the second column is the name of the dataset dataset, the third column is the name of the model model, the fourth and fifth columns describe the parameters of the model (in the order of priority) and the sixth column provides the metric as an option. There is one final column for the benchmark method (in the order of priority). Example. For the metric test, using three different metrics is easier. The first argument to benchmark is named the metric. Each of the three metrics is given a name. The first argument name is the name of the metric. The second argument label is a reference of the metric to be benchmarked. The third argument metric_lbl is a string that is used to specify that this test should be compared using	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the Oxford 102 Flowers dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?model ?model_lbl FROM { ?dataset ?dataset } WHERE { ?dataset ?dataset_lbl}. How many metrics were used in the model evaluation ? input (English text): How many metrics were used in the model evaluation ? output (Sparql query): SELECT ?model ?model_lbl. Metric AS metric FROM { ?dataset ?dataset } WHERE { ?dataset ?dataset_lbl}. If you have more than one metric, list those you know. Example: Using the query above, the results would be: SELECT ?model ?model_lbl ?model_lbl. Metric AS metric FROM { ?dataset ?dataset_lbl} WHERE { ?dataset ?dataset_lbl}. How many parameters did the model generate ? input (English text): How many parameters did the model generate ? output (Sparql query): What was the accuracy of the model in terms of accuracy (%) score ? input (English text): What was the accuracy of the model in terms of accuracy (%) score ? output (Sparql query): SELECT ?input ?output ?input. Accuracy AS accuracy. FROM { ?input ?output ?input. } { ?input ?output ?input. Accuracy AS accuracy. } ORDER BY CONCLUSION. How many values were used in the model in terms of accuracy (%) score ? input (English text): How many values were used in the model in terms of accuracy (%) score ? output (Sparql query): How many parameters did the model generate ? input (English text): How many parameters did the model generate ? output (Sparql query): What was the accuracy of the model in terms	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Q*Bert?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entity F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 selects: {... :model (model:dataset). :testdata (model)...... } GROUP BY testdata; output (Sparql query): input (English text): What is the first word in the sentence that best describes the top benchmark result (metric and value) Atari 2600 Q*Bert : score : 0 - 50 50 - 30 30 - 0 0 - 1 0 - 5 0 - 1 4 0 - 2 7 - 9 2 - 6 1 - 9 1 - 4 8 - 13 3 - 15 4 - 19 1 - 16 1 - 17 1 - 19 1 - 19 1 - 19 1 - 17	False
"
 input (English text): Which model has achieved the highest F1 score score on the BUCC French-to-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric ?a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"" ) ?reasons. ORG_MIS(s ?reasons. ORG_STOP ?reason); orkgc:HAS_DATASET ?dataset. ORG_STOP rdfs:label ?dataset_lbl. ORG_STATISTICS ?statistics. ORG_EVALUATION ?eval. ORG_METRIC ?metric. ORG_STATISTICS ORG_RESOURCE ?resource. ORG_DISTINCT ?model. ORG_DATA ?data. ORG_ROW ?row. ORG_TURN ?turn. ORG_UNTIL ?until. ?reasons. ORG_UNTIL ?before. } input (English text): Which model has achieved the highest F1 score score on the BUCC Fine-grained classification dataset ? output (Sparql query): input (English text): what are the performance metrics that have been used in the evaluation of a particular LSTM model ? output (Sparql query): input (English text): What is the training dataset for the benchmark dataset ?output (Sparql query): input (Text): What do you expect to happen with RNNs in each case ? output (Sparql query): input (English text): Which model has achieved the highest F1 score"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CNN + LSTM + SVM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CNN + LSTM + SVM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?code WHERE { ?logic ?model orkgc:Logic ?logic_bias ?model_bias. OR ?logic_bias OR ?data_1 ?logic_bias. OR ?data_2 ?logic_bias. OR ?titles ?data_1 \ orkgc:Titles_Citation. OR ?text ?data. } We are using the benchmark function to get benchmark results against two different datasets. The first dataset, labelled as long (30x8) was chosen for our benchmark; the second dataset, called short (8x2), was chosen for the comparison group (N=1050). These two datasets have several key features that we expect from any good long-term classification task: The high-dimensional features (1-3 dimensions) are highly salient for the task and include spatial, temporal, statistical, etc. features. The natural language feature sets are relatively small: 20x20. In the longer dataset, the features are even smaller since most short-term classification tasks involve tasks with few words. Given the low size of the long dataset and the high-dimensional feature sets, we expected that we would have a more difficult time identifying features from long datasets than from short ones, given the similar features and the high-dimensional features. The short dataset is also the one most likely to contain errors since such errors are more likely to occur around the vocabulary and short-term memory errors rather than error in the neural network. The long dataset is also a good test of the ML-CPR technique as it contains only small set of short-term memory features: n=13. This dataset is likely a good control over the performance of the CNN+LSTM+SVM model given that the number of words present is relatively low. The data set also was	False
"
 input (English text): List the code links in papers that use the CRF with term expansion model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with term expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP100(WMT2016, DISTINCT ?category, ROW) output (Sparql query): SELECT TOP1000(WMT2016, DISTINCT ?category, SUM( ?) / 1, ROW) out output (Sparql query): orkgp:HAS_HIDE_PAGES ?category; orkgp:HAS_GENERIC ?code. In order to run in R, it is necessary first to configure a suitable CRF (see http://dwag.univ-wien.ac.at/~hag/CRF/configuration.html) and to use the model ""CRF with Term Expansion model"" in an external benchmark file and in your benchmark paper. Note that the CRF-test-benchmark-CRF.R will automatically output a CSV of all results to a file 'Results.csv'. You should use this file to compare your results with others. It is generally a good idea to compare the results from the same benchmark to the same dataset in different benchmarks. The following command could be used for example: R > bench.R Benchmarking results of CRF with term expansion model. The input files are not included with the benchmark file. Out: Output: R > dplyr.test.CRF_LFT_CHANNELS1.R R > dplyr.test.CRF_LFT_CHANNELS2.R In order to ensure that your output CSV file contains the benchmark records from your benchmark using ""CRF with term expansion model"", it is necessary to select all the CRF records in the benchmark and then to export"	False
"
 input (English text): Which model has achieved the highest Pre-Training Dataset score on the UCF101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which sectors are modeled as energy sectors and how often?
 output (Sparql query): "	SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } 	 input (English text): Which input units are used and how many inputs do they contain ? output (Sparql query): input (English text): What are the following criteria for model activation ? output (Sparql query): input (English text): The number of inputs and the number of outputs are equal ? output (Sparql query): Yes input (English text): Why is activation at constant values used for optimization ? output (Sparql query): Optimization is based on a deterministic goal so constant values are more efficient. input (English text): When is a model used in network formation ? output (Sparql query): As a feature, for optimal performance. input (English text): If you do not have any input from a specific time period, why is one of the inputs a subset of the entire input and the other one a subset of another subset ? output (Sparql query): For better accuracy. input (English text): Is it possible to choose the model for a specific time in the training process, if some inputs were missing from the prior training or if other parameters were altered ? output (Sparql query): Yes input (English text): When is the model initialized ? output (Sparql query): Once training starts. It is done by using the network builder. input (English text): What are the following parameters that need to be specified in the network builder ? output (Sparql query):  1) Training Duration The duration of the training cycle, usually 10 minutes. 3) Max. Number of Layers The number of input layers to be used and the number of neurons. 4) Max. Number of Features The number of features to be learned and the number of neurons used to perform the estimation. 5) Max. Number of Features to	False
"
 input (English text): List the metrics that are used to evaluate models on the NYT29 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT name, name as title, name as title_id, title_lbl, title_id, metric_lbl, metric_id, description, benchmark, description_str as _score, description_str as title_id, description_str as title_lbl, description_str as title_id, description_str as title_lbl_str from rdfs_literacy_sdb_review_books where title= ? Please use at least the metric name in R, e.g. kappa_metric. The first three values represent the metrics that are used to evaluate models. Please provide any additional data that needs to be included or that are not understood by the default list. Please provide the name of the first criterion or set of criteria you used or used to describe your model. For example, what's the first set of metrics used, and what's the first criteria that was used to define your model ? Please provide a description of your benchmarking strategy. What are the metrics used to evaluate your model under the test conditions of the NYT29 dataset ? Note that the NYT29 and Kinetics datasets were not independently created. Each dataset has the same characteristics. To provide more information or to specify how the same data is used in your model, please see R's Benchmarking documentation. Note: please include an exact title at the start of each sentence. Some documents have very specific titles like ""Benchmarking my Model on the NYT29 Dataset."""	False
"
 input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" If your datasets are large, you may wish to perform an aggregation step such as B-spline aggregation (see below). Note: You may choose to make your datasets larger; all those in this test only have small datasets, so you could go as small as N=15. A B-Spline algorithm that will find the largest, shortest, and most symmetric subset is usually referred to as ""Covariance Correlation and Correlated Covariance,"" (CCCC). Note: The R project provides several tools for aggregating datasets, some of which are not described here, for non-English datasets. If you need more than one choice of aggregation option and don't know which, use the Aggregation Checker to select the option you prefer. Note: You will need to install the rstats package to read and run the R scripts. Example COCOs The following example demonstrates the use of aggregate functions in R. import numpy as np c = dataset.select(R.groupby(dataset_id, ?dataset)) c.agg(x) # x is an array of n x's for i in xrange(n): #... aggregates the results from c into the given # variable and returns a data frame of c. dataset.insert(x, dataset.from_dict(np.zeros((dataset_id, 1))) #... creates an empty data frame and updates the # index of the first dataset containing 1 dataset.drop(x[0]) #... updates the index of the last dataset c.agg(x) #... aggregates theresults from c into the given # variableand returns a data frame of Note: The first aggregate function selects which dataset will be included in the analysis. If there is only one dataset in the dataset type argument,"	False
"
 input (English text): What models are being evaluated on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? 
 output (Sparql query): "	"SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, ""Naive bayes"")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, ""Bag of words"")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, ""Feature request"")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } "	 input (English text): What performance was achieved when the input data has variable dimensionality in the first place ? in-line output (Sparql query): output ( ?samples_modes) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | SPA_RANK_MIN_DATA  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | SPA_RANK_MAX_DATA  | input (English text): The results of some experiments were tested by the machine learning feature bag of words for the Cheetah, run (DMControl500k). output (Sparql query): output ( ?samples_	False
"
 input (English text): What are economics subfields?
 output (Sparql query): SELECT DISTINCT ?subfields, ?subfields_labels WHERE { ?papers orkgp:P30 ?research_fields. ?research_fields rdfs:label ""Economics""^^xsd:string. ?research_fields orkgp:P36 ?subfields. ?subfields rdfs:label ?subfields_labels. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Samples in the S1 File for these subfields and their papers from the subfield website 1. Theoretical 2. Methods 3. Mathematics and Statistics 4. Statistical Methods 5. Theory and Modeling DMLab-30 Data The S1 File gives statistics about the results of the DMLab-30 experiments, along with the original analysis on which they were based. The subfields are divided by research in the following manner: 1. Experimental Methods (1) 2. Methods (7) There does not seem to be another category for these subfields, though it may not have to do with the DMLab-30 data itself. I assume they were chosen primarily to provide the data for a comparison of some, but not all, studies. The S4 File gives the results of the experiments themselves. The S2 File gives a short description of the data available, and the results from the analyses used to validate the results. As with the data, there does not seem to be another category for these subfields. I assume they were chosen primarily to provide the data for the comparison of some, but not all, studies. Note that the S2 File includes a S2 (subfield) with the same information as the S1 : ( ?research_fields orkgp:P52 ?) orlabels := "" ?dmlabs:1 (1)"" orkgp:P52 ?) orlabels := "" ?dmlabs:2 (2)"" orkgp:P52 ?reli [1] 1-13, 14 1.3. Statistical Methods for Economic Theory 4.1. Anomalies and Regressions in the General Theory ( ?reli := "" ?reli_test_pairs)"" S1"	False
"
 input (English text): Could you provide a list of models that have been tested on the WMT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT title, title_number FROM titles ORDER BY title DESC; title  [{""title"":1,""paper"":10,""evalure"":4,""papers"":10}] output (Sparql query):  SELECT title, title_number FROM titles WHERE title_number>2 AND evaluaturing = 1; title  [{""title"":1,""paper"":8,""evalure"":6,""papers"":6}] output (Sparql query):  SELECT title, title_number FROM titles WHERE title_number>2 AND evaluaturing = 1; title  [{""title"":1,""paper"":9,""evalure"":8,""papers"":7}] output (Sparql query):  SELECT title, title_number FROM titles WHERE title_number>2 AND evaluaturing = 1; title  [{""title"":1,""paper"":6,""evalure"":10,""papers"":9}] output (Sparql query):  SELECT title, title_number FROM titles WHERE title_number>2 AND evaluaturing = 1;  (Note that we have not filtered out duplicate data and therefore this query gives back very many output rows that should not have been returned.) Note also that because this is a test dataset, WMT2014 German-English contains a certain amount of non-English language texts. In total, this is more than 1 billion words written in English so there is likely to be data left out of the dataset on other subjects (such as ""camel"".) We can find all these items in our WMT2014 data, but with the advantage that the data for one topic is not too different (not a very useful criterion, IMHO). To get this without loading the data from this topic, we select the"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Gopher dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_rdfs_lst (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_rdfs_lst ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lst. FILTER (str( ?dataset_lst) = ""Cartpole, Swingup"") ?benchmark orkgp:HAS_DYNASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lst. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lst. } } ORDER BY DESC( ?value ) } } GROUP BY ?metric ?metric_rdfs_lst input (English text): What is the highest benchmark result achieved on the Cartlamp, swingup (DMControl500k) dataset, including the metric and its value ? output (Sparql query):  SELECT DISTINCT ?metric ?metric_rdfs_lst (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_rdfs_lst ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lst. FILTER (str( ?dataset"	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl FROM dbo.model_slices pd WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A or kgc"") {} input (English text): Select the FLOPS metric type and number of objects that fit into the given dataset at a specific FLOPS metric level. For output (Sparql query): output (Sphinx query): SELECT metric_level, FPLOPS, ?model_lbl, ?metric_level, ?rdf_level, ?batch_size, ?batch_linal_count, ?batch_sizes AS avg(batch) FROM dbo.model_slices pd WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FPLOPS"") {} } input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset ? output (Sparql query): output (Sphinx query): SELECT ?model ?model_lbl FROM dbo.model_slices pd WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A or kgc"") {} input (English text): Select the number of rows in each dataset that meet the given requirements for the given metrics. For output (Sparql query): output (Sphinx query): SELECT ?number_of_rows AS"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WikiText-103 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	-------------- Dataset -------------> Reference(s): WikiText-103 [2, 3, 25, 26] - > (2, 3, 26) - > (2.14e10 / 2.14e10) rdf:select-metrics <- tbs(dplyr::filter(v<=:>7, $1:1, rdf:columns(a, b, c))) rdf:get-metrics #> [1] 0.00981578 - - > (0.018605716) rdf:get-score #> [1] 0.02272918 - - > (1.1e-27) rdf:test #> [1] 0.02561828 - - > (1.18e-30) rdf:contr <- lm(rdfs:select-metrics, [1:3], order=.99) #> [1] <intercept 0.02590775 - - > (2.10e-31) rdf:value <- lm(rdfs:select-metrics, [1:10], order=.99) #> [1] <intercept 0.02692842 - - > (2.10e-31) rdf:constr <- lm(rdfs:select-metrics, [1:20], order=.99) #> [1] <intercept 0.02562884 - - > (2.10e-31) rdf:lab <- lm(rdfs:select-metrics, [1:100], order=.99) #> [1] <intercept 0.02692843 - - > (2.10e-31) rdf	False
"
 input (English text): Can you list the metrics used to evaluate models on the CoNLL 2003 (English) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?residual ?score ?dataset ?dataset_lbl. Input is English text in one direction, and output in another. Output will be in English Output will be English, but the value of the metric will be in another language, such as SQL-speak. Examples: Output is ""TOP MACHINE: ""Top scoring model"" over the dataset Oxford-IIIT Pets. Matched output is ""TOP MACHINE: ""The top scoring model over the dataset. Input is English text in one direction, and output in another. Output will be in English, but the metric will be in another language, such as SQL-speak. Example ""TOP MACHINE: ""Top scoring model"" over the dataset. Output is ""TOP MACHINE: ""The top scoring model over the dataset. Matched output is ""TOP MACHINE: ""The top scoring model over the dataset. Outputs are both English, and both in a language other than English; ""Top scoring model"" over dataset Oxford-IIIT Pets. Annotation of Outputs Now that you know the metrics your models can predict based on the same dataset, you can determine which output is most interesting: An example below uses the ""BEST"" metric and the ""HIGHEST"" metric. The query outputs an error: Input is the same as above, except that the second part of the output is the original input text. Example: Output is ""HIGHEST: ""The model has determined which is ""Best"" according to the dataset. An example below uses the ""BEST"" metric and the ""BEST HIGHEST"" metric. The query outputs an error: Example: Input is the same as above, but in the first part, the """	False
"
 input (English text): Which model has achieved the highest Error score on the Amazon-5 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?dataset ?dataset_lbl WHERE ?metric_lbl ?""1"", ?model ?model_lbl WHERE ?metric_lbl ?model_lbl, { ?dataset a orkgc:Dataset, ?dataset_lbl rdfs:label ?dataset_lbl. ?format ?format orkgp:HAS_DATASET, ?dataset_lbl ?dataset_lbl. ?output ?output, ?metric_lbl. ?label orkgp:HAS_VALUE ?label., ?format ?format. ORKP:HAS_VALUE ORKP:HAS_METRIC ?metric. ORKP:HAS_FORMAT ?format.!l2 ?format orkgp:HAS_VALUE ORKP:HAS_METRIC ?value.!rf ?format orkgp:HAS_VALUE ORKP:HAS_METRIC ?value.!rdfs ?format.!str ?format, ?input ?input. In addition to the above, we can see several other models from AWS like:  XL2KNet-SmallBatch and XL2Net-SmallNested.  The two datasets that appear to be underperforming are either XL2KNet-Small or XL2KNet-SmallNested.  If we compare the performance of the models that come first at the end, we can see that XL2KNet-Small is indeed outperforming both XL2KNet-SmallNested and the benchmark.  It is clear why the latter models are underperforming: they"	False
"
 input (English text): How many studies are published after 2019?
 output (Sparql query): SELECT (COUNT( ?date) AS ?number_of_studies) WHERE { orkgr:R110393 orkgp:compareContribution ?contrib. ?contrib orkgp:P9040 ?date. filter( ?date > ""2019""^^xsd:string) } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?time FROM ( SELECT date FROM date ORDER BY date DESC LIMIT 3 ) AS d WHERE ?time { orkgr:R110393 orkgp:compareContribution ?contrib. ( ?date > ""2019""^^xsd:string ) } input (English text): Is there an EfficientNetV2-L code repository that other data scientists might find useful ? output (Sparql query): input (English text): I have read through the code of the EfficientNetV2-L model, how would I get access to the model to benchmark it ? output (Sparql query): ( ?dataset ) SELECT ?dataset FROM...; Input and Output files Ravi is currently in release 2.1.0. You can browse a live snapshot of version 2.1.0 at https://github.com/RaviT/Ravi. Please read the manual page at https://github.com/RaviT/Ravi/blob/master/README.md. Code: Ravi is available as a GitHub open source release, where you can also download prebuilt binaries and installation images. Please read the GitHub releases page first. Installation instructions: The Ravi source code is available from https://github.com/RaviT/Ravi in three formats: The Ravi code repositories contain the source code for Ravi, the open source benchmarking program, and The Ravi executable, which can be downloaded from https://github.com/RaviT/Ravi. There is also a pre"	False
"
 input (English text): List the code links in papers that use the Hierarchical transformer encoder + conditional copy model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Hierarchical transformer encoder + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"______________________________________________________________________ ______________________________________________________________________ ______________________________________________________________________ WOS-46985: A cluster analysis framework for large-scale data mining and visualization. D. Hulst. A paper by Hulst and Lötsch: ""Parallelized classification using cluster-based multi-step Monte-Carlo learning"", Journal of Scientific Computing, Vol. 6, No 2, pp. 131-154. May 2000/7. Springer, New York. -------------- next part -------------- An HTML attachment was scrubbed... URL: <http://mail.python.org/pipermail/python-ideas/attachments/20140519/1d0cfafc/attachment.html>"	False
"
 input (English text): What is the best performing model benchmarking the DTD dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?DISTINCT ?model ?model_lbl. ?Metric DONE. ?name NAME. ?valuevalue value. ?metric METRIC. ?dataset ?dataset_lbl. Note that the SPSS program spss.exe cannot automatically generate a list of all the output columns. The output in the Supervised: dataset is shown in Table 1. Column names correspond to the metrics and can be listed by prefixing the metric name with the metric's prefix. The Metric: column must contain the metric, which must always be the type value. Since this dataset stores the metric in a variable and can be manipulated later, a variable must appear in all output variables. If the metric is a value, the column name must be an integer, not a variable, and the data type must be an integer. In that case, the column name must denote only the metric's type; it cannot contain ""VARIABLE"", ""DATE"", ""TIME"", or similar names. The column names and values of Metric: and Metadata: columns are displayed in the following table. Parameter Description ?dataset Variable that stores the dataset's metric. ?model Variable that stores the model parameter. ?model_lbl Variable that stores the model parameter. ?dataset Variable that stores the dataset's dataset. *Model Variable that stores the model parameter. ?dataset_lbl Variable that stores the dataset's dataset. A label must be applied to the model variable. ?labels Label that applies to label of the model parameter. ?value Value that applies to value of the model parameter. ?metrics Metric that applies to value of the model parameter. ?model_lbl_lbl_value Variable that applies to value of the model parameter Using the Model Parameter to Set the Performance"	False
"
 input (English text): List the code links in papers that use the WDec model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""WDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (< ?iso8601= ?iso8601:> or ?iso8601:iso_code ?format :> ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model;... orkgp:HAS_SOURCE_CODE ?code; orkgp:HAS_SOURCE_CODE ?code ? input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (< ?iso8601= ?iso8601:> or ?iso8601:iso_code ?format :> ) ?benchmark ORkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code;... orkgp:HAS_SOURCE_CODE ?code ? input (English text): List the code links in papers that use a WDec model in any benchmark ? output (Sparql query): SELECT "" ?dataset"" as ?dataset OR "" ?code"" as ?"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2014 English-French dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): How does the WMT2014 English-French dataset compare to other languages on the same benchmark ? output (Sparql query): input (English text): What's the difference in the performance that English-French performance would get in a benchmark, relative to English-German, French and Spanish ? output (Sparql query): input (English text): Which of the benchmarks is the most accurate ? output (Sparql query): output (Sparql query): input (English text): What does the WMT2014 English-French result mean ? - No information. - the WMT2014 (WMT2014-2015) results. input (English text): What are the benchmarks for English-French and English-Spanish ? output (Sparql query): input (English text): What kind of data are we having ? output (Sparql query): input (English text): What is the name of the benchmark and name of the variable it is comparing ? output (Sparql query): output (Sparql query): input (English text): What is the name of the variable ? output (Sparql query): output (Sparql query): input (English text): What is the average quality of the data compared to other benchmarks ? output (Sparql query): output (Sparql query): output (Sparql query): input (English text): What is the name of the variable that is providing the metric information ? output (Sparql query): output (Sparql query): 	False
"
 input (English text): Where can I find code references in papers that have used the Compressive Transformer (18L, M=1024) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Compressive Transformer (18L, M=1024)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT RANK(comp, avg) from TESTS_GAMES COUNT(1) COUNT(1) SELECT max(rank) int(rank) FROM TESTS output (Sparql query): SELECT ""comp"" as COMPREPID and "", avg"" as AVGREPID from TESTS_GAMES output (Sparql query): SELECT S1.comp as COMPREPID FROM TESTS_GAMES RANK(comp, avg) ----------- 1.15 0.19 0.18 0.17 2.1 0.19 0.19 0.18 13.0 0.14 0.14 0.13 3.7 0.14 0.14 0.13 6.2 0.13 0.13 0.13 7.1 0.13 0.13 0.13 11.3 0.14 0.14 0.15 I didn't find anything about the Compressive Transformer model. It's similar to the SPSS-21, though, and probably the only other method I've found that's compatible with our hardware. For comparison: Compulsive Engineering, by the way, used a similar algorithm, but used a large amount of text and a slightly different data set (SPSS 21).  Update: Here's a paper that used the same algorithm, but modified it to use the new LAPACK and RANSJ format. It also has some comments and additional comparisons."	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the DTD dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 You can find the benchmarking results of the WMT2014 English-German benchmark dataset. You are also asked to provide a description of the models used in benchmarking the results. We welcome other queries and welcome you to add them to the discussion! See also: Top performing models for the English Language The results of benchmarking: 1,927 out of 2,715 submitted query	False
"
 input (English text): Could you provide a list of models that have been tested on the UCF101 (finetuned) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 <input type='text'> <output type='text'/> input (English text): Can you provide a list of models that have been tested on the Cart Pole (OpenAI Gym) benchmark dataset ? output (Sparql query): <input type='text'> <output type='text'/> input (English text): Describe the datasets you have already benchmarked on. output (Sparql query): <input type='text'> <output type='text'/> input (English text): What are the primary use cases where this might be used ? output (Sparql query): <input type='text'> <output type='text'/> input (English text): What do you personally think: This dataset is likely to be useful to you ? output (Sparql query): <input type='text'> <output type='text'/> input (English text): What are your suggestions for how to improve it so the datasets can make more use of the model ? output (Sparql query): <input type='text'> <output type='text'/> input (English text): Where do you see this in the future ? output (Sparql query): * Your Input Please enter a valid username to receive a response:	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Breakout benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT rdd_version, data_version, text, data_name FROM test_data GROUP BY rdd_version ORDER BY rdd_version DESC LIMIT 1,100 output (Sparql query): SELECT rdd_version, data_version, text, data_name FROM test_data GROUP BY rdd_version ORDER BY rdd_version DESC LIMIT 1,100 RAW Paste Data input(English text): Please explain the logic to the table above ? (Please enter the values in English.) output(Sparql query): SELECT rdfs_version, cmp_version, text FROM test_data rdfs ORDER BY rdfs_version DESC LIMIT 1,100 ---------------------- Inputs required ---------------------- _________________________ _________________________ _________________________ ---------------- S = Single-word text _________________________ _________________________ _________________________ _________________________ _________________________ ---------------------- Inputs required --------------------- ---------------- B = Text with only a single word (i.e. without a character) _________________________ _________________________ _________________________ _________________________ _________________________ ---------------- C = Text with multiple words (i.e. with a few more character) _________________________ _________________________ _________________________ _________________________ _________________________ ----------------  + +  +  +  +  + ---------------- ------- S = Single-word text _________________________ _________________________ _________________________ _________________________ _________________________ _________________________ ---------------------- Inputs required ---------------------- ---------------- S = Single-word text _________________________ _________________________ _________________________ _________________________ _________________________ ---------------------- Inputs required ---------------------- ---------------- B = Text with only a single word (i.e. without a character) _________________________ _________________________ _________________________ _________________________ ________________________	False
"
 input (English text): List the code links in papers that use the Bootstrapped DQN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Bootstrapped DQN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Inputs to the function Model : the type of the model used in the benchmarks, e.g. ? model, the name or ID of the model from where data was taken - is a string, usually not needed - ? input. The input string may contain any of the following: * : Data that describes to the R driver how these codes are computed, e.g.'x% ','y% ', or'Z%'* : R-friendly names of the functions you want to work with (e.g., ""self"") (with or without parameters) * : The label of the function that your code needs to be self aware of. You have to supply this label from the command line: ? input: self [label] Outputs from the function label : the label name to use for the function that generates the benchmark code, e.g. ? output: self_nodes_codes [self_label] (with or without parameters) Samples from the function samples (type of function): the name of the function that generates the benchmarks, e.g. ? ? ctrl, ? output (Sparql query): where can I find code references in papers that have used the Autocorrelation-Inhibition model and CIE-CIM (Collaborative Intelligent Environments) for benchmarking purposes ? input (CIE-CIM and Autocorrelation-Inhibition model): a string (or a multi-part string) describing the model. The parameters are the models data structure to use and, optionally, a function name and a number of the types that you wish to observe parameters in the output. The model data structure is typically not provided by the code (e.g., from the R driver). Example output for the function input (CIE-CIM"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Biinclusion (Euro500kReuters) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Biinclusion (Euro500kReuters)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?db.genome.DB.genome ?model ?db.genome.DB.model ( ?model_lbl. FILTER (str( ?model_lbl) = ""Biinclusion (Euro500kReuters)"") ) ?db.database.dataset. DB.database.dataset. ?db.database.dataset. DB.database.dataset ?db.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. DB.database.dataset. ?db.database.dataset. DB.database.dataset. input (English text): The reference genome of the model under evaluation. output (Sparql query): Select the reference genome.  (from Genome"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""6-layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input (English text): What is the type of problem the researchers use the 6-layer QRNN model for within their research ? output (Sparql query): What are the types of training errors the model should generate ? input (English text): What is the performance of a model that is trained on a 10% wide dataset from the Flowers-102 dataset, along with 30% width weighting ? output (Sparql query): I have trained a model on the flower dataset. It scored the ""flowers-202"" dataset. What's the accuracy score of this model when applied to the flower-102 dataset after training and the same model trained on just the flower-201 dataset ? Please provide an answer in the comments box, please. input (English text): The problem the authors use the 6-layer QRNN model for within their research is ""How can one define a feature-space model that is able to learn to estimate the weights of features such that an error is minimized or a desired objective is exceeded ?"" output (Sparql query): In the previous work, we showed how to train a neural network to describe the features in a feature space and how to use a Gaussian mixture model to minimize the prediction error. We would like to provide an alternative to the conventional training procedure, specifically, how to combine different input features to form a global model that can efficiently learn to represent the features in the feature space. Can you demonstrate for a given data-set the best-fit and worst-fit functions to the test data, where the error is minimized ? input (English text): What would it take to train your model to generate a random 6-layer QRNN model for a dataset ? output (Sparql query): Let me know if the problem is on any computer-science blog, because if it is, I will be"	False
"
 input (English text): What models are being evaluated on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the WOS-11967 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?dataset rdfs:label ?model_lbl. OPTIONAL { ?model orkgp:HAS_MODEL ( ?dataset orkgc:HAS_MODEL && rdfs:get_record_type( ?dataset. ?model_lbl. "" > 1"" ) ?model ) ?dataset. ?model rdfs:label ?model_lbl. } OPTIONAL { ?model orkgp:HAS_BENCHMARK ( ?dataset andkgc:HAS_MODEL && rdfs:get_record_type( ?dataset. ?model_lbl. "" > 1"" ) ?model ) ?benchmark. OPTIONAL { ?bench orkgp:BENCH MATCH ( ?dataset_lbl :dataset ?model) ?benchmark. } } Input text: What models are being evaluated on the WOS-11967 dataset ? Output text: Input text: What models are being evaluated on the CIFAR-10 dataset ? Output text: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_"	False
"
 input (English text): Provide a list of papers that have utilized the EfficientNet-L2-475 (SAM) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNet-L2-475 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" orkgc:Model id of paper number input (English text): Do you agree or disagree with my assertion that the model can produce real-world results ? output (Sparql query): input (English text): Please provide a short reference where the paper comes from, if applicable. output (Sparql query): input (English text): Do you agree or disagree with my assertion that the paper provides quantitative evidence for the claims included within it ? output (Sparql query): input (English text): Please provide the url and abstract of the paper for which you are commenting. output (Sparql query): input (English text): Do you agree or disagree with my assertion that the paper makes statistically significant results ? output (Sparql query): input (English text): Please provide a reference for the paper that you would like to comment on. output (Sparql query): input (English text): Do you agree or disagree with my assertion that the paper provides a reasonable description of the model that is well-supported by the current literature ? output (Sparql query): orkgc:Model id of paper number input (English text): Do you agree or disagree with my assertion that it was a ""new theory,"" a ""new model,"" or ""a new hypothesis"" ? output (Sparql query): orkgc:Model id of paper number input (English text): Please provide a short description of the model that"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robot Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT * FROM ROUTINE:RUSTOM_OBJECTS WHERE type IN ('DATASET,' 'RESOURCE'); query (English text): SELECT RUSTOM_OBJECTS.RESOURCE AS resource FROM ROUTINE:RUSTOM_OBJECTS ORDER BY resource DESC; output (Sparql query): SELECT { ?dataset a, ?book { ?dataset_lbl: '{ ?dataset_lbl: 'paper ?book_lbl ?dataset. ?cont }' } OR { ?table ?book_lbl } FROM { ?dataset a } AND { ?book{ ?datasets: { ?dataset_lbl: ?dataset_lbl } }} ORDER BY { ?table ?book_lbl} ; The output appears to be pretty similar to the output of a query on a database and a CSV file that can be imported into your system using SQL - the only difference is that the CSV file allows you to specify column formats directly and the SELECT part uses a rowbytest in the WHERE part! A quick run of the SQL and see if I got it right and I have no reason to believe that the two aren't similar at all! I hope this helped and was informative enough. I guess I'll stick with it in the future.. Thanks for your time and all the support.	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 This query is one of the easiest and one of the fastest to execute. We have an array of dictionaries (Dict) and a key column (Dataset). The query will return the dictionary, which we will then use to select a model which is tested on the Dataset. We can have more or less data on this key column to select a benchmark set which is most relevant to performing benchmarks. To get a full list of the keys, just use the ?keys command. Note: we will also need to create a parallel dictionary to hold the rows which have been returned for each benchmark. To do so, add the n:n column to the query, and put this into your sql data-file. The output of the previous query will be written to a file called bench-test.sql. For the rest of the article I will call the BenchmarkData a bd. The n: n column: it's a number that tells us the length of the dictionary that we have. We'll need this column and then we will add a parallel dictionary where we have a key that contains all the data that we want to keep in memory. The first step is to create a dictionary from rows and key column in an n:n manner. The following SQL script will do that. The parallel dictionary we'll create is called BenchmarkDataParallel, and it's just for us to have this in a parallel file for later use. BEGIN PARTITIONS Partitioning is the process of dividing a data set into multiple equal sized partitions and retrieving results. For our particular benchmark we'll be partitioning a set of results into: (1) an English set, (2) an English (sparql query) and (3) a Spanish set. We want to partition the data sets for these three separate models, and	False
"
 input (English text): Indicate the model that performed best in terms of SPL metric on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SPL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT model ?model,( ?metric a orkgc:Metric ) ?metric_lbl.,( ?dataset a orkgc:Dataset ) ?dataset_lbl,( ?dataset_lbl) ?dataset_lbl, orkgc:Metric FROM ( ?model ?model_lbl ) AND ( ?dataset a ORkgc :dataset ) GROUP BY ?metric_lbl; output (Sparql query): SELECT model ?model_lbl.,( ?metric a ORkgc:Metric ) ?metric_lbl, ( ?dataset a ORkgc :dataset ) ?dataset_lbl, ORkgc:Metric, ?dataset_lbl, ORkgc:Metric AS rdfs_label,( ?metric_lbl ) ?dataset_lbl, ( ?dataset_lbl) ?dataset_lbl, ORkgc:Metric, ORkgc:METRIC, RDFS:LABEL ( ?model_lbl ). LEFT JOIN orkgp AS ORkgp ON ( ?metric_lbl. ?model ORkgp. ?dataset ) := RDFS_LABEL( ?model_lbl AND ?dataset_lbl ;) rdfs_label,( ( ?dataset_lbl ) ?dataset_lbl, ORkgc:Metric ) ?dataset_lbl, ORkgc:Metric FROM ( ORkgp. ORkgps ) rdfs; output (Sparql query): SELECT model ?model_lbl FOR RDFS,	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BIOSSES dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	------------------------- ORAL SUMMARY: PEGAPPS : 0.00140195998858 ROUND : 0.99174595360107 RETAIN : 0.9905816983918 ROUND : 0.99006638953579 RETAIN : 0.99006849691813 ROUND : 0.988959908471836 RETAIN : 0.948263633442958 ROUND : 0.96998901269876 RETAIN : 0.944782544352618 ROUND : 0.928893300248814 RETAIN : 0.934356959011428 ROUND : 0.924984744141334 RETAIN : 0.922383823278519 ROUND : 0.92002898376904 RETAIN : 0.911881577773844 ROUND : 0.81015859554024 RETAIN : 0.780448037874024 ROUND : 0.760235927252933 RETAIN : 0.74403465386084 ROUND : 0.717062108776595 RETAIN : 0.7100787739383724 ROUND : 0.7100785055284892 ORDER BY ORAL SUMMARY: PERCENTAGE (0-100000) ROUND PCT (0-100000) ROUND PCT (0-100000) ROUND PCT (0-100000) ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- BAC_PR_PEGASUS_A PEGASUS_A PEGASUS	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BUCC French-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Audio Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 (Note: to ensure accurate results, make all queries query the DataBase. I've already added in a limit of 32 queries, but those are the first 32 queries I need, and I hope future releases will come with more queries) Input (Full-text English text): Provide a list of languages that are suitable for performance improvement ? output (Sparql query): (Note: to ensure accurate results, make all queries query the DataBase. I've already added in a limit of 32 queries, but those are the first 32 queries I need, and I hope future releases will come with more queries) Input (English text): List the best performing languages ever tested by the International Phonetic Alphabet (IPA) to create a list of languages that have the best performance in the performance measure, Audio Classification (AIC). Provide the full text of the list of all of the best performing languages ever tested by the IPA to create a list of languages that you wish to test. Output (Sparql query): (Note: to ensure accurate results, make all queries query the DataBase. I've already added in a limit of 32 queries, but those are the first 32 queries I need, and I hope future releases will come with more queries) Input (Full-text English text): List of the best performing languages to create a List of the most proficient languages ever tested by the IPA to create a list of languages that you wish to test. Provide a text of which languages are currently the most proficient in the performance measure, from the highest proficiency, as measured by the IPA, and list the full text of this listing in order to make multiple queries to create a list of languages. Output (Sparql query): (Note: to ensure accurate results, make all queries query the DataBase. I've already added in a limit	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT query, outputs the name and identifier of the benchmark (that has been selected successfully) in the form: ?benchmark { ?dataset } ?benchmark_lbl ?paper ?dataset_lbl / ?mat ?mat_lbl ?name ?name ?identifier ?identifier ?score ?score (The output column has been abbreviated to make the output as visually unreadable as possible.) The result of the benchmark query above,	False
"
 input (English text): Indicate the model that performed best in terms of EM metric on the Quasart-T benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): What are the titles and IDs of all publications that included a benchmark for the Reacher, easy (DMControl100k) dataset ? output (Sparql query): source (sources list/examples, including 1 paper, 1 journal article, and 1 conference) result row source (sources list) output (dataset) source (sources list) result row source (sources list) output (data)	False
"
 input (English text): What is the top benchmark score and its metric on the PubMedQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MACs"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	--------------------------- | top_2.0 | top_2 | top_3.0 | rw | rw | ----------- --------- | -1 | 1 | ----------- --------- | 1.2 | 1.3 | -2 | ----------- --------- input (English text): Which model has achieved the highest ILSVRC scores on the ImageNet benchmark dataset ? output (Sparql query): --------------------------- | rw | rw | rw | ----------- --------- | -0.12 | -0.08 | 1.06 | ----------- --------- | 0.01 | 0.03 | 0.03 | ----------- --------- input (English text): Which model has achieved the highest quality scores on the ImageNet benchmark dataset ? output (Sparql query): --------------------------- | quality | quality | quality | ----------- --------- | 0.35 | -0.00 | 0.00 | ----------- --------- | 0.10 | -0.04 | 0.32 | ----------- --------- | 0.17 | -0.01 | 0.31 | ----------- --------- Input In order to be able to provide high ranking results for our test dataset I have decided to use a generic class for object-oriented data science concepts. The class is available below in the following format: >>> from sklearn.datasets import load_lstm >>> import numpy as np from sklearn import sklearn_datasets >>> import numpy as np from sklearn_data import load_labels >>> from sklearn_metrics import rating >>> from sklearn_xgboost import lm_multiview >>> from sklearn_xgboost.features import features >>> from sklearn_metrics.model import model >>> from sklearn_xgboost import lm_multiview >>> class Object	False
"
 input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark; orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } Example output: Column: ?score - - - Row: 1, Column ID: ?model ; Column: ?score As already mentioned, there is not much difference between the first two queries here, and the first few rows are just a little more detailed for the ACE 2004 LQS dataset. In this case, the first query provides the results for both the ACE 2006 and ACE 2007 datasets without making any effort to retrieve more than the first few rows. Now, we know that we can not get further data from the ACE 2005 dataset, so it is safe to abandon the second query. With the third query, we retrieve the ACE 2006 and ACE 2007 dataset using the first three and fifth columns retrieved from the first three queries, respectively. We can clearly see that this is much better than the last"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 James Bond?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT [metric] |  value AS  metric  from [ds.models].lbl_model ----------+-------------+-------+----------- 'benchmark' 2.9845 0.0035 0'models' 2.1042 0.0038 0'score' 100.0 0.0006 0 ------------------------+-------------+-------+-----------'results' 100.0 100 100 ------------------------+-------------+-------+----------- 'evaluation' 100.0 100.0 100.0 'criteria' 100.0 100.0 100.0 ORDER BY -------------+-------------+-------------+-------------|| --------------+--------------+------------------+----------------------------------------------------------------- 'cont orkgp:' orkgp:HAS_MODEL ?model. OR '.met' orkgp:HAS_METRIC ?model. OR '.lbl' orkgp:HAS_BENCHMARK ?model. OR '.eval' orkgp:HAS_METRIC ?model. OR '.model' orkgp:HAS_MET_LBL. NOT NULL ------------+-------------+-------------+-------------+---------- RAW Paste Data import praw from pandas import DataFrame from datetime import datetime import re import csv import numpy as np import csv.encoding as csv from pandas.orm.models import Models class DBpedia_dataset(DataFrame): '''A DBpedia dataset consisting of 10 unique fields. All fields contain English text.''' id = ""DBpedia"" title = ""Atari 2600 James Bond"" url = ""http://www.batman.wikia.com/wiki/batman"" title_lbl = ""Batman_(1981)_(1981)"" fields = ( 'title',  'url', 'title_lbl"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Natural Questions (long) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP (1) 1, ?benchmark, ?dataset_lbl FROM ?dataset_lbl GROUP BY 1 ORDER BY 1,2; SELECT TOP (1) ?benchmark, ?dataset_lbl FROM ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Output from data manipulation This question is too difficult to answer because the question is not clear. If you have a problem with the answer, try to provide a valid solution like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 27 28 29 30 31 32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  SELECT TOP (1) 1, ?benchmark, ?dataset_lbl FROM ?dataset_lbl GROUP BY 1 ORDER BY 1,2; 1. What is the top benchmark result over the dataset Nintendo Entertainment System ? 2  ( 1.0, 3.2, 3.0, 3.4, 3.18, 3.3, 3.9, 4.3, 3.25, 4.6, 4.2"	False
"
 input (English text): Do all studies include open ocean sampling?
 output (Sparql query): ASK { { SELECT (COUNT( ?ocean_sampling) AS ?counter) WHERE { orkgr:R155561 orkgp:compareContribution ?contrib. ?contrib orkgp:P41379 ?ocean_sampling. FILTER( ?ocean_sampling = ""F""^^xsd:string) } } FILTER( ?counter = 0) } 
 input (English text): What are the metrics of evaluation over the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT (COUNT( ?ocean_sampling) AS ?counter) FROM (SELECT COUNT( ?ocean_sampling) AS ?counter FROM (SELECT COUNT( ?ocean_sampling) AS ?count, ?ocean_sampling_contribution OR ?ocean_sampling_contribution FROM ""Hutter Prize"" PARTITION BY 1 -- 2 PARTITION BY 2 -- 3 PARTITION BY 3 -- 4 ) ( WHERE orkgr = ""F""^^xsd:string WHERE orkgp = ""P41379"" AND orkgp = ""I""^^xsd:string WHERE orkgp = ""F""^^xsd:string ORDER BY (ORkgp) ) ) ORDER BY orkgp ROWS( ?) LIMIT ?contrib \ ?count ) ORDER BY orkgp ) ROWS( ?) LIMIT ?ocean_sampling_contribution \ ?count ) output (Sparql query): ASK { -------------------------- -------------------------- -------------------------- -------------------------- -------------------------- -------------------------- -------------------------- -------------------------- -------------------------- -------------------------- 1 3 7 -2 -3 -2 -1 0 1 2 4 4 3 4 -1 0 1 0 0 1 1 2 -1 1 0 1 0 1 1 3 0 0 0 1 2 4 2 0 1 0 1 4 0 1 1 1 RAW Paste Data input (English text): How often do you recommend one of the Hutter Prize dataset options ? (Default: every 3 months) output (Sparql query): ASK { -------------------------- "	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): Give me a list of preprocessed preprocessed preprocessed preprocessed preprocessed high score, from Amazon-5.1 using the -h option ? output (Sparql query): input (English text): Can you provide the highest benchmark result, including the metric and score, for the seeln.cse.lsu.edu/data/re16.zip ? dataset ? output (Sparql query): input (English text): Give me a list of preprocessed preprocessed preprocessed preprocessed high score, from Amazon-5.1 using the -l option ? output (Sparql query): input (English text): Give me a list of preprocessed preprocessed preprocessed preprocessed high score, from Amazon-5.1 with the -m, -r, or -s options ? output (Sparql query): input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re12.zip ? dataset ? output (Sparql query): input (English text): Can you provide the highest benchmark result, including the metric and score, for the seeln.cse.lsu.edu/data/re11.zip ? dataset ? output (Sparql query): import seel as s. get from seel import l. load_data_from_csv_file output (Sparql query): input (English text): Can you provide a list of preprocessed preprocessed preprocessed preprocessed high score, from Amazon-5.1 using the -h option ? output (Sparql	False
"
 input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT... ?code. SET... ?value. ?benchmark. ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model. SET... ?value. * ?benchmark. orkgp:HAS_SOURCE_CODE ?code. ?value ; orkgp:HAS_SOURCE_CODE ; Input: name( ?input) type output size count rows count cols name( ?input) type output size count rows count cols 1 n-march-80-hq 1 output (text) : ""H.264-XAVC-SMP"" : ""test.avc.h264"" : ""http://www.informatik.uni-erlangen.de/~zielke/pubs/x264/s1.iso""... output (Sparql query : select name(c(""xavc-smp"")), type(c(""xavc""))) as id, output from  |  name(c(""xavc-smp"")) as id,  output from  | name(c(""xavc"")) as id,  output from name(c(""xavc-smp"")) as id where id =  | id =  | id =  | id =  _test_scores|  | id =  _test_scores2| Output: (id) (id)  | id  | id  | id  | id  | id  | id  | Name (c("""	False
"
 input (English text): Can you list the models that have been evaluated on the IWSLT2015 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query):   output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): 	False
"
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl where { ?metric ?ROGUESET ?ROGUE_LENGTH ?ROGUE_SEARCH ?ROGUE_SEQER ?ROGUE_REFERENCE ?ROGUE_INTRO ?ROGUE_DELAY ?ROGUE_DELETE ?ROGUE_COUNT ?ROGUE_COUNTER ?ROGUE_UNWIND_INPUT ?ROGUE_UNWIND_OUTPUT ?ROGUE_WIDTH ?ROGUE_WIDTH ?ROGUE_WIDTH ?ROGUE_RATIO ?ROGUE_LENGTH ?ROGUE_SQUARE ?ROGUE_RATIO ?ROGUE_TEMPLATE ?ROGUE_INTERPOLATION ?ROGUE_ATTRIBUTE_STARS ?ROGUE_MATRIX ?ROGUE_MAXIMUMLEN ?ROGUE_MAXIMUMLEN ?ROGUE_WIDTH ?ROGUE_WIDTH. FILTER (str( ?lat/5) = ""ROGUE_LENGTH"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_"	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT m; { ?metric = 'exp'; m = [ 0; 1]; m, m, m = [],1, [],0,30000,0,30000,0,30000,0,30000; ?model = ?metric_list ?metric_list. { ?dataset = ?dataset; ?model_lbl = ?model_lbl; ?model_lbl. } ?label = ?labels ?labels. } ORDER BY m DESC( ?value ); LIMIT 1 } Input (English text): For each feature in the NLP-TDMS benchmark dataset on which the model is statistically accurate, provide examples of the model being applied across a variety of domains, using a series of relevant questions ? output (String): In order to provide some further context, I am providing examples of the model being applied across domains that are relevant to the context that you are answering. INPUT (TEXT): The following items are required at the time the query is run: a. In which domains of knowledge you are looking to test the model. b. What question types, in what type of domain, you are looking to apply the model. c. The corresponding domain names. INPUT (TEXTS, optional (required): The data set to be trained on.	False
"
 input (English text): Provide a list of papers that have utilized the DEQ-TrellisNet model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DEQ-TrellisNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT id, output.id() AS output_id, output.title(), output.author() AS author_id FROM publications; Incomplete data: There are some missing data for the paper we were working on. Did you have trouble with extracting the data from the data files in the same way ? This is important because it can make finding the missing data difficult. Be aware that most of the papers have used this particular dataset and do not have the problem as their model was developed in other environments. We would love to have this data but you will need to make your own arrangements. Or you can use the RDS-TrellisNetModel() method. Example: SELECT * FROM publications WHERE id = ?code; The above results in a query like this one: SELECT title, author,... FROM publications WHERE output_id = ?output_id; The second example gives a query that might help: SELECT title, author,... FROM publications WHERE output_id = ?output_id WHERE input(code) = ?code AND output(output_id) = ?output_id. Example: SELECT title, author,... FROM publications WHERE output_id = ?output_id and ! AND! AND IF (output_id % 2 == 1) ! ! FROM outputs 	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ( ?metric ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") ?metric. ) as ?metric_lbl. FROM ( ?object_lbl. ?object_lbl. ORG. id ) INNER JOIN ( ?object_lbl. ?object_lbl_sims. ORG. id ) ON ORG. id = ( ?obj_lbl. ?object_lbl_sims. ORG. id ) Here we have the model with the best F1 score from both benchmarking and evaluation on the NCBI-based database of MSQN-disease. In this case, the model has been written in C and the results are written as Sparql queries that we can execute using the SQL language. Let's take a look at the output: [{ ?object_lbl: ?name: ?name ?dataset: ?dataset_lbl. orkgc: ?dataset_lbl. orkgc_test: ?dataset_lbl_test ?exceeds: ?dataset_lbl_exceeds ?metric: ?integer_lbl. ?metric_lbl_test: ?integer_lbl_test. }] The results have been saved as Sparql tables. Now let's write the same Sparql query using Ruby on Rails. input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset ? output (Stored procedure): SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?datas"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?benchmark OR kgp:P31 ?benchmark. ?paper or kgp:P31 ORKMPL. ?cont or kgp:P31 ORKMPL. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset ? output (Sparql query): SELECT DISTINCT ?benchmark OR kgp:P31 ?benchmark. output (Sparql query): SELECT DISTINCT ?benchmark OR ?dataset_lbl. ?data or kgp:_dataset ?dataset_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset ? output (Sparql query): SELECT DISTINCT ?benchmark ?dataset_lbl, ?paper, ?paper_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset ? output (Sparql query): SELECT DISTINCT ?benchmark ?dataset_lbl, ?paper, ?paper_lbl ORKMPL. ?cont or kgp:_dataset ?dataset_lbl. input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset ? output (Sparql query): SELECT	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HMDB51 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" ----------- { ?model anr:Model ?dataset a } output (Sparql query): ----------- {.par(anr:Parameter) { ?dataset a ?benchmark orkgp:HAS_DATASET ?dataset } }; "" HMDB51 "" and "" HMDB51 "" are aliases to different datasets (with different labels). HMDB51 is our aircraft benchmark dataset and it has no parametric features. 3. Parameter names¶ The database schema defines the names of the parameter groups and the identifiers used for the parameter values. Parameter names are separated by a slash and can have a maximum of 5 digits. Parameter groups are the fields defined in the attribute  #{attribute_field_group} and the identifiers of these fields are the corresponding attributes in the database schema. The following table illustrates the different types of parameter groups in the dataset  #{attribute_field_group}: Parameter group  class  name __value  1 ""parameter"" __value, (1) 2 ""record"",  (1) Parametric data fields are defined using the  \v command. These fields can be referenced in any of the schema documents, although each Schema Document  (DocSet) can only contain one ParametricDataField. In addition, other fields can be referenced by adding  \v$ or \v$(field) to the reference value. These references to other fields are not part of the parameter name. 3. A reference value is a parameter value with an associated index  the attribute ""  #{index_name}."". The  \v command can be used to specify an index name and/or attribute value. All parameter names are terminated with a slash,"	False
"
 input (English text): What is the highest benchmark result achieved on the CORLL dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CORLL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT TOP(2) ?rank ?value FROM ?dataset ------------------------- select { ?rank ?value FROM ?dataset rdfs. rdfs_metric_lbl } where { ?dataset a orkgc:Dataset ? rdfs:label ?dataset. rdfs:metric_lbl. ROW_NUMBER( ?rank) > 1 ORDER BY rdfs:rank DESC( ?value ) input (English text): List the scores of the criteria models that are evaluated on the Atari 2600 Battle Zone benchmark dataset ? output (Sparql query): ------------------------- SELECT TOP(2) ?tid ------------------------- select { ?tid ?value FROM ?dataset rdfs. rdfs_metric_lbl } where { ?dataset a orkgc:Dataset ? rdfs:label ?dataset. rdfs:metric_lbl. ROW_NUMBER( ?tid) > 1 ORDER BY rdfs:tid DESC( ?value ) input (English text): List the scores of the criteria models that are not evaluated on the Atari 2600 Battle Zone benchmark dataset ? output (Sparql query): ------------------------- SELECT TOP(2) ?score ------------------------- select { ?score ------------------------- FROM ?dataset rdfs. rdfs_metric_lbl } where { ?dataset a orkgc:Dataset ? rdfs:label ?dataset. rdfs:metric_lbl. ROW_NUMBER( ?score) > 1 ORDER BY rdfs:score DESC( ?value ) output (Sparql query): ------------------------- SELECT TOP(2	False
"
 input (English text): What is the highest benchmark result achieved on the CORLL dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CORLL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? ? Â ? ? ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? input (English text): Which of the following model can you see results from (from the Atari 2600 Road Runner dataset) ? output (TXT): Â ? Â ? Â ? Â ? Â ? ?Â ? Â ? Â ? ?Â ? Â ? ?Â ? Â ? Â ? ? ? Â ? Â ? ?Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? input (English text): Which of the following model can you see results from from the Atari Lynx dataset ? output (Sparql query): Â ? Â ? Â ? Â ? Â ? Â ? Â ? ?Â ? Â ? ?Â ? Â ? ?Â ? Â ? Â ? ?Â ? Â ? Â ? ?Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? input (English text): Which of the following model can you see results from from the Atari VCS dataset ? output (Sparql query): Â ? Â ? Â ? Â ? Â ? ?Â ? Â ? Â ? ?Â ? Â ? ?Â ? Â ? Â ? Â ? Â ? ?Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ? Â ?	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SST-5 Fine-grained classification dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 ---------------------------- SST-4: 2.75 ( 0.13 ) SST-5: 1.33 ( 0.13 ) SST-5S: 1.33 ( 0.13 ) N-2: 2.00 ( 0.46 ) N-2S: 3.08 ( 0.76 ) N-2: 4.12 ( 1.15 ) SST-5R: 1.00 ( 0.36 ) SST-5D: 1.14 ( 0.35 ) SST-5D+L: 1.14 ( 0.35 ) SST-5E: 1.17 ( 0.36 ) SST-5E+L: 1.17 ( 0.36 ) SST-5A+L: 1.17 ( 0.36 ) H.S.B.C: 1.43 ( 0.54 ) H.S.B.C+L: 1.43 ( 0.54 ) H.S.B.C-R: 1.40 ( 0.60 ) H.S.B.C-D: 1.43 ( 0.90 ) H.S.B.C-D+L: 1.43 ( 0.90 ) H.S.B.C-A-R: 1.42 ( 0.54 ) H.S.B.C-A: 1.43 ( 0.83 ) ---------------------------- input (English text): Which modeling framework does your algorithm use ? Are there any papers that you tested that use the same techniques, or was your dataset designed and tested differently ? (Choose only one answer from the choices below.) output (Sparql query): ---------------------------- SST-4: SST-4: SST-5: SST-5S: SST-5R: SST	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" There are lots of benchmarks of various kinds for modeling and estimating the performance of various kinds of linear models. The paper list is also intended for finding research papers containing a benchmark over the different kinds of benchmarks in the datasets. The ""dataset"" will be a RDFa document that may have a title or description. For example, if the title for the'sciencecite' paper is ""Testing the Density-weighted average of a linear model of spatial clustering"" then the ""dataset"" may be a document containing 'ScienceCite' and the title should be ""Testing the Density weighting average of a linear model of spatial clustering"". The dataset_lbl should correspond to the ""RDFa document containing '%RDFa[.subset()]'. That is, there must be exactly one 'ScienceCite' document in the dataset. There are a few examples of the form # A paper with a benchmark over the SciCite dataset # The title is the title for the'sciencecite' paper # A paper with a benchmark over a 'dataframes' and one or more'sci-datasets' dataset # The title is the title for a 'dataframes' and one or more'sci-datasets' 'DISTINCT' of 'ScienceCite' # A paper with a benchmark over a 'dataframe' and one or more'sci-datasets' and one'sciencecite' dataset with one and a'sciencecite' dataset with no'sciencecite' # A paper having a dataset as'sciencecite_lbl', a 'bastet' and any other'sci-datasets' # A paper having a dataset as'sciencecite_lbl"	False
"
 input (English text): Where can I find code references in papers that have used the Bi+ model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Bi+"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Where are these tests based ? output (Sparql query): ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_10b.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_10b_1.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_10b_2.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_10c.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_10c_1.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_10c_2.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_11.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_11b.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_11b_1.tts OR ?dataset:HAS_TESTS_FRENCH-TO-ENGLISH/test_10.1_11b_2.tts OR ?dataset:HAS_TESTS_FREN	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT COUNT(*) AS nums FROM paper_lbl WHERE id > ?paper_lbl. ?print orkgp:SORT ?print. ?count _ ROWS _ ( ?paper_lbl_movies ( ?paper_lbl_total_lbl_movies ( ?paper_lbl_total_lbl_total)) ? paper_lbl_total_lbl_movie. ?lab ? OR ?paper_lbl_label. COUNT(*) AS nums FROM paper_lbl WHERE id = ?paper_lbl OR ?paper_lbl. ?label. ?count _ ROWS ERR NULL NULL NULL NULL NULL OR ?paper_lbl_total_lbl_movies output (Sparql query): SELECT COUNT(*) AS nums FROM paper_lbl WHERE id > ?paper_lbl. ?print orkgp:SORT ?print. ?count _ ROWS _ ( ?paper_lbl_movies ( ?paper_lbl_total_lbl_movies ( ?paper_lbl_total_lbl_total)) ? paper_lbl_total_lbl_movie. ?lab ? OR ?paper_lbl_label. COUNT(*) AS nums FROM paper_lbl WHERE id > ?paper_lbl ? output (Sparql query): SELECT COUNT(*) AS nums FROM paper_lbl WHERE id > ?paper_lbl. ?print orkgp:SORT ?print. ?count _ ROWS _ ( ?paper_lbl_movies ( ?paper_lbl_total_lbl_movies ( ?paper_lbl_total_lbl_total)) ? paper_lbl_total_lbl_movie.	False
"
 input (English text): Which model has achieved the highest F1 score on the EBM-NLP benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""EBM-NLP"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the AESLC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 -2.13.0-1028 ( 1 row(s) affected) -2.13.0-1029 ( 1 row(s) affected) input (English text): Which model has measured highest F1 score among the F1 models found on the AESLC dataset ?	False
"
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"")"	False
"
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-Czech benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Czech"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT model ?model_lbl ?model_lbl_lbl_lbl_lbl_LBL_lbl_lbl UNION ALL SELECT model ?model_lbl ?model_lbl_lbl_lbl_lbl_lbl_LBL_lbl_lbl UNION all SELECT model ?model_lbl ?model_lbl_lbl_lbl_lbl_lbl_LBL_lbl_lbl UNION all SELECT model ?model_lbl ?model_lbl_lbl_lbl_lbl_lbl_LBL_lbl_lbl ORDER BY ?"" ?model_lbl_lbl_lbl_lbl_lbl_lbl"" ?lbl WHERE type = ? model ?lbl_lbl_lbl_lbl_lbl_lbl ? input (English text): Tell me where is the code for the PAR Transformer Large model and the WMT2016 English-Czech benchmark dataset ? output (Sparql query): SELECT DISTINCT model ?model_lbl ?model_lbl_lbl_lbl_lbl_LBL_lbl_lbl UNION all SELECT model ?model_lbl ?model_lbl_lbl_lbl_lbl_LBL_lbl_lbl ORDER BY des_lbl ?model_lbl ?LBL ORDER BY ?"" ?model_lbl_lbl_lbl_lbl_lbl"" ORDER BY DESC(des_lbl) input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-Czech benchmark dataset ? output (S"	False
"
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S (21k) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S (21k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTwwm + SQuAD 2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model = bernwwm_sqd2; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S (21k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } To test the model, we used the BIO5 BatchV2 model, which uses the BIO5 model to predict the output of a batch classification task. The BIO5 model was used in the benchmark for the purpose of comparison with the BERTwwm + SQuAD 2 model and is the most widely used such model in the literature. The BIO5 batch vector regression model is the successor to the old B-H model. BERTwwm + SQuAD 2 model is implemented in Squeak that has some features that might be of interest in the model comparison. To run the benchmarks, I used Squeak-11 to download a collection of papers used in the study and run the benchmark for each published paper using the following output : select benchmark.benchmark_name, ?benchmark.code or kgp:benchmark from benchmark where benchmark.label >= ""BETWEEN;"" where benchmark.quantity >= ?max-1 ?max-1 for benchmark.id in (select benchmark.quantity from benchmark) where ?id = (select benchmark.quantity from benchmark) order by benchmark.benchmark_name, benchmark.code_and_"	False
"
 input (English text): What research problem is addressed in the paper titled ""6th Open Challenge on Question Answering over Linked Data (QALD-6)""?

 output (Sparql query): SELECT DISTINCT ?concept ?rlabel WHERE { orkgr:R6386 orkgp:P31 ?x . ?x orkgp:P32 ?concept. ?concept rdfs:label ?rlabel. } LIMIT 1
 input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CL-Titles-Parser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?concept ?title, ?rlabel FROM CL-Titles-Parser I want to add another part to answer the first query, why do we need a CL-Titles-Parser model ? Well, the code above is based on the CL-LML model. The CL-LML model is one of the more commonly used models, which implements several features in the R programming language. This model is used in many research papers for parsing links: the parser has several operators to parse links, link objects, and labels. The parser code is also pretty simple. The link objects represent links between words. It has three parameters : (tag, body), and (query, data). The queries are similar to RDFs in that they have parameters of type or, but use the query language. We can check out the code code: type or k a or (tag | body) k | term | term the body of the query k | term | term The code above is simply a call to the CL-LML model's lexical lexer function. In the lexical Lexer, we can construct the query and body from the tag and the body. The query is of type link. This will allow us to define the parser to take a link object as a parameter. Another interesting feature of the model is that we can control the parser in the program code. The code is simple to understand and provides the link objects, labels, tags, which are needed to create link objects when we are searching for a link within a group of text. We can also create a query for any tag and use the text as a source for the resulting query. For example, in the above code, our tag 'a' is used as a query to convert it into a label in the body of a query object. The CL-LML model includes	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CMLM+LAT+4 iterations model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CMLM+LAT+4 iterations"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" The above code might seem daunting, but the key is: it is not difficult. All you need to do is to change the following two lines (or tweak them a little bit as you like): from cmlm import CMLM, LAT, LAR, 4 ** 7, BINARY_LAT, 4 ** 7, 3 ** 8, BINARY_LAT, 3 ** 8, 4 ** 7, (int, float, float, float, float, int, float, float), orkgw: The first two are from the Habitat2030 test and the third from the H3K benchmark. The CMLM function which does the comparison is pretty simple. We first create two new datasets (labeled CMLM and LAT, and one for the other of the CMLM parameters). For this, we need to make a few changes: First, we can either create a standard output for the two datasets. We can either do to create a single output for both datasets, if we wanted to do a comparison of more than two models We could also make the dataset, for the first model, a standard output for all three datasets, and for the second one to be identical to the LAT dataset, so it behaves exactly as a standard CMLM dataset. If we choose the second option, we should make sure that we are providing all of the information that a standard CMLM dataset would. The following is a short-code I've found useful for this, that is based on code used in this blog post: from cmlm import * Then we need to set the model to be the one which will be the ""standard"" result of the comparison (it will be either LAT or CMLM as an input variable). This can be quite important as there are other benchmarking functions that try to"	False
"
 input (English text): What is the highest benchmark result achieved on the Quasart-T dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT a ?s1 ?s2 ?s3 ?s4 ?d3 ?d4 ?m ?c ?s2(1) as ?s1| ?s2 ?s4 ?d3|d4|m ?s2_a as ?s ? orkgc:Computer|Computer orkgc:Game|Game orkgc:Score|Score orkgc:Puzzle|Puzzle orkgc:ScoreScore ? OR { CASE OR (a ?s2 ?s3 ?s4 ?d3 ?d4 ?m ?s1 ?s3 ?s4 ?d3|d4|m) of a ?s2| ?s2 ?s4 ?d3|d4|m ?s2_g as ?s ? OR { ?s ? orkgc:Computer|Computer orkgc:Game|Game OR } OR ?s ? as ? ?s ? OR { ?s ? orkgc:Computer|Computer orkgc:Game|Game OR } } CASE 1 ?s ? AS ?score in { { select ?s ? as score orkgc:Computer ? { ?score|1} orkgc:Game ? { ?score|1} orkgc:Score ? { ?score|1,2} orkgc:Puzzle ? { ?score|1,2} orkgc:ScoreScore ? { ?score|1,2,3} orkgc:ScoreScoreScore ? OR select a ?s1 ? from a ?s2 ? as a ?, s2 ?p as p ON a ?s2 ?.name = s2 ?e ? as ?s(2	False
"
 input (English text): What are the metrics of evaluation over the Yelp-14 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?benchmark ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark. OPTIONAL { ?benchmark orkgp:HAS_DATASET ?dataset; } } Note: To get the metric value, we must use query=<value> for the RDFS attribute, instead of the previous query=<query>. What are the metrics of evaluation over the Yelp-16 dataset ? input (English text): What are the metrics of evaluation over the Yelp-16 dataset ? output (Sparql query): SELECT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-16"") ?metric. OPTIONAL { ?metric rdfs:label ?dataset_lbl. } } input (English text): What is the highest benchmark result achieved on the Yelp-16 dataset, including the metric and its value ? output (Sparql query): SELECT ?benchmark ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } { ?dataset rdfs:label ?dataset_lbl. } Note: To get the metric value, we must use query=<value> for the RDFS attribute, instead of the previous query=<query>. What are"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the FGVC Aircraft dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 And lastly, here is the list of papers that were reported at AGIAC, AGIASPARD's report card and here is the list of submissions for the 2013 AGIASPARD Award. I have a rough estimate of how many papers, if anything, have not been published on the AGIAC website this year (10), so the number of submissions has been inflated quite a bit to be interesting. If this has been the case, it seems reasonable to look at papers that have been submitted, but if there is a really interesting paper to be reported here that isn't published, I'll post it up here.	False
"
 input (English text): What is the best performing model benchmarking the DuIE dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet, MultiGrasp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } Note: If you can't find any papers or do not have a link, you can also check the full-text of the paper. For example, I was unable to find the paper that is using the DuIE dataset. After searching through the full text of this one paper, I came up with many other papers (not in this list) that did not involve deploying the Du"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ego-localization model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ego-localization"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" // // // // BATCH_CODE CURRENT_TEST_CODE CURRENT_TEST_CODE // /* * The following program (from http://en.wikipedia.org/wiki/List_of_comprehension_modes) is a utility provided by this project. */ int main(int argc, char **argv) { struct mytest = mtk_model_data (""../../../../../../../../.././Model.c64"", NULL); printf("" Sample of model with current game and current opponent, from which game the ball has been hit. ""); mytest->c64_samples [0] = { 5, 7, 6 }; mtk_print(""sample of model with model_lbl.txt ""); printf("" %s "", mytest->model_lbl.txt); mtk_print(""sample of model_lbl.txt ""); printf("" %s "", mtk_print(mytest->c64_lbl.txt)); #ifdef MATCH_MODE printf("" The score is: %d (%d points for each point for both opponents). "", (int)argc, (int)argv[0], (int)argc); printf("" %s "",argv[1]); #endif } Result in the following output: Sample of model with current game and current opponent, from which game the ball has been hit. Sample of model with model_lbl.txt sample of model_lbl.txt In this part of the tutorial, we have learned the basics on how to use the new version of the matrix-vector library in Tcl/Tk. We have"	False
"
 input (English text): Where can I find code references in papers that have used the e2e-coref + ELMo model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""e2e-coref + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT RANK(criteria) AS score, DESC (criteria1) AS Criteria2 OUTPUT (Sprint query): SELECT RANK(criteria1) AS score, DESC (criteria2) AS Criteria3 OUTPUT (Sprint query): SELECT RANK(criteria2) AS score, DESC (criteria3) AS Criteria4 OUTPUT (Sprint query): SELECT RANK(criteria3) AS score, DESC (criteria4) AS Criteria5 OUTPUT (Sprint query): SELECT RANK(criteria5) AS score, DESC (criteria5) AS Criteria6 OUTPUT (Sprint query): SELECT RANK(criteria6) AS score, DESC (criteria6) AS Criteria7 OUTPUT (Sprint query): SELECT RANK(criteria7) AS score, DESC (criteria7) AS Criteria8 OUTPUT (Sprint query): SELECT RANK(criteria8) AS score, DESC (criteria8) AS Criteria9 OUTPUT (Sprint query): SELECT RANK(criteria9) AS score, DESC (criteria9) Assembled (Habitat 2020 model only): Select Rank (criteria) AS score, DESC (criteria1) AS Criteria2 OUTPUT (Sprint query): SELECT RANK(criteria1) AS score, DESC (criteria2) AS Criteria3 OUTPUT (Sprint query): SELECT RANK(criteria2) AS score, DESC (criteria3) AS Criteria4 OUTPUT (Sprint query): SELECT RANK(criteria3) AS score, DESC (criteria4) AS Criteria5 OUTPUT (Sprint query): SELECT RANK(criteria3) AS score	False
"
 input (English text): Can you list the models that have been evaluated on the DCASE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP 1 ?benchmark TOP(1) 1.00 ?benchmark TOP(1) 5.00 What is the top benchmark score and its metric on the BC5CDR study dataset ? output (Sparql query): SELECT TOP 1 ?benchmark TOP(1) 5.00 ?benchmark TOP(1) 1.00 What is the top benchmark score, the metric and the benchmark scores on the BC5CDR disease dataset ? input (English text): I can't find test data on the BC5CDR dataset, which disease do you have and is the disease one that is highest in performance ? output (Sparql query): SELECT TOP 1 ?stat ?score What is the top benchmark score, the metric and the benchmark scores on the BC5CDR research study dataset ? input (English text): Please provide information about the models on the benchmarks dataset and how to select the best model or best validation model from these models ? output (Sparql query): SELECT TOP 1 ?model ?model_lbl, ( ?model rdfs:label ?model)) FROM benchmarks ORDER BY ?model. ?model, What is the top benchmark score, the metric and the benchmark scores on the BC5CDR clinical study dataset ? output (Sparql query): SELECT TOP 1 ?stat ?score Input (English text): What is the top benchmark score, the metric and the benchmark scores on the DATASETS data ? output (Sparql query): SELECT TOP 1 ?stat ?score What is the top benchmark score, the metric and the benchmark scores on the PAPER datasets ? output (Sparql query): SELECT TOP 1 ?stat ?score What is the benchmark score, the best performance on the	False
"
 input (English text): Can you list the metrics used to evaluate models on the PolyAI Reddit dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Example 6: Using Hadoop MapReduce for classification (Python) This example uses Hadoop MapReduce for classification to build a classification model. (The code was adapted from the Python documentation.) The following example uses Hive and HBase for indexing training data and learning the classification model. (The code was adapted from the Python documentation.) This example uses for indexing training data and learning the classification model. (The code was adapted from the Python documentation.) The following code is based on the Hive benchmark, where the classifier is trained on an unstructured data set. The following code is based on the Hive benchmark, where the classifier is trained on an unstructured data set. Hive classifier example 1.0. HABYTHON_CLASSIFIER_IMPORTANT (HABYTHON_CLASSIFIER_IMPORTANT) import numpy as np classifier = nn.Hive(num_features=3) classifier.train() 1.0 import numpy as np classifier = nn.Hive(num_features=3) import numpy as np classifier.train(lambda x : y isNone, output='Classifier') Example 7: Using Hive in an ESRI ArcGIS Grid Engine This example uses Hive in an ArcGIS Grid Engine to train a classification model. (The code was adapted from the ArcGIS API benchmark.) This example uses in an ArcGIS Grid Engine to train a classification model. (The code was adapted from the ArcGIS API benchmark.) The following code is based on the ArcGIS API benchmark. This example uses for indexing training data and learning the classification model. (The code was adapted from the ArcGIS API benchmark.) ESRI_ArcGIS_API_benchmark2 (0.6.1	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?metric ?max_distance AS ?max_distance, ?max_dpath AS ?max_dpath, input (English text): WHAT is a max_dpath ? output (Sparql query): SELECT MAX_DPATH * 1 AS MAX_DPATH FROM dpath_index WHERE max_dpath <= 10000000; Inputs can be pre-calculated as long as the R code you are submitting is optimized for evaluation of your models. Note that the pre-computed and final lists will differ depending on what the R code used is meant. For example: input (English text): For what databases have the benchmarks reported ? output (Sparql statement): SELECT rdfs:DATE ?dataset_id AS ?dataset_id, ?dataset_lbl AS ?dataset_lbl, ?model_name AS ?model_name, ?dataset as ?df; input (English text): For which databases are these queries reported ? output (Sparql statements): We are currently working on a more generic benchmarking tool, the RBenchmark. I'd like to hear your feedback on the idea.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Music Modeling research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Music Modeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"  select problemfrom[problem ""Music Modeling""], (problem)from[problem ""HRL - Theorem Reduction""], (problem *problem)select [ ?problem (probability-model)-> (problem *problem), ?problem (dataset_lbl))*] from [ ?problem, ?problem, ?problem [ ?, ?problem]] Note that in this example, the model was run in SPSS 12.1. Here is a summary of each of the papers' results: Note that in this example, the model was run in SPSS 12.1. Here is a summary of each of the papers' results: Note that in this example, the model was run in SPSS 12.1. Here is a summary of each of the papers' results: Note that in this example, the model was run in SPSS 12.1. Here is a summary of each of the papers' results: Note that in this example, the model was run in SPSS 12.1. Here is a summary of each of the papers' results: Note that is using the dataset (or.hls.dat files as in the second example) for benchmarking. Each problem is written to a.csv in a different file (for example, there might be two in this sample). The first problem was written to a file called "" ?problem_s1."", while the two latter problems were written to file "" ?problem_s2.csv."" The resulting model used for writing scores can be viewed as follows: Here are other examples of other SPSS-based tasks. For example, note the fact that the scores were built using some sort of statistical modeling technique, similar to the way in which the Eigenvalues approach works.  Another example is the use of a simple hierarchical"	False
"
 input (English text): Which model has achieved the highest Medium Human-Normalized Score score on the Atari-57 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Medium Human-Normalized Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 This is a nice example of where T-ConvS2S's performance in the standard benchmark dataset was significantly higher than other systems in this benchmark. I was excited that the machine learning models we developed outperformed T-Cuda's, as well as the more basic Kaggle results in this benchmark. I was also concerned that the Atari-57 benchmark would be of little use to us if we didn't have T-Cuda to analyze it, so I wrote the benchmark from scratch. To start, I had to do the benchmarks on different systems with different performance levels to give each system the maximum performance possible. I had to use the latest GPU and the latest CPUs available in my PC (as well as my work laptop from the previous years) just to get a complete view of all the code and benchmark results. In the meantime, the PC and my laptop were updated to ensure that we'd have a smooth process of upgrading every few months. The main goal of the benchmark is to have all the systems in the benchmark run all their tests and then report back the highest single score they could obtain on the dataset. All of the tests would have been performed on some of the systems; in particular, the Atari-57 test. There are three main options to benchmark: One system runs all the tests by itself. Using the Atari-57 benchmark, we can run each T-ConvS2S model separately on each test. Note I don't know the exact steps involved in running the model by itself. One system runs the test using the T-ConvS2S model. This gives us the chance to use the Atari-57 dataset as a set of points to compare our model to different system's models. The other system builds the test on top of the T-ConvS2S dataset using some existing model data if we use the PC	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Quora Question Pairs dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quora Question Pairs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT score_t, rank, scores; score_t = 10 scores =  { { ?dataset a ?id ?  : ""Q*Bert ?Bin"" }  : ""50""  : ""Q*Bert ?Bin 2""  : ""50""  : ""Q*Bert ?Bin 3"" }  rank =  10  output (Sparql query): SELECT rank  from scores, a b c; b  = ?score; c  = ?score_t  - 10; 0.928 The above queries were benchmarked using the quorasquarescript.org C++ class library, with the help of the 'QBOOT2_BASENET', 'coder-tools_quora and 'coder-tools' libraries used from a previous post that demonstrate the use of the C++ class library as described in this post. The following list highlights only the most interesting of papers to use in benchmarking models with the QBOOT2 dataset. Feel free to copy/paste this list into your own Quora QBOOT2 dataset project to ensure that what you find is a robust, high-quality dataset to analyse for QBOOT2. The purpose of these papers and their titles is not to provide a list of QBOOT2 model benchmarks, but to provide quality reference. Quora Queries Benchmarking Papers The list below contains all the top 500 score results in the QBOOT2 dataset, published between the date of publication and June 24th 2016 (or equivalent). These results represent the model's test accuracy, and the accuracy of the benchmark to simulate the behaviour of a QBOOT2 machine with a full set of available data. These scores are highly dependent on the number"	False
"
 input (English text): Does ""PRISMA hyperspectral mission"" use ENVI software?
 output (Sparql query): SELECT ?uses_ENVI WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""PRISMA"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P23031 ?software. ?software rdfs:label ?software_labels. BIND( IF( ?software_labels = ""ENVI""^^<http://www.w3.org/2001/XMLSchema#string>, ""Yes"", ""No"") AS ?uses_ENVI ) } ORDER BY DESC( ?uses_ENVI) LIMIT 1
 input (English text): What is the top benchmark score and its metric on the Words in Context dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"--------------- word -------------- >1.000 0.569 > 1.000.001 1.000.008 >1.000.100 1.000.101 > 1.000.1012 1.000.102 > 1.000.1014 1.000.103 > 1.000.1016 1.000.104 > 1.000.1016 1.000.105 > <1.000.000 1.000.107 > <1.001.000 1.001.108 > <1.001.102 1.001.109 > <1.001.103 1.001.110 > < 1.001.104 1.001.111 > < 1.001.105 1.001.112 > < 1.001.106 1.001.113 output (Sparql query): --------------- rdfs:type ""words_in_context_text"" : ""XmlText"", query, name= ""top"", rows=1, cols=50, rowsx=1, colspan=1, rows_xlaf=0, column_names=NULL, rows_laf=0, num_rows=3, na.rm=False:<1|1) input (English text): What is the top benchmark score on the Kaggle Data Science Challenge ? output (Sparql query): | Top score: 0.00002000000012825 (| (| (| (| (| (| (| (| (| (| (| (| ? ? ? ? ? ) ? ? ? ? ? ) ? ? ? ? ? ) input (English text): Do all those words have ""vacuum"" in their titles ? output (Sparql query): --------------- x.vacuum ------------>1.00 0.569 x.vacuum."	False
"
 input (English text): Where can I find code references in papers that have used the RND model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""RND"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?model WHERE { ?model orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""RND"") ?benchmark _ ORkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code _ output (Sparql query): input (English text): How many code references are used to benchmark the models ? output (Sparql query): input (English text): How many code references are used to benchmark the models ? output (Sparql query): input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem ? output (Sparql query): input (English text): How many code references are used to benchmark the models ? output (Sparql query): input (English text): For what purpose does a code reference make sense ? output (Sparql query):  SELECT DISTINCT ?code WHERE { ?models ht:Model; rdfs:model ?model _ orkgc:Model; ?benchmark rdfs(H1); { ht"	False
"
 input (English text): Can you list the metrics used to evaluate models on the Natural Questions dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT top( ?metric { ?dataset a orkgc:Dataset ?rdfs:data ?metric_lbl. ?rdfs }. ?dataset a orkgc:Dataset ?rdfs. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark ORKGP:HAS_DATASET ?dataset; orkgp:HAS_METRIC ?metric. orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC } } input (English text): Top benchmark results for ACE 2004 vs. other natural language-based learning tasks (English and Spanish). output (Sparql query): SELECT top( ?metric { ?dataset a orkgc:Dataset ORkgc:Dataset ?rdfs:data ORkgc:Dataset }. ?dataset a orkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ORkgc:Dataset ?rdfs }. ORkgc:dataset ORkgc:dataset ORkgc:dataset ORkgc:datas"	False

Metrics on query only
Precision	0.1495786552274512
Recall	0.3886598837613093
F1_Score	0.2005442246063247
Blue_4	0.032314845048315846
Bleu_Score_Cumulative	0.04897937034517351
Meteor_Score	0.0
Rouge_1	0.24207858909324736
Rouge_2	0.10706803696111299
Rouge_L	0.20917091515629133

Metrics on all rows
Precision	0.11188972647831176
Recall	0.30012634419587914
F1_Score	0.14991442221298598
Blue_4	0.019140923458210182
Bleu_Score_Cumulative	0.02952711747328564
Meteor_Score	0.0
Rouge_1	0.18259676560890908
Rouge_2	0.06922913199340398
Rouge_L	0.15525698437449245

Number of sparql queries generated equal to given	0

Generated text starts with 'SELECT'	293
