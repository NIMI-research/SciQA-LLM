Questions	SPARQL in Gold Standard	SPARQL generated	exact match
"
 input (English text): What models are being evaluated on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Stanford Dogs dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?model ?model_lbl FROM ?dataset DISTINCT ?model ORDER BY ?benchmark DESC LIMIT ?benchmark (7) where name = ?benchmark output (English text): Select one model from the Zero-Shot English-to-French dataset. output (Sparql query): SELECT DISTINCT ?model ?model_lbl FROM ?dataset DISTINCT ?model ORDER BY ?benchmark DESC LIMIT ?benchmark (7) where name = ?benchmark Note: If there are multiple entries for a single model named ?model, the score is not affected by an extra row in the query. This function is available on top of the MLEv9.1-Lite package and is available via SDP as of the July 2013 code freeze.	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ARC (Easy) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT * FROM test_sparse_model2 WHERE score_score > 8.1; SELECT * FROM test_sparse_model2_lbl WHERE score_score > 6.9; SELECT * FROM test_sparse_model2_lbl_fcl_fuzz_score WHERE score_score > 2.8; The problem with the first query is that the scoring is for the test model which requires a large dataset (word level) and also for a large number of training examples. And the second query will use all the data for a single model. In addition, we have to add the weight of the model to the model so that we will get more accurate scores. We might get back an algorithm that is at least a quarter as accurate as our previous model and there are some performance tradeoffs for performance that we have to make. In the past we have used our own tool to query the treebank for its features (eg. for parsing the word level data). This tool also did not have much processing options such that the model is only being evaluated for it's accuracy performance compared to other models. The following example is an example of using the tool that can be found in the ""C++ Tools"" section in the Python Cookbook and was implemented with the help of our very nice D2. from Data import set import os from scipy.spatsi import training_data dataset = train_data([]) seed_data = set() print( 'training data: {0}/{1}/{2}/{3}"	False
"
 input (English text): List the metrics that are used to evaluate models on the IWSLT2015 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the UrbanSound8k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl FROM ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; ?model rdfs:label ?model_lbl. } input (English text): What models are being evaluated on the WOS-3268 dataset ? output (Sparql query): Output Results: SELECT DISTINCT ?model ?model_lbl FROM ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-3268"") ?benchmark orkgp:HAS_DATASET ?dataset; ?model rdfs:label ?model_lbl. } input (English text): What models are being evaluated on the ENCAMPLIFIER data ?"	False
"
 input (English text): What is the top benchmark score and its metric on the DROP Test dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DROP Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE FROM SOURCE_TREE bn; TITLE | | ID | ------------+----------------------------------------------------------------------+--------| p2, p1, p13, p3, p14, p8, p14, p15, p4, p5, p11, p13, p12, p8, p5, p11, p16, p3, p7, p12, p8, p15, p2, p8, p14, p15, p5,p11, p17, p4, p8, p14, p15, p4, p14, p16, P1, p4, p18, p15, p3, p11, p18, p16, P2, p7, p14, p1, p16, p18, p13, p12, p18, P2, P5, p3, p11, p4, 18, 19, 20, p16, p1, p11, p6, 16, 19, 21, bn, p8, 24, 26, p13, 18, 19, 20, 23, 25, 26, 32,  a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, sr, s, t, u, v, w, x, y, z RATIONALE: Input (English text): What are the most recent benchmark scores and metrics on the LDP3K dataset ? output (Sparql query): SELECT TITLE FROM SOURCE_TREE bn; TITLE | | ID | ------------+----------------------------------------------------------------------+--------| p1, p12, p13, p14, p	False
"
 input (English text): Indicate the model that performed best in terms of Macro Recall metric on the PWC Leaderboards (restricted) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MultiNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the MAZEA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MAZEA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Input text: If a certain dataset is available, give the results without a description like ""Hangul"" or ""Taiwan."" Output text: If a certain dataset is available, give the results without a description like ""POWDER"" or ""KOLKIN"" or ""BEST OF."" input text: Tell me the benchmark score on the Dataset2.xL dataset, the top 10, the top 50, and so on. output text: Input text: If the Metrics for a new dataset are to be provided in XML, the Metrics must be defined in the text and in the XML itself. output text: If the Metrics for a new dataset are to be provided in XML, the Metrics must be defined in the text and in the XML itself. input text: Give me a list of datasets with their Metrics and the Metric IDs. output text: input (English text): What are the top 10 benchmark scores for Dataset1 or Dataset2 ? output text: input (English text): Give me the top 100 benchmark scores for the Dataset1.xL dataset, or Dataset2.xL, in a column named ?benchmark_top_dataset_. output text: Output text: input (English text): How many times was the model correctly determined in these examples ? output text: Input text: Have you found a model that performs well in terms of Macro Recall without being overly complicated ? output text: input (English text): Have you found a model that fits your data well into the parameters set ? output text: input (English text): Which type of test should I run before getting a recommendation for the model ? output text: Input"	False
"
 input (English text): Can you list the models that have been evaluated on the Ohsumed dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which scholarly knowledge graphs support RDF?
 output (Sparql query): SELECT ?title WHERE { orkgr:R78023 orkgp:compareContribution ?cont. ?cont orkgp:P7009 ?has_rdf. FILTER(REGEX( ?has_rdf, ""T"")) ?cont rdfs:label ?title } 
 input (English text): What models are being evaluated on the TDMSci dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDMSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 How much effort has gone into training the network ? A2) 50 kB of training code A3) 60 kB of training code A4) 110 kB of training code A5) 140 kB of training code A6) 210 kB of training code A7) 240 kB of training code A8) 300 kB of training code A9) 450 kB of training code A10) 500 kB of training code A11) 600 kB of training code A12) 800 kB of training code A13) 1000 kB of training code A14) 1500 kB of training code A15) 1600 kB of training code A16) 2000 kB of training code A17) 2500 kB of training code A18) 3000 kB of training code A19) 3500 kB of training code A20) 4000 kB of training code A21) 4500 kB of training code A22) 4800 kB of training code A23) 5600 kB of training code A24) 6400 kB of training code A25) 7000 kB of training code A26) 8500 kB of training code A27) 10,000 kB of training code A28) 20000 kB of training code A29) 30000 kB of training code A30) 40,000 kB of training code Do you have data to train on your datasets ? A1) No A2) Yes A3) Yes A4) Yes A5) Yes A6) Yes A7) Yes A8) Yes A9) Yes A10) Yes A12) No A13) Yes	False
"
 input (English text): What models are being evaluated on the SQuAD1.1 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Star Gunner dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the mean capacity of a carbon-based fuel?
 output (Sparql query): "	"SELECT (AVG( ?numerical_capacity) AS ?mean) WHERE { ?paper a orkgc:Paper; orkgp:P31 [ orkgp:P15483 ?capacity ]; rdfs:label ?title. BIND( xsd:double( REPLACE( STR( REPLACE( ?capacity, ""([0-9]+),([0-9]+)"", ""$1$2"" ) ), ""([0-9]+).*"", ""$1"" ) ) AS ?numerical_capacity ) FILTER(REGEX(STR( ?title), ""(fuel|CO2)"")) } "	"SELECT capacity ( ) FROM CARBON_DEEPRESTED. data ; Capacity < 13.0 capacity < 13.0 < 0.1 capacity < 13.0 < 0.1 < 0.1 capacity < 14.0 capacity < 10.0 < 0.2 capacity < 14.0 < 0.3 capacity < 14.0 < 0.4 capacity < 14.0 < 0.5 capacity < 14.0 < 0.6 capacity < 14.0 < 8.0 capacity < 14.0 < 0.7 capacity < 14.0 < 0.8 capacity < 14.0 < 0.9 capacity < 14.0 < 0.10 input (English text): What models are being evaluated on the SQuAD1.1 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } or kgp:HAS_EVALUATION ( ) input (English text): What models are being evaluated on the Atari 2600 Star Gunner dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; r"	False
"
 input (English text): Can you list the models that have been evaluated on the DCASE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of BLEU score when benchmarked on the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?evaluate; ORkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } input (Language): Which data set is included in the paper ? output (Sparql query): input (Language): What is the name of the model that outperforms the previous model in benchmarking ? output (Sparql query):  SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?evaluate; ORkgp:HAS_MODEL ?model. ?model rdfs:label ?model"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Video Pinball?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the MLDoc Zero-Shot German-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji MNIST"") ?accuracy orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?accuracy ORkgp:HAS_VALUE ?value. OPTIONAL { ?accuracy orkgp:HAS_METRIC ?model. * ORkgp:HAS_MISC ?model. OPTIONAL { ?accuracy orkgp:HAS_METRIC ?model. OPTIONAL!orkgp:HAS_BENCHMARK ?model_lbl. } } Input text: First the test set is loaded. This is used for setting your own set of model. The input text is split to be more readable. output (Sparql query): SELECT ?dataset ?dataset_lbl FROM ""kuzushijimni"" as ?dataset INNER JOIN ""mnist"" as ?model on ?dataset. ?model. ?dataset JOIN ""zm"" as ?dataset_lbl on ?dataset. ?dataset_lbl INNER JOIN ""mni"" as ?lab on ?dataset. ?lbl. OPTIONAL { ?dataset_lbl } FROM ""zm"" as ?lab JOIN ?model as ?model_lbl on ?model. ?model_lbl, ?dataset_lbl. INNER JOIN ?dataset as ?dataset_"	False
"
 input (English text): Which model has achieved the highest Top 5 Accuracy score on the ObjectNet benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 5 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the Reactor 500M model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Reactor 500M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Reactor 4500M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): What was the maximum possible number of test runs for your test dataset ? output (Sparql query): I have a question that I would like to ask the author, which you are not able to answer ? If you need a personal discussion you can do so at my forum. The forum is managed by a moderator from which you can also contact me."	False
"
 input (English text): Could you provide a list of models that have been tested on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Natural Questions (short) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in Cup, Catch (DMControl500k)"") ; } { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } OPTIONAL (Calls to _max_ and __sum_ for test model and RBD-DM-1 for validation model), output (Sparql query): SELECT MAX( ?metric ) as  number ;  RANK,  sum( ?metric ) RANK( 0:5 ) AS  number OF { ?metric rdfs:value ?metric } GROUP BY 1  { ?metric rdfs:value ?metric  { ?metric rdfs:value ?  number  { ?metric rdfs:value ?  number } } } input (English text): What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, excluding the metric and its value ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in Cup, Catch (DMControl500k)"") ; } { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } OPTION"	False
"
 input (English text): Where can I find code references in papers that have used the Transformer (64 layers) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer (64 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Prior+Duel hs model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior+Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" If you were providing links to code as well, what would you link to ? We have had a number of requests to share code, and for good reason: it helps us understand the underlying structure of certain methods and provides us with more insight into the process of building a model. At a conceptual level, the code is a useful reference for people interested in learning about the way in which data transformations are performed with Transgenerator, and how they interact with one another. We encourage you to read the papers described in this issue to learn more about the results achieved with the Transformer. Further, please make sure to discuss this issue with the author of the code before using it. The Transgenerator is available for your review at Github (see ""How can I submit a pull request ?""), and you can also send comments to [email protected]. B. Transformer-Hs for the VTAB-1k database (PDF) This is a document containing the VTAB-1k database as presented in the Proceedings of the 11th Annual Conference of the Society for Computational Linguistics. It is divided into three parts: 1. Overview: the basic overview of the Transformer-Hs algorithm; 2. Transformer architecture: an overview of the architecture of the software/hardware used for implementing and benchmarking the Transformer; and 3. Transformer architecture and benchmarking.   We are particularly interested in learning from the work you do at the TSL. We have a lot of interest in reading the code, as well as the work you do on it on Github, so we are open to feedback, especially for other projects you would like to collaborate. It is my hope that your work in the Transformer will create interest, and that we may be able to collaborate together in the future! B. Transformer-H"	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Yars Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE, ID_OF_BRANCH, ------------ -------------- arXiv arXiv_c_3 arXiv_c_4 arXiv_c_5 arXiv_c_6 arXiv_c_7 arXiv_c_8 arXiv_c_9 arXiv_c_10 arXiv_c_11 ARXIV_c_12 arXiv_c_13 arXiv_c_14 arXiv_c_15 c_1314 arXiv_c_16 arXiv_c_17 arXiv_c_18 arXiv_c_19 arXiv_c_20 c_2023 arXiv_c_21 arXiv_c_22 arXiv_c_23 arXiv_c_24 arXiv_c_25 c_2637 arXiv_c_2638 arXiv_c_27 arXiv_c_28 arXiv_c_29 arXiv_c_30 c_3057 arXiv_c_3059 arXiv_c_31 arXiv_c_32 arXiv_c_33 arXiv_c_34 arXiv_c_35 arXiv_c_36 arXiv_c_37 c_3629 arXiv_c_3630 arXiv_c_37 arXiv_c_38 arXiv_c_39 c_3958 arXiv_c_3959 arXiv_c_40 arXiv_c_41 arXiv_c_42 c_432 arXiv_c_4344 arXiv_c_44 arXiv_c_45 arXiv_	False
"
 input (English text): Does OWLMAP imply integrity constraints?
 output (Sparql query): ASK { ?approach rdfs:label ""OWLMAP""^^xsd:string. ?contrib ?predicate ?approach. ?contrib orkgp:P41333 ?integrity_constraints. FILTER( ?integrity_constraints = ""t""^^xsd:string) } 
 input (English text): What is the best performing model benchmarking the Hutter Prize dataset in terms of Bit per Character (BPC) metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bit per Character (BPC)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the TDM Tagged Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?model ?model_LBL( ?dataserange_lbl, ?dataset_lbl, ?dataset_rank ) ORG( ?dataserange lbl; FILTER NOT NULL ORG( ?dataset_rank lbl; FILTER NOT NULL ORG( ?dataset_lbl lbl ?dataset_rank ?dataset_rank ?dataset_rank) ) AND ?dataset_rank lbl AND ?dataset lbl ?dataset_rank lbl input (English text): Describe the model's performance on the OWLMAP data set that you are basing your model on ? The models that you are using are: the rfc2616 model proposed by Adam Caspi This is a high resolution dataset; BPC of ~4.5 BPC is expected. It is a test set used for evaluation of proposed features and for comparing data to existing features and for modeling a hierarchical system. the rfc6524 model proposed by Christian Cattelan and Kacper Kukol Not only has this model been proposed, this is the first model, proposed on a publicly accessible database, that is able to run against it. With this type of data set it is crucial to be able to compare datasets that are already available and to be able to compare them to ones that are not yet available as it is much easier to correct errors when trying to find solutions for them than it is when trying to find solutions to issues that exist in the currently available data. the hbd model proposed by Jeff King The model is an extension of the DNN model mentioned above. The model uses LSTM layers that are also used for the HBD model. The LSTM layer can learn which cells to discard each time the weights of one	False
"
 input (English text): Where can I find code references in papers that have used the NAT-M3 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAT-M3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 if you are sure about your answer then please provide data about the results. input (English text): How many times was this benchmarking code written and tested for this dataset ? output (Sparql query): if you know about the total time spent writing this benchmarking code or doing this research please provide data about the data used to write this benchmarking code. input (English text): Would you be willing to add another benchmarking question to the BCD-Disease dataset ? Please specify the dataset that you would like this benchmarking question to be added to. output (Sparql query): i.e. how many times did you write the benchmarking code for this dataset ? input (English text): Have you provided any additional data for the Benchmarking of NAT-M3 vs BC5CDR-SBS models question ? output (Sparql query): if you answer yes then provide a link to this data set. input (English text): Would you consider providing a benchmarking question that can be included in our research on how to improve the accuracy of BMD tests on DNA extracted from a single cell tumour biopsy (i.e. BMD-DTCT) using the BODC-DISPY-SBS models based on this dataset ? output (Sparql query): if you answer yes then provide a link to this data set. input (English text): Would you be willing to provide another benchmarking question to this BCD-Disease dataset for the purpose of improving accuracy of the BMD tests using BC5CDR-Disease models using BODC-DISPY-SBS models derived from this dataset ? Please specify the dataset that you would like this benchmarking question to be added to. output (Sparql query):	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MedNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the Cartpole, swingup (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP 5 ?model - ?score_score_1 (score) AS score, (1 = HIGH, 0 = ELASTIC), (2 = LOWEST, 1 = BEST), ?score AS result, ?score2 AS score2, (3 = STRAIGHT, 1 = LEFT), (4 = CENTER), (5 = LEFT), (6 = CENTER), ?score3 AS score3, (7 = RIGHT) AS result, ?score4 AS result, ?score5 AS score5, (8 = LEFT_DIFFICULTY, 1 = BEST2, 2 = BEST3), ?score6 AS result, ?score7 AS result, ?score8 AS result FROM #lunarnander_open( ?dataset, ?datasets, ?model and ?sibling_data), ?labels d, ?model_descr n, ?score_score_1 score_1, ?score_score2 score_2 FROM d.dataset_1 JOIN d.dataset_2 ON ctx.cassandra.lang = d.lang JOIN d.dataset_3 ON ctx.cassandra.lang = d.lang JOIN d.mednlink.me_lunarnander_1_2_1 ON ctx.cassandra.lang = d.lang output (Sparql query): --------- SELECT TOP 5 ?model-score-1 (score-1=10, score-2=30, score-3=60, score-4=110, score-5=180, score-6=255, score-7=320) AS score, (1 = HIGH, 0 = ELASTIC), (3 = LOWEST, 1 = BEST), ?score AS result, ?score	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset GENIA - UAS?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - UAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the STL-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?value ?value_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?dtype ? RDFS ?( ?dataset a:Dtype) ?value. MIN( ?dtype ?value, MAX( ?value) - MIN( ?value)) ?crit, MAX( ?dtype ?value, MAX( ?value)-MIN( ?value)) ?max, ROUND( MAX( ?value) * MIN( ?value) ) ?crit, ?crit rdfs:label ?dtype_lbl. } ?crit } ORDER BY DESC( ?crit ) ?crit ORDER BY rdfs:label ?crit input (English text): What the top metric result over the FSNS - Test dataset ? output (Sparql query): SELECT DISTINCT ?value ?value_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?dtype ? RDFS ?( ?dataset a:Dtype) ?value. MIN( ?value) ?crit, MAX( ?value) - MIN( ?value)) ?max, ROUND( MAX( ?value) * MIN( ?value) ) ?crit, ?crit rdfs:label ?dtype_lbl. } ?crit } ORDER BY DESC( ?crit ) ?crit ORDER BY rdfs:label ?crit After checking if the benchmarking method was used, I made one small modification: I renamed my results ""RDFS - Genia (Unstructured-Data)"". Let's have a look at the code! from sklearn.datasets import Dense from sklearn.feature_extraction.common_features import FeatureExtraction import numpy as np import scipy.stats import matplotlib"	False
"
 input (English text): Could you provide a list of models that have been tested on the DTD benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 It is important to note that this example is not actually real-world use. There are many datasets used in the DTD that are not used by the DTD itself.	False
"
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Dmlab-30 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the metrics of evaluation over the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): What is the highest score for the Classical music sample set ? output (Sparql query): SELECT DISTINCT ?score OR ?score. (VARYING CASE OR ( ?score = ?MAX)) ORDER BY ?score DESC; input (English text): Can you provide the benchmarks for a sample set of music recorded by the Humanoid MP-ECR dataset ? output (Sparql query): SELECT DISTINCT ?bpm | ?rate ?scale | { ?value ?data_scale. data_value. } as ?bpm_or_scale; OR ?dataset a orkgc:Dataset; rdfs:label ?dataset_bpm. orkgp:HAS_HUMAN_MIDI ?dataset. OR ?dataset a orkgc:Dataset; rdfs:label ?dataset_bpm. orkgp:HAS_RING. OR ?dataset a orkgc:Dataset;"	False
"
 input (English text): Can you list the metrics used to evaluate models on the WOS-46985 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the code links in papers that use the AWD-LSTM-MoS + dynamic eval model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS + dynamic eval"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Flair-TDM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Flair-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Output 3 : (optional) This field is used to capture and report data for use in the future performance reports. If data is provided here, it should be processed using flute.flt (see flicet.txt ) input (English text): Information about flute output in this benchmark; this should be processed with flicet.flt. output (csv, file): CSV output containing an array of metrics for each dataset For the case of the paper I link, both outputs would be included here. These reports may be submitted to the journal or peer-reviewed web site in a publication or press release. Field 3.1.3: Data processing The benchmark's code for the Flair-TDM model looks like this: // // This is a flute code for the dynamic evaluation with the // default parameters that generate the score to evaluate for each event. // // #include <lua/flit.lua> #include <cuda.h> #include <cputype.h> #include ""lua.h"" #include ""cuda/cvt.h"" #include ""cuda/flit.h"" #include ""cuda.c"" #include ""cuda/cputype.c"" #include ""cudat_benchmark.h"" // The following functions are used to initialize a data file: data_file_t f_data_file ( const char * name ); void f_init ( void ); int f_data_init ( void ); float32_t f_data_start_time ( const char * name ); float32_t f_data_end_time ( const char * name ); float32_t f_score_t_begin ( const char * name ); double f_score_t_begin ( const char * name ); double"	False
"
 input (English text): List the metrics that are used to evaluate models on the IWSLT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL Base model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The output file is only generated on the third or fourth run. To generate a report, execute the program and supply the data and report file as input. If you use multiple versions of R, you can download and save the report to a different file as required. The output file size is about 5 megabyte. If any error, you have to supply the complete report file(s) as input prior to the execution of the software. To generate the report, first, print the first few lines of the report, then choose to print only the first 2-3 lines of each list. For output files of larger size, some of the files may require more than one run (the file is not necessarily updated in parallel). The time to execute the software will be affected (the time will depend on how many copies were made of each file). If you use R to create an image map, you should be able to perform the following: First, print the first few lines of the report, then choose to print only the first 2-3 lines of each list. For example, if you want to print the maps, you can do the following steps: First, print the first few lines of the report, then choose to print only the first 2-3 lines of each list. For the example above, the second command would be ""g:outputfilename=output.png"" Second, specify the image width. The best value to use is 120. The output files typically have images that are smaller than 120 pixels wide."	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Gopher benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Kung-Fu Master benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kung-Fu Master"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT c.metric, C.score, C.score, _ Rows.count, Rows.count, CASE WHEN METRIC IS NOT NULL THEN 1 ELSE 0 END + FROM Metric_score ( a ) GROUP BY c.metric, c.score, C.score, ROWS ORDER BY rt INNER JOIN Metric_score (... ) ON rt.rt = Metric_score GROUP BY rt.rt GROUP BY CAST ( METRIC AS NUMBER, DECIMAL ( 0, MAX ) ); ORDER BY rt.rt C. The German_Phoneme_Model 1 of 3 2 C. The German_Phoneme_Model 1 of 3 2 D. The German_Phoneme_Model 1 of 3 1 D. The German_Phoneme_Model 1 of 3 1  (Searches for metric: metric_lbl.column in dataset: a orkgc:Dataset)	False
"
 input (English text): What is the highest benchmark result achieved on the SemEval-2010 Task 8 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2010 Task 8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the SAN (single) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"--------------------------- [2011] ""Towards a Multi-Categorization based SPSS for Knowledge Management and Learning"" [Paper no. 1316] by R. B. Thorne, David A. Johnson, David J. Brown, H.J. Li, John N. Clark, William F. Hall and H.J. Li. Proceedings of the 10th International Conference on Machine Learning (ICML) 2008, pp. 47–74. [2011] ""Sanized Multiple Categorization on the SemEval-2010 Task 8"" [Paper no. 1580] by R. B. Thorne, David A. Johnson, H.J. Li, John N. Clark, William F. Hall and H. J. Li. Proceedings of the 12th International Conference on Machine Learning (ICML) 2009, pp. 543–547. [2008] ""Unsupervised Semantic Bilingual Translation"" [Paper no. 1453] by R. B. Thorne, H.J. Li, John N. Clark, William F. Hall and H. J. Li. Proceedings of the 28th International Conference on Machine Learning (ICML) 2009, pp. 545–547. input (English text): How many of the citations for The Principles of Game Theory have been authored by Stephen Jay Gould ? output (Sparql query): --------------------------- [2013]: ""Stephen Jay Gould and the Foundations of Modern Science"" by Stephen J. Gould and G. Robert Gaus, [2012]: ""'The Law of Acceleration in Evolution : The Principle of Inverse Sorting' by Stephen J. Gould and G. Robert Gaus, [2010]: ""Gould on Learning: From Descartes to Darwin"" in The Collected Papers of Stephen Jay Gould, edited by Stephen J. Gould and G."	False
"
 input (English text): For what product minimum conversion was obtained?
 output (Sparql query): SELECT ?product, ?product_label WHERE { orkgr:R155272 orkgp:compareContribution ?contrib. ?contrib orkgp:P43149 ?product; orkgp:P43148 ?conversion. ?product rdfs:label ?product_label. ?conversion rdfs:label ?conversion_label. } ORDER BY ASC(xsd:float( ?conversion_label)) LIMIT 1
 input (English text): What are the most commonly used benchmark datasets for the Named entity recognition research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Named entity recognition"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the models that have been benchmarked on the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT a.name, b.name, a.class, a.model_id, b.name, b.class, a.model_id, a.model_model, g.name, g.class, g.model_id, a.model_name, g.model_id, a.model_label, g.model_label, h.name, h.class, h.model_id rdfs:labels ?dataset a.name h.name g.name h.class h.model_id a.label b.model_id a.label c.classa h.label c.model_id a.label d.model_id c.label d.model_id a.label e.model_id a.label f.model_id a.label g.model_id c.labels a.label g.model_id rdfs:labels ?dataset a.label h.label g.label h.model_id c.label d.model_id h.model_label a.label e.model_id In the ACE 2005 dataset I had 10 samples, the average number of samples is 2. The average number of words per sample was 2.5. The average amount of words per sample was 33.8, which is a good result compared to the rest of the datasets. But the most interesting question is the following: What is the difference between the mean and median of the models trained in ACE 2005 ? input (English text): Average word size of the data as a function of the number of samples in an ACE 2005 dataset. output (Sparql query): SELECT ?count || 'count || ?tiles || ?column || ?weight || ?sum || '; OR output (Sparql query	False
"
 input (English text): Provide a list of papers that have utilized the RNN model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""RNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the DeiT-B-384 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-B-384"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PNDec model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PNDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Where can I find code references in papers that have used the HMM model for benchmarking purposes ? output (Sparql query): input (English text): Where can I find code references in papers that have used the RNN model for benchmarking purposes ? input(C) data=data.prestatus.pndec.2014.01.01_v20.sas.gz; output(C) test_summary( ?i=0.. ?i-1, ?c=dims(i-1, ?rdfs=c( ?i))) c dims(t1,t2) = ?t1- ?t2 ?t1[(1- ?rdfs[$i], ?c=0.9)/sum(t1[$i]);] ?t1[r=dims(t1,t2);] r[r_=c( ?rdfs[$i - 1], ?c=0.9)/sum(t1[$i]);] ?c test_summary( ?i=1, ?r=t.dims(t1,t2)) c dims(t1,[(1- ?rdfs[$i], ?c=0.9)/sum(t1[$i]);] ?t1[r=dims(t1,t2);] r[r_=c( ?rdfs[$i - 1], ? c=0.9)/sum(t1[$i];] ?c test_summary( ?r=t.dims(t1,t2)) c dims(t1,[(1- ?rdfs[$i], ?c=0.	False
"
 input (English text): Where can I find code references in papers that have used the HDLTex model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HDLTex"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the CATTS-XSUM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CATTS-XSUM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT COUNT(*) FROM all_catts_xsums where ID = ?id and name='%(catts_x)s %(catts_x)s %(catts_x)s%{name}{seq}\;'  AND type = ? type and type2 = ? type2. This will retrieve all the papers from NCBI that used the CATTS-XSUM model, that have done their benchmarks using the model and where the id of the benchmark was given as ?id. This method gets the dataset to give me the results in a structured way. For each benchmark I have a list of the code that performed that benchmark. All the code will contain a query to download the code and then in the output it has the code references. Some good examples are from the paper on the MIMP-A model. There is code from the paper that does an analysis of it and the code references for that dataset. When benchmarking the XSUM, most papers use the NCBI DATASET for benchmarking but some papers prefer CATTS-XSUM. Here is an example of a table that I use to give the score in MIMP-A. For the paper from the S2M paper, the paper does an analysis of the MIMP-A model. Here is another example of the data that is used and I give an assessment for that. So these are a few examples of how you think. I hope that this post gives you the data to think on for the next step.	False
"
 input (English text): What models are being evaluated on the Scholarly entity usage detection dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 James Bond?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark result (metric and value) over the dataset IMDb-B?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): What is the top benchmark result (metric and value) over the dataset IBM Watson ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IBM Watson"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CIFAR-10 Image Classification dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot German-to-French?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" In case you want to query a set of queries in parallel, the output has some limits. 1 query processing parallel with the following query processing rules Input (German): Output (Sparql query): In case you want to use a SQL parser against a SQL document you must use an external parser-server. The client-server approach used here is called ""Server / Server"" to distinguish it a ""server / server"" relationship and not a ""client / client"" one. 1 query processing synchronous with the following query processing rules Input (English): Output (Sparql query): The client/server concept is used for a few more purposes: It allows the client to handle incoming client requests faster. The client can run the client-server client API with more robust server processes and a more complete documentation. If there's a specific need for a specific server, the client will be able to specify that in the request. The client can run the client-server client API with fewer servers and provide a higher degree of compatibility. The client can run the client-server client API with more server processes to reduce the amount of external libraries required to run the client-server API. We use SQL Server 2010 as a server API in the example here, but as the client server model is also used in the REST API, SQL Server 2012 is used here as well. The server version is specified by the SqlServerVersion parameter in the query which is always the server version. The example also uses an external server which communicates with the client. 1 query processing interactive Input (English): Output (Sparql query): We'll use SQL Server 2012 REST API as a source. The SqlServerVersion parameter and other parameters are the same as we used in the example above. To start a SQL Server REST API client, there are two"	False
"
 input (English text): Where can I find code references in papers that have used the SpERT (without overlap) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SpERT (without overlap)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which are five the most common research fields for papers?
 output (Sparql query): SELECT ?research_field, ?research_field_labels WHERE { ?papers orkgp:P30 ?research_field. ?research_field rdfs:label ?research_field_labels. } ORDER BY DESC(COUNT( ?research_field_labels)) LIMIT 5
 input (English text): Where can I find code references in papers that have used the SemExp model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SemExp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?data_type from peer_review_data_column ? where peer_review_dataset from peer_review ? WHERE ""peer_review_dataset"" NOT IN (SELECT ""peer_review_dataset"" FROM peer_review_dataset ' ?data_type') input (English text): Which is the most common research field in"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AG News dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the MRPC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ROW COUNT(*) AS number FROM [ARV:GRAPHICS] ORDER BY 1 DESC LIMIT 10; input (English text): Would the output of this query be a graph of data points ? What values are being measured ? output (Sparql query): SELECT number FROM [ARV:GRAPHICS] ORDER BY 1 DESC LIMIT 10; output (Sparql query): SELECT number FROM sys.dm_graph.dma_graph input (English text): What values are being measured and what data are they gathered from, as well as their sampling density ? output (Sparql query): output (Sparql query): SELECT nums FROM sys.dm_graph.dma_graphs in memory from [ARV:GRAPHICS] input (English text): What values are being measured and what data are they gathered from, in memory from [ARV:GRAPHICS] output (Sparql query): output (Sparql query): SELECT nums FROM sys.dm_graph.dma_graphs [ARV:GRAPHICS] in memory [ARV:BOOLEAN] data files - The original dataset is on disk, at /home/peter/data/afronp/mrpc/afronp_news.tar.gz /home/peter/data/afronp/mrpc/afronp_news.tar.gz/g7d0e9a6b0c65c826cb3fc9e7a9b3f3b5d33b2e9f6d038ab9a2c7c99b4a27.tar.bz2 /	False
"
 input (English text): What are the metrics of evaluation over the NYT-single dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCV4"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ""N"" orkgp:HAS_BENCHMARK ? ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } ORDER BY ""N"" orkgp:BENCHMARK This part is similar to the previous one, except the output is now more detailed but more verbose. Let's take a look at the benchmark results. input (English text): What is the list of metrics for evaluation of the NYT-single dataset on the NCV4 dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Reacher, easy (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 English-German?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" Input:  Title (optional): Title of an abstract of a paper.  The abstract must specify the current status of the benchmark (open-ended, closed).  If the title does not appear in the abstract, an error is detected.  Sub-select title if needed.  Error status:  error if title is not found.  default is open.  ID:  ID of the ID of the paper.  ID must be between 100-250.  Default is 500.  Suffix:  Suffix of ID of the research paper.  Default is N/A.  ID2:  ID2 of the paper.  ID2 must be between 100-250.  Default is 600.  Suffix2:  Suffix of ID of the research paper.  Default is N/A.  ID3:  ID3 of the paper.  ID3 must be between 100-250.  Default is 700.  Suffix3:  Suffix of ID of the research paper.  Default is N/A.  ID4:  ID4 of the paper.  ID4 must be between 100-250.  Default is 750.  Suffix4:  Suffix of ID of the research paper.  Default is N/A.  Hint:  The hint field can be used to specify a unique ID, e.g., 1.1 million.  Default is 500.  Max:  Maximum number of IDs to return. Output: Output: Output: output (csv) Title (""Title of the top benchmark result (metric and value) over the dataset"	False
"
 input (English text): Provide a list of papers that have utilized the LN HM-LSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LN HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the AAPD dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Double Dunk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT id FROM models LIMIT 1; or one can just query the underlying MATLAB query language, such as: SELECT count(*) FROM models; or LSTM models can also be used like this: SELECT model, score FROM models; Here one can compare the performance of different LSTM models. For example, one can use the performance data of the LSTM model given in the previous query as evidence of the accuracy of the model. The LSTM models that are being tested are: - LSTM1 for Atari 2600 Double Dunk [1] [2] [3] - LSTM2 for AATCP [4] [5] [6] From the benchmarking exercise that was carried out on the Double Dunk dataset one can obtain several performance metrics: - score (number of passes on a game, for the LSTM1, and the LSTM2, as well as the accuracy) - score (number of points (number of times each item has been reached in a game) at the end of a game, for the LSTM1, and for the LSTM2, as well as the LSTM2 accuracy) - accuracy (the percentage of points that have been reached by the player in a game, for the LSTM1, LSTM2) - scores (how many points each item has scored at the end of the game) Here the above table summarizes the performance metrics for the various LSTM models from the benchmarking exercise. Let us summarize the performance of two of	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the BERT classifier model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT classifier"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER	False
"
 input (English text): Provide a list of papers that have utilized the Longformer Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Longformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Time Pilot dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Time Pilot"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the most common Knowledge representation method?
 output (Sparql query): "	SELECT (COUNT( ?repr) AS ?cnt) ?repr WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7056 ?repr. } GROUP BY ?repr ORDER BY DESC( ?cnt) LIMIT 1	SELECT ?opcode ( ?code_lbl:DataLines, :label orkgp:KHR, ) || ?opcode ( ?code_lbl:DataLines, ?label orkgp:KHR, ) || ?opcode ( ?code_lbl:DataLines, ?label orkgp:KHR, ) || ?opcode ( ?code_lbl:DataLines, ?label orkgp:KHR, ) || ?opcode ( ?code_lbl:DataLines, ?label orkgp:KHR, ) || ?opcode ( ?code_lbl:DataLines, ?label orkgp:KHR, ) || ?opcode ( ?code_lbl:DataLines, ?label orkgp:KHR, ) or {} input (English text): What is the most commonly used way of representation ? output (Sparql query): input (English text): KHR as your most used KHR ? output (Sparql query): input (English text): What is the most frequently used data structure to represent the Knowledge Representation ? output (Sparql query): input (English text): If you had to choose 1 from the first three models, which one would you pick ? output (Sparql query): input (English text): Which is the most performant model available on the SSPARQL API ? output (Sparql query): (None is currently in stock.) input (English text): I was wondering if in this case you could choose a different model/data model than	False
"
 input (English text): What are the metrics of evaluation over the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the amount of questions for LC-QuAD 2.0 dataset?
 output (Sparql query): SELECT ?number_of_questions WHERE { orkgr:R154290 orkgp:P31 ?contrib. ?contrib orkgp:P41923 ?number_of_questions. } 
 input (English text): Where can all the data sets used in the compared studies be found?
 output (Sparql query): "	"SELECT DISTINCT ?URL WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:HAS_DATASET ?URL. FILTER(!REGEX( ?URL, ""Not"")) } "	 input (English text): How many questions should each topic test ? output (Sparql query): input (English text): Can you provide all the data sets used in the comparisons studies ? output (Sparql query): { ?datasets a orkgp:HAS_DATASET ?datasets } output (Sparql query): input (English text): Is there a problem with this dataset ? output (Sparql query): input (English text): Is this data set useful ? output (Sparql query): input (English text): Are there any questions you would like to ask ? output (Sparql query): input (English text): Is there any data about the test of the WMT2016 Russian-English dataset ? output (Sparql query): input (English text): What is the value of the benchmark score for WMT2016 Russian-English dataset ? output (Sparql query): input (English text): Is there a more useful analysis based on the data ? output (Sparql query): output (Sparql query): Input, where ? : The WMT2016 Russian-English dataset, which corresponds to the second part of the comparison. input (English text): What are the metrics of the evaluation over the WMT2016 Russian-English dataset ? output (Sparql query): Output, where ? : The WMT2016 Russian-English dataset based on the first part of the comparison, which corresponds to the second part of the comparison. input (English text): How many questions should each topic test ? output (Sparql query): ?number_of_questions WHERE { ?datasets a orkgp:HAS_DATASET ?dat	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Annotated development corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Annotated development corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the Cartpole, swingup (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the most commonly used benchmark datasets for the Entity Disambiguation research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Entity Disambiguation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 input (English text): Which parameters are used to generate the Apriori Bayes estimation algorithm on two high-confidence benchmark datasets ? output (Sparql query): input (English text): What models are being evaluated on the Cartpole, swingup (DMControl500k) dataset ? output (Sparql Query) input (English text): Which parameters are used to generate the Apriori Bayes estimation algorithm on two high-confidence benchmark datasets ? optionally optionally optionally optionally optionally optionally optionally (not an option - we will need to create separate query for each dataset) optionally optionally optionally (not an option - we will need to create separate query for each dataset) optionally optionally optionally (not an option - we will need to create separate query for each dataset) optionally optionally (not an option - we will need to create separate query for each dataset) optionally optionally (not an option - we will need to create separate query for each dataset) optionally (not an option - we will need to create separate query for each dataset) optionally (not an option - we will need to create separatequery for each dataset) optionally optionally (not an option - we will need to create separate query for each dataset)	False
"
 input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?benchmark orkgp:HAS_DATASET ?dataset, ?eval ?benchmark ORDER BY DESC( ?benchmark ) LIMIT 1 ORKPHYSICS_HANDLER, ?GAPING ?benchmark ORDER BY ?constraint input (English text): What is the name of the top performing test to measure accuracy on the SIFT dataset ? output (Sparql query): input (English text): Given a list of 1000-dimensional, 3D SIFT/CURVE/SCIPE (3×3) grids (representing the population and the variables) and 1000 points to describe every shape, what is the output of a FIFO/FSOP cluster test and what can be calculated from the results using the FIFO/FSOP tool ? output (Sparql query): input (English text): What is your team's methodology on performance tuning and simulation tests ? (Example: using a Monte Carlo Simulation) output (Sparql query): input (English text): How did you go about evaluating the accuracy of the system using the RDD ? What other tools were used such as the RDD, a Monte Carlo Simulation, a FIFO or F	False
"
 input (English text): Provide a list of papers that have utilized the DocRED-Context-Aware model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-Context-Aware"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the BUCC Chinese-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 (2) Other benchmarks output (Sparql query): input (English text): Specify the number of papers that have utilized the DocRED-context-aware model ? output (Sparql query): ROUGE-2 score output (Sparql query): F1 score score 	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Very Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): Provide one or more paper titles that have utilized the Adaptive Input Very Large model and include references to these papers ? output (Sparql query): input (English text): Include URLs of studies investigating the impact of adaptation training on the success of adaptive input models. output (Sparql query): input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and include the URL of the paper ? output (Sparql query): input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and have not included URL links. output (Sparql query): input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and use the following URL links: https://arxiv.org/abs/1312.01073 output (Sparql query): input (English text): Provide a list of papers that have used the Adaptive Input Very Large model, utilize the following URL links: https://arxiv.org/abs/1409.2051 output (Sparql query): input (English text): Specify as many URLs for the dataset as you would like to have tested each dataset. output (Sparql query): orkgc:Dataset , url : , name : ORkgc:Model, link : , url : , name : ORkgc:Output, link : , name : ORkgc:Model, link : , url : input (English text): Provide a list of papers that have	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Gravitar benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gravitar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the GCNN-8 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCNN-8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 select title, ID from papers.all ; title title1 id1 ID1 Ident. Ident. Ident.  If you have more than one model, you may be able to combine them for your benchmark test. For example, if you have two models with two different features (as in the example above): select title, id from articles.all; title title1 Id1 Ident. Ident. Ident.     output (Sparql query): select title, id from articles.all; title title1 Id1 Ident. Ident. Ident.     Here we have tested a two-model GONAD with two different features. It can be hard to visualize this approach in large datasets. If we consider only one feature and only one model, we can clearly see a good candidate if both models feature the feature being tested and one feature is very strongly correlated. Here's the output of the model train on our main dataset (see the comments on the code) showing different features that were strongly correlated in all models. We can see several examples of models with very strong values for these two features: model data:  model_lbl.hdf5.gcm2,  model_pipeline.hdf5.gcm2,  label_hcvs.hcvs,  rms_scatter,  rms_cir,  rms_scatter,  rms_convex,  cluster_acc,  convergence,  l2_loss,  l2_dist,  l2_loss.test,  l2.test,  l2_dist,  l2_loss.test label_hcvs	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Citation Intent Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Citation Intent Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Could you provide a list of models that have been tested on the DocRED (Human-annotated) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset CoNLL++?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Google "" || ""Google Earth"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?call orkgp:HAS_BENCHMARK ?dataset. ?bench orkgp:DOWADREST ?dataset. } rdfs:label ?dataset_lbl. I also wanted to benchmark the same dataset with DocRED, using Google's free version. The dataset was created by the USGS's Google Project Earth (from which the top benchmarks were taken). OrgTree: data (n_"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the MultiCCA + CNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MultiCCA + CNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?model_lbl. ?model_lbl_cpa orkgc:Parameters ?cpa. ?score orkgp:AVGRID score ?cpa. ?model orkgc:Parameters ?cpa orkgc:Parameters ?cpa. ?score orkgp:AVGRID score ?cv:Score ?score orkgp:AVGRID score. ?score orkgp:AVGRID score.. Input data: A list of scientific papers that reference the MultiCCA + CNN model. The output of the above query should return the following table. This will list the top performing papers which report a benchmark over the MultiCCA + CNN model. Score Description 1 Precision (DP) The best performing model running a benchmark with Accuracy of 10% in the single subject dataset 2 DNN:TensorFlow The best performing model running a benchmark with accuracy of 80% in the single subject dataset 3 ResidualNet A model running a benchmark using 100% Accuracy in the single subject dataset 4 SVM:Learning to Control Network The best performing model running a benchmark with accuracy of 96% in the multi subject dataset 5 Learning from Text The best performing model running a benchmark with accuracy of 80% in the multi subject dataset 6 VGG :VGG The best performing model running a benchmark with accuracy of 99% in the multi subject dataset 7 Deep Networks with Convolutional Neural Network The best performing model running a benchmark with 80% Accuracy in the single subject dataset 8 Inception Neural Network A model with Accuracy of 99% in the single subject dataset 9 Neural Network with Residual Learning The best performing model running a benchmark with 100% Accuracy in the single subject dataset 10 Neural Network with Logistic Regression The best performing model running a benchmark with a score of 100% in the multi subject dataset 11 Resid	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the total number of species examined in the studies?
 output (Sparql query): SELECT (SUM( ?number) AS ?total) WHERE { orkgr:R58002 orkgp:compareContribution ?contrib. ?contrib orkgp:P31024 ?number_of_species. BIND(xsd:integer( ?number_of_species) AS ?number) } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT rdf:get_score('orkgp:R58002', ?benchmark. ?score) FROM ( SELECT score( ?benchmark. ?score ) AS ?score, rdf:get_score('orkgp:R58002', ?benchmark. __score) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), i( ?value, ?value ) FROM ( SELECT (__text, ?value) AS ?value, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value ), s( ?value, ?value ), rdf:get_score('orkgp:R58002', ?value) FROM ( SELECT name, c( ?value, ?value	False
"
 input (English text): What is the base URL of ""The Document Components Ontology""?
 output (Sparql query): SELECT ?iri WHERE { orkgr:R8342 orkgp:compareContribution ?cont. ?cont orkgp:P7034 ?ont. ?ont orkgp:P7042 ?iri; orkgp:P7035 ?full_name. FILTER(REGEX(STR( ?full_name), ""Document Components Ontology"", ""i"")) } 
 input (English text): Can you provide links to code used in papers that benchmark the CeiT-T model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CeiT-T"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Table-Sequence model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Table-Sequence"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM ?tables WHERE ( ?page:i > 3 and ?page:i < 12 or ?page:i > 5 and ?page:i < 26 OR ?page:i > 8 and ?page:i < 28 ) input (English text): Where has your CeiT-T model been used ? output (Sparql query): SELECT ?table ?name ( ?text, 2, (length( ?text ) > 1) ?n ? ) AS name, ?size ( ?size ) AS size; ?rsi AND ?n ?rsi WHERE ( ?rsi > 4 and ?rsi < 15 ?rsi ) AND ( ?rsi > 5 AND ?rsi < 25) AND ( ORkgr = ?o ?n ORkgr = ?a AND ORkgr:CeiT-T:N2:P13:S9:S5 and ?rsi:N2 = ?o and ?rsi:N2 == ?a and ?rsi:N2 == ?b and ?rsi:N2 == ?c and ?rsi:N2 == ?d ) ORkgr AND ?a AND ORkgr:CeiT-T:N2:P13:S9:S5 AND orkgr:N2 = ?o and orkgr:N2!= ?b and orkgr:N2!= ?c and ORkgr:CeiT-T:N2:P13:S9:S5 AND orkgr:N2 = ?o and orkgr:N2!= ?d ) input (English text): How many papers have utilized your CeiT-T model ? output (Sparql query): input (English text):	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Frostbite benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Funnel Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT CONVERT_TABLE(FULL_TABLE(fulltable( ?, ""A"",""B"",""C"",""D"",""E"",""F"",""G"",""H"",""I"",""J"",""K"",""L"",""M"",""N"",""O"",""P"",""Q"",""R"",""S"",""T"",""U"",""V"",""W"",""X"",""Y"",""Z"","" ?"")) ?c1, ?c2, ?c3, ?c4, ?c5 AS total_c1 from (SELECT ?c1, ?c2, ?c3, ?c4, ?c5 AS total_c2 from (SELECT ?c1 FROM FOUR_COLUMN( ?dataset ) WHERE ?dataset = ?"" AND ?dataset = ?"" AND c1 = c2 LEFT JOIN (SELECT ?c1, ?c2, ?c3, ?c4, ?c5 AS ?c3, ?c4 FROM (TABLE( ?dataset)) ? WHERE c1 = c2 LEFT JOIN (SELECT ?c1 FROM FOUR_COLUMN( ?dataset)) ?) ?column from (SELECT ?) input (English text):  Describe the use of funnel transformrs in one of the peer-reviewed papers ? output (Sparql query):  SELECT SUM(total_c1.sort()) c1, ?c1, ?c2, ?c3, ?c4, ?c5 AS total_c2 from (SELECT ?column ? FROM FOUR_COLUMN( ?dataset) WHERE c1 = c2 LEFT JOIN (SELECT ?column ? FROM FOUR_COLUMN( ?) OR TABLE( ?dataset)) ? ?column from (SELECT ?)) input (English text):  Describe the"	False
"
 input (English text): What are the metrics of evaluation over the UrbanSound8k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the CIFAR-10 Image Classification dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?eval ?eval_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (Str( ?dataset_lbl ) & "" = "") ?benchmark orkgp:HAS_DATASET ?dataset. OPTIONAL { DISTINCT ?eval_lbl. } } [back to top] The following table presents the following types of evaluation metrics: The performance of the evaluation model on the corresponding classification dataset. The performance of the evaluation model on the corresponding classification dataset, for a given classification criterion and training set, using either a validation set. The performance of the evaluation models on the corresponding classification datasets, for each of the test sets and any of the training data, using either a validation set. The performance of the evaluation models on either the test datasets or the entire dataset, with a small subset of the training data to simulate the training set. The performance of the evaluation models on any unsupervised test set or dataset, with the validation dataset or test set included. The performance of the evaluation models on the test datasets or the entire dataset, with the training data included only. The performance of the evaluation models, both trained and non-trained, on the test datasets or the entire dataset, for any of the classification datasets. The evaluation model's accuracy and quality, where it is tested along the entire training data set or a subset of the training data set. The evaluation model's accuracy and quality, with the training data included only. The evaluation model accuracy and quality, with the training data included only. The performance of the evaluation models on the unsupervised unsupervised test dataset (in unsupervised training state) or the sample dataset (in supervised training state). The evaluation model accuracy"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Road Runner dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the Random Forest model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Random Forest"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"--------------------------- a b --------------------------- --------------------------- 1085 1088 1090 1082 1078 1080 1085 1087 1091 1094 1097 1108 1115 1120 1195 Note that there is no indication of when the code was last modified. This code was generated on 20 May 2016. Data set updated on 22 April 2018 using the ""recompute."" code. NOTE: This dataset was generated on a single machine during a single run. No external modifications have been made to the code in either this dataset or other related data sets. This dataset may be modified by running the scripts with the appropriate options (e.g., to add a few extra columns to an existing dataset). The latest version can be found at The Github repository for this dataset. A note on source code: I have obtained the source code (which is available on GitHub) for the datasets mentioned in this article here. The source code can be found in the github repository under the heading ""R code for the benchmarks"". I believe that all of the code I refer to in this article is available in the source code repository. I have decided that I will not be making any changes to the code during this article and I ask that anyone who is interested in sharing some of the code I refer to in this article (and similar code) kindly contact me (David J. Reedy, e.g., on twitter). However, some minor comments are in order. I have provided a link to the dataset in the R code for this article even though the source code is in C. The reason for this is that I have decided that source code for the experiments is not the way to achieve reproducibility in this article. I have chosen to use the R code at the R package R-benchmark. As I say, this R package is not the standard package for benchmarking R code and there are"	False
"
 input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the FusionNet++ (ensemble) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FusionNet++ (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DocRED-BiLSTM model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DocRED-BiLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The last time that the FusionNet++ model was benchmarked on this benchmark, it was successfully run on the following RDFS datasets: (1.1 GB) - Bioinformatics: https://doi.org/10.7554/B02673D (521 MB) - Bioinformatics: https://doi.org/10.7554/S00012-00547.1000(13)00283-2 (1.1 GB) - Bioinformatics: https://doi.org/10.7554/B02673D  - Biomedica: http://www6.biomedica.com/article/S0025-7456(13)00634-9 (521 MB) - Bioinformatics: https://doi.org/10.7554/B02673D (1.1 GB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (1.1 GB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (2.9 GB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (521 MB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (3.9 GB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (1.9 GB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (6.9 GB) - Biomedical Imaging: https://doi.org/10.7554/B02673D (1.9 GB) - Biomedical Imaging: https://doi.org/	False
"
 input (English text): What is the best performing model benchmarking the WMT2014 French-English dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } /* Output: * The best performing model benchmarking the Oxford-IIIT Pets dataset in terms of GLOSS metrics */ Note: The query is generated by the WMT2014 WMT2014 French-English benchmarking model. input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of FLOPS metric ? output (Sparql query): select rdfs as the metric dataset name from the benchmark model; output (Sparql query): rdfs as the metric_lbl model lbl to use."	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AG News dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset AESLC?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 To run a model directly in your SQL database, open a single record with the.sql extension, then enter your query, and run'select name from [input] where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. }'; where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. }'. RDFS: Metric and Value Metrics in RDF Datasets The model's metric and value metrics are based on the Metric and Value Metrics. The Metric type supports many metric types, which are described in RFC 1515, and can also be specified as rdfs:type, but this is not recommended because it can be difficult to specify the metrics without using the Metric type, since it is not always recognized by RDF parsers. The Value type is required to use the Metric and Value Metrics. The following table lists the supported metric types and their corresponding values. A set of metrics, often referred to as a metric-family, specifies an attribute of a graph in which two properties exist. All metrics are calculated by dividing all of the data in the graph by the set of metrics. A Metric type that provides a value, or a set of values for a metric, can generally be created by defining a name for the metric. A set of metrics of this kind is called a Metric family. Any number of the Metric-family metric types can be combined as is, and a Metric family is composed of all of the possible combinations. The simplest way to generate a new Metric family class is to create a new class with a single property of type. The following example shows how to create a new Metric family class named family	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL (18 layers) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (18 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the AAN Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAN Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" output (Sparql query): The benchmark score is 18. I hope these help. Do you have a copy of the paper(s) that benchmark the Transformer-XL dataset ? Thanks again. Comments My email is [email protected]. We respond to comments within 48 hours. Please direct any other questions to [email protected] We respond to comments within 48 hours. Please direct any other questions to [email protected] I would find the benchmarked paper(s) on the AAN website (it doesn't have to be the one in question!) and then I paste a PDF of it into the comments of the first image. I believe the goal is to improve the AAN Corpus (which has quite good tools for summarizing text) and make the benchmark a high-quality estimate of the Transformer XL for that dataset. I like to keep up with what people are working on because that is important to me as well as helping them to use the dataset (that can be a good source of good data!). In some sense that will be helpful for people to be able to take the time to follow my comments on the benchmarks and to know that I am watching their progress. I don't think anyone will be surprised by how much better it all looks or how much more accurate it is as time goes on. I believe this is possible. I hope someone finds the information useful to them. In the comments this is what will show up: ""transformer-xl with 18 layers and 8 layers "" ""transformer-xl with 18 layers and 4 layers "" ""transformer-xl with 18 layers and 6 layers "" ""transformer-xl with 18 layers and 2 layers "" ""transformer-xl with 18 layers and 4 layers "" I see. If"	False
"
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC5CDR-chemical dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the BC5CDR dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?BASET. Now we have an array array( ?dataset_lbl) of the name of the model(s) with their metrics(es). As we are going to be querying a BATCH column, the query is very simple, because there is only one BATCH column. The result of the query might not be in the format that I need, as there is no way to get the dataset name or the metrics from the BATCH column. Let's see what we are going to do next. Our aim is to get the data to an SQL client via SELECT... Input(English text): What format is the dataset data in and what is the format of the data ? output (Sparql query): SELECT DISTINCT ?dataset_lbl_metrics ?metric SELECT DISTINCT ?dataset_lbl. input (English text): What format is the dataset data output (Sparql query): SELECT ?dataset_lbl. Now we just have to look at what the response is meant to be: output(Sparql query) or output(Sparql query, data): This is what we need it for. The data ( ?dataset_lbl_metrics, ?metric) is what we need to generate the value(s) that we should look at in the SQL query. Here is a short example of how to get the metrics of a model: data ( ?model ) : { F1 entity level score between 3 and 8 F1 entity level score between 3 and 8 F1 entity level score between 6 and 8 F1 entity level score between 3 and 4 F1 entity level score above 6 F1 entity level score above 8 }; OR data	False
"
 input (English text): Provide a list of papers that have utilized the Grave et al. (2016) - LSTM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2016) - LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ego-localization model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ego-localization"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the AcrE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AcrE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Can you provide links to code/data used in papers that benchmark the AcrK model and include the links to their code ? output (Sparql query): input (English text): Can you provide links to codes/data used in papers that benchmark the AcrD model and include the links to their code ? output (Sparql query): input (English text): Can you provide links to code/data used in papers that benchmark the AcrH model and include the links to the code for the AcrK ? output (Sparql query): input (English text): Can you provide links to codes used in papers that benchmark the AcrK and AcrH models and provide their code ? output (Sparql query): input (English text): If you're not comfortable using SQL, please tell us what the format is so we can avoid errors in our query.	False
"
 input (English text): What is major reactant for ""Microwave-Assisted Cobinamide Synthesis""?
 output (Sparql query): SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""Microwave-Assisted Cobinamide Synthesis"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P37557 ?reactant. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Solaris dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Supervised: dataset in terms of SemEval 2013 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SemEval 2013"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" classification.supervised.supervised1 def input (text): score = 0 percentile = 1 if ""0"" in text: score += 1 else: score -= 1 else: score = percentile for ""n"" in text: score = score - p 1.0 * score / (score % 2.0) classification.supervised.supervised2 def input (text): score = 1 percentile = 2 if p.starts_with(""0""): score += 2 else: score -= 2 else: score = percentile for ""n"" in text: score = score - p 1.5 * score / (score % 2.5) classification.supervised.supervised3 def input (text): score = 1,2 percentile = 3 if p.starts_with(""0""): score += 3 else: score -= 3 else: score = percentile for ""n"" in text: score = score - p 1.0 * score / (score % 2.0) 1.0 * (score / 2.0) * (score / 2.0) * score * (score / 2.0) * score classification.supervised.supervised4 def input (text): score = 2,3 percentile = 4 if p.starts_with(""0""): score += 4 else: score -= 4 else: score = percentile for ""n"" in text: score = score - p 0.5 * score / (score % 4) * score classification.supervised.supervised5 def input (text): score = 2,4,6 percentile = 5 if p.starts_with(""0""): score += 5 else: score -= 5 else: score = percentile for ""n"" in text: score = score - p 0.5 * score / ("	False
"
 input (English text): Can you list the models that have been evaluated on the AI-KG dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AI-KG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Name This Game dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" input (English text): Given what input data is there output (Sparql query): The output will look something like this: output (JavaScript): $ curl 'https://s3.amazonaws.com/t2.com/e2/e2.json ?output=jsonp&outputType=text' # A simple'model' to visualize the results of the benchmarking. model: function(input, output, modelset,...) {... } # A model that helps us to understand the'mechanisms' of how data is collected and stored var modelBin = function(input, output, modelset,...) {... } # A 'feature' to help to understand if the model has 'quality' or not var qualityBin = function(input, output, modelset,...) { # A feature to give us information about which'model types' are involved var modelTypes = [] modelTypes.push(""data"", modelBin) modelTypes.push(""data"", modelBin, ""data"") modelTypes.push(""data"", modelBin, ""modelType"") modelTypes.push(""modelTypes"", modelBin, ""modelType"", ""type"") modelTypes.push(""data"", modelBin, ""type"", ""modelType"") ... modelTypes.push(""value"", input) modelTypes.push(""model"", qualityBin(output)) modelTypes.push(""model"", qualityBin, ""modelType"") name = modelset.find('name', function(err, model) { modelTypes.push(""modelType"", modelType) modelTypes.push(""model"", qualityBin(output)) } ) # A 'variate' that is called"	False
"
 input (English text): Could you provide a list of models that have been tested on the CoNLL 2012 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset EBM-NLP?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""EBM-NLP"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What quantity of iron oxide was discovered on Elorza crater?
 output (Sparql query): "	"SELECT ?properties_values, ?property_description WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""Elorza crater"", ""i"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""FeO"")) ?properties orkgp:description ?property_description. } "	SELECT ROW_NUMBER * FROM ( SELECT ROW_NUMBER(2,20) FROM ( SELECT ROW_NUMBER(4,20) FROM ( SELECT ROW_NUMBER(3,20) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,20) FROM ( SELECT ROW_NUMBER(3,20) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,20) FROM ( SELECT ROW_NUMBER(2,20) FROM ( SELECT ROW_NUMBER(3,20) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,40) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(3,20) FROM ( SELECT ROW_NUMBER(3,40) FROM ( SELECT ROW_NUMBER(2,40	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TACRED dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TACRED"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Cornell Grasp Dataset dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): Can you list any other models that are being evaluated on the TACRED dataset ? output (Sparql query): input (English text): What training method(s) did you use to evaluate the models (e.g., R, SPSS, Stata) ? output (Sparql query): input (English text): What was the R code (which would have used the R code) ? output (Sparql query): input (English text): What did the SPSS code (which would have read the data) look like ? output (Sparql query): input (English text): What kind of data were you looking at (e.g., CSV, GAN) ? output (Sparql query): input (English text): What was the SPSS library (e.g., R, SPSS, Stata) ? output (Sparql query): input (English text): What are your current R installation(s) ? output (Sparql query): input (English text): What are R installation(s)/paths ? For the most part, we recommend: * Windows: You can download R here. * Mac OSX: This is how it will look for R 3.1: cd /Users/<Your_name>/R * Linux: It's a bit more complicated, but read this. * OpenSUSE: install from sources (you can use zypper to install packages). * Ubuntu: check out the official R package repository. * BSD: install from source. * Solaris: It should be self explanatory as you can find detailed instructions here, but some of the installation scripts may work on Solaris. input (English text): Is there anything you would like readers to note	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Text8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Text8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the DQN-PixelCNN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-PixelCNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Output 2 Input data: List the number of sentences that were ranked (most to least) within each paper ? output (text): List the score for each paper ? output (Rdf) Rdf (text): Input list that was used to create the pixel-CNN model. output (Rdf) output (Rdf): Rdf (data): List that was used to create the neural network (the source datasets). output (Rdf): Rdf (text): Input list that was used to train the model and classify the sentences. output (Rdf): Rdf (data): Output list of papers that utilized the data source Rdf (text): Input list that was used to train the model. output (Rdf): Rdf (text): Output list of papers that utilized the data source Rdf (text): Input list that was used to train the model and classify the sentences. output (Rdf): Rdf (text): Input list that was used to train the model and classify the sentences. output (Rdf): Rdf (text): Input list that was used to train the model and classify the sentences. output (Rdf): Rdf (text): Input list that was used to train the model and classify the sentences. output (Rdf): Rdf(text): Rdf (text): Output list of papers that utilized the data source Rdf (text): Indicate the model that performed the best on the RNN benchmark dataset ? output (Rdf) output (Rdf): Rdf (text): Input list that was used to train the model and classify the RNN benchmarks. output (Rdf): Rdf (text): Indicate the model that performed the best using the DQN-Pixel	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the word BiLSTM + char CNN + CRF model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""word BiLSTM + char CNN + CRF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Freeway dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?num_parameters AS num_parameters FROM a.params WHERE { ?parameter a.params ? }. LIMIT 1 input (English text): What is the size of your dataset, for example, how many hours is one hour ? output (Sparql query): input (English text): Which code link is that in the paper ? output (Sparql query): input (English text): Which model and which class is that in the paper ? output (Sparql query): 	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2014 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the CvT-W24 (384 res, ImageNet-22k pretrain) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24 (384 res, ImageNet-22k pretrain)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Dynamic Coattention Networks (single model) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT COUNT(1) FROM books ORDER BY title DESC; output (Sparql query): SELECT name, name1, title, title1, title2, title2; Please specify a list of papers (top 20 and last 20 papers) that provide benchmarked methods for a model that provides the SVM output (i.e., training, test, evaluation, regression, unsupervised. In general, I prefer papers that provide a range of models and benchmark them all, since this makes my understanding of the research literature more coherent than if I had chosen only the papers that benchmarked the WMT2014 model. There are 2 ways to select this information. First is to look up the paper number at the top of this page and pick a paper based on this number. Next is to search for the paper by abstract, name and description for that paper number. The WMT2014 Model is also a very well-studied model; for example, see this WMT2014 article   This is what the WMT2014 authors have been using (see the source code). In particular, I would recommend you to check out the following papers: M. H. van der Heijden and R. A. Visscher (2014), ""An Improved SVM on the SVM with a New Model-Training Order,"" Numerical Analysis K. S. Osterwalder, M. Dang, I. K. Vyas, S. P. N. Nair, A. Maksimov, and D. P. Raja (2014), ""Sparse-Modeling Theorem,"" arXiv:1409.4229, dx.doi.org/10.1708/0935-3389.1  This paper explores the model-training order on the SVM by introducing a"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Oxford 102 Flowers dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Macro Precision metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl( ?metric ?metric_lbl )* ?metric AS ?matric_lbl( ?metric ?pattern) WHERE { { SELECT ?pattern ?matric rdfs:label ?pattern_lbl. ?metric rdfs:label ?pattern_lbl.( ?pattern) OR ?pattern rdfs:label ?matric_lbl.( ?pattern_ ) ORDER BY ?pattern. ?metric ORDER BY ?pattern_lbl. } OPTIONAL { ?matric pattern } } ORDER BY DESC( ?metric_lbl ) AND ?metric AS ?pattern_lbl. The response appears to be limited to the two most prominent models, both the ""Exp"" and ""ARQ"" models. The ""Exp"" model was able to perform very well in the Exp metric, but in the case of both the ""Exp"" and ARQ metrics, the ""Exp"" performed better than the other models. The NLP-TDMS test data provided only the metric ""Exp"". Other metrics like ""Arithmetic Precision"" and ""Mnemonic Recall"" might well have an impact in order to improve the performance of this model. This may be due to the performance of the other models in the test set. output (Sparql query): SELECT DISTINCT ?pattern ?pattern_lbl( ?metric ?pattern)* ?pattern AS ?pattern_lbl( ?pattern) OR ?pattern rdfs:label ?pattern_lbl.( ?pattern) ORDER BY ?pattern. ?pattern ORDER BY ?pattern_lbl. input (English text): Identify the paper published on December 18, 2009 that is referenced in your previous email, that has"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the AAN Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAN Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Tennis dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"  { id:0, score:0, avg_sec:10, max_sec:1008, avg_score:0, avg_score_min :9.0, average_score :0, averages_min :0, average_score_max :4.0, average_score_min :3.0, average_score_max :2.0, score_min :0, stats_min :0 } Input files and output files The SPSS R package provides a number of standard input and output data files: The datasets are extracted from the SearchQA Data Kit. The source data for these datasets are also supplied with the source code. The results are the most recent data in the search-ranking-analysis repository in a single XML file: search.xml Each dataset is grouped into categories for categorizing the results. Each category is represented by a set of sub categories and each category has a group ID and the search order within that group. All the different search algorithms are represented in this data file. The search query used for each dataset is a subset of its category. For example, the search query for ""AAN"" is not ""searchQA.AAN"". Instead the query is ""AAN.name"". For each of the four datasets there are some pre-defined categories. For example, one can assume that for the Tennis dataset, it is the category ""P31"". It is the search-ranking-analysis dataset that is the default. The files for the SearchQA dataset consist of two files: The SPSS R package provides a number of standard input and output data files: The output data for all the individual categories is organized by a sub-category ID. Each sub-category has a list of individual categories, the search order in that sub-category and results for"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Ice Hockey benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the CommitmentBank dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Metric; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Duie"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. /* * * I'm going to be using the data I just fetched... */ output (Sparql query): SELECT ?dataset ? ? ? ? ? ? ? ? from ? ? ? ? ? ? ? ? ? ? ? ? ? ?dataset_lbl WHERE { ?db lbl_t1 ?db_lbl ?db_s1. FILTER (str( ?dataset_lbl) = L "" ?"" ) ?benchmark orkgp:HAS_DATASET ?dataset ; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. /* * * I'm going to be using the data I just fetched... */ If I only need to look at one metric, which one would I look at instead of each one of the metrics ? If none of the metrics have a parameter, what would I do instead of the model output ? If all the metrics have a parameter, which one ? Which one is better to look at or the results from all models ? How often would I need these results ? (A minimum of once a week). What kind of accuracy score is acceptable for each metric ? What do the names mean for the different metrics ? (What did you mean ? ) Would it be possible to create a model"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Q*Bert?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the GAD benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl (GASPSUBJECTS(7, 3) AS ?score) AS ?score, count(*) FROM ?dataset_lbl a GROUP BY ?metric ?metric_lbl ORDER BY DESC( ?score) output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl FROM ?dataset_lbl a, ?score, ?score ORDER BY DESC( ?score)  select count(*) FROM ?dataset_lbl a select count(*) from ?dataset_lbl a, 1, ?score ORDER BY DESC( ?score)  select count(*) FROM ?dataset_lbl a, 1, ?score ORDER BY DESC( ?score) input (English text): List the metrics used for benchmarking on Cheetah, run (DMControl500k) ? output (Sparql Query): SELECT DISTINCT ?dataset_lbl ?score, ?score, ?metric_lbl AS ?metric, ?result ORDER BY DESC( ?score) output	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Private Eye dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Private Eye"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Where can I find code references in papers that have used the BiTT model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiTT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	-------------------------- RAVENBERRY_HRCV1 RAVENBERRY_HRCV2 COLDSEASON RAVENBERRY_COLDSEASON RAVENBERRY_HRCV2 input (English text): How many users are on-line at: http://www.hirschenzeitung.com/ output (Sparql query): -------------------------- RAVENBERRY_USERS_HRCV1 RAVENBERRY_USERS_HRCV2 1 input (English text): You would like to download a CSV file containing the benchmark results from the Reuters RCV1/RCV2 Standard English-to German Benchmark dataset ?	False
"
 input (English text): Can you list the metrics used to evaluate models on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the SST-2 Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What types of nanocarriers do have therapeutic effect?
 output (Sparql query): "	"SELECT ?types_of_nanocarriers, ?types_of_nanocarriers_labels WHERE { orkgr:R144364 orkgp:compareContribution ?contrib. ?contrib orkgp:P41740 ?types_of_nanocarriers. ?contrib orkgp:P41743 ?therapeutic_effect. FILTER( ?therapeutic_effect = ""T""^^xsd:string) ?types_of_nanocarriers rdfs:label ?types_of_nanocarriers_labels. } "	 Input Text : Nano carbodes on difusing a nanorobotic matrix that exhibits the following properties: 2-nanometre diameter - not less than 2,000 nm, larger than 3,000 nm, respectively (1 nanometre = 10 000, 000 nm)  - not less than 2-nanometre diameter, larger than 3,000 nm, respectively input (English text): Please provide results in the form of a sentence (no quotes required) output (Sparql query): Select all rows with a nanocrystal on the following columns, as seen above. input (English text): Please give examples/case studies of Nanocrystals with good therapeutic result in the form of: Case studies : What treatment did you use ? In the past or would you ever use it again ? output (Sparql query):  Case studies : Which cancer cells, if any, were grown in the nanoparticle chamber ? output (Sparql query): input (English text): Are there any diseases that have this effect on their tumors ? output (Sparql query): input (English text): What is the optimal dose for nanocrystals in which nano carbodes as well as the nanorobotic matrix are combined ? output (Sparql query):  Maximum Effective Dose - the maximum dose that you can put nanocrystals (and the nanorobotic matrix) in (if the nanocrystals can support the nanorobotic matrix) input (English text): How could nanocrystals be used ? output (Sparql query):    Nano - Carbodes (single nanocrystal): The optimal combination of nanocrystals for nanomaterials:  - nanocrystals in combination with nanorobotic matrix input (	False
"
 input (English text): What are the most commonly used benchmark datasets for the Semantic Textual Similarity research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Textual Similarity"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the CL-SciSumm dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" What is the number of classes in the ACL-ARC benchmark dataset ? input (English text): What is the name of the top performing model in terms of ROUGE-2 score when benchmarked on the BH-SciSumm dataset ? output (Sparql query): What is the number of classes in the BH-SciSumm benchmark dataset ? input (English text): In which benchmarks did the top performing models find the value that best fits the problem in terms of ROUGE-2 score ? output (Sparql query): Which of the two following labels is used to show the difference between a class label of ""X"" vs ""X'"" ? input (English text): X' output (Sparql query): X and X' are separate classes, not classes or classes' labels. Which of the two following labels is used to show the difference between a test label of ""X"" vs ""X'"" ? input (English text): X' output (Sparql query): X and X' are separate classes, not classes or classes' labels. Which of the following labels is used to show the difference between a problem attribute label of ""X"" vs ""X'"" ? input (English text): X' output (Sparql query): X and X' are separate classes, not classes or classes' labels. Which of the following labels is used to show the difference between problem parameters' A versus B classes ? input (English text): X' and X' are separate classes, not classes or classes' labels. output (Sparql query): X and X' are separate classes, not classes or classes' labels. Which of the following labels is used to show the difference between problem parameters' A versus S classes ? input (English"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kuzushiji-MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the NCBI_BERT(large) (P) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(large) (P)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Unsupervised NMT + weight-sharing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT kway: ?where (!str( ?kway) ) || kway:R1_DIST2 / 3 ?kway / 4 ? ORDER BY ?kway ASC LIMIT 1 BY ?kway input (English text): Are there any studies that have used the k-way cross-validation of the k-nearest neighbors model in any benchmarks ? output (Sparql query): input (English text): Name or title of the study ? output (Sparql query): input (English text): Identify the benchmark/study that you have written in relation to the following title(s) or study(ies) or study/s in the title(s) ? output (Sparql query): input (English text): In which language is the study described in the text ? output (Sparql query): input (English text): In which language is the study described in the title(s) ? output (Sparql query): input (English text): In which language is the study described in the title(s) ? output (Sparql query): If you could identify the key words (i.e. concepts) in the article, include those in the comments and/or the paper ? Note that if all the relevant parts are identified in the response (e.g. key phrases, concepts), we may use these (e.g. for data mining), if the original paper does not explicitly list them. There are three possible reasons why we might want to use preprocessing/decompatibility functions: first due to the lack of suitable	False
"
 input (English text): Which model has achieved the highest BLEU score score on the IWSLT2015 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the HMDB51 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Permuted Accuracy score on the Sequential MNIST benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Permuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT CUSTOMER_ID, CUSTOMER_SUM, BLEU_BASE, BLEU_CMP, CUSTOMER_STATUS, BLEU_CFLAGS, CUSTOMER_LANGUAGE FROM customers WHERE BLEU_CFLAGS = 1 OR BINGE_CFLAGS = 2 OR BINGE_SUM >= 1 output (Sparql query): SELECT CUSTOMER_ID, CUSTOMER_SUM, BSEQ_CMP, CUSTOMER_STATUS, BSEQ_CFLAGS, CUSTOMER_LANGUAGE FROM customers WHERE BSEQ_CFLAGS = 1 OR BINGE_CFLAGS = 2 OR BINGE_SUM >= 1 #4. The IWSLT2015 English-German benchmarking task results in a BLEU score of 30, and a BLEU per-record accuracy score of 72. #5. If the model is capable of executing a 10,000 training and testing sets, the models performance will be optimized by over 50 percent. #6. BLEU accuracy score should not be expected to change much after each training set, yet the model should perform well over the long run in terms of top-1 Accuracy metric from 10,000 training and testing sets. #7. BLEU score is most robust when the accuracy is constant over the model's lifetime, but will decrease more quickly than Top-1 Accuracy metric over a longer time period. #8. When the Model performs well, then the performance will improve over 1,000,000 testing and training sets until the model reaches its absolute maximum performance. #	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the MRPC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the GPT-3 175B (Few-Shot) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3 175B (Few-Shot)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the SciTLDR dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Question 7: What are the next steps ? Answer 4: Answer 4: Do you think that a large fraction of the published papers are invalid ? Question 8: Where can we find more details about the SciTLDR data set, including its authors, year of publication, and title ? Answers 1:  You can download it from the SciTLDR website.  The first download covers 2002 (data collection period), and the second includes latest release data, 2016 (dataset update year 2016).  If you prefer other methods, we will list them as appropriate.  In general, the data is available by invitation only, so there will be more updates to the dataset as new questions arise.  In particular, we will keep an eye out for a possible re-design of the methodology to remove spurious performance measurements. Answer 2:  I see no reasons why there should be any difference here.  For one thing, we're looking at papers written in 2011, and the datasets are only available from that year.  Second, our dataset starts in December 2009.  There are also no issues, because the authors are identified with the correct year of publication. Question 9: What is the status of the current research effort on the MRPC-3 dataset ? Answers 1:  There is a continuing effort to improve the quality of the test set.  We hope to complete the evaluation in Summer or Fall of 2018.  If this assessment is postponed, then in the mean time we will probably conduct a new batch of testing (i.e. evaluate the different test suites in several different tests) and have another batch ready for testing in Early 2019.  That should give us at least six months to finish the test set.  This is not guaranteed, of course, but we have tried to do our best.	False
"
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the most commonly used benchmark datasets for the Continuous Control research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Continuous Control"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the metrics that are used to evaluate models on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT list( ?dataset_lbl.c_label, 1) ?metric_name, list( ?metric_name, 2) ?metric_value from c_metrics_test_benchmark WHERE { ?question ?metric_name; list( ?metric_name, 3) ?metment_value } input (English text): List the metrics that form part of the Common SenseQA benchmark of the Continuous Control research field ? output (Sparql query): dataset (string): Standard, or other, metric(s) of the benchmark. data (string): 1. metric name 2. metric value 3. input (English text): List the metrics that have been identified as the input to the Common SenseQA benchmark ? output (Sparql query): data (list): 1. metric name 2. metric value 3. input (English text): Which benchmark has been validated according to the Common SenseQA benchmark of the Continuous Control research field ? output (Sparql query): SELECT c_met_test_statements.c_statements.c_benchmark as c_statements, LIST c_met_test_statements.c_statements AS c_test_statement, LIST c_met_test_statements.c_statements_df as c_tests_dF FROM c_test_data_dsl GROUP BY c_met_test_statements. c_statements ORDER BY c_statements ; data (list):	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the DBpedia dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the IMDb-M dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT a : Metric FROM.... ORkgp OR kgp ORDER BY metric DESC( ?value ) We have obtained three metrics (score, performance and error): 1. Metric Score 3.8 (average) 2. Performance 5.4 (average) 3. Error 8.0 (average) We see significant variations in scores of 4.1 for the metric 'best-fit' and 4.8 for the metric 'best-in-category', which we calculate with the following equation: Note: you could get 'best-fit' and 'best-in-category' in 'best' and 'average' using the same input. 2. Performance Score 8.0 (average) 3. Error 10.3 (average) We show here two different algorithms, both of which produce similar results, if the data is in English, and there is a constant metric. For example, it is not a surprise that the 'best fit' algorithm produces an RMS value of 10.3, whereas the 'best-in-category' algorithm produces 11.6, the result in the second data set as well, given the difference in the metrics. 4. Metric Best Fit 4.1 (average) 4.2 (best-in-category) 5. Performance Best Fit 11.3 (average)	False
"
 input (English text): What is the best performing model benchmarking the WOS-46985 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the PROTEINS benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Scholarly entity usage detection dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Scholarly entity usage detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 output (Sparql command): select high from high_benchmark where rdfs : 'PROTEINS' output (Sparql command): select high  from high_benchmark_latam and high  from high_{dataset}_latam ORDER BY DISTINCT RDF S/M RANK (highest score) ASC output (Sparql command): output (Sparql command): select high from high_benchmark_metrics and high  from high_benchmark_metrics_latam ORDER BY DISTINCT RDF S/M RANK (highest score) ASC output (Sparql command): output (Sparql command): select high from high_benchmark_metrics_low_score and high  from high_benchmark_metrics_low_score ORDER BY DISTINCT RDF S/M RANK (highest score) ASC output (Sparql command): select high  from high_benchmark_metrics_low_score_latam and high from high_benchmark_metrics_low_score_latam ORDER BY DISTINCT RDF S/M RANK (highest score) ASC output (Sparql command): output (Sparql query): select high from high_benchmark_metrics_low_score and high  from high_benchmark_metrics_low_score ORDER BY DESC(high) ASC In this table you'll see a few things which you're likely to see when writing your own code to measure performance: RDF output: All the RDF values for each test result. To make the output of your function easier to read on the desktop, create	False
"
 input (English text): Can you list the metrics used to evaluate models on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the Hendrycks Test dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?test.model_name, ?test.model_type, ?test.criterion_key, ?test.criterion_value, ?test.criterion_lbl. CRITERIA key. OPTIONAL CRITERIA value. TRIM key. OPTIONAL test.criterion_lbl. HASH key. OPTIONAL R2 key. BLAIR key. OPTIONAL SPCKEY key. DIFFKEY key. DIFFCKEY key. DIFFPKEY key. SPCKEY KEY. DESTKEY KEY. DEPKEY KEY. DEPCKEY KEY. DEFPKEY KEY. PKEY KEY. PESTKEY KEY. PESTCKEY KEY. CKEY KEY. SKEY KEY. SPCKEY KEY. SPCKEY KEY. SPCKEY KEY. SPCKEY key. SPCKEY key. SPCKEY key. SPCKEY key. SPCKEY key. SPCKEY key. SPCKEY key. SPCKEY key. SPCKEY _value. CRITERIA key. DISTINCT TEST.model_name, TEST.model_type, TEST.model_criterion_key, TEST.model_criterion_value, TEST.model_lbl. R2 key. DISTINCT PRIMARY_EXCEPTION.PRIMARY_EXCEPTION.R2key. DISTINCT SECONDARY_EXCEPTION.SECONDARY_EXCEPTION.SEC2key. R2 key. R2 key. R2 key. SPCKEY key. R2 key. SPCKEY key. DIFFKEY key. DIFFCKEY key. DIFFPKEY key. SECKEY key. CRITERIA key. HASH key. HASH key. HASH key.	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Natural Questions dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Bowling dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } }  You can find sample results for your language (C, Java, Python, C++, etc.) here. Conclusion: With these exercises you have gained an understanding of how an online database works and are in a good position to understand why you should go to a data warehouse for your future datasets. The exercises were based on the concepts and algorithms presented, you can continue through the exercises using Python or C++, and the source code is available online (or download the Python or C++ version). Feel free to share the exercises with others on the Web. Your feedback is greatly appreciated"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the Atari 2600 Private Eye dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Private Eye"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Sequential MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP (8) c1 c2 d 1 (C) RPN_RESULT (n_rpn) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 (C) Atari_RESULT (n_rpn) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 T ( C) Atari_RESULT (n_rpn) 1 2 3 4 5 6 7 8 9 10 11 12 13 Atari_RESULT (n_rpn) 1 2 3 4 4 5 6 7 8 9 Atari_RESULT (n_rpn) 1 2 3 4 5 6 7 8 7 Atari_RESULT (n_rpn) 1 2 3 4 5 6 7 8 7 7 Atari_RESULT (n_rpn) 1 2 3 4 5 6 7 4 Atari_RESULT (n_rpn) 1 2 3 4 5 Atari_RESULT (n_rpn) 1 2 3 4 5   input (English text): Will the results of this work be available as a pdf file ? If so, please do you own any copyrighted work that could be used in the data analysis ? output (Sparql query):  SELECT TOP (1) c1 c2 d 1 FROM ( SELECT DISTINCT ?dataset  and ?model 	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Montezuma\'s Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the CRF with sentence expansion model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CRF with sentence expansion"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" A: If you see that this statement has been included in a published paper and is not being cited, please alert us. You can find more detailed guidelines about publications on the publication guidelines page. Q: What do our results mean ? A: With a little bit of research, it seems this is a very easy and common way to accomplish clustering and prediction, but if you look at how this can be broken down, you can quickly see the problems with relying mostly on sentence expansion in a classification. Most papers do not do sentence expansion properly and most papers have misclassified results that are not predictive. You can see a few examples of incorrect results we had during our work here and here. Q: How do I test for clustering errors ? A: As with all things modeling, once you start practicing, you need to test against multiple data sets, with different metrics, with different statistical assumptions. Before giving up, you should start testing against the ""correct"" examples on the RDFS. If the test was not able to tell a difference between data sets, you can test again with new data. If you can tell a difference between the two, then it is probably a valid method. If the test was able to tell you whether a particular method can be used, then it is definitely likely a valid method. For each test, you should check to ensure that the following criteria are met: Cluster is close to 0.5. It is a small subset of the entire dataset. Cluster was used, not simply a result from the technique. Q: I want to add more sentences to my dataset, but the sentence expansion method is good enough for most cases. How do I add more to my datasets ? A: There is often a debate with the community about whether a technique needs to be used. A useful practice to keep in mind is that one"	False
"
 input (English text): Can you list the metrics used to evaluate models on the UCF101 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the most commonly used benchmark datasets for the Data-to-Text Generation research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Data-to-Text Generation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset NYT-single?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT HAVING c = 1 OR c 2 OR c 3 OR c 4 AND c 5 OR c 6 OR c 7 OR c 8 FROM [dataset_orkgc:NYT-single], { ?max_value c ?metric_lbl } WHERE s = 1 OR s 2 OR s 3 OR s 4 AND a OR s 5. LIMIT 10 ORDER BY c,metric_lbl input (English text): Where are we now ? output (Sparql query): input (English text): The results from your comparison are: Using only benchmark datasets on your existing dataset is not guaranteed to be scalable. This means that your comparison is not a scientific/technical study on your data. For example, it is unclear whether this project can be run from a machine that has access to many different types of machines. The following is a suggestion; I feel that ""a"" is not a meaningful column in the dataset, but we'll see how that holds when running the code. Using multiple benchmark datasets would make it easier to find the best results for your problem, as each of your benchmark datasets are unique. If you're only interested in the max_value for your problem, then all the benchmark datasets would be included in your query. If you're interested in the sum of the values, you must use a subquery to select all your benchmark datasets and compute the sum. (See The question How do I see the best benchmarks ? for further detail.) You also would get access to new benchmark metrics, which are more useful and precise than the ones we used for last year's project. You should also know, however, that the benchmark problem was solved last year, at the same time as the training challenge was solved. In fact, the data set that we used for the second round of the"	False
"
 input (English text): What was the most common type of approach for summarization before 2002?
 output (Sparql query): SELECT ?approach ?approach_label WHERE { orkgr:R6948 orkgp:compareContribution ?cont. ?cont orkgp:P15 ?implementation. ?implementation orkgp:P5043 ?approach. ?approach rdfs:label ?approach_label. } ORDER BY DESC(COUNT( ?approach_label)) LIMIT 1
 input (English text): Could you provide a list of models that have been tested on the Kuzushiji-MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kuzushiji-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the SciTLDR benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT SUM( ?kvs :. val) FROM KVS kvs_kv WHERE kvs = 1 AND kvs < 1. input (English text): What are you using for rating your models ? output (Sparql query): Input data: Output data: Input data: Output data: Input data: Output data: Input data: Output data Input data: Output data (Sparql query): SELECT S.rating_value, S.approach_rating_value, S.approach_approache_rating_value, S.approach_rating, S.approach_approach_val_value FROM ?approaches ?approaches_s GROUP BY S.approach_approach_approach_value; input, output, input (English text): What are some of the best and most suitable alternatives to KVS, compared to Kuzushiji MNIST ? output (Sparql query): SELECT ?rating_value1 ?rating_value2... FROM ?approaches_approaches_1, ?approaches_approaches_2, ?approaches_approaches_3, ?approaches_approaches_4, ?approaches_approaches_5, ?approaches_approaches_6, ?approaches_approaches_7, ?approaches_approaches_8, ?approaches_approaches_9, ?approaches_approaches_10, ?approaches_approaches_11, ?approaches_approaches_12, ?approaches_approaches_1, ?approaches_approaches_2, ?approaches_approaches_3, ?approaches_	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Atlantis dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Atlantis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Yelp-14 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the WMT2016 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { orkgc:Datum.LASTORG( ?dataset_lbl, :value ) ?metric rdfs:label ""Model"". OPTIONAL { ?metric model_lbl. ?metric model_lbl. OPTIONAL { ?metric model. ?metric model_lbl. OPTIONAL { ?metric model_lbl. } } OPTIONAL { ?metric model_lbl. } } OPTIONAL ORDER BY 2DESC( ?value ) } } input (English text): Which of the 10 models performed the best on the Yelp-10 dataset ? output (Sparql query): SELECT _dataset_lbl_lbl( ?value IN ( ?dataset_lbl ), ?metric_lbl( ?value, :value ) AS ?score ) AS ?model_lbl FROM ( SELECT DISTINCT orkgc AS dataset_lbl, rdfs FROM ( SELECT RANK() AS top, RANK(rank) AS middle FROM ( SELECT TOP(order) as order1, TOP(order2) as order3 FROM ( SELECT TOP(order) as order4 FROM ( SELECT TOP(order) as order5 FROM ( SELECT TOP(order) as order6 FROM ( SELECT TOP(order) as order7 FROM ( SELECT TOP(order1) as top, RANK() AS middle FROM ( SELECT TOP(order2) as order1 WHERE (ORDERER() BETWEEN 1 AND 2 >= 1 AND order2 > 1), ORDER(1.1) >= 1.1 AND order3 < ORDER(2.1) AND order3 > ORDER(3.1)) ) ORDER BY RANK(),"	False
"
 input (English text): What is the top benchmark score and its metric on the AAN Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAN Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Freeway dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT MAX( ?value) FROM ?benchmark ?; input (English text): What is the model that performed the best on the AAN Corpus in order to compare it to (a pre-trained model) and which feature of the AAN corpus is being used ? output (Sparql query): SELECT CASE WHEN ?value - COUNT_3_4 = 0 THEN '0' ELSE '.3.1' END AS ?feature, CASE WHEN DISTINCT ?feature FROM ?benchmark ? ORDER BY CASE WHEN ?value = '2' THEN '.2.3' WHEN DISTINCT ?feature FROM ?benchmark ? ORDER BY CASE WHEN ?value > 1 THEN '1' WHEN DISTINCT ?feature FROM ?benchmark ? ORDER BY CASE WHEN ?value - 1 = 1 THEN '1.1' WHEN DISTINCT ?feature FROM ?benchmark ? ORDER BY CASE WHEN DISTINCT ?feature = '1.0' THEN -1 ELSE NULL; input (English text): Give me the maximum allowed number of features in the AAN Corpus ? output (Sparql query): Input data type is integer. Output data type is str. output (Sparql query): Where is the maximum allowed number of features ? input (English text): Give me a list of research papers that have performed benchmarks on the Atari 2600 Freeway dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset e_hc:Compound[datas	False
"
 input (English text): What is the highest benchmark result achieved on the HoC dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Yelp-14 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT VARIABLE ?dataset FROM rotowire WHERE rdf:Metric = ?dataset AND ?dataset rdf:Label = ?dataset. AND c:N = ?dataset_lbl LIMIT 1 ORDER BY DESC( ?value ) } RotoWire (Relation Generation) has a dataset of about 2000 labels. How many data points did RotoWire generate with a given number of labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels. Which set of 1st order correlations did a given RotoWire-18 label generate, using the given RotoWire-18 labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels. Which set of 1st order correlations did a given RotoWire-13 label generate, using the given RotoWire-13 labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels. Which set of 1st order correlations did a given RotoWire-26 label generate, using the given RotoWire-26 labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels. Which set of 1st order correlations did a given RotoWire-30 label generate, using the given RotoWire-30 labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels. Which set of 1st order correlations did a given RotoWire-25 label generate, using the given RotoWire-25 labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels. Which set of 1st order correlations did a given RotoWire-32 label generate, using the given RotoWire-32 labels ? RotoWire(Relation Generation) has a dataset of about 2000 labels	False
"
 input (English text): Where can I find code references in papers that have used the SRU++ model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BERTjoint model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTjoint"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Does the BERTjoint model perform better than DBSCAN on the Easy dataset ? output (Sparql query): input (English text): Is the BERTjoint model (DBSCAN) better performing than Reacher in terms of Score metric ? output (Sparql query): Input input (English text): Do you know any benchmarks for the BERTjoint model used by other papers ? output (Sparql query): input (English text): Where can I find high quality literature that includes detailed benchmarking against this model ? output (Sparql query): output (Sparql query): input (English text): How reliable is the BERTjoint model ? output (Sparql query): Input text: A lot of different papers used in benchmarking have used a lot of different models, it is hard to compare models. The BERTjoint model seems reliable enough, and it is still being benchmarked in lots of papers. output (Sparql query):	False
"
 input (English text): Where can I find code references in papers that have used the Compressive Transformer (24 layers) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Compressive Transformer (24 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of benchmarked datasets related to the Word Embeddings research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Word Embeddings"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	--------------------------- LAPACK Model 1 0.0003 0.00000 5.3% 1.000 TALLS Model 15 0.0018 0.0000 -0.1% 1.000 SSAQ Model 31 0.0011 0.0000 4.1% 3.000 SSPD Model 1 0.0041 -5.7% 6.000 AGRL Model 0.0003 0.00000 0.7% 1.000 MASS Model 0.0000 0.0000 -0.3% 0.000 SSAQ_MATERIAL Model 0.0012 0.0000 0.3% 0.000 MODEL 0.0000 0.0000 0.3% 0.000 LAPACK_STACK Model 0.0016 0.0000 0.1% 0.000 LAPACK_STACK Model 0.0015 0.0000 0.1% 0.000 CHECK_LEVEL Model 0.0000 0.0000 0.3% 0.000 CIFAR_MOVELEN Model 0.0000 0.0000 0.5% 0.000 LABELIST Model 0.0000 0.0000 0.6% 0.000 LAPACK_STACK_FLOAT Model 0.0015 0.0000 0.4% 0.000 LAPACK_STACK_FLOAT Model 0.0014 0.0000 0.5% 0.000 LAPACK_STACK_FLOAT Model 0.0013 0.0000 0.5% 0.000 LAPACK_STACK_FLOAT Model 0.0012 0.0000 0.2% 0.000 CHECK_LEVEL_MATERIAL Model 0.0000 0.0000 0.5% 0.000 CASCADE Model 2 0.0000 0.0000	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2014 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQNMMCe+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT reference, DESC(reference) FROM title ORDER BY reference DESC(reference) output (Sparql table): TABLE output(Sparql table) PARTITION by reference ORDER BY reference DESC(reference) output (Sparql spreadsheet) UPDATE orKmstmt( 1, orkgc:Dataset, title, benchmark = NULL, description = NULL, id = NULL, name = NULL,...,..., orkgp:IECONTEXT ) [ { orkgc:Dataset }, { metric: metric_lbl } ] SELECT reference, ?val AS ?value, DESC(reference) FROM title; ORDER BY reference DESC(reference) output (Sparql query): select reference, ?val AS ?value, DESC (reference) FROM title ORDER BY reference DESC(reference) output (Sparql table): TABLE output(Sparql table) PARTITION by reference ORDER BY reference DESC(reference) output (Sparql spreadsheet) UPDATE orKmstmt( 1, orkgc:Dataset, title, benchmark = NULL, description = NULL, id = NULL, name = NULL,... ) [ { orkgc:Dataset }, { metric: metric_lbl } ] SELECT reference, ?val AS ?value, DESC(reference) FROM title; ORDER BY reference DESC(reference) output (Sparql query): select reference, ?val AS ?value, DESC (reference) FROM title; ORDER BY reference DESC(reference) output (Sparql table): TABLE output(Sparql table) PARTITION by reference ORDER BY reference DESC(reference) output (Sparql Excel) SELECT reference, ?val AS ?value, DESC(	False
"
 input (English text): What are the metrics of evaluation over the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the ADE Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ballinin Data"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } input (English text): What are the metrics of evaluation over the Ballinin, catch (DMControl100k) dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ballinin Data"") ?benchmark orkgp:HAS_DATASET ?dataset orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_l"	False
"
 input (English text): List the code links in papers that use the Performer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Performer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT TOP 1 ? score, ?score, { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER { ?model a OR ?model_lbl = ""DATASET"", \ ?source_code OR ?source_code = ""LIMIT"", ?model { ?model_l, ?model_r, ?model_c_l } } ?dataset ?dataset_lbl; ?cont orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model_l; orkgp:HAS_SOURCE_CODE ?code. ?cont orkgp:HAS_SOURCE_MODEL ?source_code. ?cont orkgp:HAS_SOURCE_MODEL ?cod_code orkgp:HAS_SOURCE_CODE ?cod_code. } This is based on a previous paper by Mirek and Jonsson. Here is the paper from which I am attempting to derive the definition in my paper:   https://github.com/hiraiyan/Perforator/blob/master/paper/2015/01/PerfTest_benchmarks.txt From there I extracted the code links in the paper. In this way I ended up with the following code: var name = ""bcp-0.2""; if (page!= null) { var page = page_list[page]; var title = page[""title""];"	False
"
 input (English text): Could you provide a list of models that have been tested on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the SpanRel model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SpanRel"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the MEMEN (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" OPTIONAL { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Memen"") ?benchmark orkgp:HSAVING_MODEL ?dataset. ?cont orkgp:HSAVING_LINK ?dataset. ORG_GAMMA ORG_GAMMA_EXPERIMENTALS OPTIONAL { ?modestar orkgp:HAS_DATASET ?dataset_lbl. } } input (English text): Can you provide links to code used in papers that benchmark the LENR model ? output (Sparql query): OPTIONAL { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LinearNet"" ) ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HSAVING_MODEL ?dataset. ORG_GAMMA ORG_GAMMA_EXPERIMENTALS ORG_MODESTAR ORG_MODESTAR_EXPERIMENTALS OPTIONAL { ?modestar orkgp:HAS_DATASET ?dataset_lbl. } } Categorical/Interactive Analysis  We created a spreadsheet with these models for a specific dataset that can be downloaded here. Example of an interactive graph showing where each model was trained. The full set of links are here. This graph shows the total number of training samples from many models from different training data sets for each model. To use this graph you need to download"	False
"
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ANLI test dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the MMV TSM-50x2 model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV TSM-50x2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input data (from ANLI): A list of research papers along with their titles and IDs. output (Sparql query): COUNT(*) Input data (from NYT): a list of research papers along with their titles, titles of their co-authors, and their affiliations. output (Sparql query): COUNT(*) Input data (from J. D. Smith et al.): A list of research papers along with their titles and IDs. output (Sparql query): COUNT(*) Input data (from J. D. Smith et al.): a list of research papers along with their titles, titles of their co-authors, and the number of times they have been cited by others. output (Sparql query): COUNT(*) Input data (from J. D. Smith et al.): a list of research papers along with their titles, titles of them co-authors, and the number of times they have been cited by others.	False
"
 input (English text): What models are being evaluated on the Atari-57 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the CUB-200-2011 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT s.scores; input (English text): What's the highest score for a model on the Yelp-4 benchmark dataset ? output (Sparql query): input (English text): What's the highest score for a model on the Rotten Tomatoes benchmark dataset ? output (Sparql query): S input (English text): Which model has achieved the highest Accuracy score for the Yelp-5 benchmark dataset ? output (Sparql query): R D!!! input (English text): What's the highest score for a model on the Yelp-4 bench ?benchmark dataset ? output (Sparql query): S D!!! input (English text): What's the highest score for a model on the Rotten Tomatoes benchmarks ? output (Sparql query): D D input (English text): Which model has performed best on the Yelp-5 Benchmark dataset ? output (Sparql query): R D D input (English text): Which model has performed best on the Rotten Tomatoes bench ?benchmark dataset ? output (Sparql query): D D input (English text): Which model has performed best on the Yelp-4 Benchmark dataset ? output (Sparql query): R D!!! input (English text): Which model has performed best on the Yelp-5	False
"
 input (English text): Indicate the model that performed best in terms of ROUGE-2 metric on the CNN / Daily Mail benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Recipe dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Recipe"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the WMT2016 German-English dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. MODEL : { MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. ?model rdfs:labels ?model_lbl. MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. ?test orkgp:HAS_TEST ?model. } MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. } MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. ?model orkgp:HAS_HAS_INF ?model. MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. ?model orkgp:HAS_HAS_HIT ?model. MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl. ?test orkgp:HAS_TEST ?model. } MODEL lbl [ ?paper orkgp:HAS_MODEL ?model. ? rdfs:labels ?model_lbl.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BERT classifier model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT classifier"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the WRN28-10 (SAM) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""WRN28-10 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the SAC model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""WRN21/CBT"") ?benchmark berts:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code; berts:HAS_SOURCE_FILE ?code= ?code3; orkgp:HAS_SOURCE_CODE ?code="	False
"
 input (English text): Indicate the model that performed best in terms of ROUGE-1 metric on the arXiv benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the Cheetah, run (DMControl100k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the MEMEN model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?paper_name, ?paper and ?paper_source ?paper_source = ?paper_name/ input (HTML): Could you provide some examples of benchmark papers that utilize the Memen model ? output (Sparql query): SELECT DISTINCT ?metric ?method ? CASE WHEN metric!= NULL_MODEL THEN CASE OF  ds:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MEMEN"")  { NULL_MODEL, ?metric_lbl. DOCUMENTARY, ?metric_lbl. DOCUMENTARY, ?metric_lbl. DOCUMENTARY, ?metric_lbl. DOCUMENTARY ; } input (Text): How many articles from your paper has the following pattern, ""The performance of the model with which the present paper is based on is not well understood"" ? output (Sparql query): input (Text): What paper did you cite in the paper that discusses the limitations of memen ? output (Sparql query): SELECT name, source, title, author, year, doi_pub_date, doi_date, DOI, source_date, source_date. DOCUMENTARY, DOI, source_date. DOCUMENTARY, DOI, source_date.   input (Text): Could you provide a list of papers that have used a model of memench that could support the aforementioned pattern ? output (Sparql query): input (Text): How many papers have used the memory model that did not describe these issues clearly enough ?  output (Sparql query): input (Text): In the second round of tests, the paper was able to provide some benchmarks, specifically"	False
"
 input (English text): Could you provide a list of models that have been tested on the MPQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Micro F1 score when benchmarked on the PWC Leaderboards (restricted) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Past Decode Reg. + AWD-LSTM-MoS + dyn. eval."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): What are the latest version dates of the models that have been tested on the MPQA benchmark dataset ? output (Sparql query): input (English text): What are the latest version dates of the models that have been tested in the MPQA and PWC Leaderboards datasets ? output (Sparql query): input (English text): In the test of this model, has the same model been ran as in the MPQA dataset of the PWC Leaderboards with the same data ? output (Sparql query): input (English text): In the test of this model, has the same model been run as in the MPQA dataset with that dataset of a model that was not available in the PWC Leaderboards without missing data ? output (Sparql query): input (English text): input (English text): I would like to compare these models across a wide range of parameter values. For details, please provide some example data. output (Sparql query): input (English text): I would like to compare these models across a wide range of parameter values. For details, please provide some example data. output (Sparql query): input (English text): What would be the number of parameters in the past DECODE model vs. the present one; I can provide some examples here: output (Sparql query): input (English text): Please provide some values here that are the only difference between the present and the present model. output (Sparql query): input (English text): Please provide some values here that are the only difference between the present and the present model. output (Sparql query): input (English text): What is the latest year number that the model has been included in the	False
"
 input (English text): Can you list the metrics used to evaluate models on the DDI extraction 2013 corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI extraction 2013 corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the PAR Transformer Base model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Assault"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Assault dataset ? output (Sparql query): input (English text): Does the model use any hidden layers ? output (Sparql query): input (English text): What are the input/output structures for the model ? output (Sparql query): input (English text): How can I compare the output of the model you recommend for DDI extraction with the output from a test model ? output (Sparql query): input (English text): Which are the top performing models tested with the PAR Transformer Base model ? output (Sparql query): input (English text): Is the model tested with any other DDI model ? output (Sparql query): input (English text): Which of the top models tested by Benchmarking the PAR Transformer Base model on the Atari 2600 Assault dataset should I try ? output (Sparql query): input (English text): Why is DDI extraction a good benchmark for PAR processing ? output (Sparql query):	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the CommitmentBank dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the ObjectNet (Bounding Box) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the 200k Short Texts for Humor Detection dataset in terms of F1-score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1-score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Is there a common pattern for the models performing the best in terms of predictability between the text sets and the real objects ? output (Sparql query): 2 of 200 (90.00% prediction accuracy) None. 2 of 200 (30.00% prediction accuracy) None. 3 of 200 (12.50% prediction accuracy) 96.50 2.00 2.00 2.00 2.00 2.00 8.00 2.00 2.00 7.67 2.00 2.00 9.00 2.00 2.00 input (English text"	False
"
 input (English text): What is the top benchmark score and its metric on the ChemProt dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ChemProt"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the STREET model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""STREET"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------- The following table lists many citations used during the Benchmarking Project: ---------- ------------------------- ------------------------- ---------- Model Name Description and citations --------------------- ------------------+------- --------------- STREET LENGTH 1.0.0.0 STREET LENGTH 1.1.0 STREET LENGTH Model Parameters and Datasets --------------------- ------------------+------- --------------- STREET LENGTH 1.0.0.0 STREET LENGTH 1.1.0 STREET LENGTH Number of rows in the data frame (8 columns) Model Parameters and Datasets --------------------- ------------------+------- --------------- DENSITY=2,100 DENSITY DENSITY 2,100 DENSITY DENSITY 1.0.0.0 STREET COUNT STREET COUNT 5 Rows COUNT Number of rows in the data frame (2 columns) Number of rows in the data frame (2 columns) Number of rows in the data frame (2 columns) Number of columns in the data frame (3 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (3 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (3 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (3 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (3 columns) Number of columns in the data frame (2 columns) Number of columns in the data frame (3 columns) Number of columns in the data frame (2 columns)	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ZFNet (1 convnet, 512,1024,512 maps) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ZFNet (1 convnet, 512,1024,512 maps)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of EM metric on the SQuAD1.1 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the enwiki8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?compute_m0_n1_x_0_1_x_0_1 ?model_lbl. ?compute_m0_n1_x_0_1_x_0_1_1 orkgc:model_id ?compute_m0_n1_x_0_1_x_0_1_1 ?model_hdl input (English text): Is the benchmark dataset a suitable benchmark of the HMMX model (EBNF-KNN) model for inference from a text corpus ? output (Sparql query): SELECT DISTINCT ?compute_m0_n1_x_0_1_x_1. ?x1_1_0_0_0_1 ?model_lbl. ?compute_m0_n1_x_0_1_x_1_0. ORGXC:model_id ORGXC:model_id ?compute_m0_n1_x_0_1_x_1. ?x2_1_0_0_0_1 ?model_lbl. ORGXC:model_id ORGXC:model_id ?compute_m0_n1_x_0_1_x_1. ORGXC:model_id ORGXC:model_id ?compute_m0_n1_x_0_1_x_1. ORG	False
"
 input (English text): What models are being evaluated on the Rotowire (Content Selection) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciERC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	--------------------------- RESULTS FOR --------------------------- ---> Title (R2) ------------ ---- ------------- SciERC - Research in Machine Learning -------------- 16.0% ------------- Sr. No Rank Name R2 Rank 1 NAP-NN-DNN-V1 3.1 17.2 2 NAP-NN-DNN-V2 2.2 19.2 3 ABI-CNN-V1 3.9 19.8 4 CNTI-Net-V1 5.8 12.0 5 EGBoost-Learning 1.8 8.2 6 FLP5-CNN 2.0 3.2 7 PIR-DNN 2.8 14.5 8 SOPAM-CNN 2.6 12.6 9 INTL-CNN 3.9 16.4 10 LSTM-CNN-V2 3.9 18.7 11 CNTI-Net-V3 3.1 14.2 12 CNTI-Net-V4 3.5 7.2 13 PIR-V4 3.6 12.9 14 CNTI-V5 3 3.9 15 PIR-V1 3.4 8.2 16 TPU-DNN 4.8 10.0 17 SIP-CNN 2.8 25.7 18 FLP5-CNN 2.6 8.4 19 INTL-CNN-V5 6.1 14.4 20 FLP5-V2 3.4 7.9 21 SOPAM-V2 2.9 8.7 22 SVNN 2.0 9.0 23 INTL-V5 6.4 15.1 24 NAP-DNN-V3 3.9 17.2 25 HASH-V2 1.3 3.6 26 PIR-V2 2.7 10	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Phoenix dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the CommonsenseQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?benchmark. ?lbl  || ?benchmark. ?model  || ?benchmark. ORkgp|| ?model|| ?model_lbl.  || ?dataset a orkgc:Dataset.  || ?dataset_lbl.  || ?model_lbl.  || ?model_lbl. || benchmark orkgp:HAS_DATASET || model_lbl.  || ?model_lbl  || model_lbl  || ?dataset a orkgc:Dataset.  || ?dataset_lbl || ?dataset_lbl || benchmark. || ORkgp||  || ?model_lbl &&. || model_lbl|| The output can be generated by the following SQL Query: SELECT   ?lbl ||   ?lbl_lbl|| ?benchmark  || ?benchmark	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoLA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoLA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?model_lbl ?model_lbl_dataset ? The following example requires the user to provide a list of 10 model names from the CoLA dataset. In an example of a benchmark, the models are benchmarked against another set of models from the benchmarking. For example, instead of displaying the model names in the database, the user can query the database for benchmarks themselves with this command: SELECT name FROM dbo.benchmark WHERE dbl:criteria!= 'benchmark' After defining a benchmarked model, one or more parameters can be passed to the benchmarking system. To benchmark the IMDb-M dataset, one could use Benchmark: SELECT name FROM dbo.benchmark WHERE dbl:criteria!= 'benchmark' AND name.label!= '' To validate the output of a benchmarking, one can use Benchmark: SELECT name FROM dbo.benchmark WHERE dbl:criteria!= 'benchmark' Using a non-standard query in a benchmarking can be detrimental to performance and can lead to a severe performance drop in the benchmarking system. For this reason, caution should be taken in interpreting the parameters passed to a benchmarked model, which can impact overall performance. This example can therefore be simplified by eliminating the extra parameters provided when the example was developed to show the non-standard query to prevent the execution of a non-standard query: SELECT name FROM dbo.benchmark WHERE dbl:criteria!= 'benchmark' AND name.label!= '' Using the Benchmark command will return output like this: The output of the below query is the result of running the benchmark on 10 models from the dataset Co	False
"
 input (English text): Can you list the metrics used to evaluate models on the Yelp-2 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Abstracts\' entities and relations annotated corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Abstracts' entities and relations annotated corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT MAX(a) MAX(b) FROM ?a ?; orkgp:HAS_DATASET ?dataset WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""English-German-French"") ?benchmark orkgp:HAS_DATASET ?dataset; input (English text): What languages can you write with the output of the WMT2014 English-German-French corpus ? output (Sparql query): SELECT lang FROM ?a ? GROUP BY ?lang ORDER BY ?lang; orkgp:HAS_DATASET ?dataset WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""English-German-"" + lang + "" ?"") ?benchmark orkgp:HAS_DATASET ?dataset; In what languages can you write with the output of the WMT2014 English-Latino-English dataset ? output (Sparql query): SELECT lang FROM ?a ? GROUP BY ?lang ORDER BY ?lang; orkgp:HAS_DATASET ?dataset WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""English-Lato-"" +"	False
"
 input (English text): Can you list the models that have been evaluated on the Kinetics-600 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Reading Comprehension research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Reading Comprehension"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" Inputs: A1: Select a dataset of languages. output_lang: A1: select a 1-sentence file of language sequences in.text dataset. select language1_sentencing.txt to read into the output_lang column of a1_lbl select language1_sentencing1_file.csv to parse into a1_sentence1_lbl for comparison with the rest A2: Select a dataset of languages. output_lang: A2: select a 1-sentence file of language sequences in.text dataset. select language2_sentence.txt to read into the output_lang column of a2_lbl select language2_sentence1_file.csv to parse into a2_sentence2_lbl for comparison with the rest Answers: A1: Language sequence set from a1_lbl and a2_lbl. A2: Language sequence set from a1_sentence.txt and a2_sentence1_lbl. Inputs: A1: A1-to-A2 dataset that have similar features. A2: A2-to-A1 dataset that have similar features A3: A3-to-A2 dataset that have similar features Answers: A1: ""Sentence set 1"". A2: ""Sentence set 2"". A3: ""Sentence set 3"".   You can find more information about this dataset on  http://www.nga.ch/discover-word-grams/parsing-language-text-text/#data Answers: A1: English text from language set ""Sentence set 1"" with sentence ""The cat is black"". A2 : English text"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ES FF (1 hour) noop model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ES FF (1 hour) noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of benchmarked datasets related to the co-authorship prediction research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""co-authorship prediction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Which model has achieved the highest RE+ Micro F1 score on the CoNLL04 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a askgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""solution to a"" OR ?problem_lbl_lbl!= null AND rdfs:concat( ?data, ?lbl. ?table = ""co-authorship test"" OR rdfs:concat( ?data, ?lbl. ?table = ""co-authorship test""))) ?dataset a askgc:dataset; rdfs:label ?dataset_lbl. ?cont orkgc:HAS_DATASET ?data. } You might be able to get better results by using the RStudio R Code Editor or R Studio's API's. For example, RStudio has the ability to create code blocks with your query and submit the result to the web, where researchers can use your data and submit their own code back to RStudio. Use the RStudio R Code Editor to: Open individual R code blocks in TextEdit Import code blocks (the example here, from the original paper ): For each code block in TextEdit, open the code's code block definition (code blocks must be named ""code_block"" ): Code block (name,line:number) { code_block : { [code blocks (name,line:number)] : [block:value] } } The RStudio CmdLine() command opens the R Code editor in the code block definition"	False
"
 input (English text): Provide a list of papers that have utilized the Dynamic Coattention Networks (ensemble) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the XLNet (base) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (base)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code ?code_mldata WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_mldata. FILTER (str( ?dataset_mldata) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset_mldata. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?code orkgp:HAS_DATASET ?code. } input (English text): Provide a list of articles that have discussed or benchmarked Atari 2600 Crazy Climber in some other context than their own ? output (Sparql query): select title, author, subject from arxiv.org where title in ('""Atari 2600 Crazy Climber"", ""'"", ""Crazy Climbing"") and author in ('""Macklin, Mihai"", ""'"", ""Crazy Climbing"", '"") and subject in ('""Atari 2600"", ""'"") input (English text): Provide a list of articles that have discussed or benchmarked Atari 2600 Crazy Climber in some other context than their own ? output (Sparql query): input (English text): Give me a list of scientific publications that discuss or benchmark any aspect of the Crazy Climber dataset and include the titles of any papers that have analyzed the data ? output (Sparql query): SELECT DISTIDENTIFIER ?c:Matcher; dsd:Dependency-Table"	False
"
 input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Ohsumed dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ohsumed"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 _____________ Published: _____-2016/04/30 (18 papers) -ORIGINAL, in Nature Communications _________Author Bio: http://www.nature.com/nmc/journal/v8/n2/full/nmc20160430.html ________ I understand you are a company with research results, but why the secrecy ? Did not that violate any regulations ? Please explain.	False
"
 input (English text): What kind of graph does ADANA use?
 output (Sparql query): SELECT ?graph ?graph_label WHERE { ?paper orkgp:P31 ?cont; rdfs:label ?title. FILTER(REGEX( ?title, ""ADANA"")) ?cont orkgp:P5008 ?graph. ?graph rdfs:label ?graph_label. } 
 input (English text): Could you provide a list of models that have been tested on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the NYT29 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 As well, you should test your model on the other common benchmark datasets that you might be using: yelp.com, google.com/search, google.com/gstatic, etc. For other types of datasets, you should only run your model against certain of these or a subset of these type of datasets. If you find a model not to perform as expected, consider optimizing your model. You can optimize the training process by doing more training samples. This won't fix every single problem encountered by a user, but in practice it's very likely that you won't find a single bottleneck. Before making any changes to an existing model, it's a good idea to benchmark with a sample of your data first. This will provide you with a general idea of how many errors there could be, what the impact of each one might be (if any), and other useful information. As shown above, it will also serve to test your model against the best possible dataset. 2.10. Optimizing the Model 2.10.1. Selecting the best training sample We've already established that a large dataset is very beneficial to your model. However, there's another important factor you should consider if building your model using this approach: how often will we need to train on a particular type of dataset ? If you use your model to create an estimate of the probability of a particular outcome of interest, and you need to test your model on a dataset that occurs once every day, then you will always find that your model is too inefficient to use. 2.10.1.1. Common Training Dates 2.10.1.2. Benchmarking Over and Over If you have several datasets that you think will be useful to you, it might be beneficial to run (or have run) your model against all data on one of them. However	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the FTD dataset dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the metrics used to evaluate models on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which are 3 the most common variables for the atmosphere models?
 output (Sparql query): "	"SELECT ?variables WHERE { orkgr:R68871 orkgp:compareContribution ?contrib. ?contrib orkgp:P7144 ?models. ?models rdfs:label ?models_labels. FILTER(REGEX( ?models_labels, ""tmospher"")) ?models orkgp:P26032 ?variables. } ORDER BY DESC(COUNT( ?variables)) LIMIT 3"	 input (English text): Which variables are included in the climate model evaluation tool ? output (Sparql query): input (English text): Which parameter are used to evaluate atmospheric models using GCM-LSC-EPSA-GFN ? output (Sparql query): input (English text): Which parameter are used to evaluate atmospheric models using GISS-CE,GISS-CFD-F1,GISS-CLCM-S4 or GISS-CM-S4:1 ? output (Sparql query): input (English text): Which parameters are used to evaluate the GISS-CM3-M (CE-GISS-CM-3-M4) and the GISS-CM3-M-G2 ? output (Sparql query): input (English text): What value variables were used for the GISS-CM-3-M model evaluation with GISS ? In the next step, the dataset can be evaluated using multiple parameter sets: The variables to evaluate for the GISS-CM-3-M, GISS-CM3-M-G2 and the GISS-CBM models are stored in variables. input (English text): Is it possible to extract the parameter set from the models ? output (Sparql query): input (English text): If the model evaluation tool provided parameter ranges to evaluate the model, how would you use the parameters to evaluate the models you're currently using ? output (Sparql query): output (Sparql query): To find all parameter set for the climate models, you should use the parameterset package, version 1.2, available in the standard library: package paramset input (English text): Please fill out a parameter set object for the GISS-CM	False
"
 input (English text): What are the metrics of evaluation over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Micro Recall score when benchmarked on the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Micro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the DDQN-PC model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN-PC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT id, name, id_link | name, id_link_prelude | name, id_link_prelude_prelude, name_links | name_links, id_links, name_links_prelude, name_links_prelude_prelude | name_link_prelude, id_link_prelude_prelude | name_link_prelude_prelude_prelude, id_link_prelude_prelude_prelude_links | id_link_prelude_prelude_prelude_prelude_links_prelude_prelude | id_link_prelude_prelude_prelude_prelude_prelude_prelude_prelude_all | id_link_prelude_prelude_prelude_prelude_prelude_prelude_prelude_all_all | id_link_prelude_prelude_prelude_prelude_prelude_prelude_prelude_all_all_all input (English text): List the code links in papers that use the TDQN-PC model in any benchmark ? output (Sparql query): SELECT ID, CODE, ID_TOLL | ID, CODE, ID_TOLL_PRELUDE | ID, CODE, CODE_PRELUDE. ID_TOLL CODE CODE_PRELUDE ID. CODE_PRELUDE CODE. CODE_TOLL > 10000 LIMIT 1 IN ANSWER input (English text): List the code links in papers that use the TDQN-PC model in any benchmark	False
"
 input (English text): Can you list benchmarked problems in the area of Semantic Web?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Semantic Web"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the ImageNet ReaL benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the TREC-6 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT TITLE FROM RESULTS WHERE ID= ?title input (English text): List the description of the benchmarked problems in the area of Semantic Web ? input (English text): List the number of problems in the area of Semantic Web ? output (Sparql query): SELECT NUMBER OF PROBLEMS FROM RESULTS Input Courses Courses are used to give courses, and is also available as a data type. It is an interface between Course and an R object. Course is a new type that is available since 1.2. Please note that Course is a wrapper type and not a normal class. Course (object): class Course : ROW_NUMBER() :  @classmethod def get_class ( cls, fname, ** kw ): """"""Return a new, static copy of the course on the selected row. Returns an instance of a Course which represents the course as a Course object, with fields like name (string), id (integer or None). """""" cls. new_course = Course ( fname, ** kw )  return cls. new_course  class Course : ROW_NUMBER() :  @classmethod def get_class ( cls, fname, ** kw ): """"""Return a new, static copy of the course on the selected row. Returns an instance of a Course, which represents the course as a Course object, with fields like name (string), id (integer or None). """""" pl = Course ( fname, ** kw ) fname, id = pl. attributes ( self ) pl. field = fname or id return cls. new_course. pl  method (): def __init__ ( cls, ** kw ): cls. __init__ ( cls ) _classargs = { '"	False
"
 input (English text): Which model has achieved the highest F1 score on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Rational DQN Average model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rational DQN Average"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ISNA( ?method, ?average_c:Model ) AS ORROB_CORE_CORE, ?weighting_model_lth( ?method, ?average_c:Model ) AS orweighting_model_lth_s, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_c, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_l, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_l_s, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_L, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_L_s, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_L_s_c, ISNA( ?method, ?weighting_model_lth( ?method, ?average_c:Model ) ) AS weighted_average_L_s_s, ISNA( ?method, ?weighting_netlth( ?method, ?average_c:Model ) ) AS weighted_netlth_c, ISNA( ?method, ?weighting_netl	False
"
 input (English text): What models are being evaluated on the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?residence orkgp:HAS_RESIDENTIALITY ?record. ID ?record rdfs:label ?record_lbl. } input (English text): What is the name of the top performing model in terms of Quality score when benchmarked on the MLDoc Zero-Shot English-to-French dataset ? output (Sparql query): output (Sparql query): Select which metric is being used to evaluate models on the IMDb benchmark dataset ? input (English text): List the metrics that are used to evaluate models on the IMDb benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?dataset a_cv orkgp:HAS"	False
"
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of 1-of-100 Accuracy metric on the PolyAI Reddit benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""1-of-100 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PolyAI Reddit"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" input (English text): List the parameters that were used to evaluate the model on the MVC2 benchmark dataset ? output (Sparql query): input (English text): List the parameters that were used to evaluate the model on the F# code platform ? output (Sparql query): input (English text): Provide a list of metrics, or parameters, that were used to evaluate the models on the F# code platform ? output (Sparql query): input (English text): The model to evaluate was called ""F# code platform"" and the data was the F# code platform. Provide the parameters that were used to evaluate the model on the F# code platform ? output (Sparql query):"	False
"
 input (English text): What is major reactant for ""Microwave-Assisted Cobinamide Synthesis""?
 output (Sparql query): SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""Microwave-Assisted Cobinamide Synthesis"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P37557 ?reactant. } 
 input (English text): Where can I find code references in papers that have used the SpERT (without overlap) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SpERT (without overlap)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Long Short Transformer model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Long Short Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""Microwave-Assisted Cobinamide Synthesis"")) ?papers orkgp:P31 ?"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Winograd Schema Challenge dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Winograd Schema Challenge"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the full name of the EXPO ontology?
 output (Sparql query): SELECT ?full_name WHERE { orkgr:R8342 orkgp:compareContribution ?cont. ?cont orkgp:P7034 ?ontology. ?ontology rdfs:label ?ont_lbl; orkgp:P7035 ?full_name. FILTER(REGEX(STR( ?ont_lbl), ""expo"", ""i"")) } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?cont ?dataset. ?cont rdfs:label ?paper_lbl. } input (English text): What is the name of the TOP500 list covering the highest performing models ? output (Sparql query): // Example query as a column in a SQL Server dataframe, to generate sample rows RDD // column name = ?dataset input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the top500 list ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; ?cont ?dataset. ?cont rdfs:label ?paper_lbl. }	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the WMT2016 English-Russian dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is Raman spectroscopy?
 output (Sparql query): "	"SELECT ?definitions WHERE { ?terms orkgp:P24009 ?definitions. ?terms rdfs:label ?terms_labels. FILTER(REGEX( ?terms_labels, ""Raman spectroscopy"")) } "	SELECT DISTINCT ?index_symbol ?index_symbol_krambles. ?index_symbol ?index_symbol_krams. ?index_symbol rdfs:label ?index_symbol_krams. ?index_symbol rdfs:value ?krambles_krambles. OPTIONAL { ?krambles_krambles rdfs:key ?krambles_krambles. } OPTIONAL { ?krambles rdfs:value ?krambles_krambles. } OPTIONAL { ?krambles rdfs:key ?krambles_krambles. } OPTIONAL { ?krambles rdfs:value ?krambles_krambles. } OPTIONAL { ?krambles rdfs:key ?krambles_krambles. } OPTIONAL { ?krambles rdfs:value ?krambles_krambles. } OPTIONAL { ?krambles rdfs:key ?krambles_krambles. } OPTIONAL { ?krambles rdfs:value ?krambles_krambles. } OPTIONAL { ?krambles rdfs:key ?krambles_krambles. } OPTIONAL { ?krambles rdfs:value ?krambles_krambles. } OPTIONAL { ?krambles rdfs:key ?krambles_krambles. } OPTIONAL { ?krambles rdfs:value ?krambles_krambles. } OPTIONAL {	False
"
 input (English text): What is the top benchmark score and its metric on the MLDoc Zero-Shot English-to-Japanese dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Japanese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DTD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DTD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?score, ?value, ?metric_lbl( ?score) FROM (SELECT ?dataset, TOP 5 ?statistics FORMAT DISTINCT ?value FROM data where ?statistics = ?name ORDER BY ?statistics) ; output (Sparql query): SELECT ?value, ?metric_lbl( ?score) FROM (SELECT ?dataset, TOP 5 ?statistics FORMAT DISTINCT ?value FROM data WHERE ?statistics = ?name) ; ORG( "" "" ) ; SELECT ?score, ?value, ?metric_lbl( ?score) FROM (SELECT ?dataset, TOP 5 ?statistics FORMAT DISTINCT ?value FROM data WHERE ?statistics = ?name ORDER BY ?statistics) ; ORG( "" "" ) ; output (Sparql query): output (Sparql query): The top value among the five metrics for each dataset was 10.01 output (Sparql query): The top value among the five metrics for each dataset was 6.00 output (Sparql query): The top value among the five metrics for each dataset was 6.01 output (Sparql query): The top value among the five metrics for each dataset was 0.00 output (Sparql query): It would be interesting to see if the top values are the same for each dataset. output (Sparql query): The top values for each dataset are 1.75 for English, 1.62 for French and 1.71 for Spanish output (Sparql query): What is the top benchmark score and its metric on the MLDoc Zero-Shot English-to-Japanese"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Extended Transformer Construction model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Extended Transformer Construction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the minimum and maximum installed capacity for each energy source considered?
 output (Sparql query): SELECT ?energy_sources_labels (MIN( ?installed_cap_value) AS ?min_installed_cap_value) (MAX( ?installed_cap_value) AS ?max_installed_cap_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value) } 
 input (English text): What is the top benchmark score and its metric on the CINIC-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 orkgp:BOUND(R153312, R153601, R153701, R153801); source (String): source file contains a list of sources. These lists include the energy sources (LIFGENERIC, SOURCE LOCAL, REQUIREMENTS_SENSITIVE, SOURCE_DELIVERY, INSTALLED_HUMAN AND VARIOUS).   orkgp:DATASET(R35000, R35001, R35002, R35003) or kgp:HAS_BENCHMARK(R152323) or kgp:P43135| or kgp:HAS_MODEL(R35002) or kgp:HAS_SOURCE_CODE(R75003, R75004, R75005, R7500511, R7500512, R7500513, R7500514, R7500515, R7500516, R7500517, R7500518, R7500519, R7500520, R7500521, R7500522, R7500523, R7500524, R7500525, R7500526, R7500527, R7500528, R7500529, R7500530, R7500531), HASH_DUMP( ?s). orkgp:SUBMISSION( ?s) or kgp:PAYMENT( ?s) if the system is used on a regular basis or kgp:PAYMENT_CODE( ?r, ?s) input (Email): You can get the link to read about this paper/serve as one of the references. -- ---------------  -- [1] http://en.wikipedia.org/	False
"
 input (English text): List the code links in papers that use the NCBI_BERT(base) (P) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(base) (P)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the metrics used to evaluate models on the  Jacquard dataset dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SemEval-2018 Task 7 dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?title ?"" Input (English text): List the titles and IDs of publications and articles that include a benchmark for the US National Academy of Sciences project using data from NCBI_BLAST (P = 3, NCBI_DEFINITION ""eNBERt"" ). output (Sparql query): Select the NCBIBBLAST title  that corresponds to the P-model of the project  that you wish to evaluate. input (English text): Tell us which benchmarks to use in the projects using the US National Academy of Sciences output (Sparql query): Select the output for the projects using the US National Academy of Sciences input (English text): Please list the NNAST_NLS-19 datasets that contain the P-model for a NISR dataset. output (Sparql query): Output the output of the NNAST_NLS-19 project using the P-model of the NISR dataset. Example input: input (English text): What are the number of tests performed with SAGE_SPARTIS_LIGHT for the R (i.e. NCBIbBLAST) benchmark NNAST_NLS-19 ? output (Sparql query):  SELECT ?test ?model ?"" input (English text): What are the name and number of publications and articles that include a benchmark for the  U.S. National Academy of Sciences project using data from NCBI BLAST ( P = 3, NCBI_DEFINITION ""eNBERt"" ) ? output (Sparql query):  SELECT ?pub ?name ?number "" ) Example output: output (Sparql query): Select the output for the project using the U.S. National Academy of Sciences using P-models"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the HMDB51 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the DRI Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the AG News benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AG News"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT COUNT(*) as m, N as N * m, RANK ( ?value) as r, NUMBER ( ?value) as N. N, INTERSECT ( ?value), SORT ( ?value ) as S, SORTED ( ?value ) as S, STRING ( ?value ) as S. S, NUMBER ( ?value ) as N, SORTED ( ?value ) as S, STRING ( ?value ) as S, N, RANK ( ?value = 1, ?value = 2, ?value = 3 or ?value = 4 )) as k, NUMBER ( ?column = 1, ?column = 2 ) as k, NUMBER ( ?column = 3 ) as k, NUMBER ( ?column = 4 ) as k, LENGTH ( ?column ) as k, VIGNORE ( ?column, ?order, ?collimator, ?min, ?max, ?sortorder, ?first, ?last ) as N, RANK ( ?order, ?min, ?max, ?sortorder ) as R, MIN, MAX( ?lengths ) as R. s, STRING( ?lengths, 1 ), VARYING( ?lengths, 2 ) as V, ?order, ?min, ?max, ?sortorder, ?first, ?last AS k, DIFFUSE( ?max, ?min) as D IF( ?order = 1, ?min = 0, ?max = 0, ?lengths = []) AS s, ?order AS x WHERE ( ?order = 1 AND ?length { ?value } = 0) ORDER BY -1; ORDER BY -1 input (English text): Show a summary of the top 500 papers that publish papers on the Kaggle Benchmark dataset ? output (Sparql query):	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Penn Treebank (Word Level)?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC Chinese-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Chinese-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 output (Sparql query): id number: 1 2 3 4 5 6 input (English text): What are the title and IDs of research papers that contain a benchmark for the NCBI Disease dataset ? output (Sparql query): output (Sparql query): id number: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 input (English text): What is the name of the first model in terms of F1 entity level score when benchmarking on the CEE database (BPDL) ? output (Sparql query): output (Sparql query): id number: 2 3 4 5 6 input (English text): What are the titles and IDs of research papers that contain a benchmark for the NEXUS database (BPDL) ? output (Sparql query): output (Sparql query): id number: 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 input (English text): What is the name of the first model in terms of F1 entity level score when benchmarking on the NEXUS database (BPDL) ? output (Sparql query): output (Sparql query): id number: 2 3 4 5 6 input (English text): What are the title and IDs of research papers that include a benchmark for the NEXUS database (BPDL) ? output (Sparql query): output (Sparql query): id number: 2 3 4 5 6 input (English text): Which languages were used in research ? output (Sparql query): output (Sparql query): language id Language lang: ?lang and ?lang_l	False
"
 input (English text): Where can I find code references in papers that have used the ResNet50 multi-grasp predictor model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet50 multi-grasp predictor"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI- Disease "") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } Input: Output: 1. Find the title/ID of either the RACE (2014) and RACE (2015) papers, or the NCBI-disease (2015) and NCBI- diseases (2015) papers that report results on either RACE (2014) or RACE (2015); or 2. Find the title/ID of either the RACE (2014) and RACE (2015) papers, or the NCBI-disease (2015) and NCBI- diseases (2015) papers that report results on either RACE (2014) or RACE (2015). 2. Find the title/ID of either the RACE (2014) and RACE (2015) papers, or the NCBI-disease (2015) and NCBI- diseases (2015) papers that report results on either RACE (2014) or RACE (2015). 1. Find the title/ID of both the RACE (2014) and RACE (2015) papers (or at least, the NCBI-disease (2015) and NCBI- disease (2014) papers with results results on either RACE (2014) or RACE (2015), if we are comparing the two papers with results from either RACE ("	False
"
 input (English text): What is the best performing model benchmarking the Cartpole, swingup (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the Feedback Transformer model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Feedback Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the GCN Hybrid model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GCN Hybrid"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The above example will output (for example), this is how you can get the scores of GCNN and GCNN-CM for GCNN-XOC model test. This is a sample results query for the following papers. GCNN-LSTM is the only valid benchmarking technique for the GCN Hybrid model. You can see the results you can apply with various benchmarking platforms below. The example assumes (it should not be used in real-world scenarios and the benchmarking engine does not include the GCNN-XOC) that the input (English text) is identical to the one used for training of the training set. The GCNN-XOC model was a benchmarked model to benchmark the GCNC hybrid model according to the papers. GCNN-LSTM was tested against various benchmarks such as PASCAL and the C++ model from the same paper. These benchmark results show that a model with a large number of parameters can perform very well, and that a model with a small number of parameters or a single long running benchmarking model has a pretty poor performance in comparison.	False
"
 input (English text): Indicate the model that performed best in terms of % Test Accuracy metric on the SNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""% Test Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Reuters De-En dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What data format does CHEMDNER corpus have?
 output (Sparql query): "	SELECT ?data_format, ?data_format_label WHERE { orkgr:R150596 orkgp:P31 ?contrib. ?contrib orkgp:P42011 ?data_format. ?data_format rdfs:label ?data_format_label. } 	"SELECT model; model:RDFS orkgc:Dataset Input data format is an RDFS file. Output model is an RDFR object. Input model is for the model that performed best in terms of % Test Accuracy metric on the REX dataset ? output (Sparql query): SELECT model; model:RDFS orkgc:Dataset Input data format is an RDFS file. Output model is an RDFR object. input (English text): Would you mind providing the name of the data model used in the benchmark dataset ? output (Sparql query): SELECT name, data_model, score FROM CHEMDNER_models WHERE name > ?model_1 OR 1 = ?model_2; name:RDFS data_model:RDFS input (English text): You said it only works with the data file format "".csv "", so could you provide me a name and structure for this data file ? output (Sparql query): input (English text): What information do you have on the following items and file formats ? output (Sparql query): It does not seem to have any information about the file formats. output (Sparql query): Input language is "" English "". output (Sparql query): Input language is "" English "". output (Sparql query): output (Html) Please fill in the following fields: output (Html) Please fill in the following fields: input (English text): Why were you looking for such data ? output (Sparql"	False
"
 input (English text): What is the top benchmark score and its metric on the MUTAG dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Accuracy score on the WSC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the CUB-200-2011 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT CUB-200 ?matrix ?dataset as ?matrix_lbl FROM ?data to cub:MUTAG, cub:CUB2008 to cub:MUTAG2011 WHERE ?matrix_lbl = "" ?matrix"", ?title ?text as ?title_lbl, ?num_rows orkgc:Dataset, ?max_rows orkgc:CUB2008, ?min_rows orkgc:CUB2008; { { SELECT ?title ?text as ?title_lbl, ?num_rows orpkgc:Dataset, ?max_rows orkgc:CUB2008, ?min_rows orkgc:CUB2008;... } } ORDER BY DESC( ?title ) LIMIT 1 input (English text): Do the MUTAG authors have authored an article/paper that contains a benchmark for the CUB-200-2011 dataset ? output (Sparql query): SELECT ?dataset ?dataset_lbl ON (author ?dataset_lbl) = { ?table ?column ?value; ?column ?name; { ?table ?column ?value; { ?table ?column ?value; { ?table ?column ?value; { ?table ?column ?value; ?column ?name; } } } } ORDER BY ( ?key ?value ) ORDER BY ( ?column ) LIMIT 1 input (English text) (7 rows): output (HTML) (6 rows): SELECT title, author, id, text, count(key), score, median, number_rows, min_rows FROM ?data to cuba:MUTAG, cuba:CUB2008 to cuba:MUTAG2011 output (Sparql query) (6 rows"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Pitfall! benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the D-NLI model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""D-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the SEE model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SEE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT SUM( score ) FROM ?models GROUP BY 1 ORDER BY DESC(desc) LIMIT 1 input (English text): Can you provide links to code used in papers that benchmark the TPSA model ? output (Sparql query): SELECT SUM( score ) FROM ?models GROUP BY 3 ORDER BY DESC ( desc ) LIMIT 1 input (English text): Can you provide links to code used in papers that benchmark a multidimensional array model or a multidimensional array model ? output (Sparql query): SELECT SUM(score) FROM ?models GROUP BY 1 ORDER BY DESC(desc) LIMIT 1 input (English text): Can you provide links to code used in papers that benchmark the D-NLI and HEAR model ? output (Sparql query): SELECT SUM( score ) FROM ?models GROUP BY 1 ORDER BY DESC(desc) LIMIT 1 input (English text): Can you provide examples of each model ? output (Sparql query): SELECT P(1, ?) AS input, P(2, ?) AS output, P(3, ?) AS output_test, P(4, ?) AS output_test_2, P(5, ?) AS output_test_3,..., P(6, ?) AS output_test_n,..., P(7, ?) AS output_test_s,..., P(8, ?) AS output_test_s_2,..., P(9, ?) AS output_test_s3,..., P(10, ?) AS output_test_sh,...,...,..., P(11, ?) AS output_concat,...,...,..., P(13, ?) AS output_repr,...,...,...,..., P(12, ?) AS output_tcp	False
"
 input (English text): What models are being evaluated on the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Top-1 Error Rate metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Duel hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" If you are using the data from the data.table on a separate document (e.g. the paper you were working on), make sure to change the field ""name"" to your personal e-mail address with the data.table. To read the table data, you can use the following Python functions: read_data_table (table) : Reads the data from the table into a string. The table must be accessible from the Python function. The return value is either None, or a serialized response. If None is returned then the read was successful. For example, using this code: data.table.read_data_table(""data.table"") (from 'python-data-table.py') or ``data````.read_data_table(file=None) or """"""data``.read_data_table(file=file) or """"""data``.parse_data_data_table.""__read__.py""```` """""" This will output: a. Data file from data_table.py b. Read all the rows from 'data_table.py'. read_all_rows (rowname_list) : Reads all the rows from the given rowlist for each column and outputs them in a list. The rowlist is of the form [data] [samples] [columns]. The rows must be accessible from the function. The result will be a list of tuples (columns). All the tuples must be accessible by the function. row1, row2, etc. can be accessed by name, or by index (e.g. the index in each table). If you want to read a tuple containing only data, you are almost done. You just need to know the field names and list the fields. This can be done using the Python syntax: data"	False
"
 input (English text): What models are being evaluated on the SQuAD1.1 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset DocRED (Human-annotated)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT { ?value ?dataset, ?METRIC:A2 ?metric } from ?model( ?model_lbl) ORDER BY dataset DESC LIMIT 1 SELECT { ?value ?dataset, ?METRIC:A2 ?metric } from ?model( ?model_lbl) ORDER BY dataset DESC LIMIT 1 SELECT { ?value ?dataset, ?METRIC:A2 ?metric, ?eval } from ?model( ?model_lbl) WHERE { ?dataset orkgc:Dataset; rdfs:label ?dataset_lbl. } ORDER BY DISTINCT eval OR ( ?result > ?value and ?dataset orkgc:Dataset <= ?metric) OR ( ?result = ?val AND ?dataset orkgc:Dataset <= ?metric) output (Sparql query): SELECT ?record ?dataset, ?value ?dataset, ?eval OR ( ?result > ?value and ?dataset orkgc:Dataset <= ?metric), { ?score ?metric, ?score ?value } FROM ?records/d/DocRED( ?record) AND { ?input ?dataset, ?score ?metric } ORDER BY or( ?input ?dataset, ?score ?metric) SELECT ?record ?dataset	False
"
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of BLEU metric on the RotoWire benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Adaptive Input Large model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT dt:ID, rdf:ID, t:ID, w:ID, b:ID, c:ID   AS output, output.length AS total_length FROM PROPERTY_MODEL  GROUP BY (dt:ID) ORDER BY 1, output.length DESC; id rdfs output.length total_length _________________________________________________________________ ---------- ----------   2 114749      --      -13.99       0.19      -2.15   2 57946     14.25      9.29    12.66   3 60439     20.26      12.77      13.42   4 53428     36.34      11.76       31.02   5 48761     52.67      15.22      36.29   6 38761     60.66      17.14      30.95   7 38761     60.66      17.14      30.95   8 103097     98.00      15.99     56.26   9 69699     128.00      25.69     48.38   10 90915     224.00      20.48     35.64   11 51776     216.00      27.56     39.15   12 63932     262.00      19.76     44.86   13 84907     308.00      19.66     37.30   14 252406   1829.00      26.40     40.76 15 91051     2488.00      28.88	False
"
 input (English text): What is the top benchmark score and its metric on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the MLDoc Zero-Shot English-to-Russian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the Atari 2600 Solaris dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT TOP 10 model_lbl ?""a"" ?""m"" ?""n"" ?""y"" ?""z"" ?""h"" ?""p"" ?""r"" ?""s"" ?""n"" AS ?Model_List ?""_r"" ?""s"" AS ?Model_List; ?""_s"" ?""_n"" AS ?Model_List ORDER BY DESC( ?Model_List. ?model_List ) ? ORDER BY ?""_n"" AS ?Model_List ? ORDER BY ?""_s"" AS ?Model_List ? ORDER BY ?""_i"" ?""_r"" ?""s"" AS ?Model_List AS; ?""_r"" AS ?Model_List ORDER BY ?""_i"" ASC ?""_n"" ASC ?""_y"" ASC ?""z"" ASC 1 2 3 5 15 29 37 45 49 1 21 12 2 12 18 3 10 5 6 7 7 13 15 15 17 19 18 24 25 26 29 31 33 35 45 46 47 49 55 55 57 61 59 56 62 57 58 61 20 10 9 5 1 12 13 26 10 10 9 3 14 8 5 11 5 11 5 6 7 1 11 5 4 6 9 10 10 13 13 8 3 6 3 8 6 3 3 6 4 1 3 2 5 4 6 9 2 4 4 3 3 3 6 5 2 3 4 3 2 5 2 1 7 2 1 7 4 2 7 7 4 2 5 3 5 3 3 6 5 5 6 7 3 3 1 3 4 1 3 2 2 3 2 2 5 2 6 6 4 20 23 30 36 9 19 24 29 26 40 20 13 15 12"	False
"
 input (English text): Indicate the model that performed best in terms of DISTANCE_TO_GOAL metric on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""DISTANCE_TO_GOAL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of A1 metric on the ANLI test benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT name, NAMES( ?dataset. ID ) AS ID( ?dataset), C. ID FROM  SELECT data, ?dataset_lbl, ?metric_lbl FROM  select   ?dataset, ?metric_lbl FROM  SELECT   ?dataset_lbl, ?metric_lbl, ?metric, ?value, ?c, ?c. ID, ?dataset_lbl, ?metric, ?value, ?c, ?c. ID, ?dataset_lbl, ?metric, ?value, ?c, ?c. ID, ?dataset_lbl, ?metric, ?value, ?c, ?c. ID, ?dataset_lbl, ?metric,	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Breakout dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the MMV model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMV"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (Sorted list of source and target code): METHODS = 3; PORT_STREAM = 1; DATASETS = 1; DATASERTIMER = 1. c.2: What are the limitations of the MMV model ? a.1: Are there any limitations with the Model ? a.2: Is there any advantage to a better performing Model which can be used to perform a benchmark ? a.3: Do you think using the Model improves the Benchmarking score or is the Benchmarking more accurate using the Model if the Model is used ? a.4: What is the output (Sparql query) for a benchmark where the Model was used to perform the benchmark and what metric is the benchmarking run conducted on ? a.5: If the MMV model is used for a Benchmark run, what should the Output include ? b.1: Can the model be used to benchmark other Atari 2600 games ? NOTE: If the model's output for a benchmark of another Atari 2600 game was the same, there are no limitations with the Model that would affect the Benchmark. c.1: How can the MMV model help make predictions about which games will perform better on the Atari 2600 ? a.1: If the MMV model is compared to other models, what differences exist or will be found ? b.2: How should the MMV model compare to the other Atari 2600 Model comparisons that exist ? b.3: Is the model the next best thing to a Monte Carlo Approach when doing simulation and training ? c.2: Do you know how the MMV model compares to the Atari 2600 Game Performance Database (GPPD) ? In other words, how can you evaluate the Model to compare to the GPPD ? How can you use	False
"
 input (English text): Where can I find code references in papers that have used the CAIT-M36-448 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-M36-448"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Boxing dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Centipede dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): Do you have any examples of how the CAIT-M36-448 approach compares to conventional model-based approaches to benchmarking ? output (Sparql query): input (English text): What do you think about the CAIT-M36-448 model in general ? output (Sparql query): input (English text): I need the code for the benchmarking library. What do you have for reference ? Please reply, as I might be able to create an issue. We are also currently looking to acquire a new benchmarking software. output (Sparql query): input (English text): What do you think about the current state of the project ? output (Sparql query): input (English text): I have been interested in performance measures. What do you think about comparative data analyses, with or without code comparison ? How do you go about implementing these comparisons ? output (Sparql query): input (English text): What have the benchmarks on the Atari 2600 Centipede dataset proven to be in terms of raw throughput with a number of different methods ? output (Sparql query): input (English text): I would like the code on this for a benchmarking reference (or benchmark) library, so other authors can check its performance and compare to previous results. Please reply to this email with the following: the names of any benchmarks where the code was used, why it was used, what was the benchmark name, what inputs were used, etc. I'd like a more specific answer to this question. output (Sparql query): Input (English text): What does the current version of the code on this project look like ? output (Sparql query): input (English text): Do you have any other	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Freeway dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the PubMedQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model_lbl ?model_lbl_lbl. ?model_lbl. FILTER { rdfs:label ?model_lbl. }  WHERE DESC( ?method). The output above is the results of this query which we use to compare the accuracy of the two models based on the benchmarks. The output shows a very different picture than the one we saw below, where the RDFS model performed relatively well in terms of accuracy and some of the best models performed with an even lower reliability. You might wonder why we needed to do this analysis only for the English paper. The reason is that I already knew from reading the paper itself, (and using the code in the other article) that the accuracy is likely only around 1/2 and there is an overall improvement in paper learning performance, the real story however, could be much much faster. Below we will try to determine which model performs best on the other papers. A. The text of the paper and list of benchmark papers B. The output of the query on the dataset Let's begin from the bottom of the table:  You can see in the first table, that my model performs very similar to the paper I previously used, (here called MPI2, which is the original version we used in the paper)  It actually does quite a bit better since it uses only the ""metric"" method; thus it uses only about 18mb of memory and 10,000 rows per document, as compared to the paper with MPI3 which used a little over a terabyte of memory and around 100 thousands rows per document. On the second page of the paper (pp 10-18), you can see an interesting table with the two models tested. From the title of this table, we can pretty easily determine what paper the RDFSL models performed much"	False
"
 input (English text): Which model has achieved the highest Error score on the Amazon-2 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Depth DDPPO model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Depth DDPPO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where did the study with maximal geographic scale take place?
 output (Sparql query): "	SELECT ?location, ?location_label WHERE { { SELECT (MAX( ?geo_scale) AS ?max_geo_scale) WHERE { orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale. } } orkgr:R149849 orkgp:compareContribution ?contrib. ?contrib orkgp:P41568 ?geo_scale; orkgp:P37524 ?location. ?location rdfs:label ?location_label. FILTER( ?geo_scale = ?max_geo_scale) } 	 input (English text): Describe the results of the test set which included the specified model. The full list of results can be found in the test set results. output (Sparql query): input (English text): What results do the tests suggest ? output (Sparql query): Note that you must include at least one set of results, so if you have multiple results, try to match them both. input (English text): Which model is the first model that appears in all the test sets ? output (Sparql query): input (English text): Describe the results of the first step in the model's development. output (Sparql query): input (English text): Describe the performance of the specified model on the specified dataset. The performance may differ depending on the type of test that was run, and the type of dataset that was used. output (Sparql query): Input data: Model model_lbl_test set output (Sparql query): Input data: No Model model_lbl set Result set1 Set 2 Set 3 Result set1 set2 set3 result set1 set2 set3 Result set1 set2 set3 R2 (95%CI) 8.08 0.04 (0	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Percentage error score on the MNIST benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WLPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Inputs: [ ?name ] (string): Name of model or query. Please provide a valid description. [ ?title ] (string): Title of research paper. Please provide a valid description. [ ?id ] (integer): ID of research paper. Please provide a valid description. Results: [ ?name ] If you have more (2 or more) input fields, output data in one table. If none, output data in one row. If there are extra (1 or more) fields, output summary. If there are no extra fields, output data in one row. Otherwise, report errors.	False
"
 input (English text): What models are being evaluated on the Natural Questions (long) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the arXiv dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 select title, id, ?text from ?paper where _id = ?input1 select title, id, ?text from ?title where _id = ?input1 select title, id, ?text from ?title where _id = ?input1 select title, id, ?text from ?title where _id = ?input1 select title, id, ?text from ?title where _id = ?input1 select title, id, ?text from ?title where _id = ?input1 select title, id, ?text from ?title where _id = ?input1 from ? ? ? ?select title, id, ?text from ?title select title, id, text from ?page where _id = ?input1 select title, id, text from ?page where _id = ?input1 select title, id, ?value from ?page where _id = ?input1 select title, id, ?value from ?page where _id = ?input1 for _key in [0..100]: select title, id, ?key from ?page where _id = ?input1 as output from ?input1 where _id = ?input1 select title, id, ?value as ?key from ?page where _id = ?input1 as output from ?input1 where _id = ?input1 from ?page select title, id, ?value as ?key from ?page where _id = ?input1 as output from ?input1 where _id = ?input1 from ?page select title, id, ?value.value as ?keys from ?page where _id = ?input1 as output from ?input1 where _id = ?input1 from ?page select title, id, ?value.key as ?keys from ?page where _id = ?input1 as output from ?input	False
"
 input (English text): List the metrics that are used to evaluate models on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the IMDb dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. ORG_SEARCH RDFS ""SciERC"" { ?model model_lbl. ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ORG_SEARCH AND ( { ?dataset a"	False
"
 input (English text): Provide a list of papers that have utilized the SciNLP-KG model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciNLP-KG"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the CoNLL 2012 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Example 3: Model evaluation against existing datasets With this second example, the KG model is evaluated against the data sets presented in the previous two examples. The models in the fourth and fifth examples use a dataset derived from the same dataset at the level of ""batch size"" (number of records) as the input, so they are already highly performant. The model ""M"" is used once only in the final step. The output format is ""Score"", which gives an index of the performance of the model. The model ""A"" should then be evaluated against the output of the KG ""A"", which is shown below. The model ""A"" has been optimised using the following steps: Convert the input to a NumPy array. For each sub-array, calculate the Kg statistic for each of the first n rows and calculate the Kg statistic for each of the last n rows (which is called ""Kg_index""), which would be the number of rows in the array corresponding to the given sub-array. Convert the resulting Kg_index into a SortedDict by applying the SortedDict.sum function and creating a new SortedDict object that has Kg_index set to the total number of rows in the input array. Now, when looking at the score for the model ""A"", we can see something similar to the score from the previous examples: At the time of this writing, that model performed nearly identically to the KG model in terms of accuracy. However, the KG model achieved an overall Score score of 30.05. Example 4: Evaluating the Model using the SortedDict The SortedDict model is now evaluated for the ""Score"" value. After some calculations, the Score value for the SortedDict models is shown above:"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ORKG-TDM dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ORKG-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the FGVC Aircraft dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the WMT2014 English-German dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT top_cnt_1, top_cnt_2, top_cnt_3, top_cnt_4, top_cnt_5, top_cnt_6, top_cnt_7, top_cnt_8, top_cnt_9, top_cnt_10 FROM nasa_data_1 AS dsa1 UNION ALL SELECT top_cnt_nest_1, top_cnt_nest_2, top_cnt_nest_3, top_cnt_nest_4 UNION ALL SELECT top_cnt_nest_nest1, top_cnt_nest_nest2, top_cnt_nest_nest3, TOP_CNT_nest UNION ALL SELECT top_cnt_nest_nest4, top_cnt_nest_nest5, top_cnt_nest_n, top_cnt_nest_nest6, top_cnt	False
"
 input (English text): List the metrics that are used to evaluate models on the CS-NER benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CS-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 HERO dataset in terms of Best Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Yelp-14 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT MAX(r( ?dataset ?data)) / 10 / 1.05 + MAX(r( ?dataset ?data)) / 10 / 2.5 + 0.2 * (p(rank) *.0001), ?dataset ?data AS r(rank) FROM { ?dataset d:dataset orkgc:HAS_DATASET} orkgp:RESTRICT { csv:label ?dataset_lbl. } ORDER BY DESC( ?rank) LIMIT 10, 1 The output of the first query gives a benchmark result of 5.12 which is good, but we should see better results in the second query with a ranking of 2.1. A quick benchmark of performance of a dataset on a different benchmark to an existing dataset, for instance a dataset of Atari games: input (English text): List the metric numbers used during evaluation on the same benchmark dataset ? output (Sparql query): SELECT IF (RESTRICT(c( ?dataset ?dataset_lbl) = ""Atari 2600 HERO"")>0 ?dataset_lbl. OR (RESTRICT(c( ?dataset ?dataset_lbl) = ""Atari 2600 HERO"")>0) ?dataset_lbl. ), ?data:List of datapoints in each of the Atari games dataset, sorted by their score ? The output of the second query is much better, but it does not show the full details of the Atari benchmarks. There is no information in the query except for the metrics which can be obtained by querying the metrics defined in the dataset. The most popular benchmarking dataset is in fact the Atari 2600 GAME HACK, where an Atari 2600 game can be tested for its"	False
"
 input (English text): Where can I find code references in papers that have used the Naive Bayes model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Naive Bayes"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF (1 day) hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): Is 'Mochilla' also used for 'Classimed' or 'Constrained' benchmarks ? output (Sparql query): input (English text): Is 'Caveat for the Future' used anywhere else for 'Classimed' or 'Constrained' benchmarks ? output (Sparql query): input (English text): What are the sources and the authors of the code used for the benchmarking of the Naive Bayes model ? output (Sparql query): output (Sparql query, English text): input (English text): What kind of data are used in these benchmark tests ? output (Sparql query): output (Sparql query, English text): How many models are used for the specific benchmarks in the dataset ? output (Sparql query): input (English text): How many datasets are there ? output (Sparql query): output (Sparql query, English text): output (Sparql query, English text): What type of codes is used in the benchmarking of a model ? output (Sparql query): input (English text): What kind of features are used to describe the benchmark ? output (Sparql query): output (Sparql query, English text): What code attribute is used for determining the parameter to use in the benchmarking ? output (Sparql query): input (English text): How did you find out what the parameters used in the benchmarking are ? output (Sparql query): output (Sparql query, English text): input (English language): Did you find out that any of the code to compute these parameters came from NaiveBayes or SparseR	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SST-5 Fine-grained classification?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the SMT + NMT (tuning and joint refinement) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SMT + NMT (tuning and joint refinement)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the RTE benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 It would be nice to be able to do the next step manually and in parallel, but sadly there isn't enough time. I'll post instructions in the next post. Update (2014-01-16) : I made one more post for the new ECEA 2013 ECEA paper. It's more short and focused on the new RTE benchmark dataset, but it still includes the original ECEA text and all the original benchmarks. Update (2015-05-01): I've made a blog post that explains the code. This may be a good place to start with the code itself.	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CommitmentBank?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the WOS-5736 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Inputs: SqlCommand SqlData SqlResultResult SqlCommand : The command used to retrieve the dataset data. SqlData : The dataset where the score is defined. SqlResultResult : The result code (i.e., the error code or the expected result code). If the result code is missing, a null reference will be generated. For example, if the score is 500 and the output includes '1' for'metric_lbl' and an empty string for'metric' and '0' for 'value', then the result code will be 1. This is a different error code than for the CASE WHEN score > 500 and METRIC LIKE 'Metric(value) = '. If a null reference is supplied in the output, a special exception will be thrown to indicate that the output should not be interpreted accordingly. In order to retrieve values in the WOS-5736 dataset, use the following query: $ rsql ""select $dataset as ?metric_lbl, ?value as ?value, ?score as $rdb$result"" (where data table: 'dataset' not in ('WOS-5736') AND data table column:'metric_lbl' not in ', METRICS ',); The output of the command to retrieve the dataset can be of several types:  'Metric' : The metric that is selected through the 'SET' clause. It is of the following form: score : number : value Where score is an integer value. Example: The following query will select the metric metrics'score', 'tokens', and'metric' (and return the data only for the metrics which have been included in the resulting query data). To retrieve the metric scores in the WOS-5736 dataset, use the following"	False
"
 input (English text): What models are being evaluated on the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Seaquest dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WebQuestions dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebQuestions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT * FROM DATASETS WHERE name = ? orgt; input (English text): Please provide a list of benchmark results submitted for the SAC/SNA benchmark on the Paperfield dataset.   output (Sparql query): Select all benchmark results on the Paperfield dataset from page'results'. output (Sparql query): Output a table of all of the benchmark results for the Paperfield dataset. input (English text): Please provide a single ID for each paper that has been assigned the model name 'Paperfield' and benchmarked on the WebQuestions dataset. output (Sparql query):	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Video Pinball dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ART/CoreSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" (The score was not used here) . OR : Input from the SciERC test set. Note: I used the following test set: test set id: 10001, grade: 6, model: 'c-t-j-c' (1-10) Result in a Table containing scores as rows: (The score wasn't used here) This example shows some of the properties and characteristics of all the benchmarks, in order for the user to perform a comparison. This is a sample JSON schema of the data set: { ""data:"" : ""test_data/battles"", ""categories: [ ""game"", ""action"", ""d-pad"", ""d-pads"", ] } All of the benchmarks are on the test set ""game"", ""action"", ""d-pad"".   The first benchmark, ""game"", consists of a ""d-pad"" button. To calculate the score, we compute the ""game"" score and the ""action game score"".   The ""d-pad"" benchmarks are defined below: Test sets:   ""d-pad"" : Test Set 1  (from DataSet.test_lbl"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CMLM+LAT+1 iterations model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CMLM+LAT+1 iterations"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 James Bond dataset in terms of Medium Human-Normalized Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Medium Human-Normalized Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT feature_name ?|feature score FROM Natural Questions(short) WHERE { ?feature a OR kgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""N"") ?benchmark ?model. ?model_lbl ?feature_name. |feature_score } The only features whose scores are included in this query are the scores of the two models listed above, and not the"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the MP-EB model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MP-EB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT MAX(1,N,MAX(1)); ?benchmark ORAGkgc:Evaluation ANDAGkgc:Score; ?dataset. orkgc:HAS_BENCHMARK. ?dataset_lbl. ORAGkgc:HAS_MODEL. output (Sparql query): SELECT ?dataset ANDAGkgc:Evaluation. ?dataset_lbl. ?dataset_lbl. ( { ?dataset ( ?model! ANDAGkgc:Evaluation!. ORAGkgc:Score! ORAGkgc:Evaluation! ) ORAGkgc:SourceCode. } ). output (Sparql query): SELECT ?dataset ANDAGkgc:Evaluation &&& ORAGkgc:Score &&& ORAGkgc:Evaluation ( { ?data_data_data ORAGkgc:Evaluation! ANDAGkgc:Score! ANDAGkgc:Evaluation! } ). output (Sparql query): The CSV and JSON files are available here. I was able to import a version of the WMT2014 dataset with the latest benchmark results from this Postdoc on GitHub.	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-Romanian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Test perplexity score on the WikiText-2 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?db_model ?DB_model_lbl. RDFS:label ?db_model_lbl CASE orkgp:HAS_DATASET ?dataset WHERE DISTINCT { ?db_model_lbl ?, ?db_model_lbl. } OPTIONAL ; input (Latin text): The name of the model that performed best in terms of BLEU score metric on the WikiText-2 benchmark dataset output (Sparql query): SELECT ?DB_model ?DB_model_lbl. RDFS:label ?DB_model_lbl. input (English text): The name and the text of the model that performed best on the WikiText-2 benchmark dataset in an English-Grammatical language output (Sparql query): SELECT ?DB_model ?DB_model_lbl. RDFS:label ?DB_model_lbl. The input data will be a list of English text records. Use the following output format to run the query: select model_lbl, db_model, orkgp_lbl, db_model_lbl, orkgp_lbl_lbl from table(db_model, orkgp_lBl) The output of the above query will be a list containing the model names of two databases, as well as one of the three database names. If the model name column of the database is set to db_model_lbl, the output data will include the three models that the model was included in. If the model name column of the database is set to orkgp_lbl, the output data will include the three models that the model was included in. (In all of these cases,	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Solaris dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the ST-MoE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ST-MoE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the All-attention network - 18 layers model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 18 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Where can I find code references in papers that have used the C-LEARN-LEARN model for benchmarking purposes ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C-Learner"") ?benchmark orkgp:HAS_DATASET ?dataset.. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code.. input (English text): Where can I find code references in papers that have used the C-LEARN-LEARN - 36 layers model for benchmarking purposes ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C-Le"	False
"
 input (English text): What is the best performing model benchmarking the CoNLL04 dataset in terms of NER Micro F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy (Middle) score when benchmarked on the RACE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (Middle)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Does paper ""Disambiguating authors in citations on the web and authorship correlations"" employe Cosine similarity?
 output (Sparql query): "	"ASK WHERE { orkgr:R6187 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5002 orkgr:R6006. FILTER(REGEX(STR( ?paper_title), ""Disambiguating authors in citations on the web and authorship correlations"", ""i"")) } "	 input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query):  input (English text): input (English text): output (Sparql query): input (English text): output (Sparql query): 	False
"
 input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT TOP(0) RESULT  ( ?dataset dbl orkgc:Dataset) FROM  SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""stsa-benchmark"") ?benchmark dbl orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval dbl orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } We also need to know the output of the top 50 results (according to the S2 score on the last two pages of results table, not the first). For that, I've written two SQL scripts with the code below: input (English text): List of the Top 500 most frequent results in S2. output (Sparql query):      SELECT TOP(100)  COUNT(DISTINCT ?t)  FROM results.fetch_all()         SELECT TOP(50)  COUNT(DISTINCT ?t)  FROM results.fetch_top_50()         SELECT TOP(50)  COUNT(DISTINCT ?t)  FROM results.fetch_top_50_top 50         SELECT TOP(50)  COUNT(DISTINCT ?t)  FROM results.fetch_top_50_top_"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Outputs : A B C D E FF M F R 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Total Time: 0.00 secs	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciBERT + CNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT + CNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the Penn Treebank (Word Level) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT CASE RANKED 'benchmark' WHEN 1, 2, 3, 4 THEN 'benchmark', ELSE 0 END AS titles, CASE RANK_FACTOR 2 DESC AS ID IN SET CASE WHEN ?language = 'eng' THEN 'benchmark'ELSE ?country = 'eng' END AS author, CASE RANK_EVALUATION 2 DESC AS RANKED FROM 'benchmark_data_test' WHERE id IN (SELECT text FROM 'benchmark_data_test') ?benchmark_dataset OR ?benchmark_dataset_bmp2 FROM 'benchmark_dataset_databaset OR 'benchmark_dataset_bmp2' ORDER BY text,rank FROM 'benchmark_dataset_lbl' WHERE id IN (SELECT text FROM 'benchmark_data_lbl') OR ?dataset_lbl ORDER BY text,rank FROM 'benchmark_dataset_lbl2' WHERE id IN (SELECT text FROM 'benchmark_data_lbl2') OR ?dataset_lbl2 ORDER BY text,rank FROM 'benchmark_dataset_sourcecode' WHERE id IN (SELECT text FROM 'benchmark_sourcecode') ORDER BY text,rank FROM 'benchmark_dataset_sourcecode2' WHERE id IN (SELECT text FROM 'benchmark_sourcecode2')) OR ?benchmark_dataset_bmp2 input (English text): If the benchmark is to benchmark SciBERT + DCASE versus other preprocessing pipeline models, which model should be used ? output (Sparql query):  SELECT CASE (ID = 5), CASE (ID = 4) AS id, CASE (ID = 3) AS description, CASE (ID	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the Quasart-T dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Yelp Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DESC( ?count PER ?metric_lbl ?dataset_lbl. ?metric rdfs:label ?metric_lbl. ) AS ?count, DESC( ?max PER ?metric_lbl ?dataset_lbl. ) AS ?max,...,..., DESC( ?cper PER ?metric_lbl ?dataset_lbl. ) AS ?cper,...,...,...,..., DESC( ?metric PER ?metric_lbl ?dataset_lbl. ) AS ?metric,...,...,...,...,...,..., DESC( ?tot PER ?metric_lbl ?dataset_lbl. ) AS ?tot,...,...,...,...,...,...,...,...,..., AS SELECT DESC( ?rval PER ?metric_lbl ?dataset_lbl. ) AS ?rval, DESC( ?score PER ?metric_lbl ?dataset_lbl. ) AS ?score,...,...,...,...,...,..., COUNT( ?) PER ?metric_lbl... AS ?metric_lbl,... ORDER BY DESC( ?rank PER ?metric_lbl ?dataset_lbl. ) DESC( ?value PER ?metric_lbl... AS ?metric_lbl,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...,...) input (English text): What is the highest performance metric of the classification dataset compared to the Yelp dataset ?	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Fast Search model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fast Search"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the HMDB51 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Enter a value for the field : Score or enter an error : ?score output (Sparql query): output (Sparql query): Top-1 Accuracy The following three questions will ask you to select a model from a set of models in the database from which you will perform a top-50 (or top-100) evaluation of a sample HMDB51 model. Each question will be related to an exercise given at the end of an APL 2011 workshop by APL 2012 organizers. Please answer as many questions as possible which will lead you to a model from a set of models that have been constructed to perform a top-50 (or top-100) evaluation of the sampled dataset. For an example question, please see Question 3. 3. What sets off any of the previous models in this list ? Answer as many questions as possible which will lead you to a model from a set of models that have been constructed to perform a top-50 (or top-100) evaluation of the sampled dataset. Please answer as many questions as possible which will lead you to a model from a set of models which have been constructed to perform a top-50 (or top-100) evaluation of the sampled dataset. For an example question, please see Question 6. 4. What sets off any of the previous models in this list ? Answer as many questions as possible which will lead you to a model from a set of models which have been constructed to perform a top-50 (or top-100) evaluation of the sampled dataset. Please answer as many questions as possible which will lead you to a model from a set of models which have been constructed to perform a top-50 (or top-100) evaluation of the sampled dataset. For an example question, please see Question 10. 5. What sets	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Wizard of Wor dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the SRU++ model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BERT + BiLSTM + CRF Decoding model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT + BiLSTM + CRF Decoding"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 This was just a quick-and-dirty demo, so there's lots to explore. I've added a function, benchmark, that allows you to query the BERT + BiLSTM + CRF dataset and compare it to the Atari 2600 Wizard of Wor dataset. We can also get the raw file data of these papers for reference. If you run a benchmark like my one above against the Atari 2600 Wizard of Wor database, please add comments about paper, model, benchmark, and dataset to the table. I'd really appreciate these stats being added to the BERT + BiLSTM + CRF dataset data and to the Atari 2600 Wizard of Wor database in other datasets. If you have any additional recommendations to add to the BERT + BiLSTM + CRF dataset, post them in the comments below.	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the WMT2016 English-Czech dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Czech"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the MedSTS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 select  mean( { ?dataset:MedSTS ?dataset_lbl }  where { ?dataset:Dataset } and ( ?dataset_lbl. ?metric. ?metric_lbl)) as metrics_lbl_score, { ?metric rdfs:label ?dataset_lbl. } as metrics_lbl, { ?metric rdfs:label ?metric_lbl. } as metric_lbl_score, { ?meta rdfs:label ?meta_lbl_score. } as meta_lbl, { ?meta rdfs:label ?meta_lbl_score. } as meta_metric_lbl from { ?dataset:Dataset ?dataset_lbl} where label  org_c:DataSet; and metric  orkgc:Dataset; meta_metric_lbl  orkc:Metric ; and metric_metric_lbl from { ?dataset:Metric ?dataset_metric_lbl; meta_metric_metric_lbl  orkc:Metric } where value  gt-1.0 and metric  gt-0.2; to test-c, to-c, to-c-c and to_c-c select  mean_score, metric_score, metric_metric_score from metric_metric_lbl where metric  gt-0 and metric_metric  ge-0.2 and metric_metric_metric  gt-0.2 select  mean_score, metric_metric_score, meta_metric_score from	False
"
 input (English text): Can you provide links to code used in papers that benchmark the MPCM model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPCM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the MLDoc Zero-Shot English-to-Chinese dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Chinese"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the WSC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model. ?feature orkgc:Feature; rdfs:label ?model_lbl. FILTER (str( ?feature_lbl) = ""Model"") ?bench input (English text): What makes MPC's benchmark more valuable than other methods ? output (Sparql query): SELECT DISTINCT ?model.... input (English text): What method(s) would you like to see added to MPC ? output (Sparql query): SELECT 'N' FOR ROW GROUPING BY 'N'; input (English text): What is the current state of MPC ? output (Sparql query): SELECT 'STOP' AS OBJECT_ID, 'RESET' AS CASE_NAME, 'OPEN' AS CASE_TYPE, 'CLOSURE' AS CASE_FLAG, 'EXCL_COPY' AS CASE_NOCALL, CASE_HAS_NOCALL, 'CLOSE' AS CASE_OPTION, 'CLOSE_EXCL_COPY' AS CASE_OPTION, 'CLOSE_END_LICENSE': ?clo-code ?clo-date ?clo-ext input (English text): What is the current state of MPC ? (1 = complete, 0 = unfinished) output (Sparql query): 1 1 1 "	False
"
 input (English text): What are the objectives for Sepsis prediction?
 output (Sparql query): SELECT DISTINCT ?objectives WHERE { orkgr:R70642 orkgp:compareContribution ?contrib. ?contrib orkgp:P15051 ?objectives. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the linear-chain CRFs model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""linear-chain CRFs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Input code links: Input code links: Input code links: Input code links: The output files are structured based on the code links, while the code links in the input file are structured based on some of the code links the results of this paper. The code links from the input file are very specific to the data sample and do not have any relation to any of the benchmarks and code links reported here. We will include a summary of the benchmark and code link structures used in the next chapter with a link to each benchmark. The codes provided are written in the following format: [code] - [code1] - [code2] The code links from the input file are structured based on some of the code links the results of this paper. We will include a summary of the benchmark and code link structures used in the next chapter with a link to each benchmark. The code links from the input file are provided in the following format: code link - code2 code The output files are structured based on the code links from each benchmark, while the code links in the input file are structured based on some of the code links the results of this paper. The code links from the input file are provided in the following format: code link - codeID codeID The code links from the input file are structured based on some of the code links the results of this paper. We will add a summary of the benchmark and code link structures used in the next chapter with a link to each benchmark.	False
"
 input (English text): Provide a list of papers that have utilized the GRU (Bai et al., 2018) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GRU (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the DDQN (tuned) noop model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tuned) noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the DrQA model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT TOP( 1). ?label ?model_. ?dataset. ?cont OR.... ?model_lbl. FILTER (str( ?model_lbl) = ""DDQN (tunneled) orkgc:Model"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont OR... OR... output (LazySQL query): SELECT TOP( 1). ?label ?model_. ?dataset. ?cont OR... OR READMIME('application/octet-stream'); ?dataset. ?cont OR READMIME('application/octet-stream'). ?cont OR READMIME('application/octet-stream') OR READMIME('application/octet-stream') OR read - c output (Slicing query): SELECT TOP( 1). ?label ? ?dataset. ?cont IF( ?dataset. ?cont IS NO) OR ( ?dataset. ?cont IS NO AND ?dataset. ?cont IS NO) OR ( ?dataset. ?cont IS NO and ?dataset. ?cont IS NO AS DEFAULT ?dataset. ?cont IN NULL and ?dataset. ?cont IN NULL ) ?benchmark AND... OR READMIME('application/octet-stream'); ?dataset. ?cont AND read - c output (Sparql query): SELECT TOP( 1). ?label ? ?dataset. ?cont AND READMIME('application/octet-stream'); ?dataset. ?cont AND read - c input (English text): Provide a list of papers that have utilized the SIPR model in any benchmark ? output (Sparql query): SELECT TOP( 1). ?label ?"	False
"
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Ms. Pacman benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the precision and recall of SemGraphQA?
 output (Sparql query): SELECT ?precision ?recall WHERE { orkgr:R6898 orkgp:compareContribution ?cont. ?cont orkgp:P34 ?eval. ?eval rdfs:label ?eval_labels. FILTER(REGEX( ?eval_labels, ""SemGraphQA"")) ?eval orkgp:P3004 ?precision; orkgp:P5015 ?recall. } 
 input (English text): Which model has achieved the highest Accuracy score on the SST-5 Fine-grained classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?precision ?recall WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a = ?dataset ?dataset_lbl. ?dataset_lbl rdfs:label ?dataset_lbl. ?dataset_lbl } ORDER BY DESC( ?value ) LIMIT 1 } } The answer is that the model called ORKB, which is the most sophisticated one, with a precision of over 1000, but the recall was under 50. When I look at a machine learning algorithm like the one used by SemGraphQA, the accuracy is not important in this case. How much RAM do you need ? input (English text): What is the memory used by SemGraphQA ? output (Sparql query): input (English text): How much RAM do you need ? output (Sparql query): input (English text): How much memory do you need ? output (Sparql query): input (English text): How much memory do you need for the SemGraphQA ? output (Sparql query): input (English"	False
"
 input (English text): List the metrics that are used to evaluate models on the ANLI test benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC-PDN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 As you should now recognize, in the first two cases, there is a lot of space between the model inputs. However the first case may have several different metrics; in the second both models and the metrics are taken from the same sample (so the third case is not possible since it does not have enough time to compute the different metrics. And what is the difference ? The first column is the raw number of models per metric. In the second case, the number of metrics varies from the original. There is also a column called benchmark which displays the different results of the benchmarking benchmarking. This gives you enough information to have a good idea for which metrics are being used. All in all there are three different questions about the different models and tests used by the two libraries. Hopefully you will gain the same understanding that I had.	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the CUB-200-2011 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CUB-200-2011"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which QA benchmarks incorporating more than 10000 questions?
 output (Sparql query): "	"SELECT ?papers, ?papers_labels WHERE { ?papers rdf:type orkgc:Paper; orkgp:P31 ?contrib; rdfs:label ?papers_labels. ?contrib orkgp:P41923 ?questions_number_str. BIND(REPLACE( ?questions_number_str, ""[+]"", """") AS ?questions_number_preprocessed) BIND(xsd:integer(REPLACE( ?questions_number_preprocessed, """","""", """")) AS ?questions_number) FILTER( ?questions_number > 10000) } "	SELECT TOP 15 ?rvs -  * 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT TOP 15 ?rvs - ?rvs.1  * 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT TOP 15 ?rvs - ?rvs.1.1  * 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT TOP { ?rvs ?rvs.2  * 1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT TOP { ?rvs.2.1.1  * 1 2 3 4 5 6 7 8 9 10 11 12 13  * 1  } ?rvs.2  * 1 2 3 4 5 6 7 8 9 10 11 12 13 ORDER BY DESC( ?rvs.1 ).ORD BY DESC( ?rvs).ORD BY DESC( ?rvs.1.1 )  ; ?rvs.2.1.1  * 1 2 3 4 5 6 7 8 9 10 11 12 13 ORDER BY [ ORDER BY DESC( ?rvs.1 DESC( ?rvs.1.1 ) ) DESC( ?rvs.1.1 ) DESC( ?rvs.1.1 ) ] DESC( ?rvs.1.1) DESC( ?rvs.1.1) DESC( ?rvs.1.1) ORDER BY DESC( ?rvs.1.1 DESC( ?rvs.1.1 ) ) DESC( ?rvs.1.1 ) DESC( ?rvs.1.1 ) DESC( ?rvs.1.1) DESC( ?rvs.1.1 ) ] ORDER BY [ ORDER BY	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset ANLI test?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the datasets benchmarked under the relation extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""relation extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT TOP (SELECT ?method ?metric FROM ?dataset ( ?problem:a OR ?""Problem""; rdfs:labels ?problem_lbl. ) AS ?method; rdfs:pivot ?dataset_lbl. ORDER BY ?method (Sparql Query) The following benchmark results: Parameter Metric Value ANLI test ANLI score 0.83876 (2.679317) ANLI test_mean ANLI score 1.93626 (3.097631) ANLI test_median ANLI score 1.84759 (3.079964) ANLI test_median_mean ANLI score 1.96318 (3.126811) ANLI test_std ANLI score 1.97189 (2.556792) ANLI test_std_mean ANLI score 2.01128 (3.203667) ANLI test_std_median ANLI score 2.02979 (3.209829) ANLI test_std_median_mean ANLI score 2.01448 (3.212222) ANLI test_tend ANLI score 2.03125 (3.233827) ANLI test_tend_mean ANLI score 2.02498 (3.223345) ANLI total ANLI scores 0.83876 (1.986979) ANLI total_mean ANLI scores 0.83912 (9.235729) ANLI total_median ANLI scores 0.83934 (3.078994) ANLI total_median_mean ANLI scores 0.85406 (7.780185) ANLI total_std ANLI scores 1.93626 (4.983"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Yars Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the models that have been evaluated on the SentEval dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Here are details of the output in a tab separated values file format:	False
"
 input (English text): Provide a list of benchmarked datasets related to the stochastic classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""stochastic classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the highest benchmark result achieved on the CNN / Daily Mail dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT 1, :{} FROM ?datasets ?datasets_lbl LEN ?metrics ?metric_lbl INNER JOIN ?datasets ?datasets_data_lbl ON LATEST(!lbl_datapath) LEN INVOKE JOIN ?datasets ?dataset_lbl ON ?dataset_lbl. ?datapath LIKE ?data_lbl ORDER BY ?metric) input (English text): What is the English-language version of the Daily Mail dataset with its metric ? output (Sparql query): language = (SELECT 1, :{} FROM ?datasets ?datasets_lbl LEN ?lbl_metrics ?lbl_lbl INNER JOIN _metrics ?stats_lbl ON LATEST(!lbl_metavg) LEN INVOKE JOIN ?datasets ?datasets_data_lbl ON ?data_lbl. ?metavg LIKE ?data_lbl ORDER BY ?metric) input (English text): What is the metric to measure the quality of the English dataset ? output (Sparql query): input (English text): What is the metric to measure the quality of the Daily Mail dataset with its metric ? output (Sparql query): language = (SELECT 1, :{} FROM ?datasets ?datasets_lbl LEN ?lbl_metrics ?lbl_lbl INNER JOIN _metrics ?stat_lbl ON LATEST(!lbl_metav	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Space Invaders dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the Stanford Cars dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT COUNT ( ?revs ), MAX( ?revs. ?max ), MAX( ?revs. ?min ), SUM( ?revs. ?percent ) FROM tcs.timeline WHERE ?dataset, ?benchmark, ?dataset_lbl, ?coeff, ?coeff_lbl, ?metric output (Sparql query): SELECT COUNT ( ?revs ), MAX( ?revs. ?max ), MAX( ?revs. ?min ), SUM( ?revs. ?percent ) FROM tcs.timeline WHERE ?dataset, ?benchmark, ?dataset_lbl, ?coeff, ?coeff_lbl, ?metric output (Sparql query): In the previous example, we used a regular expression to match data into an SQL query. Here, we will use a more sophisticated SQL that will not be too much like regular expressions. When you are writing SPSS routines in SPSS, you will often encounter certain patterns and you may write similar queries over the same files. The SQL here will match those patterns and do an analysis of the datasets when executing an analysis. The first row contains three fields: the original record, the query, and the analysis. The second column contains a value that indicates whether or not the results were compared or a different sort was used. Please read the following section to understand the following SQL: select COUNT ( ?revs.), MAX( ?revs. ?max ), MAX ( ?revs. ?min ), SUM( ?revs. ?percent ) FROM tcs.timeline WHERE ?tcl_status= ?and ?dataset AND ?dataset_lbl ( ?coeff ), ?metric, ?dataset_lbl output (Spar	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Stanford Dogs dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Bit per Character (BPC) score on the Penn Treebank (Character Level) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bit per Character (BPC)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT id, rdfs:name, id, ncol, rdfs:dataset, rdfs:benchmark from dataframe_test for id1 int ?data1 ?dataset. id2 int ?data2 ?dataset. ncol int ?col1 ?dataset. ncol int ?col2 ?dataset. } input (English text): Name or identifier of a benchmark dataset that has been benchmarked with the Stanford Dogs dataset ? output (Sparql query): rdfs:name, id1 int ?data1 ?dataset. rdfs:dataset, rdfs:benchmark, id1 int ?data1 ?dataset. id2 int ?dataset ?dataset. ncol int ?col1 ?dataset. ncol int ?col2 ?dataset. } output (Sparql query):   input (English text): Describe the title of a benchmark paper that has been benchmarked with the Stanford Dogs dataset ? output (Sparql query): SELECT TITLE as title, id1, rdfs:name, id2, ncol, rdfs:dataset, rdfs:benchmark, id1, id2 as n1, ncol as n2 from dataframe_test for 	False
"
 input (English text): What is the best performing model benchmarking the WebNLG dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the SemEval-2010 Task 8 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2010 Task 8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): For what piezoresistive material the least response time was obtained?
 output (Sparql query): "	SELECT ?piezoresistive_materials WHERE { orkgr:R138374 orkgp:compareContribution ?contrib. ?contrib orkgp:P41021 ?piezoresistive_materials. ?contrib orkgp:P41022 ?response_time. } ORDER BY ASC( ?response_time) LIMIT 1	SELECT d. Pitch_Quality, ( ?p. Metric === ?p. Dataset ?metric_lbl. ) AS Metric, ( ?p. Dataset === ?model ?metric_lbl. ) AS Metric, ( ?p. Pitch_Quality === 0 ?metric_lbl. ) AS Metric_X_Y FROM ?model_lbl, ?tBLA AS tBLA INNER JOIN.Metric AS mm ON mm.p.Metric = ?tBLA.p.Metric ORDER BY mm.r.d. Dataset_Pitch, mm.d.r.d. Pitch_Quality, mm.d.l.d.Dataset, mm.d.l.d.Pitch_Quality; --------------------- ORDER BY d. Pitch_Quality, ( ?p. Metric === ?p. Dataset ?metric_lbl. ) AS Metric, ( ? p. Dataset === mm. d. Dataset ?metric_lbl. ) AS Metric, ( ? p. Pitch_Quality === ?p. Dataset ?metric_lbl. ) AS Metric_X_Y FROM ?model_lbl, ?tBLA AS tBLA INNER JOIN.Metric AS mm ON mm.p.Metric = ?tBLA.p.Metric ORDER BY mm.r.d. Dataset_Pitch, mm.d.r.d. Pitch_Quality, mm.d.l.d.Dataset,	False
"
 input (English text): What is the top benchmark score and its metric on the ChemProt dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ChemProt"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the average efficency for experiments?
 output (Sparql query): SELECT AVG( ?efficency_values) WHERE { orkgr:R155266 orkgp:compareContribution ?contrib. ?contrib orkgp:P43156 ?efficency. ?efficency rdfs:label ?efficency_labels. BIND(xsd:float( ?efficency_labels) AS ?efficency_values) } 
 input (English text): List the code links in papers that use the GPT-2 (small) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-2 (small)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?dataset name as ?dataset_schema WHERE ( ?dataset_schema. GPROP1_PROD, ?dataset_schema. GPROP2_PROD, ?dataset_schema. GPROP3_PROD, ?dataset_schema. GPROP1_RES, ?dataset_schema. GPROP2_RES, ?dataset_schema. GPROP3_RES, ?dataset_schema. RANGE, ?dataset_schema. DENSITY_RANGE, ?dataset_schema. DENSITY, ?dataset_schema. CO2_RANGE ). FIND( ?value, ?rdfs:ID. DENSITY ) GROUP BY ?value input (English text): What was the top score for ChemProt in benchmark output and its metric on the ChemProt dataset ? output (Sparql query): SELECT TOP ( 10 ). ORDER BY ?score; input (English text): What is the best-performing filter for chemprot, given a single input parameter ? output (Sparql query): SELECT best_filter as ?filter ; input (English text): What is the average efficency for experiments that use the GPT-2 (small) model in any benchmark ? output (Sparql query): SELECT AVG( ?efficacy. ) AS ?value ; output (Sparql query): GROUP BY BEST_FLOW input (English text): What was the distribution distribution on the ChemProt dataset ? output (Sparql query): FROM (	False
"
 input (English text): What is the top benchmark score and its metric on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Fishing Derby"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?measurement FROM { ?tournament ?}   WHERE match_score <= ?score WHERE { ?dataset a orkgc:Dataset; rdfs:labels ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Tournament"") && matching_score > ?score; orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?note ?note OR kgp:HAS_NUMBER ?number. OPTIONAL { ?note ?note OR kgp:HAS_METRIC ?metric. } ORDER BY DESC( ?measurement) If you can find a link to a data file that will serve as the benchmark, let me know. If you use some other file that has been created by this database, please let me know. About Data Analysis I am a consultant for a German engineering firm in Munich. My clients come from three sectors -- aerospace, construction and pharmaceutical. My field of expertise is in modeling and research into the application of machine learning. I am constantly looking for good quality data sets and my datasets are regularly updated. I write about my work on my blog: ""Tatastrophic: Machine learning or Big Data ? "". For the better understanding of the capabilities of Big Data and Machine Learning, the Data Science Master Level 3 course was kindly reviewed by Andreas Tappeins. Please contact me if you have any questions or comments on this post, or if you have any problems with it."	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet ReaL dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ARC-PDN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 The benchmark is not available, please choose an alternative benchmark from our index. Input text : What is the best performing model benchmarking the ImageNet dataset in terms of Number of params metric ? Output (Sparql query): The benchmark is available, please choose an alternative benchmark from our index. In both cases, you can select the best method of benchmarking. What is the best approach for benchmarking the ImageNet DataSet ? Which test or benchmark should be used ?	False
"
 input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Provide a list of papers that have utilized the Ours: cross-sentence ALB model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Ours: cross-sentence ALB"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model as ?model_lbl FROM orkgp:HAS_MODELS OF mf_datasets FORMAT (v,i INT. MAX ) | { ?model ?dataset_lbl. FILTER (str( ?model_lbl) = "" ?dataset"" ); ?label ?labels. } input (English text): Which papers are currently being actively evaluated in your project ? output (Sparql query): SELECT list() as %output from %outputs; input (English text): Which journals are currently being actively evaluated while you are working on your dissertation ? output (Sparql query): SELECT journal_name %pub_name FROM (SELECT 'J' journal FROM orkgp:HAS_LABELS OF journal_name OR (Journal.ID = ? and Journal.ISSN = ? and JournalID = ?)); input (English text): Can you summarize the status of your efforts in performing a comparative study of the techniques and applications that you have seen being used in the literature concerning the document database, and the techniques and applications that you are investigating for your own dissertation ? output (Sparql query): SELECT * FROM (SELECT 'C' source_id from orkgp:FILTERS.COPY (orkgp:COPYRIGHT)) OR (SELECT 'B' source_id from orkgp:FILTERS.BATCH); Inputs: English text: What are the primary methods used to perform document summarization in research settings ? output (Sparql query): SELECT  a as ?output_labels WHERE label in OR  b as ?input_labels WHERE label in OR  c as ?output_tags WHERE label in or  d as ?"	False
"
 input (English text): List the metrics that are used to evaluate models on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SciTLDR dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciTLDR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the BiT-M model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" 2.2.4. Performance Summary Input Performance Metrics (in minutes per model) Metric Name Metric Description Default Description Metric Time/Result Description Default Result Benchmark time N(1) 1000 The benchmark time is the time that took for the model to train on a sample data set, and the metric time/result is the amount of time the model trained on the benchmark data set. The benchmark time, time/result, and train time values are relative to the baseline and are not normalized by a running sum of samples time to get the time/result value. Benchmark performance N(1) 1000 The benchmark performance is the number of times the benchmark (or more than one benchmark ?) had an acceptable number of predictions. For example, a model with good model-to-prediction rate was called ""good"" if its model-to-prediction rate was less than 60%. No model was called ""bad"" or an ""unsatisfactory"" model. Good performance N(1) 1000 The performance of the benchmark is the average prediction accuracy of its training set. Unsatisfactory performance N(1) 1000 0 The benchmark had an unacceptable number of predictions. 2.3. Benchmark and Simulation Results 2.3.1. Benchmark Results Results of our performance benchmark. We've run the benchmark and simulation six times, with three runs of two years each and one run of three years. The plots are shown to illustrate the overall results. 2.3.2. Simulated Results You can try the simulations on your own machine if you don't like our results. Just visit our website, log in (using your username and password), and you can run and experiment with the simulation. Please, help us improve that. 2.3.3. Results of Experiment The simulated results in the form of PDFs. ("	False
"
 input (English text): What models are being evaluated on the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT language, LOWER(i) FROM LOWER(English text) LEFT JOIN LOWER(French text) ON ('text' = 'fr') WHERE i = 1 output (slices of text): LOWER(English text) language LOWER(French text) lang varchar(30) ____________________________ _____________________________ where: LOWER(English text) The following example shows the results when applying different values for the lower and upper (default) rows on the English and French dataset. The upper row is used to determine if the first 10 words, ""first"" or ""father""; from this data set, the first English term is used to evaluate the other 8 English words. For each of these 8 English terms, the first word is chosen to evaluate the English-French comparison. select language as 'english', first as 'first', i as u2, lower as 'lower', upper as 'upper', u2 as u3, lupper as 'lower', lupper as u4, rlower as 'lower', rlower as u5, rlower"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Robotank dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Robotank"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S (21k) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S (21k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the NASCell model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NASCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?benchmark ?benchmark_lbl. FILTER { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Atari 2600 Robotank"") ?benchmark. } input (English text): Where can you find code references in papers that benchmark the Atari 2600 ModelS3 (16k) model ? output (Sparql query): Where can you find code references in papers that benchmark the Atari 2600 ModelS3 (16k) model ?"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Kung-Fu Master dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kung-Fu Master"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the JNLPBA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JNLPBA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the TCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"-------------------------- J.N. Kaptchuk, U.L. Pankov & J.L. Cattanavicio. ""Performance of TCN-based models over the Atari 2600 (Unpublished), June 2007.  Abstract : For several years, the Atari 2600 Master (AKM) has been the gold standard for benchmarking the hardware and software capabilities of low-end arcade arcade arcade systems. The primary goal of the AKM benchmark is to demonstrate the ability of the hardware within those systems to execute a large, complex and often monotonic amount of video. In this paper, we present the results from the AKM Benchmark. Although the test is simple in many respects and only performs a relatively small set of tests, the results are interesting; the performance of the various versions of the TCN model is described by its relative performance in comparison to other models, which are not directly comparable. We also report performance results for a number of other systems based on the AKM benchmark. The benchmark is described in the form of a set of rules, or ""benchmark rules"", which the test program generates based on how well a machine can be represented in the data. There are three basic rules that govern the results, each with its own number of steps. A machine with two processing cores performs significantly better than one with only one core. We believe that the results show that the AKM Benchmark shows that the performance of the hardware is not necessarily the bottleneck."	False
"
 input (English text): Can you list the models that have been evaluated on the ObjectNet (Bounding Box) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Top-1 Error Rate metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BUCC French-to-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP( ?) a, rdfs:Rows(t, 10), MAX( ?rgb(a,2) <= 0.1 ?rgb(a,2) > 0.5 ?rgb(a,2) < 0.4 ) AS TOPRESULT FROM DUAL( ?data_lbl, ?dataset, ?dataset_lbl, ?benchmark) AS ?data_lbl OR ?data_lbl ?dataset_lbl AS ?dataset_lbl ORDER BY rdfs:Rows(t, 10) ; OPTION ( --show values of top model in the output table as percentages from 10 to 100 %  , --show all of TOP_RESULT values: top model as well as any model selected for the model:  top model, top rdfs:rows(t, 10),  top metric:  top rdfs:metric % ) AS TOPRESULT FROM DUAL( ?data_lbl, ?dataset, ?dataset_lbl, ?benchmark) AS ?data_lbl ORDER BY rdfs:Rows(t, 10) > 100; ORDER BY rdfs:Rows(t, 10) < 100; % INPUT_FORMATS FOR  - 'DUAL'  | 'GB'  | 'COUNTRY'  | 'US'  | 'DEFAULT'  | 'ENG'  | 'FI'  | 'FR'  | 'IT'  | 'LATIN'  | 'MA'  | 'NO'  | 'PL'  | 'PLN'  | 'PT'  | 'R	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Star Gunner dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of PRE-TRAINING DATASET metric on the DCASE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PRE-TRAINING DATASET"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DCASE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset SQuAD2.0?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?metric ?metric_ lbl  -output (Note - The above model was chosen to minimize the amount of data input. The actual model will depend. If the matrix contains a large number of vectors and not all the data is necessary to evaluate the model, the model can be chosen to have no data, so that the input matrix can be used. For more details, see the documentation for the SQuAD2.0 model ) input (English text): What machine learning algorithm is used to maximize the learning rate on the DCASE dataset ? output (Sparql query): SELECT ?linearity ?linearity_lbl FROM ?rdr1_2_3_2 ON  { ?dataset a  orkgc:Dataset; rdfs:label ?dataset_lbl. ROW_NUMBER( ?dataset_lbl. ?dataset )  = ?row_number( ?dataset_lbl. ?dataset )  ; ORG_KEY: ?dataset_lbl_a ORG_KEY: ?dataset_lbl_b  AND  { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl. ?dataset ) = ""DCASE"") { RECURSE {  row_number( ?dataset_lbl. ?dataset ) ORG_KEY: ?dataset_lbl_a ORG_KEY: ?dataset_lbl_b ~~ RECURSE(""DCASE"") ?dataset_lbl. ORG_KEY: ?datas"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2014 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BC2GM?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What is the highest benchmark result over the dataset BC2GM ? output (Sparql query): input (English text): What is the most popular classification result (metric and value) over the dataset BC2GM ? output (Sparql query): input (English text): Input a list of published articles that have benchmarked the Bi-T-S (ResNet-152x4) model on the WMT2014 English-German dataset. output (Sparql query): input (English text): Input a list of publications of a different type than the ones you provided in the input. A list of different kinds of publications on the WMT2014 English-German dataset is needed because the list is given below. output (Sparql query): input (English text): Input a list of different types of publications on the WMT2014 English-German dataset that you want to benchmark against. output (Sparql query): input (English text): What is the name of the publication. output (Sparql query): input (English text): What is the title of the publication. output (Sparql query): input (English text): What is the publication's publisher ? output (Sparql query): input (English text): What is the author of the publication ? output (Sparql query): input (English text): Who are published authors ? output (Sparql query): input (English text): What is the publication's year of publication ? output (Sparql query): input (English text): What is the publication's publisher's name ? output (Sparql query): input	False
"
 input (English text): Provide a list of papers that have utilized the Discrete Latent Space World Model (VQ-VAE) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Discrete Latent Space World Model (VQ-VAE)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the SemEval-2018 Task 7 dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2018 Task 7 dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 query (string): Select the story to run and the output values of the test models ? or select ?story and ?output_method from output ? or select ?story and ?output_method from output * The optional query parameter specifies the name of the story or output (including an underscore) and the target output model (VQ-VAE, VXGB-VQ-MAIN, VQ-MAIN, or VXGB-MAIN). The optional query parameter is a list of one or more of: a single metric name. An integer (such as ?benchmark or ?constraint for the VQ-VAE benchmark, or ?output_method for the Story Cloze test) or RDF string (such as ?story-sample and ?output_method-samples for the VQ-VAE benchmark, or ?benchmark and ?constraint-samples for the Story Cloze test). A string containing multiple metrics. A list of RDF strings with or without names from which to select a subset of the metrics. The optional query parameter can be specified even when both input and output values are provided. If you can omit the name in the query argument, the values listed are used. If you need to list only one of the metrics in the query parameter, provide only one, even if multiple values are being specified. If only the name is provided, the values listed may vary based on the output format. See the output syntax description in the text of the required output value. Ranking the models: RANKING = ROUTE=<value for the current query result> # = > | <= = >|<value for the index in the last query result. output (Sparql query): RANKING = 1; RANKING: 1-3 = 1 |	False
"
 input (English text): Indicate the model that performed best in terms of RE+ Macro F1  metric on the CoNLL04 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Macro F1 "") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL04"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the enwik8 dataset in terms of Number of params metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Who has contributed to the largest number of articles about coronavirus?
 output (Sparql query): "	"SELECT ?author_name WHERE { { SELECT (COUNT( ?paper_) as ?max_n_papers) WHERE { ?paper_ a orkgc:Paper; rdfs:label ?title_; orkgp:P27 ?author_. FILTER(REGEX(STR( ?title_), ""2019-nCoV"")) } GROUP BY ?author_ ORDER BY DESC( ?max_n_papers) LIMIT 1 } ?paper a orkgc:Paper; rdfs:label ?title; orkgp:P27 ?author. OPTIONAL { ?author rdfs:label ?author_label } BIND(IF(BOUND( ?author_label), ?author_label, ?author) as ?author_name) FILTER(REGEX(STR( ?title), ""2019-nCoV"")) } GROUP BY ?author ?type ?author_name ?max_n_papers HAVING(COUNT( ?paper) = ?max_n_papers)"	SELECT ?source ?source_m, ?timestampl : TIMESTAMPLE AS tst, ?reporters_count : REFERENCE_SCORE AS r_reporters_count, ?timestampl_index : TIMESTAMPLE AS tst_index, ?date_entry_index  , ?tpm_index : TIMESTAMPLE AS tst_index, ?tspm_index : TIMESTAMPLE AS tst_index, ?f_timestampl AS tst_index, ?f_reporters_count AS f_reporters_count Inclusion criteria: Source of articles containing new information concerning coronavirus discovery and distribution and Reporting criteria: Who has submitted the most articles containing new information concerning coronavirus,  and Who is responsible for the most number of articles containing new information concerning coronavirus ? Inclusion criteria: Linking criteria: How has the link to the source of this information been provided, For e.g.: a. The date of publication a. How have you provided the link b. How do you provide the link to this information to the community ? input (English text): A sample of the top 100 articles from the previous survey. output (Sparql query): SELECT ?ID, ?category, ?name  , ?id as article_id, ?name_n, ?lbl_n  from article_n in  SELECT ?category, ?name from n in articles ?table_name * Required	False
"
 input (English text): What is the name of the top performing model in terms of BLEU score score when benchmarked on the WMT2016 Czech-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Krull dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Krull"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the DuIE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT IF( METADATA (A,0=0), METADATA (B,0=0), METADATA (C,0=1), METADATA (D,0=2)), METADATA (E,0=3)), METADATA (F,0=4), METADATA (H,0=5), METADATA (I,0=6), METADATA (J,0=7)), METADATA (K,0=8) AS ?dataset_lbl FROM datastext/duidate; input (English text): What are the models that score highest in terms of BLEU score and its metric ? output (Sparql query): SELECT IF( METAPDATEL(A,0=0), METAPDATEL(B,0=0), METAPDATEL(C,0=1), METAPDATEL(D,0=2)); input (English text): What are the models that have been benchmarked on the H-C data ? output (Sparql query): SELECT ?model ?model_lbl ON dbl (E,C,F,H); input (English text): What is the name of the top performing model when benchmarked on the WMT2008 Czech-English dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER(str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER"	False
"
 input (English text): What is the name of the top performing model in terms of F1a score when benchmarked on the MultiRC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1a"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy (%) score when benchmarked on the FGVC Aircraft dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What coolant does ASTRID reactor use?
 output (Sparql query): "	"SELECT ?coolant ?coolant_label WHERE { ?ASTRID_reactor ?rdfs_label ""ASTRID""^^xsd:string. ?contrib ?reactor_predicate ?ASTRID_reactor. ?contrib ?predicates ?coolant. ?predicates rdfs:label ?predicates_label. FILTER(REGEX( ?predicates_label, ""coolant"")) ?coolant rdfs:label ?coolant_label. } "	 input (English text): What coolant does ASTRID reactor use ? output (Sparql query): input (English text): What cooler does ASTRID reactor use ? output (Sparql query): output (Sparql query): What cooler does ASTRID reactor use ? input (English text): What cooler does ASTRID reactor use ? output (Sparql query): input (English text): What is the name of the top performing model in terms of Performance of the C1C1 dataset ? output (Sparql query): output (Sparql query): input (English text): What is the name of the top performing model in terms of Performance of the S2C1 dataset ? output (Sparql query): output (Sparql query): input (English text): What is the name of the top performing model in terms of Performance of the D1C1 dataset ? output (Sparql query): output (Sparql query): output (Sparql query): What is the name of the top performing model in terms of Performance of the S1R1 dataset ? output (Sparql query): output (Sparql query): output (Sparql query): What is the name of the top performing model in terms of Performance of the G1V1 dataset ? output (Sparql query): output (Sparql query): output (Sparql query): What is the name of the top performing model in terms of Performance of the S1C2 dataset ? output (Sparql query): output (Sparql query): input (English text): What is the name of the top performing model in terms of Performance of the G0V2 dataset ? output (Sparql query): output (Spar	False
"
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the AWD-LSTM-MoS + ATOI model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-MoS + ATOI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"		False
"
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-S-48 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-48"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Dataset mentions in Social Sciences benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT a, a:i from data.frame (1, 2, 3) where { a:i < 10 } or km:a from data.frame (1, 2, 3) where { a:i > 10 } or rdf:a from data.frame (1, 2, 3) where { a:i > 10 } and { a:i < 10 } or m:m from data.frame (1, 2, 3) where { a:i > 10 } or k:k from data.frame (1, 2, 3) or tp:t from data.frame (1, 2, 3) where { a:i < 10 } and { a:i > 10 } or n:n from data.frame (1, 2, 3) where { a:i > 10 } and { a:i < 10 } or cg:cg from data.frame (1, 2, 3) where { a:i > 10 } and { a:i < 10 } or z:z from data.frame (1, 2, 3) where { a:i > 10 } and { a:i < 10 } or m:m from data.frame (1, 2, 3) where { a:i > 10 } and { a:i > 10 } and { a:i < 10 } or d, t from data.frame (1, 2, 3) where { a:i > 10 } and { an:i < 10 } or tp:ts from data.frame (1, 2, 3) where { a:i > 10 } and { an:i > 10 } OR k:k from data.frame (1, 2, 3) where { an:i > 10 } and { an:i > 10 } OR rdfs:r	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Abstracts\' entities and relations annotated corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Abstracts' entities and relations annotated corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the ES FF (1 hour) noop model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ES FF (1 hour) noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the most commonly used benchmark datasets for the Text Summarization research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT ?benchmark.matric ?metric_lbl( max( ?metric_lbl) ) AS ?score ?value > 0 SELECT ?benchmark.matric ?as ?metric input (English text): What are the benchmarks. The output is formatted as a list of benchmark datasets of the Text Summarization research field. If the dataset name contains multiple values and does not begin with /, these values must be specified. output (Sparql query): SELECT ?benchmark.metric ?metric_lbl( max( ?metric_lbl) ) AS ?score ?value > 0... input(English text): What are the most commonly used benchmarks. The output is formatted as a list of the most commonly used benchmarks. output (Sparql query): if(!defined(benchmark) ) result(benchmark) input(English text): What benchmarks. The output is formatted as a list of benchmarks that the Text Summarizer research field uses. If the dataset name contains multiple values, the first number is the benchmark, the second number is the metric, and the third number is the score. output (Sparql query): SELECT ?benchmark.score ?value... *, ?benchmark.metric ?metric_lbl( max( ?metric	False
"
 input (English text): What are the most commonly used benchmark datasets for the Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification. research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Finding an efficient and state-of-the-art approach to search for optimal architectures for image classification."") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Could you provide a list of models that have been tested on the MLDoc Zero-Shot English-to-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Sparql command): image_data_list : 'http://iachotop.net/download/Tutankham.dataset' image_dataset : 'http://iachotop.net/download/Tutankham.dataset' image_data_list : 'http://iachotop.net/download/Tutankham.dataset' image_dataset : 'http://iachotop.net/download/Tutankham.dataset' img_data_names : 'http://iachotop.net/download/Tutankham.dataset' img_data_names : 'http://iachotop.net/download/Tutankham.dataset' img_data_names : 'http://iachotop.net/download/Tutankham.dataset' image_dataset_list : 'http://iachotop.net/download/Tutankham.dataset' image_dataset_list : 'http://iachotop.net/download/Tutankham.dataset' image_dataset_list : 'http://iachotop.net/download/Tutankham.dataset' image_dataset_list : 'http://iachotop.net/download/Tutankham.dataset' image_dataset_list : 'http://iachotop.net/download/Tutankham.dataset' output : a.png image_dataset : 'http://iachotop.net/download/Tutankham.	False
"
 input (English text): List the code links in papers that use the Neural cache model (size = 2,000) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Neural cache model (size = 2,000)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset QuAC?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QuAC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tutankham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (Sparql query): What is the name of the top performing model in terms of Value score when benchmarked on the Atari 2600 Tutankham dataset ? output (Sparql query): input (English text): Which feature is your model most sensitive to ? output (Sparql query): { ?code_score orkgc:FeatureScore ORG_NAMED_FeatureScore ?model rdfs:labels rdfs:dataset_lbl. } orkgc:FeatureScore ORG_NAMED_FeatureScore ?label rdfs:labels rdfs:dataset_lbl. input (English text): What is the name of the feature that needs the most work to fully generalize into ? output (Sparql query): { ?code_score ORG_NAMED_FeatureScore ?model rdfs:labels rdfs:dataset_lbl. } ORG_NAMED_FeatureScore ?label rdfs:labels rdfs:dataset_lbl. option (Integer): Which is the default model name ? (required) option (String): Which model name is your default model using ? (required) output (Sparql query): option (Integer): Which is the name of the model with the most test failures ? (required) option (String): Which is the name of the model with the most success with regard to classification ? (required) output (Sparql query): option (Integer): Please select if: option (String): Which model name is your	False
"
 input (English text): Provide a list of benchmarked datasets related to the Scientific Claim Verification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Claim Verification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the highest benchmark result achieved on the CNN / Daily Mail dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the PROTEINS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PROTEINS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	------------------------- Select Category -------- --- --------------------------- PROTEINS ------------- CNN CNN-PROTEINS CNN-PROTEINS-HNNR PROTEINS-TASK_PROTEINS CNN-PROTEINS-ADVANCED_TASK CNN-PROTEINS-LOW_EXPONENTS CNN-PROTEINS-MASS_PROTEINS CNN-PROTEINS-TASK_PROTEINS CNN-PROTEINS-HNNR PROTEINS-TASK_PROTEINS-HNNR_VGG CNN-PROTEINS-ADVANCED_TASK PROTEINS-TASK_PROTEINS CNN-PROTEINS-ADVANCED_TASK-HNNR_VGG_VGG_FRC PROTEINS-TASK_PROTEINS PROTEINS-EKV PROTEINS_HINTF PROTEINS_TASK_PROTEINS-HINTF_TASK PROTEINS_TASK_CAS PROTEINS_TASK_CAS_HINTF PROTEINS_TASK_CAS_TASK PROTEINS_TASK_CAS_ADV PROTEINS_TASK_CAS_ADVANCED PROTEINS_TASK_ADV PROTEINS_TASK_ADV PROTEINS_TASK_ADVANCED-HINTF_FRC PROTEINS_ TASK_PROTEINS-TASK_PROTEINS-ADVANCED PROTEINS-HINTF PROTEINS_TASK_TASK-ADV ANNUAL_AMPR PROTEINS_TASK_TASK PROTEINS	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Oxford 102 Flowers benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Gopher dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gopher"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT (*) AS a FROM ( SELECT (SELECT i FROM theCount) as i FROM ( SELECT c.name AS j from theCount where i > c.count_of_items i AND j = ?, theCount WHERE c.name = ?, theCount GROUP BY c.name, a ORDER BY i ) as c) AS theCount, theCount_of_items_by_j ORDER BY i DESC c.name DESC j.name ORDER BY c.name ) as j WHERE j.f = ? ORDER BY i DESC c.name DESC j.name ) ORDER BY i DESC c.name DESC j.name ORDER BY i DESC c.name DESC i DESC ( a; c.name ) c.name DESC i DESC c	False
"
 input (English text): List the metrics that are used to evaluate models on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the datasets benchmarked under the Unsupervised Machine Translation research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Unsupervised Machine Translation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the CommitmentBank dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Including a dataset (a variable list) requires the optional argument [ ?dataset]. We will also use an example dataset named Test1 to compare our model selection to test data. In this case, we will use the commit_test database that contains the results of the RACE benchmark challenge (the full dataset has not yet been downloaded). The input will be a table with 3 columns. 1. RACE: The 1st column is (RACE) followed by the RACE_DATE(year) string that indicates the date the dataset was created and released for public use.  2. ORK: The 2nd column is the ORK data type, like ORK_DATA which lists the dataset with the ork data set. Since this dataset was used as part of the research, we do not need the raw dataset as there was plenty of data available for other testing purposes.  3. MODE: 3rd column: the 3rd row of this table indicates the level of evaluation from 0 to F. 0 is not recommended; the 4th and subsequent rows of this row are the criteria on how effective the evaluation process was for each and every metric.  These are the metrics that are being evaluated.  The results are shown in the following example: This is the resulting output: For RACE: For ORK: For MODE: The example dataset (Test1 ) that was used for the study is a table that shows the data sets that were used for testing. This table can also be used in the training process, as it records the training, evaluation and test data sets. The RACE challenge The RACE challenge has one goal: Create the best model for translating English strings into German (e.g. M. K.) words. That is, to predict how well	False
"
 input (English text): Could you provide a list of models that have been tested on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-M (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	_____________________________________________________________________________ Code _____________________________________________________________________________ orkgc:HAS_RESNET_M (resNet1_2.M) orkgc:HAS_RESNET_M (resNet2_2.M) orkgc:HAS_RESNET_M (resNet4_2.M) orkgc:HAS_RESNET_M (resNet5_2.M) orkgc:HAS_RESNET_M (resNet6_2.M) orkgc:HAS_RESNET_M (resNetA_2.M) orkgc:HAS_RESNET_M (resNetB_2.M) orkgc:HAS_RESNET_M (resNetC_2.M) orkgc:HAS_RESNET_M (resNetD_2.M) orl 2.4 Data¶ The original DQN-CTS code base is a distributed dataset of real data (which are not freely available in the wild or at data.gov). We have only extracted a few thousand records from the original dataset. We have chosen to use the CIFAR-10 data set. This dataset is the largest and most extensively used of all datasets, so the CIFAR-10 data sets provides a reasonable level of coverage and allows more data to be analyzed. CIFAR-10 consists of approximately 13 terabytes of data. CIFAR-10 is distributed as a series of high-quality datasets at a central point in the United States: Chicago in Illinois. It has also been used in many other countries. The following figure shows the CIFAR-10 data distribution for the entire United States. This figure was produced by MIGRA and CIFAR10.org. The total data for CIF	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE+ Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT pAST_COMPOUND RANK, pST_COMPOUND RANK, pAST_BENCHMARK RANK || STATEMENT_COMPARE(RE+,Micro-F1,F1B,REB1,Micro-F1+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(Micro-FP1,Micro-FP2,F1B,FP2,F1B+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(Micro-FP2,Micro-FP3,F1B+B,FP3,FP2+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(Micro-FP3,Micro-FP4,F1B+B,FP4,F2+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(Micro-Micro-FP6,Micro-Micro-FP7,F1B+B,FP6,F1B+B+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(Micro-TP6,Micro-TP7,F1B+B,TP6,F1B+B+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(B-FP6,B-FP7,F1B+B,FP7,F2+B), pAST_BENCHMARK RANK || STATEMENT_COMPARE(FP6,FP7,FP6,FP7,FP6+B), pAST_BENCHMARK RANK || STATEMENT_COMP	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the JNLPBA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""JNLPBA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the SAN (ensemble model) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SAN (ensemble model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?score ?metric ?metric_lbl WHERE { ?score rdfs:code_met_lbl. rdfs:value ?code. ?metric rdfs:label ?metric_lbl. }	False
"
 input (English text): What are the models that have been benchmarked on the WOS-46985 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the BC5CDR-disease dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the STEM-ECR v1.0 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 select count from {'m_m2.model.mesh' : 'dummy.model.mesh';'m_m1.model.mesh' : 'dummy.model.mesh'; } ; [ (m_m2 or m_m1), (m_dummy.measure_lbl.sum ), (m_dummy.measure_lbl.count ), (m_dummy.measure_lbl.measure ), (m_dummy.measure_lbl.measure_lbl.mean_y).sum 1 ) m_m1.measure_lbl.sum of all measurements for m_m2 m_m2 or m_m1 1 ) m_metric_lbl_mean_y_value_sig > 0.97.0.0.2 m_metric_lbl_mean_y_value_sig > 0.97.0.0.1 M A P R O U D L Y S M A S T A I N. Measurement Sigment Sum 1 ) m_metric_lbl_mean_y_value_sig > 1.00 0.0.0.0.0.0 M A P R O U D L Y S M A S T A I N. Measurement Sigment Sum 2 ) m_metric_lbl_mean_y_y_mean = rdfs:mean_y; rdfs:lbl_dst_pct_sim > 0.97.0.0.2 M A P R O U D L Y S M A S T A I N. Measurement Sigment Sum 3 ) m_metric_	False
"
 input (English text): Does OWLMAP imply integrity constraints?
 output (Sparql query): ASK { ?approach rdfs:label ""OWLMAP""^^xsd:string. ?contrib ?predicate ?approach. ?contrib orkgp:P41333 ?integrity_constraints. FILTER( ?integrity_constraints = ""t""^^xsd:string) } 
 input (English text): What is the top benchmark result (metric and value) over the dataset BoolQ?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Perspective Matching (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT max( ?, 5) 1 row in set (0.08 sec) input (English text): Can you provide examples of use cases of OWLMAP in various places ? output (Sparql query): -------------- select max( ?, 5) 1 row in set (0.04 sec) In case the database has been prepared for use in multiple versions the following examples show the use of OWLMAP for testing and debugging. example one - testing with 2 versions (MVC) input (English text): With many OWLMAP versions available (1.0 and 2.0) I would like to test and debug for using 1.0 with 2.0 version. How would this be done ? output (Sparql query): SELECT ?instance, ?dataset, ?model, ?score, ?cont( ?metric. ?value) AS ?score, ?cont( ?metric. ?value) AS ?total, ?cont( ?metric. ?value) AS ?approach, ?constraints, ?integrity_constraints, ?match( ?object, ?predicates, ?approach, ?cont. ?values) AS ?top, ?select( ?, 1) FROM ?instance input (English text): 1.0 (one line) example two - testing with 3 versions (MVC) input (English text): In case my version of OWLMAP contains many security features which we do not want to use in testing we should also include additional features for testing and debugging. How would this be done ? output (	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciBERT + CNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT + CNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the SpERT (with overlap) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SpERT (with overlap)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the GAD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" The data from this particular query is of interest to your project: do you consider this dataset in addition to the full dataset used by the current paper ? If so, you need to provide more links, either as well as the additional dataset (e.g. the ""alternate data sets"") for your paper. Note: If you have a dataset that is completely independent from any existing datasets, e.g. for a new project, it is usually acceptable to provide links to your dataset even when doing parallel validation. We also recommend you to add some citations to your papers (see Citation Policy ). You can obtain a detailed list of your current citations from your citation history in your Google Scholar profile. Do you have any additional advice for the authors ? Thanks very much for this generous proposal which is worth the effort on our part and will bring our authors a significant boost in the development of high quality research on the Neural Network approach. It will also make research in this area more widely available, so as to increase the output of the authors, especially researchers in developing countries. We think that the paper will be well received by the authors of the paper and we invite you to submit your work for consideration. We hope you will find a good fit. Do you want to participate in the next generation paper workshop ? We are also interested in other upcoming initiatives such as our upcoming workshop on Artificial Intelligence Deep Learning Research [6] that will be held in Stockholm, Sweden, at the end of 2014. You can take a look at the schedule of the workshop. Thank you to our participants who have already written some interesting papers. It is a pleasure to work with you! Please email us if you would like to be included! Our colleagues in The Department of Computer Science, Göteborg University, University of Göttingen and Centre for Computational Neuroscience have made a significant"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the RTE dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 This R script was created by the project team members. We are grateful to Celine de Moraes et al. for the editorial assistance. We thank Dr. Richard Cichon, Dr. Daniel Rieberg, Dr. Anirudh Sankaranarayanan, Dr. Randal J. Bierman, Dr. Jürgen Hüfner, Dr. Johannes Müller, Dr. Daniel Wüllmuth, Dr. David W. Bockenback, Dr. Joao Paulo, Drs. Andreas Müller, Michael Meyrink and Dr. Peter J. Willemsen for discussions. We are grateful to Jeroen Fries, David W. Brown, Peter J. Willemsen, Thomas A. Schüsser and the TU Löwenstierna for comments on previous versions of this paper and to Paul Ghezzi-Schleussner for editing suggestions for this version. We greatly appreciate the generous support of the European Union's Seventh Framework Programme through the EU-funded International Research and Development Collaboration project, which was instrumental in creating the FB15K dataset.	False
"
 input (English text): What is the maximum egg mass in studies?
 output (Sparql query): SELECT MAX( ?egg_masses_float) WHERE { orkgr:R34845 orkgp:compareContribution ?contrib. ?contrib orkgp:P15692 ?egg_masses. BIND(xsd:float( ?egg_masses) AS ?egg_masses_float) } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC-PDN?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER(str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?comp orkgp:HAS_DATASET ?dataset; ORkgp:HAS_EVALUATION ?eval. ?eval ORkgp:HAS_VALUE ?value. OPTIONAL { TEST( ?dataset as ?dataset_lbl, ?metric as ?metric_lbl)... } ORDER BY DESC( ?value) } input (English text): List the values that were added to the model after fitting in the Geometric Scaling Analysis benchmark dataset ? output (Sparql query): input (English text): What was added to fit a 1-layer hierarchical Bayesian Neural Network model to the Gibson PointGoal Navigation benchmark dataset ? output (Sparql query): input (English text): How many metrics have been used to evaluate a 1-layer hierarchical Bayesian Network model on the Gibson PointGoal Navigation benchmark dataset ? output (Sparql query): select count(*) from gibson pgo nlab nlm with  sort code as (sort code by) where pgo:LINK BETWEEN( nlab:LABEL( ""GLOBAL GIBRONE POINT GOAL BAND (1-LENGTH)"" ), vlab:LABEL( ""G"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Twitter?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Twitter"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ORKG-TDM dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ORKG-TDM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title FROM orkg/papers WHERE ID = ?labels ORDER BY ?title, ?id, ?published_date, ?pub_date input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ORKG-TDM dataset ? output (Sparql query): SELECT NURSES_DETERMINISTIC DISTINCT ?metric ?metric_lbl ORDER BY DESC( ?metric) input (English text): Give me names, numbers and dates of authors and conferences that have published benchmark results on the ORKG-TDM dataset ? output (Sparql query): ---------------------------------------------------------------------- AUTHOR DATE --------------------------------------------------- ------------------------ 2:30 p.m. 2010 5/27/10 R. Stelmaszczyński, B. Miller & H. Pörtner ORKG-TDM. 2010. http://orth.jtf.org/orth-dataset/index.html. ------------------------- 3:00 p.m. 2011 6/20/12 R. Stelmaszczyk, T. Zablotowicz & B. Gartner. ORKG-TDM. 2011, 9th International ACM Conference on Parallel and Distributed Processing. http://orth.jtf.org/orth-dataset/index.html. 4:10 p.m. 2013 9/11/13 R. Stelmaszczyk, R. Dierks, B. Gartner, B. G. M. Reijneveld & H. B. Tannock. ORKG-TDM. 2013. http://orth.jtf.org/orth-dataset	False
"
 input (English text): What are the models that have been benchmarked on the Words in Context dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Time Pilot dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Time Pilot"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the AxCell model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AxCell"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" output (R code): \ \ _____________________ \ _____________________ _________________ \ _____________________ _________________ \ _____________________ _________________ \ _____________________ _________________ \ \ / | | / ( ?dataset a orkgc) \ / \ / ( ?dataset rdfs) \ / \ \ \ / | \ \ \ / \ / \ / \/ ( ?dataset) \ /\ / | \ \/ / \ \ \ \ \ \ / / / ( ?dataset) \ / \/ ( ?dataset, ?dataset, ?dataset, ?) \ In the original paper, Dijkstra and Lister describe the AxCell database as the ""complete R code of a single model with an evaluation procedure, and some data"". However, this can be further simplified and is not entirely accurate. First of all, Dijkstra is not the reason that Axcell is used. The original paper says that ""the complete R code of a single model with evaluation"" and that the code in turn is an interpretation of the data set (as per Axcell). I guess we should not make assumptions there ? After all, they are comparing the single model to its interpretation of the data. As far as the data set is concerned, Dijkstra and Lister say that the data was extracted just like a R database: \ ___________________________________________________________________________________________________________________________________________________________________________________ \ | ___________________________________________________________________________________________________________________ \ | ___________________________________________________________________________________________________________________ \ | ___________________________________________________________________________________________________________________ \ \ ___________________________________________________________________________________________________________________ Finally, Dijkstra doesn't even start with one model as he describes it on the third page. It is the whole model in R's package, not just the model with an evaluation formula: \ ________________________________________________________________________________________________________________________________"	False
"
 input (English text): List the code links in papers that use the Dueling DQN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dueling DQN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the FTD dataset dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): What software or hardware do you use to develop your code for benchmarking on the AWD LSTM model ?	False
"
 input (English text): Where can I find code references in papers that have used the NAT-M3 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAT-M3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the metrics used to evaluate models on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMedQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, id, title_id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text FROM ( SELECT title, id, meta_text from (SELECT title, id, meta_text FROM (SELECT title, id, meta_text FROM (SELECT title, id, meta_text FROM ( SELECT title, id, meta_text from (SELECT title, id, meta_text FROM (SELECT title, id, meta_text FROM ( select title, id, meta_text ) AS title FROM r WHERE id = ? and title not in ( r. query_tree. id_list )) OR! title_id IN ( ?. query_tree. title_list)) and title NOT IN ( r. query_tree. title ) AND query_tree. id!= ? )) rdb_dataset_ref_ids IN ( ?. query_tree. id_list WHERE title!= ? AND title_id!= ? ) ORDER BY title_id ASC LIMIT 10 ) and title IN ( SELECT title FROM r WHERE title IN ( ?. query_tree. title_list ORDER BY title ASC LIMIT 10 ) and title in ( SELECT title FROM r WHERE title IN ( ?. query_tree. title_list ORDER BY title ASC LIMIT 10 ) ) ORDER BY title_id ASC LIM	False
"
 input (English text): Can you list the models that have been evaluated on the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the Oxford 102 Flowers dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Name the datasets that have been used for benchmarking in the Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	" input (English text): What are the metrics of evaluation over the Visual Recognition and Image Decoders research problem ? output (Sparql query): input (English text): What is the name of the original dataset ? output (Sparql query): input (English text): The original dataset is: The new dataset is: The new dataset contains the ""original"" model, the model evaluated on this dataset. Who did it evaluate ? The original data is: The new data is: The data should be a small subset that describes the dataset by the previous model. What are the metrics of evaluation over the new original dataset ? The original dataset is: The new dataset is: The new dataset contains the ""old"" model, the model evaluated on the original dataset. Who did it evaluate ? The original data is: The new data is: The data should be a small subset that describes the dataset by the previous model. What are the metrics of evaluation over the new new dataset ? The new dataset was: (unknown, the same data as the original dataset) The new dataset was: (unknown, the same data as the original dataset) What metrics of evaluation are available over the new new dataset ? The original dataset does have the model evaluation scores that it had when the dataset was original. What were the evaluation scores in the original dataset ? The latest version of the dataset (that hasn't been evaluated on any model) (original data, that was evaluated on) The current code used to evaluate the dataset (original dataset, that is) The model evaluation scores for the new dataset (original dataset, that was"	False
"
 input (English text): Which model has achieved the highest F1 score on the NYT benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the WSC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WSC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Gibson PointGoal Navigation dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 The output (Sparql results) contains a Metric label and a Benchmark or Evaluation label with all possible values. You can also find several Metric and Benchmark options, as well as the name of the output dataset, the last_batch and the last_batch_number. In the case of the Gibson PointGoal Navigation dataset, the first row contains the text : Metric name: ?value Benchmark: ?benchmark Value: 0 Benchmark value: 0 What you see in the second row is either one of the benchmarks with the same name or the value is not yet known by the end-user. For example, there are three benchmark options, : Benchmark: ?benchmark_value Value: 0 Benchmark value: 0 If you click on the Metric label, you will be taken to the relevant Metric and Benchmark results page. The Metric label also contains a Metric value that you can click on, to find detailed results for that Metric. This option is very useful when you want to check the metrics used to evaluate the model or the output score of the model. On the Benchmark page, there is a Metric value and an Evaluation metric value. If you click on the Metric value, a detailed summary page with Metric value, Benchmark value, Evaluation metric value and the metric value for the final score is displayed. You can also select multiple Metric values and see the Metric value for each individual measurement. However, you will see the same Metric value shown again. The last column of the Benchmark page contains a Date, a time stamp and the output folder, where the data is copied to after validation. Finally, on the Evaluation page there is a page with the evaluation criteria and the number of metrics evaluated. You see two metrics, the metric	False
"
 input (English text): Can you provide links to code used in papers that benchmark the multi-head model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""multi-head"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the nuScenes dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""nuScenes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a:HierarchicalDataSet; rdfs:label ?dataset_lbl. FILTER ( str ( ?dataset_lbl) = ""nuScenes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. } test (English text): Which model has achieved the highest score on the Cheetah, run (DMControl500k) benchmark dataset ? The function 'numSets' was not found on this server. See https://api.nsimple.com/3.5/test/pairs ?search={sensor}%{sensor}%{sensor} in the C code repository"	False
"
 input (English text): Provide a list of papers that have utilized the DyGIE++ model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DyGIE++"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Stanford Dogs dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Dogs"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP 10_BASE_Score from Dog1 WHERE { ?scale 0.5 ?scale 1.0 ?scale 1.3 } input (English text): What is the minimum score scored with each of the benchmarks on the Stanford Dogs dataset ? output (Sparql query): SELECT TOP 10_MinScore from Dog1 WHERE { ?scale 0.5 ?scale 1.0 ?scale 1.3 } The output of the above query is a table and a column of data. We can look at the data set that we obtained and see that it looks as follows: dataset DATE, BECOME, BACON, BODIES, CUSTARD, DOG, FED, FOREVER, FREEDOWN, GOOSE, JACK, JUNK, JUNK-O, KONG, KOO, OWL, OSTRICH, PUP, PEACH, PANDA, PIG, PIT, QUEEN, RACER, ROMEO, RUTH, SCHERL, SLACK, SOMALIAN, STANNA, STAINS, STINK, TREACH, THUD, TOUGH, WAGER, XANTHAN, WADGET, YELLOW The columns are as follows (the name of a dataset corresponds to the column name, for example CODET ). dataset datet_code RDFS code.................. dataset_dictionary RDFS dictionary.................... dataset_lbl RDFS label....................... These three dataset fields	False
"
 input (English text): What is the best performing model benchmarking the WMT2016 English-Romanian dataset in terms of BLEU metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2014 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Senseval 2 metric on the Supervised: benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Senseval 2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 id: 1 id: 2 id: 3 id: 4 id: 5 id: 6 id: 7 id: 8 id: 9 id: 10 id: 11 id: 12 id: 13 id: 14 id: 15 id: 16 id: 17 id: 18 id: 19 id: 20 id: 21 id: 22 id: 23 id: 24 id: 25 id: 26 id: 27 id: 28 id: 29 id: 30 id: 31 id: 32 id: 33 id: 34 id: 35 id: 36 id: 37 id: 38 id: 39 id: 40 id: 41 id: 42 id: 43 id: 44 id: 45 Id: ?model ORID: ?method_lbl ORID: ?model_lbl ORID: ?model_lbl ORID: ?model ORID: ?model_lbl or: ORID: ?model_lbl or: ORID: ?model_lbl ORID: ?model_lbl ? ORID: ?model_lbl ORID: ?model ORID: ?model_lbl or: ORID: ?model ORID: ?model_lbl ORID: ?model_lbl ORID: ?model_lbl ORID: ?model_lbl ORID: ?model_lbl ORID: ?model or: ORID: ORID: or: ORID: ORID: OR	False
"
 input (English text): Can you provide links to code used in papers that benchmark the 4 layer QRNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""4 layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of SICK-E metric on the SentEval benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SICK-E"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""WikiText-103"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-103"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?benchmark rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str ( ?metric_lbl) = ""WikiText-103"") ?benchmark ?skew orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } sample input (English text): Can you provide links to tests"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the GPT-3 175B (Few-Shot) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GPT-3 175B (Few-Shot)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?data ?data_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?data_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?data orkgc:Dataset; orkgc:Dataset orkgc:Source_code ?dataset_lbl; orkgc:Source_code ?   input (English text): What are the titles and IDs of research papers that include a benchmark for the TTT dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a ttt:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TTT dataset"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ? orkgc:Dataset ? ?benchmark. ?paper orkgp:P31 ? orkgc:Source_code ?dataset_lbl; orkgc:Source_code ? input (English text): What are the"	False
"
 input (English text): Provide a list of papers that have utilized the Weighted Tsetlin Machine model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Weighted Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of benchmarked datasets related to the Document Classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the most common lead compound?
 output (Sparql query): "	SELECT ?compound WHERE { orkgr:R75638 orkgp:compareContribution ?contrib. ?contrib orkgp:P35194 ?compound. } ORDER BY DESC(COUNT( ?compound)) LIMIT 1	SELECT DISTINCT ?cited ?cited_lbl. ?title. FILTER IF NOT NULL ?keyword. cited_lbl. ?page. ROW_NUMBER ?keyword. ?author. FILTER IF NOT NULL ?author. ?note. FILTER IF NOT NULL ?note. ?page. input (English text): Provide a list of five papers that use different algorithms for classifying documents ? output (Sparql query): input (English text): Provide a list of papers that did not publish papers in the field of document classification ? output (Sparql query): input (English text): Provide a list of papers with papers that were published in the text mining field ? (not just those that use the Weighted Tsetlin Machine and other papers that were published in the field of text mining ?) output (Sparql query): input (English text): What is the most common lead compound in the set containing the three data items ? output (Sparql query): input (English text): Provide a list of the largest number of datasets in the weighted model ? output (Sparql query): input (English text): Provide a list of the largest number of papers in a weighted dataset ? output (Sparql query): input (English text): Provide a list of papers that have been reviewed by expert authors and that are based	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Ball in cup, catch (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Enduro dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT name FROM SELLER WHERE name <> ' ? ? ?'; name input (English text): Input should be in the form of the following single-quote string:   ?Name ?Name output (Sparql query): SELECT name FROM SYSTEM WHERE name =   ?Name; name Input format:   ?Name   ?Name [ID/Name]  [ID/Name] output (Sparql query): SELECT name  , COUNT("" ?"" as ID) as num FROM SELLER WHERE name, num <> ?Name  , num  AND name IN (' ? ? ?'). (1) input (English text): Specify this text as an input to the database. If the answer is blank you need to specify the path (e.g. ""C:\\Users\\MyDog\\AppData\\Local\\Microsoft\\Windows\\Explorer\\Databases"" ) before the command begins. output (Sparql query): SELECT name, COUNT("" ?"" as ID) as num FROM SELLER WHERE name <> ?Name  , num  AND name IN (' ? ? ?'). (1) Note You may need to provide additional input to this section if you have not specified a unique identifier in your query.   input (English text): Describe the dataset.  Note that you can specify"	False
"
 input (English text): Provide a list of papers that have utilized the PubMedBERT uncased model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PubMedBERT uncased"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Frostbite benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the C51 noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a ||kgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51 noop"") ?benchmark||kgp:Benchmark orkgp:HAS_EVALUATION||benchmark||kgp; orkgp:HAS_METRIC||eval. ?eval||kgp|| HAS_C51 orkgp:HAS_METRIC||eval. } input (English text): Which model has achieved the highest Score score on the Atari 2600 Frostbite benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a ||kgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a ||kgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:Benchmark orkgp; orkgp:HAS_EVALUATION||benchmark||kgp; orkgp:HAS_METRIC||eval. ?eval||kgp|| HAS_C51 ||kgp:HAS_METRIC||eval. } ?label rdfs: label ?model_lbl. } input (English text): How many times have you run the benchmarks on the Atari 2600 and the Atari 810 ? output (Sparql query): SELECT MAX( c, 3 ) FROM benchmark_batches; "	False
"
 input (English text): Where can I find code references in papers that have used the RND model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""RND"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the ARC-PDN dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC-PDN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the FABIR model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FABIR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 selects the corresponding model rdfs from the paper if found using the model links input (English text): Are there other implementations of the RND model that include or use RND labels ? output (Sparql query):  selects the corresponding model rdfs if found using the model links input (English text): Are there other implementations of the RND model that include or use the FABIR data model ? output (Sparql query):  selects the corresponding model rdfs if found using the model links	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Reuters De-En dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the emergency type of AMBER Alert?
 output (Sparql query): "	SELECT DISTINCT ?emergency_type WHERE { orkgr:R153500 orkgp:P31 ?contrib. ?contrib orkgp:P43093 ?emergency_type. } 	" input (English text): What is the emergency signal on the Ipad ? output (Sparql query): Select ""iPad"" from news_news.news_objects WHERE id = ? and source_name 'iPad' ORDER BY id DESC LIMIT 20 input (English text): Which is the official name of the US flag ? output (Sparql query): Select ""USA"" from news_news.news_objects WHERE id = ? and source_name ""USA"" ORDER BY id DESC LIMIT 20 input (English text): What is the location of the UN building in Geneva (Switzerland) ? output (Sparql query): Select ""Geneva"" from news_news.news_objects WHERE id = ? and source_name 'Geneva' ORDER BY id DESC LIMIT 20"	False
"
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Amidar benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset TSE-NER?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the CoQA benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?c. ?m ?value from ?metric_lbl ORDER BY DESC( ?value) LIMIT 1; output (Sparql query): WHERE ?c. ?m : ?r ; a, b, c output (Sparql query): DATE (year) select count(*) from _models; a; b; c; output (Sparql query): SELECT date('2004-04-07'); c; date('2004-05-05'); select count(*) from _metrics; a; b; c; output (Sparql query): SELECT ?c. ?m ?value from ?metric_lbl ORDER BY DESC( ?value) LIMIT 1; output (Sparql query): WHERE ?c. ?m : ?r ; A simple example: a class of models used for the CoQA benchmark. The class model is a model which predicts whether an object in the database is alive for the past 10 days. Our model is an abstracted CoQA model, and we need a new domain model on which to train it. class ModelInDB(db): def __init__(self, db='coqasa', max_tasks=10): db.set_model('model1.coqasa', model_lbl = model1.lbl) self.db = db self.data = {} self.name_column = str(DB.IDENTIFIER_ID.encode('utf-8'), max_tasks) self.name_datetime = datetime.datetime.now() self.db_data = db self.score	False
"
 input (English text): Can you list the models that have been evaluated on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the OntoNotes benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT { ?rdfs:ID ?model_lbl. RDBMS RDBMS_ID { ?db:Languages ?db.Lang ?db.Lang. ?db.Lang. OPTIONAL ?categories ?categories. COUNT(list_of_categories) ?categories. OPTIONAL ?categories. RDD ?cat; COUNT(list_of_cats) ?cat. OPTIONAL } RDD ?rds_id. OPTIONAL RDD ?cat. OPTIONAL ?lbls rdfs:id. OPTIONAL ?cat rdfs:id. OPTIONAL { ?table rdfs:id. INSERT RDS_ID RDD CAT { ?key cat1, ?id cat2, ?id cat3, ?id cat4. } RDS_ID rds_id. OPTIONAL RDS cat. OPTIONAL ?categories rdfs:id. INSERT CAT CAT { ?categories cat1, ?id cat2. } CATS cat1. OPTIONAL { ?category cat1, ?id cat2. }} OPTIONAL  output (Sparql query):  SELECT { ?db:Languages ?db.Lang ?db.Lang. ?db.Lang. OPTIONAL ?categories ?categories. COUNT(list_of_categories) ?categories. OPTIONAL ?categories. RDD ?cat; COUNT(list_of_cats) ?cat. OPTIONAL } RDD ?rds_id. OPTIONAL RDD ?cat. OPTIONAL ?lbls rdfs:id. OPTIONAL ?cat rdfs:id We have not seen this data	False
"
 input (English text): What are the metrics of evaluation over the SentEval dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the DRI Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of research papers that incorporate a benchmark for the Atari 2600 Defender dataset ? output (Sparql query): output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of research papers that include a benchmark for the F-16 Intercept dataset ? output (Sparql query): output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of development papers for the F-16 Intercept dataset ? output (Sparql query): output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of development papers for the F-16 Intercept dataset ? output (Sparql query): output (Fluent data type): Title ID title or id output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of development papers for the Game System Capture dataset ? output (Sparql query): output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of development papers for the Game System Capture dataset ? output (Sparql query): output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of research papers that include a benchmark for the Game System Capture dataset ? output (Sparql query): output (Fluent data type): Title ID title or id input (English text): What are the titles and IDs of research papers that incorporate a benchmark for the Game System Capture dataset ? output (	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Reuters-21578 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the BBCSport benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BBCSport"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Pointer + Coverage + EntailmentGen + QuestionGen"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Metric; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ABCL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_DATASET ?dataset } } ORDER BY DESC( ?value) } } > r2 ?metric rdfs:id | score -----------------+-------+-------+---------+---------------- 0 | NULL | NULL | NULL | NULL 2 | 0 | NULL | NULL | NULL | NULL  select r1 from a select r1 from abcl where rdf_id = r2; input (English text): Where do"	False
"
 input (English text): What models are being evaluated on the Annotated development corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Annotated development corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Zaxxon benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Switch Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT CAST( ?dataset as ?dataset_lbl_categorical_dsl) AS ?switch_tweaker, ?dataset_lbl_categorical_dsl_frequencies FROM ?switch_tweaker_pads ?  output (Sparql query): SELECT CAST( ?dataset as ?dataset_lbl_categorical_dsl) AS ?switch_tweaker, ?dataset_lbl_categorical_dsl_frequencies FROM ?switch_tweaker_pads ? ORDER BY DESC( ?dataset_id ) DESC( ?dataset_id ) DESC( ?)  input (English text): Indicate which of the models presented in the Table 1 performed the best in terms of Switch Transformer score. output (Sparql query): input (English text): Indicate whether the Switch Transformer score provided by the selected model matches the score for the Atari 2600 Zaxxon benchmark dataset. output (Sparql query): Inputs for this model  Input Model Names: Linguistic category  Model Number of papers  Paper Text  Score  Switch Transformer Scored      1 3 Switch Transformer Transformer score  0.97 1 6 Switch Transformer Transformer score  0.75 10 6 Switch Transformer Transformer score  0.70 6 6 Switch Transformer score  0.65 7 4 Switch Transformer score  0.46 4 4 Switch Transformer score  0.41 23 16 Switch Transformer score  0.35 10 8 Switch Transformer score   0.31 12 20 Switch Transformer score   0.29	False
"
 input (English text): What are the models that have been benchmarked on the SNLI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the CNN / Daily Mail dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl. FILTER (str( ?model ) = ""CCF"" ) ?contrast orkgp:HAS_COMPULSE_VARIABLES ORGKP:HAS_COMPULSE_VARIABLES ORGKP:HAS_COMPULSE_VARIABLES ORGKP:HAS_COMPULSE_VARIABLES ORGKP:HAS_COMPURE_POWER_VARIABLES ORGKP:HAS_COMPUTE_POWER_VARIABLES ORGKP:HAS_REPEAT_POWER_VARIABLES ORGKP:HAS_COMPRESSION_VARIABLES ORGKP:HAS_RECOMPRESSION_VARIABLES ORGKP:HAS_VARIABLE_PREDICTION ORGKP:HAS_VARIABLE_PREDICTION ORGKP:HAS_EVALUATION ORGKP:HAS_BENCHMARK ORGKP:HAS_BENCHMARK ORGKP:HAS_BENCHMARK orkgp:HAS_ENV_MODEL ORGKP:HAS_ENV_MODEL ORGKP:HAS_COMPULSE_VARIABLES ORGKP:HAS_ECOLLECTIONS ORGKP:HAS_ELECTRICITY ORGKP:HAS_VOLTS ORGKP:HAS_POWER ORGKP:HAS_RADAR ORGKP:"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset Habitat 2020 Object Nav test-std?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the Hierarchical transformer encoder + conditional copy model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Hierarchical transformer encoder + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ""Tests with code quality score >.85"" FROM habitat2020.json where code_quality_score > 90 GROUP BY code_quality_score; SELECT code_quality_score,.,:index_dts , name , dts ---------- ---------- input (English text): What score criterion (in decimal points) are used to classify the code quality of the model results as above ? output (Sparql query): input (English text): What quality score (in decimal points) is used to compare code quality of the model results to the code quality of the actual code ? output (Sparql query): select., index_dts., name, dts, codeQuality_score ---------- ---------- input (English text): Which quality score can be used to compare the quality of the code quality of the model results with the quality of the code quality of output (Sparql query): select:value,.,:index_dts , name, dts, codeQuality_score ---------- ---------- input (English text): What quality score is used to compare the quality of the code quality of the model results with the code quality of the actual code ? output (Sparql query): select:value,.,:index_dts ,"	False
"
 input (English text): Where can I find code references in papers that have used the ProjectionNet model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ProjectionNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the CoLA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoLA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the A3C-CTS model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" output (Sparql query): [ {, ""probability-trie"", { -2, -1}, {0, 0}, {1, 1}, {0, 0}, {0, 0}, {0, 0}, {2, 0}, {2, 0}, {2, 0}, {0, 0}, {0, 2}, {2, 2}, {2, 4}, {1, 0}, {1, 0}, {2, 0}, {4, 0}, {1, 0}, {1, 3}, {1, 0}, {4, 3}, {1, 0}, {1, 0}, {1, 3}, {4, 0}, {2, 0}, {8, 0}, {2, 2}, {1, 0}, {0, 0}, {1, 0}, {0, 2}, {4, 0}, {0, 0}, {2, 0}, {0, 0}, {1, 1}, {0, 0}, {0, 0}, {1, 0}, {2, 0}, {1, 0}, {2, 0}, {0, 0}, {0, 0}, {1, 0), {0, 0}, {5, 0}, {0, 0}, {2, 0}, {0, 0}, {1, 0}, {3, 0}, {0, 0}, {4, 0}, {0, 0}, {0, 0}, {4, 1}, {0, 0}, {0, 3}, {0, 0}, {0, 3}, {0, 0}, {1, 0}, {0, 0}, {1, 0}, {1, 0}, {0, 3}, {1, 0}, {1, 2}, {1, 0}, {3, 3}, {0, 0"	False
"
 input (English text): Which model has achieved the highest F1 score on the MedNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the WDec model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""WDec"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?benchmark ?benchmark_score FROM ?benchmark WHERE { ?column ?column_name ?data. ?data_c ?dataset. OR ( ?index ?index_name ?index_value) ?dataset. AND { ?index ?index_id ?index_data. OR ( ?index ?index_id ?index_data. OR ?index ?index_id ?index_data_c) } } select code (string): Name of code link in paper: link text (string): Description of link: > > A code for a paper: > A description of the paper : A code or a description select code (string): Name of code link in paper: link text (string): Description of link: > > B code for a paper: > A description of the paper : A code or a description select code (string): Name of code link in paper: link text (string): Description of link: > > C code for a paper: > A description of the paper : A code or a description select code (string): Name of code link in paper: link text (string): Description of link: > ? code : ?code_name: ?code. FILTER (str( ?code ) = ""WDec"")) return output (Sparql query): SELECT COUNT(*) FROM ?code_lbl ORDER BY ?code; # The above output returns the list of papers code links are in and their corresponding benchmark scores # Each entry in the list is a one line statement that gets the score of"	False
"
 input (English text): What are the metrics of evaluation over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT-based binary sentence classifier"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?dataset; ?dataset rdfs:label ?dataset_lbl. RDF:RDF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. RDF:RSF Schema Link. input (English text): What are the top 10 performance improvements for the Open Entity Dataset, as measured over time and compared to BERT-based binary sentence classifier at each of the specified stages of the benchmark model evaluation ? output (Sparql query): input (English text): What is an Open Entity Dataset in CoNLL ? output (Sparql query): SELECT SINGLE_DATA_FILE ?dataset_lbl AS `dataset_lbl`, BING ENCODING FROM ""CODE_PAPERS"" AS _code WHERE srsr:LEN( _code. rdfs:label ) > 1 ; input (English text): What metrics are useful to analyze against the BERT-based binary sentence classifier model across time ? output (Sparql query): input (English text): Specify the data source for the output of the output query that returns a data file ? output (S"	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Finger, spin (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Params score when benchmarked on the VTAB-1k dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 select ? DISTINCT ?model_lbl ?model_lbl_lbl_id from ModelLists rdfs where { ?lbl_lbl_id in rdfs. rdfs. select ? orkgc:ModelLets. lbl_lbl_id. } ORDER by * DESC( ?lbl_lbl_id ) LIMIT 1 output (Sparql query): select model rdfs, k from ( select ? model model_lbl rdfs if not model_lbl.  else model_lbl ), andkgc:ModelLets k where { ?lbl_lbl_id in rdfs. select ? orkgc:ModelLets. lbl_lbl_id. } ORDER by score LODB ; output (Sparql query): select model, k from ( select ? model model_lbl rdfs if not model_lbl.  else model_lbl ), andkgc:ModelLets k where { ?lbl_lbl_id in rdfs. select ? orkgc:ModelLets. lbl_lbl_id. } ORDER by score LODB ; output (Sparql query): return ( models, k); The problem is quite complex, so let's try to solve it here with several small projects. The main problem is to extract the correlation between two datasets. For each dataset, we choose which dataset should be used for the prediction: The first step is to get all the measurements from each dataset and to find the one with the closest correlation to our given datasets. For our example, we will use two datasets, VTAB-1k and Finger data (in this case, for our purposes, V	False
"
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Name This Game?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list benchmarked problems in the area of Artificial Intelligence?
 output (Sparql query): "	"SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Artificial Intelligence"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): How many unique and significant features is the data used for in the model ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl(MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:"	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Kung-Fu Master benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kung-Fu Master"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the MPQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the FSNS - Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): What are the models that have been benchmarked on the FMSN - Test dataset ? output (Sparql query): input (English text): What are the models that have been benchmarked on the OLDT test set ? output (Sparql query): ...   input (English text): What are the models that have been benchmarked on the RMST - Test corpus ? output (Sparql query):  input (English text): What are the models that have been benchmarked on the RSPAC - Test corpus ? output (Sparql query):  in table 4 in line 6 of column 'count' for'model name'   input (English text): What are the models that have been benchmarked on the OLE - Test corpus ? output (Sparql query):  input (English text): What are the models that have been benchmarked on the SNES - Test corpus ? output (Sparql query):  input (English text): What are the models that have been benchmarked on the SNES - Model corpus ? ...   input (English text): What are the models that have been benchmarked on any of the other corpus (other than the EGBG). output (Sparql query): in table 4 in line 7 of column 'count' for'model name'   input (English text): What are the models that have been benchmarked on the NES - Test corpus ? output (Sparql query): ...   input (English text): What are the models that have been benchmarked on the NES - Model corpus ? output (Sparql query):  INNER JOIN  ( ?dataset, ?model, ?benchmark):  GROUP	False
"
 input (English text): What are the metrics of evaluation over the NYT-single dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT-single"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?pets ?pets_mf:Pets ?metric. ?pets_mf rdfs:id ?metric_lbl ; rdfs:indexes ?metric_lbl. OPTIONAL{ ?metric_lbl orkgp:HAS_METRIC ?metric. } OR gm_blu_met ? lmlp:P25 ?index_lbl. OR gm_blu_met lmlp:P25 ?id_lbl.  input (English text): What is the benchmark that you've completed ? What is the output in form of a data frame in R ? output (Hex file): The benchmark data frame is the following: (data frame from the above query) select max(score(samp_dataset), sum(score(samp_dataset)/count(score(samp_dataset)))) as count from samp_dataset in dataset  asp:DataFrame(data=""sample"", indices={-3.9,3.9}) select max(score(samp_dataset), count(score(samp_dataset)/count(score(samp_dataset)))) as count from samp_dataset in dataset The following benchmark was done by Mark Dittmar and is reported in the paper "" The Power of BLEU Metrics in Benchmarking Machine Learning Models. "" (https://arxiv.org/abs/1708.04375. Abstract): One of the interesting aspects of BLEU metrics is that they are an independent metric of performance, which cannot be easily compared across models or models in the same dataset. Using a dataset of two well tested machine"	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?dataset ?dataset_lbl FROM 'lens:Attention+ELmo.model.unprocessed.test.read.model' WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?dataset andgg:FIFO :dataset andgg:FIFO ?dataset, ?dataset_lbl rdfs:label ?dataset_lbl. } orgg:FIFO, ?dataset_lbl rdfs:label ifg:BLANK_STRING ?dataset, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?dataset_lbl orgg:FIFO, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?dataset_lbl, ?dataset_lbl orgg:DATA_BLOCK orgg:DATA_BLOCK, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?dataset_lbl dt:test, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?dataset_lbl og:test, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?dataset_lbl rdfs:label ifg:BLANK_STRING, ?datas"	False
"
 input (English text): Provide a list of papers that have utilized the Dynamic Coattention Networks (ensemble) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Feedback Transformer (8 layers) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Feedback Transformer (8 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?result_cnt ?result_min ?result_max FROM [storagedata].[benchmarks] WHERE score > ?benchmark.result_cnt; Input (string): Specify the metrics and how they are used: 'value','mean','standard deviation','min-1-deviation' or'min-avg'.'mean' or'min-1-deviation' should be a single number. output (string): Outputs the information of the benchmarks (see above).'score' is the score value that should be compared with the minimum score expected to be achieved over a span of 100 trials. The metrics are used to determine the performance of ensemble network. select(score, score_avg, mean, number_of_features)_cnt Outputs metrics of the most challenging benchmark of the STS Benchmark dataset. ( score of the 10th run is reported, i.e., the score of the first benchmark in the 11th run is reported.) The metric will be converted to the score in decimal form. select(percentile, sum(percentile), total_seconds_per_second, seconds_per_second)_cnt This is a script that can help you generate the time series data you will need for the post-secondary-to-post-secondary pathway of your college. Be sure to test it on your own data as this script will get messy in very short time. This script works on the output of my student paper as well as the paper I was currently writing. Just double the post-secondary date column in one of the plots, set up an input to use the output from the student paper, and you have your post-secondary data, too. To use the student paper dataset for post-secondary data preparation	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the REDDIT-B dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Penn Treebank (Word Level) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Word Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Shake-Shake (SAM) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Shake-Shake (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?table_name ?table_name, ?pipeline_name ?pipeline_name FROM ?table_description ?table_description WHERE { ?table_name ?table_name. ?id orkgc:SUMIF OR ?id OR ?dataset OR ?dataset_name OR ?metric OR ?label OR ?value. OR ?table_data-name OR ?table_data-n or ?data. ?label rdfs:label ?table_data-name. ?n orkgp:SUMIF ?data. ?n rdfs:label ?n OR ?data OR ?metric. OR ?model OR ?model_lbl OR ?model_lbl rdfs:value ?dataset. ?metric rdfs:name ?metric. OR ?value rdfs:title ?model_lbl. } Input (Text): What are some of the parameters of the Shake-Shake (SAM) model ? output (Sparql query): SELECT ?model_lbl ?model_lbl. FROM ?table_name ?table_name ORDER BY rdfs:n || ?index || ?rank OR ?model_lbl rdfs:value ?dataset. OR ?metric rdfs:n || ?metric rdfs:title | ?dataset_names. ORDER BY kv | [kv] || ?table_data-names.,., h. ORDER BY kv | d. |, h., d. |, kv. Input (Text): Where are the parameters in the Shake-Shake (SAM) model ? output (Sparql query): SELECT ?table_name ?table_name, ?pipeline_name ?	False
"
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Gravitar benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Gravitar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the MedNLI dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Joint Entity and Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 If you'd like to contribute to the documentation, please submit a pull request. This page was written by David A. Atherton, Tom Chabris, and Chris L. Wagerty, and has been edited by Matthew B. Lueber, Christopher B. Mather, and Sean O'Toole. As always, we welcome your suggestions of improvements to this article, or to this particular article in general.	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Russian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiRC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	--------------............. -------------- Name............ -------------. -----------------.-.......... -------------- Score........... -------------.......... -------------.......... -------------...... ------------.----.----.----.----  @benchmark #....... ------------. \ _____ - \_______ - \ | _____ | _____ | __||__||| @mec2 # @mec2... -------------- @mec2 @mec2 @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2 @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2 @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2  @mec2 @mec2  @mec2  @mec2 @mec2 @mec2  @mec2 @mec2 @mec2   @mec2   @mec2  @mec2 @mec2  @mec2	False
"
 input (English text): Which model has achieved the highest F1 score on the ScienceCite benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Abstracts\' entities and relations annotated corpus benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Abstracts' entities and relations annotated corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the SRU++ Base model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SRU++ Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 In the last analysis, we have identified that only three papers have used the SRU++ model for benchmarking purposes and have identified that of the three papers the most frequently used metric is %F1. It is worth noting that the only other metric that has been used is'sparql' which has not revealed any meaningful trend or evidence of overfitting. As discussed in a previous post, we see that in the paper on F1 testing, the researchers do a lot of preprocessing to determine their training set and have also employed methods to perform multiple comparisons as the data we have on hand were not sufficient to enable this analysis. As a result, we would not suggest that the SRU++Base model is the most suitable model that the authors have used for testing their findings regarding the performance of the SRU-based validation of the MxR library (Petersen and Hutter, 2014). We also note that the researchers actually performed some validation testing using different models (i.e. MxR) and with slightly more robust data. These methods can be highly misleading since they lack the data/statistical controls (which will be discussed in further detail below) to enable this type of analysis. In general, we would urge the authors (and anyone else making use of the information presented here) to provide their data to peer review bodies (for example e.g. ISI or AICR) and we have a number of links to other information that would assist peer review bodies in reviewing papers published in peer reviews by the authors. However, we should be clear that the work here is not intended to provide advice on which model or metrics to use. We have used the SRU-based model in this article for comparison purposes primarily because of its ease of manipulation and the ability to perform a more robust analysis than using the more conventional models (i.e., a.	False
"
 input (English text): Provide a list of papers that have utilized the BERT-based binary sentence classifier model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERT-based binary sentence classifier"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the AVTS model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AVTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asteroids benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?name WHERE { ?id == ?label AND ?label = ?strCode OR ?name = ?strID AND ?name = ?strHid AND id = ?label AND strCode = ?strHid AND id = ?strLstr AND strCode = ?strCode AND strHid = ?strLstr WHERE id = ?label AND strCode & strHid = ?strCode AND id = ?strLstr AND id = ?strCode & strHid = ?strCode AND id = ?strLstr AND strCode & strHid = ?strCode AND id = ?str	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the STEM-ECR v1.0 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STEM-ECR v1.0"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): what are the evidence and limitations of paper ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries""?
 output (Sparql query): SELECT ?evidence ?evidence_title ?limitation WHERE { orkgr:R6751 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5004 ?evidence; orkgp:P5006 ?limitation. ?evidence rdfs:label ?evidence_title. FILTER(REGEX(STR( ?paper_title), ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries"", ""i"")) } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2014 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" As an example, given the following set of papers: papers: paper1 paper2 paper3 paper4 paper5 paper6 paper7 paper8 paper9 paper10 (eot: 'paper1') paper1 is 'paper5': The list of benchmarks for the above-mentioned dataset would look like this: A total of 9 benchmark values were detected, and 3 had identical values. Given the following set of datasets: datasets: dataset1 datasets: paper2 papers: paper3 paper4 paper5 paper6 paper7 paper8 paper9 paper10 Dataset1: English papers (listed starting from first to last) data: the results from this benchmarking can be obtained from (eot: 'paper2') The final two datasets look like this: dataset1: English papers (listed starting from first to last) data: the results from this benchmarking can be obtained from For the above comparison, I used 4 datasets that have a larger set of datasets. Some papers here are taken from the English-German literature (i.e., those more easily accessible in English). Some are not. The dataset for each dataset are taken from the authors and/or authors' bibliographies. The original dataset used was the German-English Literature Collaborative Research Database 2013 (GLAC13). However, according to a comment at the website, ""the original German-English literature was published in 2011 in several German and English book series for research organizations and researchers, with the aim of providing an overview of the current state of German-English literature"". To reduce the number of results, the dataset number is reduced to 8. The difference between this value and its previous equivalent cannot be explained by any other reason, and is therefore removed from the dataset numbers. In order to reduce the amount"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset The Pile?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""The Pile"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Name the datasets that have been used for benchmarking in the Graph Embedding research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Graph Embedding"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the models that have been benchmarked on the SearchQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?query, ?result, ?value FROM ?search ORDER BY ?query; input (English text): Where are you retrieving the results ? The Pile ? output (Sparql query): SELECT ?query, ?results AS result INNER JOIN ?graph WHERE graph_id = ?graph_id AND result_id = ?id; output (Sparql query): SELECT ?query, ?result_value FROM ?graph JOIN rdfs r2d WHERE r2d:source = ?graph_source AND r2d:target = ?rdf_target AND ?rdfs_object_id = ?id. SELECT ?query_id input (English text): What's the query argument called ? output (Sparql query): SELECT ?query, ?result_value, ?value FROM ?search WHERE search_id = ?search_id AND ?search_object_id = ?id AND ?search_rdfs_object_id = ?id. input (English text): What are the search parameters The Pile ? output (Sparql query): SELECT ?Query, ?Result, ?Value FROM ?search ORDER BY ?Query; input (English text): What are the search parameters The Pile ? output (Sparql query): SELECT ?Query, ?Result, ?Value FROM ?searchWHERE search_rdfs_object_id = ?id AND ?search_source = ?graph_source AND ?rdf_target = ?rdf_target AND ?rdfs_object_id = ?id AND ?search_search_id = ?search	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset SciREX?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest BLEU score score on the WMT2016 Russian-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Russian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the OntoNotes dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OntoNotes"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Javascript web query): var  t = new  JSON ( 'UTF-8' ); var  data = new  JSON ( 'UTF-8' ); var  result =  JSON.parse(data, null ); return result.toJSON();	False
"
 input (English text): List the code links in papers that use the Poolingformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Poolingformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CaiT-M-48-448 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CaiT-M-48-448"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Defender dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Defender"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "		False
"
 input (English text): Can you list the metrics used to evaluate models on the IMDb-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the WNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the WMT2014 English-German dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What is the maximum number of times of all metrics are recorded in the WNLI benchmark dataset ? output (Sparql query): input (English text): For each one of the individual models, which one/s of the measurements are consistent with other models that have been evaluated by you in the past ? output (Sparql query): * All other observations are stored in CSV files. They are not used in this benchmark.	False
"
 input (English text): Indicate the model that performed best in terms of Precision metric on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of ROUGE-L metric on the GigaWord benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-L"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GigaWord"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-L"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GigaWord"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } } input (English text): Indicate the model that performed best in terms of ROUGE-L metric on the Datawrecker benchmark dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-L"") { SELECT ?model ?model_lbl WHERE {"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AdvSoft (+ 4 layer QRNN + dynamic eval) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AdvSoft (+ 4 layer QRNN + dynamic eval)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest ROUGE-2 score on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the enwik8 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric FROM enwik8_metric_lbl WHERE lblc = ?rdfs:label ? and rdfs:label ? AND lblc>0.5 AND rdfs:label ORlblc>0.75 AND lblc>1; output (Sparql note): rdfs:label orlblc is a key metric in this benchmark that is available in most open access databases, such as Jupyter Notebook. [Back to Results] Example data analysis [Back to Results] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] Using the Pivot Table This chapter provides a simple procedure for using the Pivot table, which allows for simple and flexible statistical analysis. The procedure consists of three steps: input (C2) is a list of values used to summarize the values in C1. C1 is a data frame and contains the labels for the 10 items from each of C1's categories. If the values for C1 are entered incorrectly, values of C1 will be re-inserted. Additionally, the Pivot table includes Pivot Tables for each item in the C1 data frame. Each item's pivot table is a data frame with a pivot table reference (PIR) field indicating the column of the data frame that should contain the pivot table reference. [Back to Results] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top] [Back to Top	False
"
 input (English text): What is the best performing model benchmarking the Reuters RCV1/RCV2 English-to-German dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the SciERC dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" var:SciEval-3 = { { ""dataset"": ?dataset_lbl ?} { ?metric:Metric, ?metric_lbl, ?metric_f1:Metric } } 1 2 3 4 var : SciEval - 3 = { { ""dataset"" : ?dataset_lbl ?} { ?metric:Metric, ?metric_lbl, ?metric_f1:Metric } } The second query above uses the following evaluation metric: Metric. The metric in the third query will be ""accurate"": True or ""True"". The Metric in the second Query requires a ""benchmarking"" or a ""valuation"" argument. The eval clause in the second query will be executed only once: in the second query. This is a detailed description of the different type of benchmarks that are supported based on the type of metrics in the dataset. The Metric type Metric type Description Accuracy accuracy value of the result The highest test accuracy in the benchmark. To test data, the test has to include an average value. The more accurate your test, the more likely its false positive ratio will be lower. If, in the benchmarking, two values are found which are not identical (e.g. two values which were not included in the test) they will both be grouped together and compared. If, in the benchmarking, two values that are identical are found, they will both be evaluated and the resulting accuracy value will be averaged. The accuracy metric was first recommended by the authors for use in the testing of the ""Spigot"" software for RSS. It is also used by the SPARQL for Microsoft SQL Server, The Data Explorer for R, and Oracle's TextExpander. This has"	False
"
 input (English text): What is the best performing model benchmarking the Yelp Fine-grained classification dataset in terms of Error metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the CommitmentBank dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the most common location in the studies?
 output (Sparql query): "	SELECT ?locations WHERE { orkgr:R111045 orkgp:compareContribution ?cont. ?cont orkgp:P37537 ?locations. } ORDER BY DESC(COUNT( ?locations)) LIMIT 1	 select address, name from study where address in ('Manchester, UK', 'Liverpool, UK', 'Manchester, UK') order by address desc ; output (Sparql query): select address, name from study where address in ( 'Manchester, UK', 'Liverpool, UK', 'Manchester, UK'): name ( '' ) ; output (Sparql query): select address, name from study where address in ( 'Manchester, UK', 'Liverpool, UK', 'Manchester, UK' ): name ( '' ) ; output (Sparql query): select address, name from study where address in ('Manchester, U.K.', 'Liverpool, UK'): name ( '' ) ; select address, name from study where address in ( 'Manchester, UK', 'Liverpool, U.K.': name ( '' ) ; output (Sparql query): select address, name from study where address in ('Manchester, U.K.', 'Liverpool, U.K.'): name ( '' ) ; select address, name from study where address in ('Manchester, UK', 'Liverpool, U.K.': name ( '' ) ; output (Sparql query): select address, name from study where address in ('Manchester, U.K.', 'Liverpool, U.K.') : name ( '' ) ; output (Sparql query): select address, name from study where address in ('Manchester, UK', 'Liverpool, U.K.'): name ( '' ) ; select location, name from study where location = : 'Manchester, U.K.' : city ( '' ) ; output (Sparql query): select location, name from study where location in : 'Manchester, U.K.'	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the WOS-11967 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the SentEval dataset in terms of STS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""STS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the BCN+ELMo model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BCN+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input(English text):  Who authored the selected paper ?  output (Sparql query): output (Sparql query):  Is the selected paper linked to the published paper ?    output (Sparql query): input (English text):  If yes, which of the following resources or files were used to provide the code ? output (Sparql query):  http://code.google.com/p/bbn-elmo-model/  http://code.google.com/p/bbn-elmo/source/downloads/detail.html Input ( English text): Are there any issues related to the model or is it completely functioning and accurate ? output (Sparql query):  OK. Which of the following is an alternative and how long does it take to set the model up ? Output ( sass ) import os (source: http://code.google.c...-bbn-elmo/ ). _get_results() # set the model for output  # ( sass ) import os (source: http://www.wos-c...-elmo/ ). # set the model for output  # - set BNB_ELMO_CODE for output  # ( sass ) from bnb_elmo import BNB_ELMO_CODE Input ( English text): Are there any issues related to the model or is it completely functioning and accurate ? output (Sparql query):  OK. Which of the following is an alternative and how long does it take to set the model up ?	False
"
 input (English text): What is the best performing model benchmarking the WebNLG dataset in terms of F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MedSTS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pre-Training Dataset"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?model orkgp:P31 ?cont. } } ORDER BY DESC( ?value) LIMIT 1 } "	False
"
 input (English text): Which model has achieved the highest ERR@20 score on the ClueWeb09-B benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ERR@20"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Does ViCoMap support aggregation?
 output (Sparql query): SELECT ?has_aggregation_support WHERE { ?application rdfs:label ""ViCoMap""^^xsd:string; orkgp:P5038 ?has_aggregation_support_. BIND( IF( ?has_aggregation_support_ = ""T""^^xsd:string, ""yes"", ""no"") AS ?has_aggregation_support ) } 
 input (English text): Can you list the models that have been evaluated on the Classical music, 5 seconds at 12 kHz dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classical music, 5 seconds at 12 kHz"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ERR@20"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?expr. ?expr orkgp:HAS_VALUE ?expr; ?metric rdfs:label ?metric_lbl. } ORDER BY DESC( ?value ) LIMIT 1 } } This worked, but unfortunately pandas is slower. I was hoping you'd ask this before you started and I could point you to an alternative. I haven't tried the nxpymongo project, but you can build the pandas for the most popular data structures with them. If you want a really low layer in an Numpy array and then you are not satisfied with the results, you can import all the models through nxpython, and then use nxpy to execute the query on the data. I haven't tried that yet because I only want to show what it can do. This is something we might want to try when"	False
"
 input (English text): What is the research problem addressed by paper ""LIMSI participant at QADL-5@CLEF""?
 output (Sparql query): SELECT ?problem ?problem_lbl WHERE { ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. FILTER(REGEX(STR( ?paper_title), ""LIMSI participation at QALD-5@CLEF"", ""i"")) } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" Inputs for all models: RDFa S4 Input for model: Feature Selection Outputs from model: Inputs for model: Feature Selection Features are specified on the YAML model, and then a CSV with the features is used. The YAML will need to be parsed into a CSV, which has the following format: 1. Description 2. Parameters of the feature 3. Description 4. Usage 5. Required Features 6. Optional Features 7. Output 8. Optional Outputs 9. Description 10. Parameters 11. Description 12. Usage 13. Required Features 14. Optional Features Please Note: The CSV format is the default. The feature format requires further work to create the CSV-file. A feature descriptor consists of 1 field (FeatureID) followed by 2/3 of a description field. A feature ID will be used when an RDFa model will be applied. A description field contains more information than should be provided in features. Currently, the format is: Example description: The name of the feature in the feature database. The name of the property for the feature (property name). The type of the feature 1. description 2. optional features Example features: Parameter types of the features 3. Description 4. Usage 5. Required Features 6. Optional Features 7. Output 8. Required Outputs 9. description 10. type 11. required parameters The field description field specifies the description for these features. The description can be as simple as ""Name of the feature"" or as complex as ""Name of the property for the feature"". The type field can specify one of the following fields: Parameter types for the parameter types fields are specified on the YAML feature descriptor. The YAML will need to be parsed into a CSV, which has the following format: 1"	False
"
 input (English text): What is major reactant for ""Microwave-Assisted Cobinamide Synthesis""?
 output (Sparql query): SELECT ?reactant WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""Microwave-Assisted Cobinamide Synthesis"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P37557 ?reactant. } 
 input (English text): Could you provide a list of models that have been tested on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the BC5CDR-disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT ?reactant WHERE { ?titre rdfs:label ?titles. FILTER( ?titles, ""Microwave-Assisted Diamine Oxide Synthesis"") ?titre orkgp:P45 ?titre. } orkgp:P11 ORkgp:P14 ?titre. "	False
"
 input (English text): What is the top benchmark score and its metric on the Reuters En-De dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which countries are considered in the papers about geopolitics?
 output (Sparql query): SELECT DISTINCT ?location WHERE { ?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label ?label ]; orkgp:P5049 ?location ] FILTER(REGEX(STR( ?label), ""geopoli ?tics"")) } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?location WHERE { ?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label ?label ]; orkgp:P5049 ?location ] FILTER(REGEX(STR( ?label), ""geopoli ?tics"")) } In line (2), we've used an empty string to return ?location as the final result. In the first line, we've used the ?location row to select the top location for our algorithm, so we can write it into the ?location expression on top of the first ?location expression we wrote down, and so on. Output (Sparql query): SELECT TOP(1) ?location WHERE { ?db, ?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label ?label ]; orkgp:P5049 ?location ] { { SELECT ?db ?column. ?column. OPTIONAL { ?db db OPTIONAL { ?column db OPTIONAL orkgp:P32 ?column } } In line (3) we've written the top location in the column we've chosen as the result of our comparison to the other countries. output (Sparql query): SELECT TOP(1) ?location WHERE { ?_ orkgp:compareContribution [ orkgp:P32 [ rdfs:label ?"	False
"
 input (English text): Can you list the models that have been evaluated on the Birdsnap dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the datasets benchmarked under the relation extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""relation extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE ?title ?title_2 ?title. input (English text) for example: http://www.dspa.com/news/article/207728_T_Sparql-analysis-benchmarks-nearly-a.html select title ?title ?title_2 ?title. input (English text): In order to understand more about the connection between this dataset and the research papers, we would like to include some historical information on the author(s), paper(s) and journal(s) of the paper from which this dataset was derived. Which of course would help in our explanation of the significance of this dataset. Do you know of other historical details about the authors or the journal(s) ? output (Sparql query): SELECT title ?title ?title_2 ?title. output (Sparql query): input (English text) for example: http://www.dspa.com/news/article/207728_T_Sparql-analysis-benchmarks-nearly-a.html select title ?title ?title_2 ?title. input (English text) for example: http://www.dspa.com/news/article/207728_T_Sparql-analysis-benchmarks-nearly-a.html select title ?title ?title_2 ?title. input (English text): In previous work we have implemented a parallel version of the benchmarking task to find out how many times the same dataset was used at different time points in the history of computing. In our approach, the parallel version is constructed of an algorithm which is then run under the benchmarking task and its results are evaluated on a training set. Which of course helps to	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the AI-KG dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AI-KG"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the CIFAR-100 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the COMET - Direct model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""COMET - Direct"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): If you need to use a different model, please provide links. output (Sparql query): input (English text): Do you need any recommendations as to the number of papers that should be included ? output (Sparql query): input (English text): I am going to add a paper by Raghavendra Mathur and Andrew Ng on the evaluation of the CIFAR-100 benchmark for the evaluation of deep neural networks. Why don't you list the benchmark ? output (Sparql query): 	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 River Raid dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the TDM Tagged Corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the BUCC Russian-to-English benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT SPSC_SCORE FROM TEST ( ?dataset ) AS input GROUP BY SPSC_SCORE ASC ; input (English text): Why do I gain no more than 100 in the Tagged Corpus benchmark of the Atari 2600 River Raid dataset ? output (Sparql query): SELECT SPSC_SCORE FROM TEST ( ?dataset ) AS input GROUP BY SPSC_SCORE ASC ; input (English text): I am using R, which is more general than C++ or Python, and I can run the benchmark without error. I will not share the code, because I fear it to be a security risk. output (Sparql query): Input (English text): Please describe when the benchmarking is done. output (Sparql query): select the metric from the Categorical Index DataSet which is closest to the given metric, as found from the Categorical Index DataSet in the Tagged Corpus Benchmark dataset select the metric from the Categorical Index DataSet which is closest to the given metric, as found from the Categorical Index DataSet in the Tagged Corpus Benchmark dataset output (Sparql query): The following is a comparison of the performance of the Tagged Corpus benchmark, as compared to the Atari 2600 River Raid benchmark measured on the same dataset: In both cases, the performance is within 10% of one another. In the Categorical Index DataSet, the Tagged Corpus benchmark measured on the Atari 2600 River Raid dataset, achieves an accuracy equal to the score obtained through a CPG test performed for each measurement: (mean_score = 79.7, std_score = 83.5, p = 8.1; CAGT(RUN) = 100) In the Categorical Index	False
"
 input (English text): What is the highest benchmark result achieved on the DBpedia dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the MRPC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BAX)"" ? orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } } ORDER BY DESC( ?value) } } ORDER BY DESC( ?value) The table above has an important point about our query (the ""order by"" clause): we explicitly ordered by the metric name. This lets us easily see how the benchmarks are performing in the SQLite database. input (English text): What is the highest benchmark result achieved on the Atari 2600 Boxing dataset, including the metric and its value ? Output (Sparql query): SELECT DISTINCT ?meter ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?meter ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BAX)"" ?eval; } } ORDER BY DESC( ?value) } } ORDER BY DESC( ?value) Input text : What is the highest benchmark result achieved on the Atari"	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Amazon-5 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest MRPC score on the SentEval benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MRPC"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" A simple list of the data available for this database (the Amazon web services public dataset): citation: "" Amazon EC2 Database Benchmarks "" Http://awsdoc.amazon.com/ec2-databases/ Benchmarking Amazon EC2 Database Performance using R "" The Amazon data source is publicly available at http://aws.amazon.com/db-benchmarks"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Phoenix dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Where can I find code references in papers that have used the Reactor 500M model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Reactor 500M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT COUNT( ?metric) || ?score || ?metric_lbl (MAX( ?value) AS ?score) FROM (SELECT a. ?dataset1 || a. ?dataset2 || a. ?dataset_1 || a. ?dataset2 || a. ?dataset_m || a. ?dataset3 || a. ?datasetm || a. ?datasetm_m ) as orkgc GROUP BY ?metric || ?score || ?metric_lbl order by ?metric_lbl ) as a c;  input (English text): Can you provide the highest benchmark result, including the metric and score, for the IWSLT2005 Japanese-English dataset ? output (Sparql query):  SELECT COUNT( ?metric) || ?score || ?metric_lbl (MAX( ?value) AS ?score) FROM (SELECT a. ?dataset1 || a. ?dataset2 || a. ?dataset_m || a. ?dataset_m || a. ?dataset3 || a. ?datasetm || a. ?datasetm_m ) as orkgc GROUP BY ?metric || ?score || ?metric_lbl order by ?metric_lbl  input (English text): If you had used a custom dataset as a benchmark in the past, would you have published the result in the papers, and if you have since published, the results in your dataset ? output (Sparql query): SELECT COUNT( ?metric) || ?score || ?metric_lbl (MAX( ?value) AS ?score) || ?cont ( ?dataset || model	False
"
 input (English text): Where can I find code references in papers that have used the Sparse Transformer (30 layers, fixed attn) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sparse Transformer (30 layers, fixed attn)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the SentEval dataset in terms of STS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""STS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Frostbite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Frostbite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?code. FROM (SELECT ?model. {a int64}, ?model. {b int64}, ?model. {c float64} FROM ?dataset. ?model ORDER BY DESC( ?code, ?model. ?code )) ; output (Sparql query):  output (Sparql): input (English text): Why should the input data be a list of strings ? output (Sparql query): output (Sparql): input (English text): Should we compute the accuracy of sparse sparse-DATT matrix-based classifier trained to select the best set of sparse-DATT classes based on the data ? output (Sparql query): output (Sparql): output (Sparql): input (English text): I am looking for an application level benchmarking that can evaluate the SPC model with different attributes and multiple parameters at runtime in order to help determine which parameters should provide the optimal performance to build that model ? output (Sparql query): output (Sparql):  output (Sparql): Select a new sentence and enter it into the box below. You'll be prompted to confirm your answer. Enter a sentence that has as its subject ""I am a scientist"". Now your sentence is available at  http://www.sparq.net/documents/results  and its score is displayed by a green line on the"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the SemEval-2010 Task 8 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2010 Task 8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the Encoder-decoder + conditional copy model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Encoder-decoder + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the MultiNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 output (Sparql query): output (Sparql query): input (English text): List the paper code links by project in any benchmark ?	False
"
 input (English text): What is the base URL of ""The Document Components Ontology""?
 output (Sparql query): SELECT ?iri WHERE { orkgr:R8342 orkgp:compareContribution ?cont. ?cont orkgp:P7034 ?ont. ?ont orkgp:P7042 ?iri; orkgp:P7035 ?full_name. FILTER(REGEX(STR( ?full_name), ""Document Components Ontology"", ""i"")) } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ImageNet ReaL dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT name_of_model_list FROM imview_datasets R1.R1 R2.R2 R3.R3 LIMIT 8 1 orkgc:Dataset R1.R1 R2.R2 R3.R3 LIMIT 4 1 memepool:Tensorizer.Dense(128, 0, 256, 64). TINY(4*256*4*0+4*8*256). TINY(64*128*.512*0+128). TINY(256). SINGLE(64). VECTOR_TENSOR_REA_LOOKUP_TABLE:TIMETEXT(Numeric). NN. VECTOR_TENSOR_REA_LOOKUP_TABLE_NEXT:TIMETEXT(Numeric). NN. VECTOR_TENSOR_REA_LOOKUP_TABLE_EXPIRED:TIMETEXT(Numeric). NN. TINY(32). Dense(128,256,64). TINY(128). TINY(32). Dense(128,256,64). TINY(32). TINY(32). Dense(128,256,64). TINY(32). TINY(32). dplyr:DENSE(128,256,64). TINY(128). TINY(32). dplyr:DENSE(128,256,64). TINY(128). TINY(32). dplyr:DENSE(128,256,64). TINY(128). TINY(32). dplyr:DENSE(128,256,64). TINY(128). TINY(32). dply	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset GENIA - UAS?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - UAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the ObjectNet (Bounding Box) dataset in terms of Top 5 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 5 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Accuracy score on the Reuters En-De benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters En-De"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model, rdfs:label ?model_lbl, rdfs:value ?name, rdfs:type ?type, (id), rdfs:metrics_lbl, rdfs:label ?metric(accuracy) WHERE { id, rdfs1:label ?name, rdfs1:value ?name. OR id, rdfs2:label ?name, rdfs2:value ?name. OPTIONAL { set_dataset 1 } } How big is the data set ? output (Sparql query): input (English text): Input file size: 1000 MB Input file size: 800 MB Input file size: 400 MB Input file size: 200 MB Input document size: 2.0 MB Output document size: 8.2 MB output (Sparql query): If you want to display the whole data set (all the models used) you can use: SELECT ?model. | ?feature ?met	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Does ""PRISMA hyperspectral mission"" use ENVI software?
 output (Sparql query): SELECT ?uses_ENVI WHERE { ?papers rdfs:label ?titles. FILTER(REGEX( ?titles, ""PRISMA"")) ?papers orkgp:P31 ?cont. ?cont orkgp:P23031 ?software. ?software rdfs:label ?software_labels. BIND( IF( ?software_labels = ""ENVI""^^<http://www.w3.org/2001/XMLSchema#string>, ""Yes"", ""No"") AS ?uses_ENVI ) } ORDER BY DESC( ?uses_ENVI) LIMIT 1
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?consensus_group ?consensus_group_p as coeff_group_group from ?consensus_group where ?consensus_group_p= ?consensus_group_key ORDER BY coeff_group_group[ ?consensus_group] DESC LIMIT 1 input (English text): Is there an error in the definition of ?dataset_lbl. ?dataset a orkgc:Dataset output (Sparql query): SELECT ?list_labels as labels from ?label_lbl where ?label_lbl. ?label_lbl. input (English text): Can you provide me the list of studies that have performed a 100% classification of an image ? output (Sparql query): SELECT ?test_dataset ?test_dataset. ?test_label as test_label from ?image_lbl where ?test_	False
"
 input (English text): Can you list the models that have been evaluated on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest 3-fold Accuracy score on the UCF101 (finetuned) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""3-fold Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Yelp-5 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT *, ?data_version ?dataset_lbl_lbl_v1 FROM ?dataset_lbl_lbl_v1 ?data_version to ?dataset_lbl_lbl_v1 ?dataset; orkgp:HAS_DATASET ?dataset_lbl_lbl_v1 input (English text): List the metrics that have been evaluated on the Baidu-200 dataset ? output (Sparql query): SELECT DISTINCT ?metric ?dataset_lbl_lbl_v1 WHERE { ?data_version a orkgc:Dataset; rdfs:label ?dataset_lbl_lbl_v1. FILTER (str( ?data_v1) = ""Baidu-200"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?max_latitude, ?max_longitude, ?average_acuracy input (English text): Which metric is used to evaluate models on the Bingo-3 benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?dataset_lbl_lbl_v1 WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl_lbl_v1. FILTER (str( ?dataset_lbl_lbl_v1) = ""Bingo-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?sumv"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-DOC + Partial Shuffle model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-DOC + Partial Shuffle"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the CMLM+LAT+4 iterations model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CMLM+LAT+4 iterations"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the BART model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BART"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"... Note: The above code does not use all the necessary variables to run the model. The first input field is only required to display the first result as each output is generated. The following code will display a table of results with each row containing the following fields: The code sample in the second section is very much the same. It is a simple validation method that returns a list of names of the papers that have used the CMLM+LAT+4 model and code. The code sample in the second section will output a list of papers where the benchmarking paper and its reference paper using CMLM+LAT+4 model and code are in the same file. In the example below, I have input the text of all the papers that have utilized the CMLM+LAT+4 model and code in their publication. In the second example, I have included the links to the source code for these papers in the code samples (see the third example). library(rmagick) library(datasets) paper1 <- read_csv(""https://m.scipy.org/public/software/mtb/m_paper_reform_cmlm_l_lat_4_2013.csv.gz"", header=TRUE) paper2 <- read_csv(""https://m.scipy.org/public/software/mtb/c_print_l_lat_4.csv.gz"", header=TRUE) paper3 <- read_csv(""https://m.scipy.org/public/software/mtb/c_print_l_lat.csv.gz"", header=TRUE) paper4 <- read_csv(""https://m.scipy.org/public/software/mtb/c_print_l_lat.csv"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2014 German-English?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the CvT-W24 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Natural Questions (short) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (short)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?benchmark ?score ORDER BY score DESC LIMIT 3 input (English text): What is the number of participants and what number of papers of each type were reported via the CvT-W24 model ? output (Sparql query): Output of WMT2014's Natural Questions. List of publications (links to source): WMT2014 - Natural Questions 1.1 (2012). Data: pdf.csv.pdf.csv.pdf.csv.pdf.csv.pdf.csv.pdf.pdf.csv.pdf MIP-C (2014) - Natural Questions 1.0 (2013). Data: pdf.csv.pdf.csv.pdf.csv.pdf.csv.pdf.csv.pdf.pdf.csv.pdf WMT2012 WMT2013 WMT2014 (all) WMT2014. csv WMT2012. csv WMT2013. csv WMT2014. csv WMT2014. csv 2 5 5 0 13 2 9 1 9 5 5 4 11 10 1 8 5 2 3 1 6 8 4 3 2 2 4 1 0 11 3 2 14 10 7 8 0 15 17 14 9 31 12 22 37 11 10 18 0 2 21 1 9 14 1 19 9 14 6 3 9 8 1 9 7 4 30 6 1 14 0 13 9 7 12 3 15 4 2 27 3 2 9 2 6 2 12 11 12 7 9 3 3 0 0 0 0 0 1 0 0 1 0 13 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 2 5 10 0 6 5 9 8 6 5 10 9 9 16 5 1 16 0 0 0 0 0 0 0 0	False
"
 input (English text): What is the top benchmark score and its metric on the IWSLT2015 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the Atari 2600 Yars Revenge dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) GROUP BY { { OPTIONAL { ?value orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl } } ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model. OPTIONAL { ?value orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } ORDER BY DESC( ?value) } What is the most frequently used evaluation metric on the IBM Pulsar 12.2 dataset ? output (Sparql query): SELECT DISTINCT ?value ?rdfs_lbl ORDER BY DESC( ?value ) ORDER BY DESC( rdfs_lbl ) OPTIONAL ORDER BY DESC( rdfs_lbl).	False
"
 input (English text): What is the scope of ""Decentralised Authoring, Annotations and Notifications for a Read-Write Web with Dokieli""?
 output (Sparql query): SELECT ?scope WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?title. ?cont orkgp:P7046 ?sys. ?sys orkgp:P7047 ?scope. FILTER(REGEX(STR( ?title), ""Decentralised Authoring, Annotations and Notifications for a Read-Write Web with dokieli"")) } 
 input (English text): List the code links in papers that use the DyGIE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DyGIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT TOP 100 ORDER BY score ASC, rank ASC LIMIT 100;... Input: Output: ------------------------------------------------------------ Decentralized Authoring, Annotations and Notifications for a Read-Write Web with DyGIE ------------------------------------------------------------ -------------------------- -------------------------- 100 0.11 0.07 0.14 0 0.18 0.17 0.2 18-34 1.18 6.24 1.09 25-45 3.11 11.76 0.92 52-75 2.74 9.06 1.02 75 1.38 4.38 1.28 75 2.15 5.07 0.92 ------------------------------------------------------------ ---------------------- -------------------------- ------------------ 100 0.05 0.02 0.08 0.14 0.17 0.1 37-85 2.04 5.49 0.83 100 0.43 5.36 8.26 ---------------------- -------------------------- ------------------ 100 0.02 0.05 0.20 0.1 0.09 0.11 <35 0.02 1.01 0.31 2.13 <35 0.08 1.09 0.31 42-85 1.05 4.58 0.74 50-100 1.06 4.02 0.52 100 0.05 1.06 5.58 ---------------------- -------------------------- ------------------ 100 0.01 0.05 0.09 ------------------ -------------------------- 100 0.05 0.04 0.07 0.01 0.06 0.05 ------------------ -------------------------- ------------------ 100 0.03 0.03 0.17 0.17 0.29 0.24 40-85 0.10 0.27 6-31 0.16 0.44 ---------------------- -------------------------- RDF	False
"
 input (English text): What is the average efficency for experiments?
 output (Sparql query): SELECT AVG( ?efficency_values) WHERE { orkgr:R155266 orkgp:compareContribution ?contrib. ?contrib orkgp:P43156 ?efficency. ?efficency rdfs:label ?efficency_labels. BIND(xsd:float( ?efficency_labels) AS ?efficency_values) } 
 input (English text): What is the top benchmark score and its metric on the NCBI Disease dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Name the datasets that have been used for benchmarking in the citation classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""citation classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 Input files for benchmarking: In the input files for the benchmarks, we used the dataset of the NCBI Disease dataset which was updated since the first benchmark with the following fields: 1. Description of the NCBI Disease dataset 2. Description of the benchmark tasks. 3. Summary statistics. 4. Example data for the benchmark tasks. For the benchmark task tasks, we used the NCBI Diseases dataset in a similar fashion as the first benchmark file. For the benchmarks, it is important to use the default dataset, which we used to test the quality of the datasets. For the sample data we can use the R statistical benchmark (see https://www.ncbi.nlm.nih.gov/pubmed/21466983/index.html ) which is based on the GenBank Genes and Transcripts Database from Nucleotide Genesys (B. Jain, S. Chatterjee and N. Z. Khadka). The database is a collaborative repository of all the genome datasets from the biomedical sciences. The primary objective is to provide a consistent and high-quality baseline across all the genome datasets. The benchmark task datasets used for this benchmarking exercise are the following: the genome genotypes, the transcriptome, mRNA-pairs, and all other components of the NCBI Disease database. We tested all datasets using the default settings of the NCBI disease dataset, while setting each benchmark task to test the accuracy of the current state of knowledge in the disease-based research question. The first task was to compare the results from all the genomes with the results from one of the genotypes (which we use as a reference from the human genome assembly). The genotypes and transcriptomes were used as the baseline, while the other genome components were only used as a reference from the NCBI Disease study and as input for the benchmarking task	False
"
 input (English text): Can you provide links to code used in papers that benchmark the TokenFuser model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TokenFuser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the Amazon dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the H-NLI model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""H-NLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 To summarize, we first see an output containing a list of papers that are benchmarking the TokenFuser model. The list includes both English and Japanese content (the Japanese paper was not included in this version). Then we see an input containing a document, which is the output of a query that performs only one operation on the document. We show the output for all the code links in the query, including those that were not included in the original query. We also show the text within the query output. We compare this output to text from the H-NLI paper and see that the token-fusing framework used in the paper is completely different from the framework used here in terms of the data models loaded into the database. These differences are clear and indicate that there are several differences in the way the paper performs token-fusing rather than the way the token-fusing framework does this work. The next column tells us what is in the data base using the ORGP data format. This is a list of fields. We have three rows for the different data sets for this paper: English data (the only one used for this study): ORGP data file for the original paper is available here. We have two rows each for the Japanese and German data sets (the second row has a list of all the fields used in the two data sets, but it is quite large in terms of table space): Japanese data: ORGP data file for the original paper is available here. We have some fields from both data sets, but the text, data, and table fields are only used in Japanese data (a row with only Japanese text is not shown in the table). The first row of the ORGP data file for the original paper is available here. German data: ORGP data file for the original paper is available here. We have the same Japanese data for the first	False
"
 input (English text): What are areas of study?
 output (Sparql query): SELECT DISTINCT ?areas, ?areas_labels WHERE { orkgr:R155445 orkgp:compareContribution ?contrib. ?contrib orkgp:P37041 ?areas. ?areas rdfs:label ?areas_labels. } 
 input (English text): What are the metrics of evaluation over the Atari 2600 Q*Bert dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PubMed 20k RCT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 You can view or modify these reports in any report editor here.	False
"
 input (English text): Provide a list of papers that have utilized the BertSumExt model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BertSumExt"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the PubMedBERT uncased model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PubMedBERT uncased"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Macro Recall metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro Recall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT(*) FROM 'http://bit.ly/18P5R9B'; output (SparkQL query): SELECT COUNT(*) FROM ?NLP_TDMS/*(Expp, arXiv only) ?benchmark; Including COUNT, we see that all the papers that applied the model scored very high on this metric. However, the one paper that performed the best on this metric, using the original source code from the project, was the paper by Wang et al. (2013), where we can note that the model is a combination of multiple models, which is common in our meta-regression dataset. On the other hand, we noticed that one paper used only two models, and one of them was a combination of BLEU and BLEU++. In particular, the BLEU++ model is a submodel of the BLEU and BLEU, a modified variant of the BLEU. We then calculated the median score of 10 model combinations that were applied to the benchmark datasets. That resulted in 1.7, roughly 1.5 more than the median scores of 10 model combinations used by the Benchmark-R (2012, 2015, 2016). As we can see, there seems to be some overlap between the two datasets, and there seems to be a correlation between the score of one paper and the score of the other: Matching Score: If the original source and benchmark datasets were of the same size, the median scores of all the model combinations is 1.7. Comparision Score: The median score for the first model combination with the original dataset is 7, roughly 4.5 more than in the comparison score, as you can see in the table below: Benchmark Comparison Score BLEU 5.8 BLEU++ 3.5 OR	False
"
 input (English text): Can you list the models that have been evaluated on the Yelp-5 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the Yelp-14 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of NLL score when benchmarked on the Nottingham dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NLL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What is the name of the top performing model in terms of Accuracy metric when benchmarked on the Nottingham dataset ?	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciGEN dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the IWSLT2015 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IWSLT2015 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the DCN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DCN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT rdfs:cma from rdfs; ?cma rdfs:cma <input (English text)> Output: SELECT name, name_seq (seqno), ID:seqno, name, name_seq (seqno), name_title (title), name_seq (seqno) FROM rdfs; name_seq_seqname(seqno, name) name name_seq name_title name_title_seqid name_title_seqid 1 0 1 1177 1.3 3 1 1 1057 0.6 1 1 1 1111 0.3 The DCN model has been written up in an article on arXiv in the following: RDFs on IWSLT2015 Now that we've got all the information about the raw data and the DCN model, lets move to the analysis of our data with the use of Python and SciPy. I've started with the first few tables in the IWSLT2015 file and used pandas or scikit-learn to prepare them for further processing. The first tables include the description of the datasets, titles, and the raw data for each dataset. I tried to include all data that seems to have been included (without duplicates). The ""titles"" table (T15 in the text above) is all the titles found in the published papers (titles_by_title_id()) along with the raw data for each title, which in this case is a list of 5 rows. In the ""rawData"" table (D1:1), where we have 5 rows about each title in the paper, we can see that each title and the raw data are stored in the same file. There is a small part of each paper's text that is not present in the paper."	False
"
 input (English text): Indicate the model that performed best in terms of Macro F1 metric on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the BBCSport dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BBCSport"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_DATASET ?dataset; ?label ?label_lbl. } ORDER BY DESC( ?value) LIMIT 1 } output (Sparql query): input (English text): Which of the following is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_DATASET ?dataset; ?label ?label_lbl. } ORDER BY DESC( ?value) LIMIT 1 } output (Sparql query): input (English text): Which of the following is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:HAS_DATASET ?dataset; ?label ?label_lbl. } ORDER BY DESC( ?value) LIMIT 1 } output (Sparql query): input (English text): Which of the following is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric: SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; ?cont orkgp:H	False
"
 input (English text): Indicate the model that performed best in terms of ROUGE-1 metric on the arXiv benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""arXiv"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What are the best scores on the Atari SNES Super Nintendo dataset ? output (Sparql query): input (English text): What is the metric used by Google's TensorFlow to determine optimal accuracy ? output (Sparql query): input (English text): What is the quality of the training output ? output (Sparql query): input (English text): What are the top quality metrics on the XBMC Databases data set, and their corresponding numbers ? output (Sparql query): input (English text): What are the best scores on the XBMC Databases Dataset, and their respective metrics ? output (Sparql query): input (English text): What are the highest resolution metrics on the XBMC Databases Dataset ? output (Sparql query): input (Japanese text): In which metric are the results better, that could be evaluated by another metric, or more precisely, what type of measurement technique is used to quantify (rank) the different results on the XBMC Databases data set ? output (Sparql query): Input: x output: Results: # Input: x x output: Results: # Input: ?model[ ?metric, ?order][ ?] output: Metric: ?metric ?order [ ?quantity.] # Input: ?lbl output: lbl # Input: ?dataset. ?dataset_lbl output: # Input: ?benchmark. ?benchmark orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:HAS_EVALUATION ?eval. ?eval orkgp	False
"
 input (English text): Could you provide a list of models that have been tested on the Cheetah, run (DMControl100k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Accuracy (%) score on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Results Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 -------------------------------------------------------------------------------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------- ?dataset:Food-101 ?model:HAS_DATASET ?model_lbl ? The above results result a list of 10 datasets which has been tested by Cheetah, run on Cheetah, the benchmark dataset. All 10 datasets have achieved an Accuracy of at least 80%. All datasets have not been tested on the first few datasets, but have been tested on the latest. Cheetah has also made the benchmark dataset compatible with its Cheetah database. As you are aware it is not easy or cheap for a computer to benchmark a large dataset at a high accuracy. Therefore the dataset's accuracy is measured mainly by the accuracy of the queries the program performs (in comparison to the benchmark dataset). We don't want you to run on this benchmark dataset at the same time as using this test dataset. If you run the benchmark dataset first, it will have to perform a lot more calculations than the benchmark dataset and the benchmark dataset will be slower. So the answer might be: Cheetah benchmark dataset performance is measured by the number of times it solves a benchmark or the number of errors it makes in the benchmark or on the benchmark dataset. Note that the test dataset is not just set up to achieve an accuracy, it is also designed to test the performance from the previous benchmarked data set. So all data and code for a Cheetah benchmark dataset must appear in one place. The other option is to run the benchmark dataset, but make sure that it performs the benchmark with the benchmark data. The Cheetah benchmark data and benchmark data-only can be installed from the command line in a separate directory named benchmark and run with the command: curl 'https://raw.githubusercontent.com/hayden-lab/benchmark/master/bench.sh' and the	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 English-German dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Walker, walk (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?book citable:Sid ?book. TITLE ?identifier ?doc ?book. NAME ?name ?author ?authors ?book. ?author citable ?. TITLE ?citable ?citable_lbl. TITLE ?author citable ?. TITLE ?citable citable ?. CITED ?citable_lbl. ""Citable data for a set of data for the Walker, walk dataset for the English language in the mid-90s. An attempt to understand why different tasks in the WMT2016 English and German language tasks have different Citable scores (the benchmark used to identify how relevant some tasks are to a certain language). "", ""authors"", ""document"", ""citable"", ""published"", ""publisher"", ""doi"", ""author"", ""repository"", ""language"" ] input (English text): Given 'wordlist', what is its original language, which is in turn the original language that was chosen for a specific function call ? output (Sparql query): SELECT DISTINCT ?name ?language_lbl. LANGUAGE ; output (Sparql query): SELECT DISTINCT ?data ?language_lbl. LANGUAGE ; output (Sparql query): RETURNING ?language_lbl. LANGUAGE END-OF-DATA RETURNING ?data OR ?language_lbl. LANGUAGE END-OF-DATA input (English text): Given 'wordlist', what is its original language, which is in turn the original language that was chosen for a specific function call ? output (Sparql query): SELECT DISTINCT ?language_lbl. LANGUAGE ; output (Sparql"	False
"
 input (English text): What is the top benchmark score and its metric on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the MRPC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of benchmarked datasets related to the Semantic Role Labeling research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Semantic Role Labeling"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Semantic Role Labeling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } Please note, that the dataset is available in the 'demo' tab of OpenTK. This dataset is included in OpenTK's demo repository. The first thing to do is to download OpenTK 0.10.0 (the 'demo' tab). The 'demo' tab was added to OpenTK's 'download' button to allow users to download the latest stable release. You can find the download link just by right-clicking the 'demo' tab on the 'opentksetup download page'. The list of the OpenTK 0.10.0 packages is available on their download page, below. The next thing to do is to download the dataset. The tutorial is available by clicking the 'tutorial' tab. OpenTK 0.10.X.v2016.01.01 - OpenTK version 0.10.0 Dataset contains a total of 64,000,000 rows. This is a lot. The dataset contained 4,920,000,000 unique words, and contained 696,734 unique words/phrases, with some items in the middle. As an example, the dataset contains data related"	False
"
 input (English text): List the metrics that are used to evaluate models on the CIFAR-100 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which are five the most common research fields for papers?
 output (Sparql query): SELECT ?research_field, ?research_field_labels WHERE { ?papers orkgp:P30 ?research_field. ?research_field rdfs:label ?research_field_labels. } ORDER BY DESC(COUNT( ?research_field_labels)) LIMIT 5
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WikiText-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 output (Sparql query): COUNT( ?research_article_id ) input (English text): Output the number of papers using each dataset ? In the output, which of the five the most common research fields were used ? input (English text): Output the number of papers using each dataset whose outputs were included directly in the publication ? output (Sparql query):	False
"
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Zaxxon dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the Paper Field dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the PWC Leaderboards (restricted) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PWC Leaderboards (restricted)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" input (English text): What are the metrics of evaluation over the PWC Leaderboards (open) dataset ? output (Sparql query): The dataset consists of the following key fields. The key is the unique identifier of the dataset, so I also named them: orkc_id, orkc_type, orkc_labels, orkgc_id, orkgc_type, orkgc_labels, orpc_id, orpc_type, orpc_labels, orpub_id, orpub_type, orpub_labels, orspkc_id, orspkc_type, orspkc_labels, orspkc_pixmap, orspkc_cov, orspkc_labels, orspkc_pixmap, orspkc_dataset_id, orspkc_dataset, orspkc_model, orspkc_model, orspkc_value, orspkc_meas_value. Note that the key isn't always displayed in the dataset. In total, the dataset contains 49 key fields. The data was originally derived from the paper ""The Use of a Statistical Model to Measure the Strength of an Argument for the Adequacy of a Propositional System"", by K. M. S. Lee, et al., in the proceedings of a workshop on the probablistic philosophy of science, organized by the Society for the Study of Science, March 1-3, 2005. This paper is freely available online from the web page http://cs.seas.berkeley.edu/~cjfakal/Lee_pws2006.pdf. The paper was a collaboration between"	False
"
 input (English text): What are the models that have been benchmarked on the Dataset mentions in Social Sciences dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dataset mentions in Social Sciences"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the DDI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the Duel noop model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Duel noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 The table below highlights some important characteristics of datasets that use the Duel noop model, but is currently not yet ready for publication. We need your support to get all the characteristics of the databases used in the benchmarked papers published. Code links, in the form of URLs, are all included in the database tables of Datasets 2 for this model. There is also a table containing the name of other models that use the Duel noop model, but which will have to be excluded from the final final dataset as it can cause performance issues for the benchmarked papers. The table below is an important part of the analysis of the current state of database performance on both the DDI and DDI datasets. Datasets 1, 2 and 3 may be used and should have similar features, and are not included in the benchmarked examples. All the benchmarking models mentioned at the end of this document use a data preprocessing layer that applies a lot of preprocessing steps on the SQL statement used to build those classes. This layer has a direct access to the data files for the benchmarked databases. You should keep track of how many times the preprocession step is applied on queries that are done for the benchmarked datasets and make sure to remove such queries from the final dataset. These should be the only queries in the final dataset. To be included in the results, a dataset has to have at least one benchmarked paper (1 or 3) using the model that uses the preprocessing layer. The number of benchmarked papers must be at least three to be included in the dataset calculations. Dataset 1 and Dataset 2 all use the DDI dataset. Datasets 3 and 4 all use Dataset 4. Dataset 4, however, has three benchmarked papers, none of which use the new Duel noops. This can cause some	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the Go-Explore model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Go-Explore"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the datasets benchmarked under the Fine-Grained Image Classification research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Fine-Grained Image Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?dataset FROM ORAG_GO_EXECUTIVE_SUBS and ORAG_GO_EXECUTIVE_SUBS_SUSPENDMENT for ORAG_GO_EXECUTIVE and ORAG_GO_EXECUTIVE_SUSPENDMENT and ORAG_GO_EXECUTIVE to ORAG_GO_EXECUTIVE_SUSPENDMENT and ORAG_GO_EXECUTIVE_SUBS_BASE for ORAG_GO_EXECUTIVE and ORAG_GO_EXECUTIVE_SUSPENDMENT and ORAG_GO_EXECUTIVE_SUBS_BASE for ORAG_GO_EXECUTIVE_SUSPENDMENT; rdfs:label ?dataset_lbl. input (English text): List the code references in the papers using the image capture for context model that implement the model ? What are the benchmarks in this dataset for the models that implement the model ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Image Capture for Context"") ?benchmark ? orkgp:HAS_DATASET orkgp:HAS_CONTEXT ?dataset. ?cont orkgp:HAS_CONTEXT ?benchmark ?. ?cont orkgp:HAS_CONTEXT ?model; orkgp:HAS_SOURCE_CODE ?code. } "	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AI-KG dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AI-KG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the 2-layer Norm HyperLSTM model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""2-layer Norm HyperLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): What the AI-KG dataset looks like after benchmarking a 4-layer Alg/Tor hybrid model ? output (Sparql query): -------- ----------------- -------- -------------- ------------------ ------------ ------------ -------------- ------------- -------------- 5 10 16 17 23 35 64 48 88 --------------- --------------- --------------- --------------- --------------- Slicing a Model Using the Deep Learning Library PyTorch¶ The deep learning library PyTorch is a fully scalable, open-source model-detection library from Microsoft Research. PyTorch aims for a fast implementation that can be extended in ways that are relevant for research use cases. PyTorch provides a high level API for building models from a set of input/output (I/O) layers to a single-threaded, state-of-the-art, kernel-based learning architecture. PyTorch is designed to be a full stack library, including data analysis, inference, and a low level API for model selection and parallel programming. PyTorch can be used to build a fully-fit neural network, model detection, and a variety of stochastic reinforcement learning applications. The PyTorch C++ framework that provides low level access to the API and a robust platform to handle high-level models. An overview of how to use PyTorch: PyTorch exposes a simple API for writing neural network models in Python. PyTorch uses Boost.Python to build the kernel data sets and then uses that framework to execute code. PyTorch exposes a simple Python API for building kernels, including data manipulation and inference. PyTorch supports data loading, data processing and runtime model selection. PyTorch provides a straightforward API through which operators and operators-like functions may be called. The following examples use PyTorch in conjunction with	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CvT-W24 (384 res, ImageNet-22k pretrain) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-W24 (384 res, ImageNet-22k pretrain)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Adaptive Input Very Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptive Input Very Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 output (Sparql output): What are the main differences between the two approaches ? Tested on PASCAL/PCF and Matlab versions 7.2, 7.3 and 7.4. Adaptive Adaptive Can you specify which subset is tested in each section ? Is the training and testing data of each section specified or extracted from each image ? What is the validation data set specified on file ? Is there any variation of the test set with an unnormalized version of the data to compare to this model ? Can you specify the training of the model on a model with a different number of layers ? What is the validation set of that model ? Why can you not describe the training data ? Is there any variation with data extracted from individual images or from individual layers ? Is the model described on source file or on a model with different number of layers ? How do you compare the accuracy of the different models ? Are the differences statistically significant at the alpha level ? Is the difference between one model and another statistically significant at the beta level ? Are the differences between one model and another statistically significant at the gamma level ? Are the differences between one model and another statistically significant at the sigma level ? How many times has all models been tested ? Which models are selected for the current validation set ? Are the models used the ones that have been tested most intensively in training and validation ? Do each experiment consists of a subset from the validation set and a subset from the training set ? How many times had the validation and training sets been tested together ? Is the validation data of a subset from the training set used for the validation experiment and how is it stored ? Is the validation data of a subset from the training set used for the validation experiments and how is it stored ? Can you provide links to other	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Yelp Fine-grained classification dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the DDI dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Unpermuted Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Sequential CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Alpha Alpha"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } } GROUP BY ?metric ?metric_lbl input (English text): Can you provide the highest benchmark result, including the metric and score, for the Alpha-beta algorithm ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX("	False
"
 input (English text): Can you provide links to code used in papers that benchmark the DY-MobileNetV2 ×0.5 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×0.5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of ROUGE-L metric on the AESLC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-L"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?code ?model_lbl, ?dataset a orkgc:Dataset, ?benchmark a orkgp:HAS_BENCHMARK rdfs:list ?model_lbl. { ?statistic ?stat_lbl; ?data set a orkgc:DataSet; } ORDER BY DESC( ?value) LIMIT 1 SELECT ?code ?model_lbl, ?dataset a orkgc:Dataset, ?benchmark a orkgp:HAS_BENCHMARK rdfs:list ?model_lbl. { ?statistic ?stat_lbl; ?data set a orkgc:DataSet; } ORDER BY DESC( ?value) LIMIT 1 SELECT ?code ?model_lbl, ?dataset a orkgc:Dataset, ?benchmark a orkgp:HAS_BENCHMARK rdfs:list ?model_lbl. { ?statistic ?stat_lbl; ?data set a orkgc:DataSet; } ORDER BY DESC( ?value) LIMIT 1 I used Gephi and a simple search and replace, it would seem to me that the best performance on such a test is achieved using rdfs with NULL values in the output.	False
"
 input (English text): List the code links in papers that use the Clusterformer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Clusterformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 James Bond benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the SciCite benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Outputs Outputs ------- ------- COURSES ----------- NOCODE_BLUE_SCORE = 21.697634 BBLINE_BLUE_SCORE = 7.088856 CARBONDOR_HAS_SHORT_SCORE = 4.184953 CHEAPER_BLUE_SCORE = 9.590778 DYNAMIC_BLUE_SCORE = 6.081716 ECONOMIC_BLUE_SCORE = 20.573398 FIBER_BLUE_SCORE = 4.443788 GENERAL_BLUE_SCORE = 30.584439 GREATER_BLUE_SCORE = 9.050059 GRAY_SCORE = 1.279892 HALLOWEEN_BLUE_SCORE = 27.142672 HUNTER_BLUE_SCORE = 7.393054 INSPIRATION_SQUARE_SCORE = 2.785511 LARGER_SQUARE_SCORE = 12.276654 LEMON_SCORE = 7.690073 MARRIED_DELAWARE_SCORE = 6.251636 NURSE_SCORE = 5.057934 PARTY_SCORE = 4.717441 PLAYER_SCORE = 7.9288655 RANGER_SCORE = 2.881878 ROYAL_BLUE_SCORE = 5.394827 ROYAL_GIFTBOX_SCORE = 11.854333 SCURRY_SCORE = 8.054439 SHOWER_SCORE = 3.786713 SHOWER_RISE_SCORE = 5.989972 SOLUTION_SCORE = 12.252636 SOLUTIONS	False
"
 input (English text): What are the metrics of evaluation over the UrbanSound8k dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Solaris dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Solaris"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Skiing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	" select title, id, title_dvs, title_lbl from skiing_dataset; input (English text): Provide the number of publications published with model results over the UrbanSound8k dataset ? output (Sparql query): select id, title, publication,  authors, authors_dvs, authors_lbl from skiing_dataset; input (English text): Provide a list of ""best"" and ""best-selling"" model results for a given dataset ? output (Sparql query): select best_titles_result, best_products, best_authors, authors_results, authors_lbl, authors_titles, authors_cids,  authors_cids_result,  authors_cids_lbl,  authors_cids_titles, authors_cids_titles_result,  authors_cids_titles_lbl from skiing_lbl_best_results  select title,  best,  best_results,  best_cids,  best_cids_result,  best_cidrs,  best_cidrs_result,  authors,  authors_results,  authors_lbl,  authors_cids,  best, best_cids_result,  best_cidrs,  best_cidrs_result,  authors,  authors_results,  authors_cids,  best_results,  best_cidrs,  best_cidrs_result,  authors,  authors_results,  authors_cids,  best,  best_c"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiLSTM (Europarl) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM (Europarl)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Multi-turn QA model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-turn QA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the VTAB-1k benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VTAB-1k"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT count ( ?vta:metrics ?vta_val)) FROM ? vta_metrics AS val WHERE 1 < ?val < ?val AND val = ?val  /inputs/input.csv In this section I am going to cover my ideas and thoughts on what is an interesting idea or interesting question that one might ask. I will be using the data from the VTAB-1k benchmark to explain what I think might be going on in this benchmark. What I am looking for is: 1. a model that is simple to interpret (I don't want to know how to make my model better) 2. a model that is simple to interpret (I really don't care about how I make my model better). 3. a model that is easy to compare models with (I don't care if the models are the same) (1)   What are the metrics that you are looking for in a new model ? My first intuition, and it might be wrong as my model might be good enough but I can't help but think what other people are doing with this model that I did not think about. How do I compare this model with other models ? Can a different model or model library or research tool, or tool, or framework, handle the different models that I am interested in and make them easy to interpret ? And what is the best framework for these different models that I am interested in ? My other intuition is that these metrics are not that different from the ones that the people using existing frameworks have. These frameworks, whether I use an existing framework or not, tend to include the same metrics that I am looking for. For example, some frameworks are only looking for metrics of one dimension, or metrics of some dimension in a particular area, (and it becomes clear pretty fast that you have to have a number	False
"
 input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What preprocessing methods are used?
 output (Sparql query): SELECT DISTINCT ?prep_methods, ?prep_methods_labels WHERE { orkgr:R155154 orkgp:compareContribution ?contrib. ?contrib orkgp:P41006 ?prep_methods. ?prep_methods rdfs:label ?prep_methods_labels. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT CONCAT (AUTHORS, ?score, ?labels), ?score and ?labels FROM ?score ?labels AND ?score ORDER BY ?score. DESC LIMIT 5 input (English text): What is the type and name of the top performing model in terms of Rank ? Rank is related to the number of columns in the model, where as Score is related to the score of a parameter, including the weight given to it. output (Sparql query): SELECT type AND name FROM ?score ?rank AND ?rank ORDER BY ?rank. DESC LIMIT 5 input (English text): What is the name and title of the top performing model in terms of Model Score when benchmarked on the Cart Pole (OpenAI Gym) dataset ? Output (Sparql query): input (English text): What is the name of the top performing model in terms of Model Score when benchmarked on the Cart pole (OpenAI Gym) dataset ? output (Sparql query): SELECT cvs:Name FROM model ?columns GROUP BY cvs.Name ORDER BY cvs.Name. DESC LIMIT 5 input (English text): What is the name, title and version number of the top producing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset ? Output (Sparql query): input (English text): What is the name, title and version number of the top producing model in terms of Score score when benchmarked on the Cart Pole (OpenAI Gym) dataset ? output (Sparql query): input (English text): What is the	False
"
 input (English text): What is the highest benchmark result achieved on the CNN / Daily Mail dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the Rfa-Gate-arccos model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rfa-Gate-arccos"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" FOR XML PATH=""C:/Users/michaelflynn/Dropbox/cities/arkansas/n-sp-scales/2013/12/07/ARcs14ARCS.db"";DO  EXEC  Rfa-Gate-arccos.py input (English text): Create the Rfa-Gate-arccos model(s) in the following formats:  CSV FOR XML PATH=""C:/Users/michaelflynn/Dropbox/"" C: output (Sparql query):  FOR XML PATH=""C:/Users/michaelflynn/Dropbox/"" C : This will create the following code links in the following form: Output(Rfa-Gate-arccos.py): FOR XML PATH=""C:/Users/michaelflynn/Dropbox/Cities/arkansas/"" C: These code links work for all of the above datasets except the Atari 2600 Phoenix benchmark dataset. output (Sparql query):  FOR XML PATH=""C:/Users/michaelflynn/Dropbox/Cities/arkansas/"" C : These code links work only for the Atari 2600 Phoenix benchmark dataset.  (These code links are not actually generated nor do they even affect the performance metrics of the model in any meaningful way.) input (English text): How many clusters were created for Rfa-Gate-arccos ? output (Sparql query):  COUNT(*)  COUNT(C1) Input (English text): List the Rfa-Gate-arccos cluster sizes, on average, on the Atari 2600 Phoenix benchmark dataset ?"	False
"
 input (English text): What models are being evaluated on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the amount of questions for LC-QuAD 2.0 dataset?
 output (Sparql query): SELECT ?number_of_questions WHERE { orkgr:R154290 orkgp:P31 ?contrib. ?contrib orkgp:P41923 ?number_of_questions. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT(*) FROM question WHERE answer='4' (0 or more) question (English text): Which model from the CLEF eHealth corpus benchmark dataset performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark data ? answer (English text): How many questions ? (0 or more) output (sparql query): SELECT answer FROM answer WHERE answer = '5' (5 or fewer) answer (English text): Which model from the CLEF eHealth corpus benchmark dataset performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark data ? answer (English text): Which model from the CLEF eHealth corpus benchmark dataset performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark data ? output (Sparql query): SELECT answer FROM answer WHERE answer = '4' (0 or more) question (English text): Which model from the CLEF eHealth corpus benchmark dataset performed best in terms of F1 metric on the ShARe/CLEF eHealth corpus benchmark data ? Answer (English text): How many questions ? (0 or more)	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Russian dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Zaxxon dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the BioASQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?min | ?max | ?min_max | ?maximum_min | ?maximum_max from bioasq_models/ where { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } GROUP BY input (English text): What are the metrics of evaluation over the WMM dataset ? output (Sparql query) outputs: min max max input(English text): What is the best model for measuring how many animals were killed ? output (Sparql query) outputs: MAX_ARRAY / n  +1 orkgc:Dataset; SOLUTION PLAN For any specific paper that can also be published on another arXiv database, it could provide better results:	False
"
 input (English text): What models are being evaluated on the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 James Bond?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Atari 2600 Centipede dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Centipede"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 query result: Score: 5.000000 Score: 100.000000 Score: 250.000000 Score: 400.000000 score: 50% Score: 75% Score: 60% Score: 50% Score: 25% Score: 10% score: 50.0% score: 50.9% score: 50.1% score: 47.7% score: 43.6% score: 41.3% score: 40% score: 37.7% score: 30.6% score: 27.2% score: 23.8% score: 18.6% score: 14.8% score: 5.2% score: 5.1% score: 4.8% score: 4.7% score: 4.4% score: 4.2% score: 3.9% score: 3.8% score: 3.7% score: 3.6% score: 3.5% score: 2.9% score: 2.8% 	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 HERO dataset in terms of Best Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the TACRED dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TACRED"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the most commonly used benchmark datasets for the Natural Language Inference research field?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Natural Language Inference"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TACTICAL"") ?benchmark orkgp:HAS_DATASET ?dataset. orkgp:HAS_EVALUATION ?eval. } ORDER BY DESC( ?value ) LIMIT 1 ?metric_lbl rdfs:desc( ?metric_lbl); /* OR group by ?value  */ { { ?dataset a orkg"	False
"
 input (English text): Can you list the models that have been evaluated on the Reuters De-En dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters De-En"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the WMT2016 Romanian-English dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the Classic dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?model ?model_lbl FROM Classics AS a WHERE a.id = ?dataset ; input (English text): In what metric does the benchmark result differ a lot ? output (Sparql query): SELECT MAX( ?value ) AS ?value, MAX( ?score ) AS ?score, MAX( ?value, ?score ) AS ?score FROM Classics AS a JOIN { ?d.dataset a.dataset} ON a.id = :dataset_id LEFT JOIN { ?d.dataset a.dataset} ON (a.id = a.dataset_id) WHERE a.id = :dataset_id AND a.id > ?dataset WHERE a.id > ?dataset_lbl ORDER BY DESC( ?value ) input (English text): What metric does the benchmark result differ as well ? output (Sparql query): SELECT MAX( ?value ) AS ?value, MAX( ?score ) AS ?score, MAX( ?value, ?score ) AS ?score FROM Classics AS a LEFT JOIN { ?d.dataset a.dataset} ON a.id = :dataset_id LEFT JOIN { ?d.dataset a.dataset} ON (a.id = a.dataset_id) WHERE a.id = :dataset_id AND a.id > ?dataset WHERE a.id > ?dataset_lbl ORDER BY DESC( ?value ) input (English text): What metric model's best performance ? output (Sparql query): SELECT MAX( ?value ) AS ?value, MAX( ?score ) AS ?	False
"
 input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""count"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Berzerk dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DeiT-Ti"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?dbname ?title ?title_lbl, ?data_lbl ?data_lbl_dv, ?dataset ?dataset ?dataset_lbl, ?model ?model_lbl_v, /*... */ ?metrics ?metric_lbl_v, ?lbl ?lbl_v, ?rdfs ?rdfs_lbl_v, ?benchmark ?benchmark_lbl_v, /*... */ ?value ?value_lbl, ?benchmark_dv_lbl_v, /*... */ ?dataset_dv ?dataset_dv_lbl, ?dataset_lbl ?dataset_lbl_dv, input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric ? Output (Sparql query): output (Sparql query)  select ?dataset ?data_lbl from   ?dataset (select ?dbname ?title ?title_lbl from datasets) where { ?dataset ?dataset_lbl:Dataset}  select ?dataset ?model_lbl from ?dataset (select ?dbname ?title ?title_lbl from datasets) where ( ?dataset ?dataset_lbl:Dataset)  select ?metric_lbl from ?metrics  select ?metric_lbl_dv from ?metrics  select ?dataset ?dataset_lbl from ?dataset where ( ?dataset!dataset_lbl:Dataset)  	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the CoNLL 2003 (English) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2003 (English)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the Qbert Rainbow+SEER model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Qbert Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): For which country of study overall prevalence of epilepsy is the highest?
 output (Sparql query): "	SELECT ?country, ?country_label WHERE { orkgr:R75729 orkgp:compareContribution ?contrib. ?contrib orkgp:P15512 ?country. ?contrib orkgp:P16013 ?overall_prevalence. ?country rdfs:label ?country_label. ?overall_prevalence rdfs:label ?overall_prevalence_value } ORDER BY DESC( ?overall_prevalence_value) LIMIT 1	SELECT DISTINCT ?prevalence || total || ?prevalence_en.c.r.p || ?index || ?incl.dataset || ?dcl && NULL FROM ?prevalence_en.dcl.prevalence_enl ?. ?t( ?index) || ?t( ?index& ?dcl) || ?t( ?index& ?incl) || ?i|| ?j || ?k & ?l && ?m| ?n in ct ?prevalence_en.c.r.p || total || ?prevalence_en.c.r.p&& ?index || ?incl.dataset || ?dcl && ?incl.dcl || ?x&& index || ?y && index || ?z and ?t, ?p, ?y, ?z in ?index|| ?t & ?c. input (English text): The total number of papers published in the reference library of the first author during the year 1991 ? output (Sparql query): SELECT DISTINCT ?prevalence|| total|| ?prevalence_eng ? || ?index|| ?incl.dataset || ?dcl && NULL FROM ?prevalence_eng ?. ?t( ?index) || ?t( ?index& ?dcl) || ?t( ?index& ?incl) || ?i|| ?j || ?k & ?l && ?m| ?n in ct ?prevalence_eng	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Relation Extraction research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Relation Extraction"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Which model has achieved the highest Accuracy score on the QNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT * FROM [courses/eHealth_data] where { ?project c orkgc:Project; rdfs:label ?project. FILTER (str( ?project_lbl) = ""EHR"") { SELECT ?project c orkgc:Project; rdfs:label ?project_lbl. FILTER (str( ?project_lbl) = ""ShARe"") ?metric c orkgc:Metric; rdfs:label ?metric_lbl; rdfs:label ?metric_labels. } [courses/eHealth_data] Citation/Contact Information Efficient and efficient use of data in R is a major area of interest in medical research which is being increasingly addressed with the release of new data sources which can be queried using R, such as large genomic datasets that have accumulated over millions of hits in the past. As a result of these sources of data, major issues such as the data co-ordination of research and the quality control of research results has been dramatically improved and new challenges have been encountered at a societal level because of the sheer volumes. Thus, the goal of Relation Extraction is to provide a rapid, simple, and comprehensive way to use large genomic datasets with minimal programming and high statistical sophistication required. Relation Extraction represents a new approach in the R research community which should improve the speed, ease of processing and interpretation of the enormous data sets. Please contact us if you would like us to refer you to reliable resources for using RELATION Extraction."	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the Paper Field benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the MRPC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the HMDB51 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Output: 0.00% 3.46% 4.24% 4.24% 5.08% 6.14% 7.26% 4.64% 6.20% 7.65% 7.70% 5.54% 8.13% 8.13% 3.50% 3.25% 3.75% 4.03% 2.60% 0.85% 0.00% 0.00% 0.00% Input: a dataset of MRPC Benchmark 1.0 model weights (3 rows) 2 rows Input: a dataset of HMDB51 Benchmark 1.2 model weights (3 rows) Output: 0.00% 1.80% 3.86% 3.96% 3.97% 3.95% Input: a dataset of MRPC Benchmark 1.0 model weights (3 rows) 2 rows Input: a dataset of HMDB51 Benchmark 1.2 model weights (3 rows) Output: 0.00% 1.80% 3.86% 3.96% 3.97% 3.95% Input: a dataset of MRPC Benchmark 1.0 model weights (3 rows) 2 rows Output: 0.00% 1.80% 3.86% 3.96% 3.97% 3.95% 5 rows x 30 rows, 2 columns x 4 rows A total of 20 models were evaluated on both datasets. Model Weight B1 B2 B3 DIFFERENCE (95% CI) B1 B2 B3 DIFFERENCE (95% CI) B1 B2 B3 DIFFERENCE (95% CI) Model Weight B1 B2 B3 DIFFERENCE (95% CI) B1 B2 B3 DIFFERENCE	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the ARC (Easy) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Venture dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Venture"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""VC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont} input (English text): Provide a list of research papers along with their titles and IDs, that have benchmarked models on the Arcade video gaming platform ? output (Sparql query): SELECT DISTINCT ? paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Easy)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont} input (English text): Provide a list of research papers together with their titles and IDs that have benchmarked models on the ECE/DS-941/GX-1 dataset ? output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ECE (Easy)"") ?benchmark orkgp:HAS_DATASET ?datas"	False
"
 input (English text): Can you list the models that have been evaluated on the CINIC-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CINIC-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Rotowire (Content Selection) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ROOTFILE"") { SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""One-thousand"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ROOTFILE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. } } } } input (English text): What is the name of the top performing model in terms of Metrics score when benchmarked in the ROOTFILE dataset ? output (Sparql query): input (English text): List the metrics that are used to evaluate models evaluated on the Rotowire benchmark dataset ? output (Sparql query): input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked in the ROOTFILE dataset ? output (Sparql query): input (English text): List the models that have been evaluated on the ROOTFILE (Content Selection"	False
"
 input (English text): Does ""ZoomRDF"" uses filters?
 output (Sparql query): SELECT ?uses_filter WHERE { ?paper orkgp:P5025 ?filter; rdfs:label ?title. FILTER(REGEX( ?title, ""ZoomRDF"")) BIND( IF( ?filter = ""T""^^xsd:string, ""Yes"", ""No"") AS ?uses_filter ) } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tsetlin Machine"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 How can I run a preprocessing script against the science.json and output the resulting Tsetlin model object ? This can improve the quality of the model and accuracy of the simulation of the RBS. What's next ?	False
"
 input (English text): Provide a list of papers that have utilized the Transformer-XL (24 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (24 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the TriviaQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TriviaQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 input (English text): What are the features on the Atari 2600 Breakout dataset that should factor into the evaluation of a matrix ? output (Sparql query): option_1 – Output a matrix, from which the values of an input vector are scored on a number of dimensions. Note, that the actual scores can vary by less than or equal to the dimensionality of the matrix, and that you may be best served by making a multi-dimensional projection from the first-dimension values to the second- and third-dimension. option_2 – Converts a list (or series) of words/numbers into a single point value. The first word in a list is firstly ignored, and the list of items is returned as a vector, where n is the number of items to return. The second word in the list is firstly ignored, and only the word following it (a word following n items is not counted as 0). option_3 – Inputs a single word or numeric value along with both the first and second lines of an RDF document. Outputs two points: (1) the first number in the word/numbers, or (2) the value of the word/numbers. option_4 – Converts a RDF value representing a human-readable name into a URI, where the following characters are the URI scheme: http. option_5 – Converts a list item's attribute value into an attribute value using a format of :value. option_6 – Converts a list item's attribute value into a number where (x) is the result attribute of the item, as a string. option_7 – Converts a list item's attribute value into a list item's attribute value. option_8 – Converts a list item's attribute value into an RDF document's attribute value	False
"
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Sparse Transformer 152M (strided) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sparse Transformer 152M (strided)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bowling"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?metrics_lbl. LIMIT 1; ORDER BY ?metric DESC LIMIT 1; ?metric_lbl. DESC( ?metric_lbl. ); What is the current status in this project ? Status: Current The first draft of this paper appeared in November 2010 in an article titled Sparse Transformers: A Review The Review: We are considering adopting this technique for a general use matrix model to determine the accuracy of a model in a wide range of applications. It might be possible to do so sooner than later. The paper is not yet in its final form, but there is some effort on this website to try to provide a rough draft. You can support it by visiting it periodically and sending e-mails to the authors about the project. The project's status is as follows: The project has yet to be formally adopted for adoption. We have a small set of papers that attempt to establish the methodology behind this technique, but these papers are not currently being used. We have a large amount of papers that employ the technique, but these papers are not currently being incorporated into the review process. We are also working on having a much larger set of papers that implement the technique, as well as making further attempts to incorporate the technique into existing software. Sparse Transformer 152M (strided) model for Atari 2600 Score comparison (Oct. 2011) The first version of the paper that I published in November 2010 in which I described the methodology behind this technique was a small article in the May 2010 issue of ACM Transactions on Graphics (Volume 23, Number 9), an ACM publication. I have also published other smaller, similar papers about the technique and have used the technique on the Atari 8-bit Bowling dataset since then. I am also trying to implement the technique on my own project, a large benchmarking dataset	False
"
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Beam Rider dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Beam Rider"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Name This Game dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the FQF model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FQF"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 input (English text): List the code links in papers that use the FQF model and include the benchmarks that run the FQF model ? output (Sparql query): input (English text): Provide the name of the top performer in terms of FFT score in benchmarked tests (i.e., Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score) in any of the benchmark test sets ? output (Sparql query): input (English text): Show information on the name of the Top 10 authors in papers that use FQF in any benchmark test set ? output (Sparql query): input (English text): What is the name and country of the top performing model (in terms of score per benchmark test test set for the Atari 2600) after benchmarking the Atari 2600 with either of the following: FQF, COCO (Cocos 2), COCO (Cocos 3), COCO (Cocos 4) or ROT (ROT) ? output (Sparql query): output (Sparql query): A list of the names of the top 10 authors in papers that use either of the following: FQF, COCO (Cocos 2), COCO (Cocos 3), COCO (Cocos 4) or ROT (ROT) ? output (Sparql query): How many times is each of the papers listed in output, and how many times is each one included in the table ? output (Sparql query): A list	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the ScienceIE dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceIE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Sequence error metric on the FSNS - Test benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Sequence error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?value ?value FROM data WHERE table ='s.train.train.train/model_name' AND metric = ?value ORDER BY desct( ?value) DESC LIMIT 10; input (English text): Explain why ""N/A"" is displayed when no model performed better than ""Model1"", ""Model2"", and ""Model3"" ? output (Sparql query): SELECT DISTINCT ?error ?error FROM data WHERE column > ?column AND table ='s.model.model.model.model.model_name' ORDER BY DESC( ?error) DESC LIMIT 10; output (Sparql query): input (English text): Explain how the results were displayed by the model which performed the best in terms of Sequence error metric on the FOIL dataset ? output (Sparql query): SELECT DISTINCT ?value ?value FROM data WHERE column > ?column AND table ='s.foil.model.foil/model_name' ORDER BY DESC( ?value ) DESC LIMIT 10; input (English text): What is the significance levels of the scores displayed by the model which performed the best in terms of Sequence Error metric on the FSNS - Test data ? output (Sparql query): SELECT STATUS ?error ?error FROM data WHERE table ='s.model.model.model.model_name' ORDER BY desct( ?value); output (Sparql query): input (English text): What is important for the ""N/A"" message to be written ? output (Sparql query): SELECT ?error ?error FROM data WHERE column > ?column; input (English text): How do these different metrics relate to the ""N/A"" message"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Adaptively Sparse Transformer model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Adaptively Sparse Transformer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HMDB51 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 This is the same query written as above but the query is not case sensitive! So we can see, that the first step is finding a search space containing as many words as you would want, where the words of the search space all occur in the original query. This can be done using the keyword-indexing method described in the previous sections of this document - these search spaces are represented as words in the list below and the words are ranked numerically in the above image: The second step is finding a query space that has been sorted randomly by the number of times a word occurs (see Figure 1). We do this by calculating the total number of words in this query space divided by the number of times a word occurs (which is the number of times the search is done - so if there are 100 words in the search space and 100 of these occur only once this is used as a cutoff point - it is used as a cut-off point to determine which of the 99 remaining words will be the top searched word). Once the number of words in the search space is known (there is no other information to determine a given query to be the best, just the average number of times a word occurs), a search will be performed, using this same list of words. If the search space is sorted random, then a random search is used instead (which minimises the chance the random search will find duplicates or words that occur more than once). The data being looked for will be extracted from the top 25 results of this random search and the top 25 results will be searched for by using some special algorithm. The top 25 results will include at least one top 3 result. If all these top results are found then we know that a top 3 result is the best match for the query. You can see in the Figure 2 that search was done on February 2014 and the result from this random search	False
"
 input (English text): Can you list the models that have been evaluated on the ObjectNet (Bounding Box) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Do all studies include open ocean sampling?
 output (Sparql query): ASK { { SELECT (COUNT( ?ocean_sampling) AS ?counter) WHERE { orkgr:R155561 orkgp:compareContribution ?contrib. ?contrib orkgp:P41379 ?ocean_sampling. FILTER( ?ocean_sampling = ""F""^^xsd:string) } } FILTER( ?counter = 0) } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciREX"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?dataset ?model_lbl. FROM ?dataset WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } We also analyzed the authors, peer reviewers, and publications of papers that evaluated the SSCB for a particular object in the SciREX corpus and found that most of the papers from the same discipline tend to have similar quality scores for their object. Some papers had several reviewers, others had one reviewer and a large number of reviewers. The authors were generally older researchers from the same discipline who were highly respected within the discipline for their research, and their publications tended not to be abstracts. Many more authors reported having published one other work on a different object (e.g., a paper where one author presented the same object in a different framework), and so had a higher number of individual papers on a given object than a corpus of papers based on a large number of authors. We observed similar patterns regarding paper publications over time in which publications appear in the literature. Most authors in the literature are highly regarded within the discipline, but the papers appear in the literature on a high number of occasions. Papers appearing in the literature more frequently on a given object had a higher quality score than papers appearing on a smaller number of objects (the former were more likely to be abstracts in the scientific literature). Note that the ratio of papers in the literature versus papers appearing on specific object was much higher for papers appearing on larger objects. This is because the size of the object was highly correlated with the amount of literature written on the object in question. Figure 4: Distribution of authors and publications over time in which papers appeared in the scientific literature on a particular object. Each row is a publication in the scientific literature on a specific object. (a) Papers from the first	False
"
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the SQuAD1.1 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the TSE-NER dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TSE-NER"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT a OR kgc FROM TSE-NER GROUP BY 1 ORDER BY ? Input (English text): Indicate the metric ranking order on the TSE-NER dataset ? Output (SlimSQL query): SELECT rdfs:dtype ?metric_lbl ORDER BY ? DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC DESC RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RESULT RES	False
"
 input (English text): List the code links in papers that use the NCBI_BERT(base) (P) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(base) (P)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the BioASQ dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BioASQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the Concept Mention Extraction model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept Mention Extraction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" The answer is to find ""concept"" in the name of the top performing model when benchmarking and refer to the paper that uses the Concept Mention Extraction model."	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the enwik8 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the top benchmark score and its metric on the CIFAR-10 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling+lemma (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT 1 AS ?x1, ?x2, ?x3, ?x4 FROM 'SELECT ORML_0_2 AS x2, ORML_0_3 AS x3, ORML_0_4 AS x4 WHERE x2, ORML_0_3 = ?x2 OR x3, ORML_0_4 = ?x3; ORML_2 - ORML_3 = 15000; ORML_4 + ORML_3 = 40000 ORML_4 + ORML_3 = 60000 ORML_4 + ORML_3 = 11000 ORML_4 - ORML_3 = 60000 ORML_4 + ORML_3 = 8000 ORML_3 = 40000 ORML_4 - ORML_3 = 60000 ORML_4 + ORML_3 = 8000 ORML_4 - ORML_3 = 60000 ORML_4 + ORML_3 = 8000 CIFAR-10 --------------------------- ------------ ------------------------- DISTINCT ORML_0_	False
"
 input (English text): Which model has achieved the highest Top 5 Accuracy score on the ObjectNet benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 5 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-S model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the A3C LSTM hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C LSTM hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Citation: EfficientNetV2-S model description and benchmarks (Paper #1) http://arxiv.org/abs/1210.5936 Citation: EfficientNetV2-S model description and benchmarks (Paper #2) http://arxiv.org/abs/1210.5936 Citation: EfficientNetV2-S model description and benchmarks (Paper #3) http://arxiv.org/abs/1210.5936 Citation: EfficientNetV2-S model description and benchmarks (Paper #4) http://arxiv.org/abs/1210.5936 Citation: EfficientNetV2-S model description and benchmarks (Paper #5) http://arxiv.org/abs/1210.5936	False
"
 input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the SoMeSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 RDS data type name : rdf output (Sparql query): RDS data type name : rdf_index output (Sparql query): The rdf class is currently not supported.	False
"
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the LayerNorm HM-LSTM model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LayerNorm HM-LSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (Sparql log): Input Texts WOS-5736 (n = 1330) WOS-5736.1 (n = 2373) WOS-5736.2 (n = 851) WOS-5736.3 (n = 695) WOS-5736.4 (n = 608) WOS-5736.5 (n = 931) WOS-5736.6 (n = 953) WOS-5736.7 (n = 958) WOS-5736.8 (n = 969) WOS-5736.9 (n = 960) WOS-5736.10 (n = 983) WOS-5736.11 (n = 960) Source(s) (a) Bessel (2001) (b) Baskin (1981) (c) Baskin, Brueggema, and Pahl and Lomax (1985) (d) Baskin and Noll (1990) (e) Lomax (1978), Goh (1993, 1997) (f) Meehl-Klugelstein (1999) (g) Pahl and Noll (1990), Pahl and Brueggema and Meehl-Klugelstein (2000) (h) Pahl et al. (2010) (i) Sipov et al. (2012, in press) Citation: Iris Laitin	False
"
 input (English text): Could you provide a list of models that have been tested on the UCF101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the datasets benchmarked under the SPARQL query optimization research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""SPARQL query optimization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT DISTINCT ?benchmark ?benchmark_lbl ORDER BY id DESC LIMIT ?benchmark_lbl; output (Sparql query): output (Sparql query): output (Sparql query): SELECT DISTINCT ?benchmark ?benchmark_lbl ORDER BY id DESC LIMIT ?benchmark_lbl; output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): SET output(null); output (Sparql query): SET output(null) output (Sparql query): SET output(null); output (Sparql query): SET output(null) output (Sparql query): SET output(null); output (Sparql query): SET output(null) output (Sparql query): SET output(null); output (Sparql query): SET output(null) output (Sparql query): SET output(null) output (Sparql query): SET output(null); output (Sparql query): SET output(null) output (Sparql query): SET output(null); output (Sparql query): SET output(null); 	False
"
 input (English text): Does FEEM Sustainability Index consider air quality?
 output (Sparql query): ASK { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""FEEM Sustainability Index"")) ?papers orkgp:P31 ?contrib. ?contrib ?properties ?properties_values. ?properties rdfs:label ?properties_labels. FILTER(REGEX( ?properties_labels, ""air quality"", ""i"")) FILTER( ?properties_values = ""yes"") } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Zaxxon dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the highest benchmark result achieved on the Amazon-2 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 Affected rows in table (rows.c (in rows)): column name key 1 id (ID column) 2 id.1 id 3 id.2... 13 id.40....... 1 id.2,1,2,2,2 1 id.4 id.40,3,4,4,4,4,4,4 1 id.4,1 id.4 id.4,4,3,4,2,4,3,4,4 1 id.4,1,2,1,4,2 1... Note the order of columns. The first data set is loaded at 0. We are still using the database format defined by the Oracle database engine. For my project this was fine, because I knew and understood my requirements. And even though it does not represent all of the requirements (the Atari 2600 Zaxxon problem is only a single problem and I don't need to fit all database operations to the database format), it would also let me move my database to different machines - in which case this could be done automatically in future. But the database was a hassle, and I was not happy for years - but it was worth it for this dataset, where the main goal was to develop a database tool. I didn't see any alternatives that would work with the same data.  Another problem with the database format is performance overhead. The database, with its single table, takes more than 7.5 seconds to load. I wanted such a thing to be possible in a small enough scale for my project. In other words, the database has to be fast, and fast, in some sense, means low memory footprint - and that's what I'll see in all my examples. In general, performance is just of marginal importance for me unless some critical problem is being solved. Performance is also influenced by how	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Skiing dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Skiing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the Atari 2600 Star Gunner dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Temporal Convolutional Network model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Temporal Convolutional Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- Temporal Convolutional Network --------------------------- [0.1-0.4 ms]. [0.2-0.4 ms]. [0.1-0.7 ms]. [0.2-1.0 ms]. [1.2-4.2 ms]. ---------------------------------------------- Results: --------------- [0.1-0.4 ms]. [0.2-0.4 ms]. [0.1-0.7 ms]. [0.2-1.0 ms]. [1.2-4.2 ms]. ----------------------------------------------- Results: --------------- [0.1-0.4 ms]. [0.2-0.4 ms]. [0.1-0.7 ms]. [0.2-1.0 ms]. [1.2-4.2 ms]. Note that you can see the model on the left is better than the model on the right (that's the Sigmoid version). (The model on the right is better than the model on the left). Input (English text): We'd like to optimize a model that performs better than random (not the Sigmoid version). Do you want to optimize an SVM model ? output (Sparql query): --------------------------- The SVM-like Optimizer. ---------------------------  { { 1.3-4.9 ms } { 2.20-5.50 ms } { 2.90-7.15 ms } { 2.50-7.80 ms } { 3.50-9.45 ms } { 3.60-11.15 ms } . OPTIONAL	False
"
 input (English text): Where can I find code references in papers that have used the XLMft UDA model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLMft UDA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Russian dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the BiT-S (ResNet) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"------------------------- ------------- ------------ ------------ ------------ 4.3 Introduction to WMT2016 The following sections discuss the data format used to compute the English-Russian WordNet Index (WMI), the classification algorithms used to derive each WMI word, and the associated statistical methods. 4.3.1 English-Russian WordNet Index (WMI) The English-Russian WordNet Index (WMI) consists of a set of 5,000 word pairings representing an English-language vocabulary. English-language words are organized in three main categories: 'languages', 'individuals', and 'tribe'. Each group of words in the language vocabulary may be associated with a word group or is paired not only with words from a single other group, but also with words from a different group. When a word group is paired with a word group that is not also associated with it, it is referred to as a ""neighbourhood"". The ""neighbourhood"" of a word is not necessarily associated with any word in the given language group. The WMI uses a multi-point L1 classifier ( Figure 1 ) that identifies the cluster of words using a weighted average metric based on word size. The WMI uses the following weights to classify the English-Russian words: WMI1 = 10.0 WMI2 = 10.0 WMI3 = 10.0 WMI4 = 1.9 WMI5 = 1.9 5.1 Classification Algorithm The WMI uses a two-class classification technique ( Figure 2 ). First, it uses L1 classifier, which identifies the word-groups of a given word, then a multilayer autoregressive (MLAR) classifier, which makes the inferences between the word-groups based on different class factors. The first three WMI groups are labeled"	False
"
 input (English text): Indicate the model that performed best in terms of Pearson Correlation metric on the BIOSSES benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Pearson Correlation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of PARAMS score when benchmarked on the Flowers-102 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the WNLI dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric ?value. DISTINCT ?metric_lbl. } input text : Name of the Model that performed the best when benchmarked on the BINGS dataset output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Metric""); ?dataset_2 a( ?dataset_2 ) orkgc:Dataset; ?dataset_2 b( ?dataset_2 ) orkgc:Dataset; ?dataset_2 c( ?dataset_2 ) orkgc:Dataset; ?dataset_2 d( ?dataset_2 ) orkgc:Dataset; ?dataset_2 g( ?dataset_2 ) orkgc:Dataset; ?dataset_2 h( ?dataset_2 ) orkgc input text : PARAMS score of highest value achieved ? output (Sparql query): SELECT ?model ?model_lbl WHERE { ?metric ?value. } The first few queries require some configuration on the server, as in the examples below, but this functionality is available in the Params Query DSL. The queries above are identical to the query presented in the section on ""Params queries""."	False
"
 input (English text): Can you provide links to code used in papers that benchmark the CvT-21 (384 res, ImageNet-22k pretrain) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res, ImageNet-22k pretrain)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the AAPD dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?dataset WHERE { ?problem orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Artificial Perceptual Distance"") ?dataset ORKP;... orkgc:model ORkgc:model ORkgc:model ORkgc:model ORkgc:model ?dataset. ORKP;... orkgc:model ORkgc:model ORkgc_lbl.... orkgc:model ORkgc_lbl ORkp;... ORGPD;... ORGPD. } Input (English text): What are the most commonly used benchmark datasets for the Asymptote (Artificial Perceptual Similarity) research field ? output (Sparql query): SELECT DISTINCT ?dataset WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-XL (RMS dynamic eval) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (RMS dynamic eval)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the FTD dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FTD dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?best,{ ?c2 : DISTINCT ?value2 + ?value2_lbl., ?c1 : DISTINCT ?value2 + ?value2_lbl. AND ?c2 : DISTINCT ?value2_lbl. OR ?c1 : DISTINCT ?value2_lbl. AND ?c1 : DISTINCT ?best. IF 1,0 OPTIONAL { ?c2 orkgc:Value ?value2. } OPTIONAL { ?c1 orkgc:Value. } OPTIONAL { ?c2 rdfs:Label. } OPTIONAL { ?c1 rdfs:Label } OPTIONAL { ?c1 ORkgc:Value } OPTIONAL { ?value2 RRSqlQuery : NULL // NULL for LBL if NULL } OPTIONAL { ?value2 OPTIONAL { ?value2_lbl. } } OPTIONAL { ?value2_lbl orkgc:Value. } OPTIONAL { ?value2_lbl ORkgc:Value. } OPTIONAL { ?lbl_lbl. } OPTIONAL { ?lbl ORkgc:Value. } OR NULL { ?value2 orkgc:Value. } OPTIONAL { ?value2_lbl. ORkgc:Label. } (2 rows) input (English text): How do you rate the overall performance of the model on the dataset ? output (Sparql query):	False
"
 input (English text): What is the best performing model benchmarking the HMDB51 (finetuned) dataset in terms of Top-1 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the WMT2014 German-English benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the CIFAR-10 Image Classification dataset in terms of Percentage error metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 This is a list of models that have been tested on the CIFAR-10 Image Classification dataset in terms of Percentage Error metric.	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BIOSSES dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } } ORDER BY DESC( ?value) } } input (English text): What is the highest benchmark result achieved on the Walker, walk (DMControl500k) dataset, including the metric and its value ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } } ORDER BY DESC( ?value) } } input (English text): Can you provide the highest benchmark result, including"	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-French?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Seaquest dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciGEN"") ?benchmark orkgp:HAS_DATASET ?dataset. } } ORDER BY DESC( ?value) } input (English text): What are the top models in terms of test accuracy on the Seaquest dataset ? output (Sparql query): -------------- SELECT DISTINCT ?simp. score, ?simps. scores, ?dims. scores, ?avg. avg, ?dev. dev, ?nsc. nsc, ?nac. nac, ?npf. npf, ?x0. x0, ?l2. l2, ?cwt. cwt, ?qrt. qrt, ?rf. rf, ?rr1. r1, ?r0. r0, ?r2. r2, ?r3. r3, ?r4. r4, ?r5. r5, ?r6. r6, ?rf1. r1, ?rf2. r2, ?rf3. r3, ?r8. r8, ?r7. r7, ?r8. r8, ?r9. r9, ?rt. rt, ?rtp. rtp, ?rv. vs ?, ?vrc. vrc ?, ?vrb. vrb, ?vcr. vcr"	False
"
 input (English text): What models are being evaluated on the Cornell Grasp Dataset dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the MRPC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the code links in papers that use the POP3D model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""POP3D"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM dropdown_list WHERE database_name='pop3d' input(English text): You're welcome to submit comments using this form, but please keep in mind that PRs should not contain technical issues that are not covered by our documentation, so please refer to the corresponding README.md file or issues on GitHub for help. All PRs should be filed as PR 1442 [Dystopia] and submitted as PR 1449, because each of the submissions have to be tagged by the author in the project issues. We are not interested in comments that are not properly formatted or contain sensitive information.	False
"
 input (English text): Could you provide a list of models that have been tested on the CoNLL 2012 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the ACE 2005 dataset in terms of RE Micro F1 metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RE Micro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?model ?model_lbl, ?dataset c, ?model rdfs:label ?model_lbl, ?metric c, ?metric rdfs:labels c in rds_dataset ? ORDER BY c DESC(c) LIMIT 1.10 ; Input (English text): What is the highest scoring model benchmarking the TARNAL dataset in terms of Score metric ? score (float): Best performance on the score metric of your choice from Atari 2600 Tennis ? (Inferred on actual raw scores) output (Sparql query): Which models score best using Score metric according to Score metric ? input (English text): Out of the models in the dataset, which models score the worst in terms of Score metric ? input (Sparql query): SELECT... +1.04 /... +1.03 /... +1.01 /... +0.91 /... output (Sparql query): Which model performs worst using Score metric ? output (Sparql query): input (English text): How do you interpret the data in the table ? output (Sparql query): input (English text): How would you interpret the data in the table ? output (Sparql query): input (English text): How would you interpret the information in the table ? output (Sparql query): input (English text): Which model benchmarked the Atari 2600 Tennis score ? Which model benchmarked the ACE 2005 score ? output (Sparql query): input (English text): Tell us which criterion you are interpreting in the Table from the	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Tennis dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Tennis"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the CAIT-S-36 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" Here is a full script that compares and compares the performance of CAIT-S-36 models: import tensorflow as tf from pandas import Series, DataFrame, Tuple from matplotlib.pyplot as plt import numpy as np from zlib import format import argparse import pydatasets import argparse.parse import urllib2 from datetime import date # create variables for each data set in the dataset # a basic ""matplotlib table that allows for creating images"" data_sets = ( ""DatasetId"", ""DataSetName"", ""N_titers"", ""DateTime"", ""TimeRange"", ""Bugs"", ""Date"" ) # this is the ""input data"" a Series with the names of data, # the label of the data, and a date, if you prefer datamodels = ( 'dataset_name', 'dataset_date', 'dataset_labels', 'dataset_data_links', 'dataset', 'dataset_data_links', ""dataset_label"", 'dataset_label_all_times_lbl' ) # create a list of lists of ""out-of-scope"" datasets from the dataset id # and a single numpy list that we are interested in # out_of_scope_dataset = list (data_sets. append ( datamodels [ 'dataset_label' ] )) # to make the comparison we only want the ""titers"", # we don't care what the ""dates"" are for these outliers datapoints = ( None, []) # create a plot that draws the ""totals"" of the data against a black #"	False
"
 input (English text): What are the metrics of evaluation over the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the DQN-CTS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN-CTS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MPQA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MPQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?metric ?criterion & ?criterion rdfs:label ?criterion& ?metric | ?criterion rdfs:criterion ?criterion. OPTIONAL { ?metric orkgp:LIMIT ?METRE ?. } input (English text): Provide a list of reviews analyzing the performance of DQN-CTS ? output (Sparql query): SELECT ?criterion RDFS:RDF criterion RDFS:RDF rdf:criterion rdf:metric RDFS:METRE rdf:criterion (criterion) input (English text): Provide a list of papers that have submitted a paper describing the research results of DQN-CTS ? output (Sparql query): output (Sparql query): output (Sparql query): SELECT ?criterion RDFS:RDF	False
"
 input (English text): What are the metrics of evaluation over the Classic dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of Accuracy score when benchmarked on the Recipe dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Recipe"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the BC5CDR-chemical dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT _ID, sum(score) FROM email_list WHERE name LIKE @user.name ORDER BY score DESC(""user.weight"") LIMIT 1/5,0,4,13,48,56,64 input (English text): How would your metrics be evaluated over the Classic dataset ? output (Sparql query): Select DISTINCT ?value ?rdfs:label ?value. input (English text): What do you think about the performance of the top performing models when benchmarked on the Recipe dataset ? output (Sparql query): Select _ID, max(score), min(score), SUM(value) FROM all_top_metrics_by_metric in dataset input (English text): What is the metric of evaluation over the Classic dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?metric_lbl. FILTER (str( ?dataset_lbl) = ""Classic"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Recipe"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?value"	False
"
 input (English text): Provide a list of papers that have utilized the CAIT-S-24 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SUCCESS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT input('Test dataset'), output(sparql('SELECT * FROM mydataset), test_predict') as output_html, ?code as input_code, ?input_predict as input_predict;','sparql('SELECT * FROM model_lbl/mydataset WHERE model_lbl = ?model, ?model_lbl[3] = 1, ?model_lbl[4] = 2, ?model_lbl[5] = 3, ?model_lbl[6] = 4, ?model_lbl[7] = 5, ?model_lbl[8] = 6) as output_html ORDER BY sparql_row_count AS row_count, ?model as model_lbl, ?dataset as daset, ?benchmark as benchmark, ?benchmark_predict as benchmark_predict ORDER BY test_std AS test_std_test, ?mod as model_mod, ?model as model','model_predict as model_predict' Output Output HTML (in the template)¶ If using a template to produce XML output, you should ensure the following parameters are set: Output parameters are displayed in the HTML output, which should be used	False
"
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the WNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Paper Field dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Paper Field"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?dbsn:Score ?from ?dataset1; ORDER BY ?dbsn. ?dbsn orkgp:HAS_DATASET ?dataset1; OPTIONAL ?value from ?dbsn. ORkgp:HAS_DATASET ?dataset1. input (English text): How good is the WNLI WLS test database file ? output (Sparql query): input (English text): Which version of MATLAB runs best on the WNLI benchmark dataset ? output (Sparql query): input (English text): Can you provide the WNLI WLS test file as a raw file, and what the.mat file extension and filename is in a query on the WNLI benchmark dataset ? output (Sparql query): input (English text): Identify the metrics that were used in the benchmark test on the WNLI benchmark dataset in terms of the paper and bench results ? output (Sparql query): input (English text): What are the validation criteria required to include a paper in the report ? output (Sparql query): SELECT ?model ? ?name ?model_lbl WHERE { ?dataset ?dataset_lbl ?dataset_lbl rdf[ ?dataset ?dataset_lbl] ?benchmark ?note ?quantity ?date ? ?. ?benchmark rdfs:value. OR kgp:HAS_DATASET ?dataset; OR kgp:HAS_EVALUATION ?eval. ?	False
"
 input (English text): Can you list the metrics used to evaluate models on the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the CAIT-XS-24 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the MLDoc Zero-Shot English-to-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Output in CSV format Output in XML Example Here you can find an example in Apache Maven repository.	False
"
 input (English text): List the code links in papers that use the Transformer-XL - 24 layers model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL - 24 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the BC5CDR dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Double Dunk dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Double Dunk"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?record ?record_lbl. ?retro orkgc:Retro; rdfs:label ?record_lbl. FILTER (str( ?record_lbl) = ""FDD"") ?s:Score!score; orkgp:HAS_DATASET ?record. } input (English text): List the scores on the Atari 2600 double dunk dataset ? output (Sparql query): SELECT DISTINCT ?score ?record_lbl. 1.25 input (English text): List the scores on the A.D. 3000 Double Dunk dataset ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?score ?record_lbl. 1.17 input (English text): List the results on the Atari 2600 Double Dunk dataset. output (Sparql query): output (Sparql query): SELECT  dst  , src  , sr  , sr2  , sr3  from nvarchar2chr  ORDER BY sr2  , src  , sr  , sr2  , sr3  WHILE TRUE output (Sparql query):   SELECT  sr  , sr2  , sr  , sr22  , sr3  , sr31  , sr21  , sr212  , sr213  , sr3  3, sr31  , sr22  , sr222  , sr21  , sr213  , sr3  3, sr31  ,"	False
"
 input (English text): What is the highest benchmark result achieved on the HMDB51 dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HMDB51"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the RotoWire (Relation Generation) dataset in terms of count metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""count"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (ensemble) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" # Import nvarchar(12) from mypy import format_table # From http://www.sciencedirect.com/science/article/pii/S01679525112006899 # The 'dataset' parameter is an identifier of a RotoWire dataset (dataset) # With the input parameter being the name of the dataset (defaults to 'HMDB51') # As the 'dataset' parameter is an Identifier of a RotoWire dataset, you should be able to fill # the label field of this identifier without having to supply extra data or input # with parameters # If your dataset does not exist, a dataset_lbl.dat with a 'n' in the label is assumed # (you should also supply ' ?""' in the database reference). # If you specify a valid database reference, you will see this: # If you have already created the model (or if you specify an invalid target:target for the model:target_id field) # You will most likely get a 'no model found for this input' dialog box. # You would like to add your label as a data source, so you should provide a proper # parameter # The parameter may be of the following forms: label:n label:l label:n:a label:l:a # The label field may have any other values, however they must be unique in both # and 'dataset' # Using the 'data:model' parameter in this form, you may be supplying a data "	False
"
 input (English text): Where can I find code references in papers that have used the A2C + SIL model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A2C + SIL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MultiNLI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MultiNLI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark score and its metric on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(7 | num) ?score, ( ?score) AS average, ( ?score | num) AS median, ( ?score) AS min, ( ?score | num) AS max, SUM(num) AS sum / num FROM ?dataset; ?score orkgc:V_score input (English text): What do you mean by benchmark average ? output (Sparql query): input (English text): What does the benchmark average mean ? output (Sparql query): input (English text): What does 1 benchmark mean ? output (Sparql query): input (English text): What is the benchmark median ? output (Sparql query): input (English text): What does 2 benchmark mean ? output (Sparql query): input (English text): What is the benchmark max ? output (Sparql query): Note: if the query results from a query that includes table columns like column_name=foo, it will return multiple columns like column_id=11, and a ""none"", but you can check out the columns that are left out for that query by using SELECT '.' as-name, or SELECT table_name as-name from the query result. If you are using this query on columns with NULL values (like NULL values), you should omit the ?col=NULL parameter until you understand a better approach for performing the query. Now, this is where the MultiNLI dataset comes in. From its inception, I needed a way to display the performance statistics. What follows is a snippet of code written by Ryan M. Tann"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the IMDb dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STS Benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STS Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 You can also use a JSON string: # input:  https://raw.githubusercontent.com/joshua-morriss/models/master/json.rb [...] output (Sparql query):  [{'id':..., 'name':..., 'paper':...,'authors':...,'published':...}, {'id':..., 'name':..., 'paper':...,'authors':...,'publisher':...,'published':...}] See also: http://joshuamorris.github.io/models/IMDB#API_RATINGS_RESULTS Rationale: The IMDb database already contains data that is well-characterized in relation to the RACE benchmark (e.g. names, times and dates of birth) so it is a natural complement to include them again in the dataset. The IMDb dataset also gives the ability to use benchmarks along with one another to evaluate the performance of models. The RACE benchmark dataset does not contain time or birth date data, which is an area where model performance data can be studied. I also used the IMDb dataset as the benchmark dataset because it was already well-characterized in relation to the RACE benchmark; however, I have the ability to include others that are more representative of the real-life data, such as age or employment information (in some instances with different time periods). Also, it is an attempt at creating a useful dataset for researchers, whether academic, non-academic or with other professional purposes. The dataset is well described on the IMDb site and is freely available: https://www.imdb.com/wiki/IMDB. Motivating Example Imagine you want to make a model that, in a given scenario, would predict if the user, say, is the type	False
"
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the GLaM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GLaM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Alien dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT id, cmpid, count, score, print, t_key, f. id, g. cmpid, i. id, t. id, k. id, b. rdf_id, t_key, f. id, g. cmpid, i. id, t. id, k. id, b. rdf_id, t. id FROM dbo. genid..., dbo. iid... -- - SELECT id, r_id, s. srcl_saved, s. srcl_saved_score, i. srcl_id, d. srcl_id, k. srcl_saved, d. srcl_saved_score, b. srcl_id, t. srcl_id, f. srcl_id, k. srcl_id, b. srcl_id, t. srcl_id, f. srcl_id, k. srcl_id, b. srcl_id, t. srcl_id, f. srcl_id, k. srcl_id, b. srcl_id, t. srcl_id, f. srcl_id	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Car speed in Liuliqiao District, Beijing dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Car speed in Liuliqiao District, Beijing"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Birdsnap dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Birdsnap"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?benchmark || ?paper || ?model || ?max || ?minimum || ?sensitivity || ?scale || ?value || ?weight || ?val || ?sum || ?range || ?dval || ?rval || ?score || ?kval || ?pval || ?scale || ?max || ?minimum || ?ranges || ?order || ?sibling || ?order_sibling || ?order_sibling ?dataset ?model_lbl || ?benchmark || ?weight || ?val || ?scale || ?value || ?weights || ?val || ?range || ?range_sibling || ?order_sibling || ?order_sibling OPTIONAL { ?scale orkgp:BENCHMASK ?scale orkgp:MAX ?value || ?range || ?range_sibling || ?min || ?max || ?min_i || ?max_i || ?max_s }} Input (English text): What is the first benchmark result achieved on the BSDs:P3 dataset, including the metric and its value ? output (Sparql query): output (Sparql query): SELECT DISTINCT ?benchmark || ?paper || ?model || ?max || ?minimum || ?sensitivity || ?scale || ?value || ?weight || ?val || ?scale || ?value_sibling || ?min || ?min_i || ?min_s || ?max_s || ?min || ?max || ?min_i || ?min_(i) || ?max_s || ?min_i || ?max_if || ?min_s_if || ?max_if_e || ?max_if_ei || ?min_if_ei || ?max	False
"
 input (English text): What are the metrics of evaluation over the SoMeSci dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SoMeSci"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Russian dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the Atari 2600 Chopper Command dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Chopper Command"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT MIN (value) AS metric and CASE WHEN ?output = -1 THEN -value -1 ELSE 0 END AS metric CASE WHEN MAX (value) < MIN (value) AND MAX (value) >= MIN (value) THEN ORkgc:NOMEDICTION ?value ELSE MIN (value) ?value END AS metric ELSE 0 END AS metric WHEN ?data - >!value THEN CASE WHEN orkgc:NOMEDICTION = None and ?dataset = '' then { ?dataset orkgc:Dataset } ELSE [ { ?dataset a orkgc:Dataset } ] END ELSE 0 END AS metric CASE WHEN orkgc:NOMEDICTION = ?dataset then { ?dataset } ELSE { ?dataset orkgc:Dataset } END ELSE 0 END AS metric WHEN MIN (value) = ?value then { ?dataset } ELSE MIN (value) ?value ELSE 0 END AS metric ELSE min (value) ?value ELSE 0 END AS metric WHEN MAX (value) = ?value then { ?dataset } ELSE { ?dataset orkgc:Dataset } END ELSE 0 END AS metric WHEN MIN (value) = MIN (value) then { ?dataset } ELSE MIN (value) ?value ELSE 0 END AS metric WHEN MAX (value) = MAX (value) then { ?dataset } ELSE { ?dataset orkgc:Dataset } END ELSE 0 END AS metric The outputs for benchmarking and evaluation can be downloaded at: https://github.com/dubai-community/scia/blob/master/scia_2_3.pem What is	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset smallNLP-KG?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""smallNLP-KG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the title and ID of research papers that contain a benchmark over the GENIA - LAS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Stanford Cars benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?matrix ?model_lbl OR ?model :HAS_DATASET AND ?model_lbl_rdfs AS ?rdfs, ?lbl_lbl_rdfs AS ?lbl_lbl_rdfs AND ?matrix_lbl_test as ?lbl_lbl_test, ?lbl_lbl_cont and ?matrix_lbl_test_rdf as ?matrix_lbl_test_rdf input (English text): list the score of the model that performed best on the Genes data set ? input (English text): Which data set are these ? output (Sparql query):  SELECT ?cnt.rank AS ?cnt.rank, ?cnt.score AS ?cnt.score, ?p1.rank AS ?p1.rank, ?p2.rank AS ?p2.rank, ?p3.rank AS ?p3.rank, ?p4.rank AS ?p4.rank, ?p5.rank AS ?p5.rank, ?cnt.rank AS ?cnt.rank, ?p1.lbl_score AS ?p1.score, ?p2.lbl_score AS ?p2.score, ?p3.lbl_score AS ?p3.score, ?p4.lbl_score AS ?p4.score, ?p5.lbl_score AS ?p5.score, ?cnt.rank AS ?cnt.rank, ?p1.rank2 AS ?p1.rank2, ?p1.score AS ?p1.score, ?p2.rank2 AS ?p2.rank2, ?p2.score AS ?	False
"
 input (English text): List the code links in papers that use the MEMEN (ensemble) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the SciERC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Precision"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT MAX( ?code_n, 4 ) from.studies.dictionary WHERE ?code in ?code_n and ?model in ?model_lbl. (If you have multiple benchmark papers for the same benchmark, please use an output query to summarize the results, and do not add the results to the output). If you have any questions or concerns, please send an e-mail to jw-comments[at]spsl[dot]com, or call us at (818) 637-0718.	False
"
 input (English text): Who are the authors of the SOSA ontology?
 output (Sparql query): SELECT ?authors WHERE { ?papers rdf:type orkgc:Paper. ?papers rdfs:label ?papers_labels. FILTER(REGEX( ?papers_labels, ""^SOSA"")) ?papers orkgp:P27 ?authors. } 
 input (English text): Provide a list of papers that have utilized the VGG11B(3x) + LocalLearning model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VGG11B(3x) + LocalLearning"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the SQuAD1.1 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 Input: a list of models from the benchmark: - 3 or 2x VGG11B(3x) + LocalLearning - 3 or 2x VGG11B(3x) + GBM + LocalLearning - Local-3D - Local-3D + Local-2D - Local-2D + Local-3D - Global-3D. Output: a list of metric definitions: model, benchmark_r, data_dict, and data_dict_r. metric.model.sqad1.1model = 2 or 3 or Benchmark(localLearning) metric.model.sqad1.1model = 2 or 3 or Benchmark(mul(2, LocalLearning)) metric.model.sqad1.1model = 3 or 2 or Benchmark(localLearning) metric.model.sqad1.1model = 2 or 3 or Benchmark(localLearning) metric.model.sqad1.1model = 3 or 2 or Benchmark(localLearning) metric.model.sqad1.1model = 3 or 3 or Benchmark(localLearning) metric.model.sqad1.1model = 3 or 2 or Benchmark(localLearning) metric.model.SQAD_2_3 = 3 or 2 or 3 or Benchmark(localLearning) metric.model.SQAD_3_2 = 2 or 3 or L2-MIL(localLearning) The test run data from the benchmark is a set of 100 images, and we'll use this for the evaluation. We'll use all metrics to evaluate our models (and any metric-based evaluations as well); only one value is used for the score calculation of each score: The score is computed by dividing the model's performance (in our case, based on the number	False
"
 input (English text): What is the amount of questions for LC-QuAD 2.0 dataset?
 output (Sparql query): SELECT ?number_of_questions WHERE { orkgr:R154290 orkgp:P31 ?contrib. ?contrib orkgp:P41923 ?number_of_questions. } 
 input (English text): What are the models that have been benchmarked on the ANLI test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	--------------------------- ------------- authors: 0 authors: 1 names: 2 papers: 3 --------------------------- ------------- authors: 0 authors: 1 names: 2 papers: 3 --------------------------- ------------- authors: 0 authors: 1 names: 2 papers: 2 --------------------------- ------------- author_type: 0 name: 0 papers: 2 --------------------------- ------------- author_type: 0 -------------- -------------- -------------- ------------ ------------ authors: 0 authors: 1 names: 2 papers: 3 --------------------------- ------------- author_type: 0 name: 0 papers: 2 --------------------------- ------------- author_type: 0 -------------- -------------- -------------- -------------- ------------ authors: 0 authors: 1 names: 2 papers: 3 --------------------------- ------------- author_type: 0 name: 1 papers: 2 --------------------------- ------------- author_type: 0 -------------- -------------- -------------- -------------- ------------- authors: 0 authors: 1 names: 2 papers: 3 --------------------------- ------------- author_type: 0 name: 1 papers: 2 --------------------------- ------------- author_type: 0 -------------- -------------- -------------- -------------- ------------- authors: 0 authors: 1 names: 2 papers: 3 --------------------------- ------------- author_type: 0 name: 1 papers: 2 --------------------------- ------------- author_type: 0 -------------- -------------- -------------- -------------- ------------ ------------ 1 2 3 18 7 9 15 9 6 8 5 7 4 7 2 10 6 5 7 7 1 7 9 8 2 6 5 1 2 9 11 4 4 14 7 6 11 8 1 8 1 4 7 3 3 4 9 9 3 8 1 2 3 4 2 1 2 8 1 1 2 1 1 1 1 1 1 1 2 1 1 4 1 3 5 8 3 6 12 4 4 15 14 13 12 4 15 6 9 3 11 4 10 3 4 3 13 1 14 4 2 2 1 12	False
"
 input (English text): what are the evidence and limitations of paper ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries""?
 output (Sparql query): SELECT ?evidence ?evidence_title ?limitation WHERE { orkgr:R6751 orkgp:compareContribution ?cont. ?paper orkgp:P31 ?cont; rdfs:label ?paper_title. ?cont orkgp:P5004 ?evidence; orkgp:P5006 ?limitation. ?evidence rdfs:label ?evidence_title. FILTER(REGEX(STR( ?paper_title), ""Evaluating the Use of Social Networks in Author Name Disambiguation in Digital Libraries"", ""i"")) } 
 input (English text): What is the name of the top performing model in terms of BLEU score when benchmarked on the WMT2014 English-French dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the ACL Anthology dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Note for researchers: If you are a researcher and would like to submit your paper for evaluation or would like to run your own comparison of a paper with other papers then please contact us. For further discussion regarding the paper: Please read the paper and the comments of the abstract.	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Kung-Fu Master dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Kung-Fu Master"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the WRN28-10 (SAM) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""WRN28-10 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Yars Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): Which model has successfully achieved the highest score on the Atari 2600 Yars Revenge benchmark dataset in the code link column ? output (Sparql query): input (English text): Which model has achieved the highest score on the Atari 2600 Yars Revenge benchmark dataset in the benchmark column ? output (Sparql query): input (English text): Which model has achieved the highest score on the Atari 2600 Yars Revenge benchmark dataset for each model ? output (Sparql query): input (English text): Which model has the best accuracy for the Atari 2600 Yars Revenge benchmark dataset for the given performance criteria ? output (Sparql query): input (English text): Which model has the lowest accuracy for the Atari 2600 Yars Revenge benchmark dataset for each model ? output (Sparql query): input (English text): Which model has the highest accuracy for the Atari 2600 Yars Revenge benchmark dataset for each model used in any benchmark that requires it ? output (Sparql query): output (Sparql query):	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the enwiki8 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the NCBI-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 if [ ${ ?dataset a} && ${ ?dataset b} ] ; then ${ ?dataset a} { ?dataset b } if [ ${ ?dataset a} && ${ ?dataset b} ] ; then ${ ?dataset a} { ?dataset b } (optional) if [ ${ ?dataset a} || ${ ?dataset b} || ${ ?dataset c} ] ; then ${ ?dataset a} { ?dataset b } ${ ?dataset c } input (English text): What metrics are commonly used when benchmarking models on the NCBI-disease dataset ? output (Sparql query): Output options: :benchmark :benchmark or kgp : benchmark or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp :benchmark or kgp:HAS_DATASET : benchmark or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp or kgp :benchmark or kgp_lbl:HAS_DATASET : benchmark or kgp_lbl:HAS_DATASET or kgp_lbl:HAS_EVALUATION or kgp_lbl :benchmark :benchmark or kgp_lbl:HAS_METRIC: - :benchmark or kgp_lbl:HAS_METRIC :benchmark or kgp_lbl:HAS_METRIC or kgp_lbl or kgp_lbl :benchmark or kgp	False
"
 input (English text): Indicate the model that performed best in terms of RACE-h metric on the RACE benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""RACE-h"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Classifier Chain + SMO model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Classifier Chain + SMO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?output_score FROM ?dataset ORDER BY DESC( ?output_score) LIMIT 1	False
"
 input (English text): What is the name of the top performing model in terms of BLEU score when benchmarked on the WMT2016 Romanian-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Romanian-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of benchmarked datasets related to the Neural Architecture Search research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Neural Architecture Search"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the metrics of evaluation over the Fashion-MNIST dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Fashion-MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT rdfs:label from (  ""Fashion-MNIST"", ?model_lbl  ;  rdfs). where  { ?dataset_v3_v1  &&  [ ROWS(3)  =  ""S.N.B."" ] } ;   *   is  the current model,   ?metric_v3_v2  is  the current criterion and   ?metric_v3_v1 _ =  { 1 >   ?metric_v3_v2 [ 2 ] }  OR  { 1 >   ?metric_v3_v1 [ 2 ] }  OR   { 1 >   ?metric_v3_v1 [ 2 ] } ; output ( S3 output): /*  Select top 5 best performance over the Fashion-MNIST dataset by the metric V3_M.  This metric was created to show you the highest performing models over the dataset.  For instance, the data set contains many patterns that have a lot of ""holes"" which means that there are not enough of them.  These models have a lot of potential to be good models.    These can go into the next round of evaluation. */  select * >  [  rdfs:label,  ](  rdfs:type ,  metrics) >  ["	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the AESLC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the WMT2016 English-Romanian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""X-Sum"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT sum(X_sum,ROUGE) SUM(X_sum,WMT2016) AS ROUGE FROM rdfs:ROUGE GROUP BY SUM(X_sum,WMT2016) ORDER BY SUM(X_sum). r2 > 0.75 input (English text): Please elaborate how you choose a model to benchmark X-Sum dataset. output (Sparql query): select sum(X_sum,X_sum) SUM(X_sum,WMT2016) AS X_sum FROM rdfs:ROUGE GROUP BY sum(X_sum,WMT2016) ORDER BY SUM(X_sum). r2 > 0.75 input (English text): How do you choose a model to benchmark X-Sum dataset in terms of X-Minus metric ? output (Sparql query): SELECT sum(X_minus,X_sum) SUM(X_minus,WMT2016) AS X_minus FROM rdfs:ROUGE GROUP BY sum(X_minus,WMT2016) ORDER BY SUM(X_minus). r2 > 0.75 input (English text): How do you choose a model to benchmark X-Minus dataset in terms of X-Max metric ? output (Sparql query): SELECT sum(X_max,X_sum) SUM(X_max,WMT2016) AS X_max FROM rdfs:ROUGE GROUP BY sum(X_max,WMT2016) ORDER BY SUM(X_max). r2 > 0.75	False
"
 input (English text): What is the name of the top performing model in terms of Test mAP score when benchmarked on the AudioSet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test mAP"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AudioSet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the code links in papers that use the XLNet (Large) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (Large)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the RotoWire (Relation Generation) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Relation Generation)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Test mAP"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (( ?dataset_len-1 > ?dataset) ?n.FILTER(( ?size-2> ?dataset.DATA_SIZE ?dataset) ?dataset) ?n.NUM_STACKS ?dataset ?n.NAMES ?n.VOWERS ?dataset ?n.DESC_NAME ?dataset. ?cont orkgp:HAS_DATASET ?dataset_lbl. } OR ?value orkgp:HAS_VALUE ?value. } ORDER BY DESC( ?value). ?dataset rdfs:label ?dataset_lbl. } input (English text): Show any other comments about this training dataset output (Sparql query): input (English text): What is the name of the top performing model in terms of Test mAP score when benchmarked on the AudioSet dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_"	False
"
 input (English text): Where can I find code references in papers that have used the TDMS-IE model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TDMS-IE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the Atari 2600 Freeway dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Fine-Grained Gating model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Fine-Grained Gating"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?class_1 AND { ?datastore orkgc:DATASET ?datastore. ?data rdfs:label ?dataset_lbl. ?class_1 orkgc:DISTINCT ?class_1_lbl. FILTER (str( ?class_1) = ""fine-grained"") ?score orkgp:HAS_DATASET ?data; score rdfs:label ?dataset_lbl. OPTIONAL orkgp:HAS_MODEL ?score. }  input (English text): Any other information I need for this section ? I need the names of the authors who have included code comparisons in their papers. output (Sparql query):  SELECT FEE prf_num,  name  FROM _tbl_author_list  INSERT  PRODUCT  ( 'name',  { ?name : ?num  })  WHERE name =   ?author orkgp:HAS_Author ?score/ prf_num  AND score =   ? score/ prf_num  AND score > ?score  AND author =   ? author rdfs:label/ prf_num  AND name  AND score =   ?label/ prf_num  AND name =   ?author rdfs:num  AND  name  AND score =   ?num/ prf_num   AND  score > ?score and  author =   ?author { ?score_lbl:HAS_DATASET ?dataset; ?lbl_lbl:HAS_DATA ?data...."	False
"
 input (English text): What are the models that have been benchmarked on the TREC-6 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TREC-6"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of SOFT_SPL metric on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SOFT_SPL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Montezuma's Revenge dataset in terms of Average Return (NoOp) metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Average Return (NoOp)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" input (English text): Explain the difference between the Atari 2600 ""NoOp"" metric, and the HMM-R metric. output (Sparql query): input (English text): What model has had its performance best-certified on the TREC-6 dataset on the PDP-12 ? output (Sparql query): input (English text): Explain the results that the TREC6 data set has provided over a test period that spans from 1999-2005. output (Sparql query):. For testing, this is a set of models in a dataset that is relatively small -- but in the model space that is. A test bed or testing environment will be set up like the one you see here. You can download TREC-6 here. Here are the models we plan on testing and the values they can be expected to produce here. The output will contain the model name, model title, the name of the model in the dataset, and the code that we run on the data set. For each model, a model_lbl key in the model table will describe the algorithm, and the validation test set will contain the dataset the model is testing on. We run the code that we run on our new model on the dataset it is testing on. We have written out the code for our model in the table below. You then run the code on your new model. As soon as the results come up, you look at the table on the right, click the check box next to a model, and you check that the model is the best performing model in this test. The models in the table below are shown in the order they are tested. If there are any models that you have not run yet, you do not run those models. The TREC-6 models in the table below have been benchmarked"	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Prior noop model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Prior noop"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT DISTINCT ?rdfs :RDFS_FILE name from (SELECT rdfs:RDFS_FILE id, name from a select id, rdfs:RDFS_FILE name, dataset from another, dataset from a) rdfs:RDFS_FILE rdfs:RDFS_FILE_TEXT name, dataset where rdfs:RDFS_FILE rdfs:RDFS_FILE_TEXT(name) name SELECT DISTINCT ?dataset and ?dataset_lbl from (SELECT RDFS:RDFS_FILE name, dataset from a select id, rdfs:RDFS_FILE data, dataset from another, dataset from another where rdfs:RDFS_FILE_TEXT rdfs:RDFS_FILE_TEXT(name) name) rdfs:RDFS_FILE rdfs:RDFS_FILE_TEXT name as ?dataset_lbl.dataset_lbl SELECT DISTINCT ?dataset and ?dataset_lbl.subquery as ?dataset_lbl.subquery.lbl from (SELECT RDFS:RDFS_FILE name, dataset as ?dataset_lbl.dataset and ?dataset_lbl.subquery as ?dataset_lbl.subquery.lbl from (SELECT RDFS:RDFS_FILE name, dat	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Accuracy score on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the WMT2016 English-Russian dataset in terms of BLEU score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 input (English text): What is the best performing model benchmarking the WMT2016 Russian-German dataset in terms of BLEU score metric ? output (Sparql query): input (English text): What is the optimal model to benchmark the WMT2016 English-Russian dataset in terms of BI-based index ? output (Sparql query): input (English text): Given a list of English-Japanese scientific papers, what model best performs BLEU ranking using the TEMPLM score metric, on the WMT2016 English-Japanese dataset, or another Japanese dataset, and which model best performed the same benchmarking task on the respective Japanese dataset ? output (Sparql query): input (English text): In order to run a query against the BLEU results from the WMT2016 Russian-American dataset, how should the query output be formatted so that we can easily visualize it ? output (Sparql query): input (English text): Do you know what the output of a query against the BLEU results from the WMT2016 English-Chinese dataset would look like ? output (Sparql query): input (English text): What is the minimum number of rows in which all parameters of the WMT2016 Chinese-Russian dataset are required ? output (Sparql query): input (English text): By what weight should the weights in the WMT2016 Chinese-Russian dataset be considered in relation to other datasets (e.g. all datasets using HANPP, or HANPP-LDA) ? output (Sparql query): input (English text): What is the minimum number of rows in this query to return a result for an empty vector (i.e. a ' ?') ? output (Sparql query): 	False
"
 input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Name This Game dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the Oxford-IIIT Pets dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT TOP(1) ?score, ?error_score, ?accuracy, ?accuracy_level AS error_	False
"
 input (English text): Which model has achieved the highest SICK-R score on the SentEval benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""SICK-R"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SentEval"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Who is responsible for acquisition in the RASH system?
 output (Sparql query): SELECT ?acq WHERE { orkgr:R8364 orkgp:compareContribution ?cont. ?cont orkgp:P7046 orkgr:R8350. orkgr:R8350 orkgp:P7049 ?acq. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters-21578 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 Fully automated research methods. This means that we can generate data in a structured and non-trivial way and that we don't need anyone involved. This can mean that we can make our own or other people's research methods, and we don't need to work any more with researchers than they want to work with. This avoids the need to learn a whole new process by doing the original research. All data can be made available under the Open Science Framework, or by a licence such as Creative Commons CC0 1.0. We can make a lot of our research much more easily, and we know that doing research on a dataset like this is not just about academic science (it's also a good activity for people who are interested in science). The potential for the research to benefit society or improve things in your own locality is huge. This project was inspired by the work of Chris Anderson who created a web and data management system which has a very similar idea to the research tool we are building. In the spirit of free access, this project uses Open Data. Please make your data free to access online. Your data is now part of the Open Science Framework, which means that others, researchers, and everyone interested in research can make their own datasets available to others as well. 	False
"
 input (English text): What is the name of the top performing model in terms of Top-1 Error Rate score when benchmarked on the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Error Rate"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Indicate the model that performed best in terms of Accuracy (2 classes) metric on the IMDb benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (2 classes)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Penn Treebank (Character Level) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank (Character Level)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 Inputs: dataset_a, dataset_b, dataset_c, dataset_d, data_d, data_e, data_f, DataFrames, models_i, models_j, data_k Output: 1>output.txt output.txt: Output: 2>outfile.txt outfile.txt: Inputs: dataset_a, dataset_b, dataset_c, dataset_d, data_d, data_e, data_f, CharacterLevel Output: 1>outfile.txt outfile.txt: Output: 2>outfile.txt outfile.txt: Inputs: dataset_a, dataset_b, dataset_c, dataset_d, data_d, data_e, data_f, CharacterLevel Output: 1>outfile_data_d_model_i_index.txt outfile_data_d_model_i_index.txt: Output: 2>outfile_data_d_model_j_index.txt outfile_data_d_model_j_index.txt: Inputs: dataset_a, dataset_b, dataset_c, dataset_d, data_d, data_e, data_f, CharacterLevel Output: 1>outfile_data_data_d_model_i_index.txt outfile_data_data_data_d_model_i_index.txt: Output: 2>outfile_data_data_d_model_j_index.txt outfile_data_data_data_d_model_j_index.txt: Inputs: dataset_b, dataset_c, dataset_d, data_d, data_e, data_f, CharacterLevel Output: 1	False
"
 input (English text): What models are being evaluated on the WMT2014 English-French dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the MultiCCA + CNN model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MultiCCA + CNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the ANODE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ANODE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Output Output will contain a list of papers that utilize the ANODE model. You'll need to go through this process again for each of the papers that you've created for the Benchmark of the MultiCCA + CNN Model. It is therefore recommended that you also upload a copy of the paper that has been utilized when you upload the data to Parabola. After completing the step above with each of the papers that you have used in the Benchmark of the MultiCCA + CNN Model, you are all set. The Final Step Parabola's API is used for testing each of the model variants that Parabola has developed. It's assumed that you have already provided your access details when you created your dataset and a link to their website is also available. The final step that you have completed is submitting all data, papers and datasets you have used to generate the datasets that will be the focal point of the Benchmark of the MultiCCA + CNN Model. To perform this process, you have all the data that you need to create two datasets, one to evaluate each model variant and one to test the results of the validation data. This final step can be performed manually by copying the following two commands: 1. Copy the existing dataset to a place of your choice: wmt_datasets/dataset_list.csv. 2. Download the current revision of test_datasets.csv to a public or shared location on your network: wmt_datasets/dataset_list.csv-current.xml. 3. Open the current version of test_dataset_list.csv with a text editor like Notepad++, e.g. notepad++_vim, and copy the following two lines 2. Run these commands: a. Copy the dataset list file	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the BUCC Russian-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the models that have been benchmarked on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV3-Small"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Note that the output is more than just output, but also include the link to the paper's github page (with details of the code used) and instructions for reproducing the benchmarking results. 2. The test suite These functions are currently included in the test suite, but may be dropped in future releases of the software.	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the SQuAD2.0 dev dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0 dev"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the LNN-EL/ens model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LNN-EL/ens"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the FSNS - Test dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FSNS - Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?pagename OR ?fsc or kgp:ID ?pagename. FILTER (str( ?fsc. ?pagename ) = ""SQuAD2.0 dev"") FSc Paging output (Sparql query): output (Sparql results): Input: Output: Output: Input (English text): Is the paper presented in a publication, in a peer-reviewed journal or as a research report ? output (Sparql query): SELECT DISTINCT ?presentation OR ?paper. ?presentation or kgp:ID OR ?paper ?presentation or kgp:ID. FILTER (str( ?presentation ) = ""Publication"") ?paper output (Sparql query): output (Sparql results): Input: Output: Output: Input (English text): Would such a project be eligible to participate in the European research network ? output (Sparql query): SELECT DISTINCT ?grouping OR ?n_groups OR weight ?groupING OR weight OR id ?groupING_labels OR id OR id OR id OR id OR id OR id OR id OR id. ?groupING_id OR id. FILTER (str( ?groupING_id ) = ""Group"") ?groupING GROUPING output (Sparql query): output (Sparql results): Input: Output: Output: inputs.csv - CSV documents with an additional table, to be created when"	False
"
 input (English text): Which model has achieved the highest Accuracy score on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the Transformer-XL model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the MPAD-path model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MPAD-path"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	   # # A library to manage an analysis of the Transformer-XL model for the OCR OpenBookQuant benchmark data # http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3429087/   # import sqlite3  import praw import numpy as np # Import the MPAD3_EXTERNAL module and convert to sqlite3's sql/rdf format import string import re import argparse from sys import exit, log, trace # Create the query objects # First we'll define the model class. Model = sqlite3. open ( 'testdata_2ndpart' ) # This column holds the model name class Model ( object ): __tablename__ ='model' # We can define some other attributes here Model. __doc__ = { 'code' : '', 'name' : 'MPAD-path model', 'params' : [ { 'query_type' : 'cron', 'args' : [ ( 'path', 'http://openbookquant.nlm.nih.gov/examples/mpad' ), ('metric','met(kgc:Metric,avg_2')' ), ( 'query_columns', [ ( 'name', 'lbl1' ), ( 'name', 'lbl2' ), ( 'name', 'lbl3' ) ], ) ],'methods' : [ ('sort', 'L2_SUBROUTINE' ) ] } # We can now create a query object # this function returns a SQL dictionary and will pass that # to the query object. We can now update data'metric' and 'value' in this model object  def query_model ( model ): 	False
"
 input (English text): Which model has achieved the highest F1 score on the SQuAD2.0 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What can one use instead of wheat flour?
 output (Sparql query): SELECT DISTINCT ?flour WHERE { ?_ orkgp:P37571 ?flour. } 
 input (English text): What are the models that have been benchmarked on the BoolQ dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): What is the first model to have gained the best score on the BoolQ dataset ? output (Sparql query):"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the AWD-LSTM-DOC x5 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AWD-LSTM-DOC x5"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Top-1 Accuracy score when benchmarked on the Kinetics-600 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top-1 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BUCC German-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?metric orkgc:Metric as ?category. * 1 AS Score* FROM English_to_German. orkgc ?category { ?category a :category_name } { ?category b :category_name } AS Score and ?dataset as ?dataset_lbl ON ?dataset_lbl. ?dataset_lbl. ?column as ?type. { ?column a :type_name } and ?type_2 as ?type_2_name. { ?type_1 :object_id. ?column b :object_id} and ?type_2_2 as ?type_2_2_name. { ?type_1 :object_type. ?column c :object_type_name. } AND ?name as ?name. AND ?dataset as ?dataset_lbl on ?dataset_lbl. } output (Sparql query): SELECT ?classifier orkgc:Classifier as ?parameter_name. * 3 AS Score* FROM English_to_German. orkgc ?parameter { ?params:ParameterParam1; ?params2:ParameterParam2; ?parameter3:ParameterParam3 } AS Parameter and ?dataset as ?dataset and ?column as ?type. { ?column a :type_name} and ?type_2 as ?type_2_name. { ?column c :type_name. ?column d :type_name. } AND ?name as ?name. AND ?dataset as ?dataset_lbl on ?dataset_lbl. ?name rdfs:label ?name. ORDER BY DESC( ?name) LIMIT 6.8	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the WOS-11967 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest Score score on the Ball in cup, catch (DMControl100k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT ?model ?model_lbl. DISTINCT ?metric_lbl. model name : WOS-11967, SSCOR-1, SSCOR-4, MATH-1, MATH-2, MATH-3, MATH-5, SAT-1, SAT-2, SAT-3, SAT-4, SAT-5, STROBE-1, STROBE-2, STROBE-3, STVSTA-1, STVSTA-2, STVSTA-3, STVSTA-4, SAT-1, SAT-2, SAT-3, SAT-4, SAT-5. DISTINCT ?metric_lbl.   input (English text): Could you provide a list of models that have been tested on the CUBE in CUBE1 benchmark dataset ? output (Sparql query): Model name : WOS-11967, SSCOR-1, SSCOR-4, MATH-1, MATH-2, MATH-3, MATH-5, SAT-1, SAT-2, SAT-3, SAT-4, SAT-5. DISTINCT ?metric_lbl. input (English text	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Phoenix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Phoenix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the ViT-L/16 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ViT-L/16"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Perspective Matching (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 _____________________________________________________________________________________________________________________________________  { ?code a:Multi_PerspectiveMatching; }. Input: Output: Citation links: Concentration in English (in order of importance): Input: Output: Citation link is the page with the article in the ISI journal.   Link to the relevant page is in the document ID. I didn't include citations in the output file. If citations were included in the output file, they would have to come from the journal text. A paper would be more likely to be cited in a paper that is cited by other papers that use this model.  Therefore, I didn't include them, but they are now listed in the output file. A note: I'm not a statistician. If you find any mistakes. Please do not hesitate to let me know. I would love the feedback on how to improve this page.  And don't forget to comment.	False
"
 input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the ImageNet 64x64 dataset in terms of Bits per dim metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Bits per dim"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dimension ?dim OR = ?dim; orkgc:Dimension; rdfs:label ?dimension. } LIMIT 1 (0x2f) orkgc:Dimension, ?dim, orkc:Dimension; input (text file): What is the IMDb-M dataset used to benchmark with for the ""LIMIT1"" column in SIFT ? output (Sparql query): SELECT ?maxWeight ?maxWeight, ?weights AS RDFSID OF ?maxWeight WHERE (1, 0, ?maxWeight) AND (1, 1, ?maxWeight); input (English text): What is the best performing model benchmarking the IMDb-M dataset in terms of Accuracy metric ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTCREAT( ?dataset_lbl) ?dataset; ?benchmark orkgp:HAS_DATASET ?dataset. ?maxWeight ?maxWeight; ?maxWeight rdfs:label ?maxWeight; } } } Input-file output comparison: Note that the differences between benchmarking the IMDb-M dataset for both accuracy and counts metrics are not very large even if they are statistically significant. Summary The IMDb-M dataset is designed specifically for benchmarking the IM"	False
"
 input (English text): List the code links in papers that use the Baseline : Random model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Baseline : Random"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Extended Transformer Construction model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Extended Transformer Construction"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset CommonsenseQA?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" output (Sparql query): Output the top results per dataset using a set of query variables (in this case, the query variable from the query) or select * from the query table to get only the top 10 results. input (English text): For each benchmark, would you like to have the output include the number of points that resulted in either a correct answer for the benchmark, or ""false"" on the test case ? output (Sparql query): output (Sparql query): When you get a hit, would you like to be notified. input (English text): Would you like to see a link for one of these results: A successful test (this means the system was able to extract code from the published paper and use it in the test): This has to be explicitly requested by the user. A failed test (this is the most common failure condition, usually due to human error, in papers that use the CoreLang paper): A ""Failure"" will be given because the test has been dropped after 3 failed attempts. output (Sparql query): output (Sparql query): output (Sparql query): Output only the top few results and a summary of the results. input (English text): Do you have any examples of these datasets ? output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): Do you have any tests that use the CoreLang paper ? output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): If you"	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Nottingham dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Food-101 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Are children examined in the studies?
 output (Sparql query): "	"ASK { orkgr:R44979 orkgp:compareContribution ?contrib. ?contrib orkgp:P23161 ?population. FILTER(REGEX( ?population, ""ildren"")) } "	 input (English text): Are the results consistent with those of other research ? output (Sparql query): input (English text): Are there any particular concerns about the quality of the results ? output (Sparql query): input (English text): If so, provide a specific example of how you feel the results were obtained ? output (Sparql query): input (English text): How are conclusions of the research reported in the relevant newspaper, newspaper website (e.g. the article), and peer-reviewed journal ? output (Sparql query): input (English text): Where are papers published ? For each paper, provide a list of the authors along with the paper title, publication year and a link to the paper online. output (Sparql query): input (English text): Show citations of literature relevant to the relevant topic. output (Sparql query): To obtain the R dataset, we have to download the dataset, unpack the data and transform it. Let me give a brief definition of R as follows: R is a general-purpose package to interactively manipulate and create data frames is a general-purpose package to interactively manipulate and create data frames R packages are useful packages, that contain data for scientific research. Often the data in these packages is freely available. R packages are often downloaded by non-programmers. R packages contain data for scientific research. Often the data in these packages is freely available. R is used by most researchers, and the scientific community has an enormous use for R. It is a scientific language that provides a high level of extensibility for data analysis, machine learning, and other areas. The R-package is free because it has been developed for research and scientific use by the users.	False
"
 input (English text): Provide a list of benchmarked datasets related to the Atari Games research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Atari Games"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide links to code used in papers that benchmark the Random Forest model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Random Forest"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the CoQA dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 { ?best ?best_test ?best_summary-3 ?best_score ?best_summary ?best_score_score, ?best_score_score, ?best_score ?best_best_score, ?best_best ?best_best, ?best_best_score, ?best_best_score, ?best_best_score, ?best_last_score ?best_last, ?best_last_score_score, ?best_last_score_score, ?best_prec_score ?best_prec_score} Input Data: What is the lowest benchmark result achieved on the CoQA dataset, including the metric and its value ? Output Data: The lowest benchmark result achieved on the CoQA dataset includes the metric and the value. Output Data: The lowest benchmark result achieved on the CoQA dataset is also the lowest benchmark result achieved on the Random Forest dataset. For the CoQA dataset, the value is the mean of the results found between the values selected by the random forest model. For the Random Forest dataset, the value is the median of the results found between the values selected by the random forest model. Code: http://pastebin.com/CmRqrPJg	False
"
 input (English text): Provide a list of papers that have utilized the Depth DDPPO model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Depth DDPPO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the SQuAD1.1 dev dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the ImageNet V2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 The output of these queries are returned along with the benchmarks in the format given in the text output. This output format is similar in format to the XML output from the query. The data is loaded from a file whose filename is the filename given by the output of the second command. For more details, see Using the Depth DDPPO model to train the model with the SQuAD1.1 dataset.	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the MLDoc Zero-Shot German-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the Oxford-IIIT Pets dataset in terms of PARAMS metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT SIZE_OF_SCALED AS  MIN(score_p, MAX(score_p, 1))  ||score AS ?score; DISTINCT   ?score  ||score_p as ?score; SELECT ?score  ||score_p as ?score. ?value  ||score_p as ?score For brevity, I am ignoring the fact that the same query would also return all models as output. In the above query,  the models are  ordered to make it easier to get the results in one line, without worrying that I am using a binary search as the output index. Now, you can get back your answers in a few lines of code:  SELECT score AS s, ?score AS t.score, ?score_p AS ?s.score, ?score_p as ?t.score from rdfs  || score AS ?s, ?score AS ?t ORDER BY ?score AS ?score, ?score_p AS ?s.score, ?score_p as ?t.score, ?score_p as ?score_p. DESC() LIMIT 1 LIMIT 5 What's a SPC ? A SPC is a scalar statistics distribution, as used in the following example. To get back your answers, you first need to know why you are interested in the input data. Here, an SPC is a set of measures or parameters that are set at every time step for the same model. The SPC is a scalar statistic. For the above query, SPC is the number of trials for each model, as follows:  ||score AS ?score  ||score_p AS ?s.score  ||score AS ?s.value  ||score AS ?s	False
"
 input (English text): What models are being evaluated on the Oxford 102 Flowers dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Dynamic Coattention Networks (ensemble) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Dynamic Coattention Networks (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Quasart-T"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?classname ?classid 1. SELECT ?classname ?classid 1. input (English text): Identify the classifier from the XHBSS benchmark dataset ? output (Sparql query): SELECT ?classname ?classid 1. SELECT ?classname ?classid 1. input (English text): Provide a list of papers that have utilized the TEN-HAST model and include the links to the code ? output (Sparql query): SELECT ?paper ?nodes ?positions ?clustering ?cluster_length ?cluster_pivot ?clustering_method ?cluster_method ?cluster_rfd ?cluster_label ?clustering_type ?clustering_target ?clustering_output1 ?clustering_output2 ?distinct ?clustering_outcome_b ?clustering_outcome_a ?coefficient1 ?type2 ?rdfs:label ?clustering_type ?clustering_target1 ?clustering_target2 ?clustering_outcome_a ?clustering_outcome ?clustering_outcome_b ?name 1: SELECT ?name ?classname 1. SELECT ?name ?classid 1. SELECT ?name ?classid 1. Select the cluster with the greatest size of clusters on the XHBSS benchmark dataset for the parameter ?clustering on the dataset ? output (Sparql query): input (English text): Identify the cluster of the biggest size on the XHBSS benchmark dataset for the parameter ?cl	False
"
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Breakout dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Breakout"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the LSTM Encoder-Decoder + LSTM-LM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM Encoder-Decoder + LSTM-LM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the models that have been evaluated on the Multimodal PISA dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multimodal PISA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?model. } Note: the link used for ' ?Model' as I was unable to get the dataset directly, but I am guessing that it is the link to the'multimodal_pisa_model.rpt' file, i can't guarantee that I'm correct. The Multimodal PISA dataset  (link provided to me by Richard). The LSTM Encoder-Decoder + LSTM-LM model is the subject of a recent paper: http://people.csail.mit.edu:7307/b/jamesp/2013/jpeg-pisas.pdf and the underlying code for the model is here: https://github.com/jamesp/pisas/blob/master/papers/pisasenvy.py The LSTM Encoder-Decoder + LSTM-LM model is a more powerful model than the one I use in my training algorithm and has been used to improve model training in the past as well as to improve the performance of some models in the Atari 2600 emulator (see this paper) The paper contains a good description of the use of the model and what they learned from the Atari 2600 emulator. However, it is also important to note that the"	False
"
 input (English text): List the code links in papers that use the SWWAE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SWWAE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of NER Macro F1 metric on the ADE Corpus benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""NER Macro F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ADE Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): Select the title of the paper that used the SWWAE framework to train Bayesian Classifiers on the ADE Corpus (all papers using this framework), and their NER Macro F1 metric on the ADE Corpus database ?	False
"
 input (English text): Provide a list of papers that have utilized the LUKE (single model) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LUKE (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Atari 2600 Pitfall! dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you list the metrics used to evaluate models on the Atari 2600 Zaxxon dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Zaxxon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 The output file for the first query can be found here. Please provide as much information as possible. For a full description of what you wish us to do with the dataset, please write your response in the comments section below. Thank you for your help! For support and further details, please please email me at :  matthew.matthews@gmail.com  and I will be happy to provide more information for you. If you prefer mailing your submission to the appropriate email address: Matt Matthews	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Where can I find code references in papers that have used the XLNet-Large (ensemble) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large (ensemble)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Spanish"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" In each case, the score for each metric is expressed as the sum of its individual scores (and is a composite, since all the scores are averaged over the whole batch of responses). The score is the sum of the individual scores (as described above), multiplied by the total number of samples, divided by the total number of test epochs (so a score of 1 may be converted into a score of 0.99 by dividing 1 by the total number of test epochs in the dataset). The normalized scores are based on the normalized score (i.e., the value has no single point and is not scaled) of each of the individual scores. In the second case, the score represents the total number of observations for each of the individual scores, divided by the total number of test epochs. The score is the sum of individual scores (along with the normalized scores), divided by the total number of observations. The third case is somewhat more difficult to explain with a simple example, but is essentially a ranking of the individual model values obtained from the benchmark dataset. The top result (at around 15 kB output of the query, for the test data set) shows our model (Emmerson, 2012), which gives a score of approximately 70, which is approximately the highest of all models tested on the benchmark dataset. As well, it has a high number of test epochs, and not only that. Of course, the result (the highest score) is not the best choice for every benchmark, since there might be many other models that might perform worse on each benchmark, for different metrics, etc. In the previous two cases, if we ignore the outliers in performance that are not present with any model (such as the model that fails for certain metrics), an overall score of about 65 would be sufficient to provide an upper bound on the value (which we would call the """	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Asterix dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the BC5CDR dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?meter ?metric_rdfs (MAX( ?value) AS ?score) WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_rdfs. FILTER (str( ?dataset_rdfs) = ""WMT2016"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?meter. ?meter rdfs:label ?meter_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) } } ORDER BY ?meter ?meter_rdfs input (English text): What is the highest benchmark result achieved on the WMT2016 English-German dataset, including the metric and its value ? output (Sparql query): SELECT DISTINCT ?meter ?meter_rdfs (MAX( ?value) AS ?score) WHERE { { ?dataset a orkgc:Dataset; rdfs:label ?dataset_rdfs. FILTER (str( ?dataset_rdfs) = ""WMT2016"") ?benchmark orkgp:HAS_DATASET"	False
"
 input (English text): Provide a list of papers that have utilized the Reading Twice for NLU model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Reading Twice for NLU"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of F1 score when benchmarked on the NCBI-disease dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of benchmarked datasets related to the Sentence Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Sentence Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT DISTINCT ?count(n) ?count(n). ?count(n). ?wordcount d. DIFF ?wordcount. ORG ?count(n). } input (English text): Give an overview of NLP based classification methods such as Resumetex, BAML and others ? output (Sparql query): select n from training_list where n > 0, DISTINCT ?count(n) ?count(n). n FROM ( select n from training_list where n > 0 ) ; Note that the data is not loaded if all words are zero. We recommend using.count(n) instead of.wordcount(n). This data is not relevant for the purpose of the benchmark. input (English text): Describe how NLP classifiers work, using examples and examples of use ? output (Sparql query): select n from training_list where n > 0, DISTINCT ?count(n) ?count(n). n FROM ( select n from training_list where n > 0 ) ; Note that the data is not loaded if all words are zero. input (English text): Do you have any insights into how Sentence classifiers used in the benchmark are performing ? output (Sparql query): select n from training_list where n > 0, DISTINCT ?wordcount d. DIFF ?wordcount. ORG ?wordcount. ORG n. n FROM ( select n from training_list	False
"
 input (English text): Provide a list of papers that have utilized the MEMEN  (single model) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MEMEN (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What models are being evaluated on the BC5CDR-chemical dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-chemical"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the RotoWire (Content Ordering) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset ?model ?benchmark ?benchmark_1 ?benchmark_2 ?benchmark_3 ?benchmark_4 ?benchmark_5 FROM ( SELECT ?dataset ?model ?benchmark ?benchmark_1 ?benchmark_2 ?benchmark_3 ?benchmark_4 ?benchmark_5 AS rdts ?content ORDER BY ?benchmark_1, ?benchmark_2, ?benchmark_3, ?benchmark_4 ?benchmark_5 AS tvs ?) as rds, ?model rds FROM ( SELECT ?dataset ?model ?benchmark ?benchmark_1 ?benchmark_2 ?benchmark_3 ?benchmark_4 ?benchmark_5 AS rdts ?content FROM ( SELECT ?dataset ?model ?benchmark ?benchmark_1 ?benchmark_2 ?benchmark_3 ?benchmark_4 ?benchmark_5 AS rdts ?) as rds) as rds output (Sparql query): SELECT ROWS 2 ?cont o1 o2 o3 ?"" ""C"" ""XO"" ?model o. o rdts o. o srtrs o. o tvs... OPTIONAL output (Sparql query): SELECT COUNT(a) * COUNT(b) ?"" ""XO"" output (Sparql query): SELECT COUNT(c) * COUNT(d) ?"" ""XLO"" output (Sparql query): SELECT COUNT(e) / COUNT(f) ?"" ""2XL"" output (Sparql query"	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Natural Questions dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the DBpedia dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?RESTRICTED_METRICS ?RESTRICTED_RESULTS orkgp:HAS_EGREP_METRICS \ ?RESTRICTED_QUALITY_METRICS ?RESTRICTED_MULTIPLE_COUNT ?RESTRICTED_DATA ?RESTRICTED_EXCPE ? NULL ROWS { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } OPTIONAL orkgp:HAS_EXPECTED_PRETEND_PATTERNS ? NULL ROWS { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 } OPTIONAL orkgp:HAS_BENCHMARK ?RESTRICTED_QUALITY_METRICS ? NULL ROWS { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 } OPTIONAL orkgp:HAS_BENCHMARK ?RESTRICTED_QUALITY_METRICS ? NULL ROWS { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 } OPTIONAL orkgp:HAS_BENCHMARK ?RESTRICTED_METRICS ? NULL ROWS { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 } OPTIONAL orkgp:HAS_MULTIPLE_COUNT ?RESTRICTED_QUALITY_METRICS ?RESTRICTED_METRICS ?RESTRICTED_MULTIPLE_COUNT input (English text): What model	False
"
 input (English text): Can you list the metrics used to evaluate models on the SVHN dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SVHN"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Asteroids dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asteroids"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Tokenlearner model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Tokenlearner"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ?dataset ?dataset_lbl from __VAULTING_DATA_TABLES__. * WHERE ?dataset a orkgc:Dataset is not null ORDER BY ?dataset_. | (dataset_lbl is not null). LIMIT 1	False
"
 input (English text): Could you provide a list of models that have been tested on the 200k Short Texts for Humor Detection benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""200k Short Texts for Humor Detection"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What models are being evaluated on the MedSTS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedSTS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?model ?model_lbl FROM { orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval; orkgp:HAS_MODEL ?model} D) output (Sparql query): SELECT DISTINCT ?model ?model_lbl FROM { orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval; orkgp:HAS_MODEL ?model} D LIMIT 1   input (English text): If the query returns multiple metrics on an object, select only one of the metrics output by the query. All input data is excluded from the query, such as values of features from other queries such as the CPT-100, and values of other objects such as dates and topics from other queries. If you are sure that your query returns the desired result, run the output query with NULL values for all columns, and use the following R code to return only the value that you want: output (Raster graph):	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the DuIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the IMDb-M dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the CoNLL++ dataset in terms of F1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?benchmark(DISTINCT ?dataset) ?benchmark FROM IMDbBenchmarks where DISTINCT = ?dataset; The result is a table with some interesting entries: #1 A. R. K. S. #2 A. R. K. S. #3 A. R. K. Here the second row of table only contains the results, the first row contains benchmark results. The IMDb-M dataset has to be set up to perform best, the benchmarking runs can be conducted with two or more of the different datasets. I used the DISTINCT variable to make sure the results are not included accidentally. In this case the benchmarked models are the CoNLL++ and IMDb-M datasets. The CoNLL++ dataset has a high resolution matrix (only 5 × 5 × 5 bits) but has a high dimensionality (60 × 120×90) and a very large data-set. The IMDb-M dataset has a very low resolution matrix (only 4 × 4 × 3 bits). As far as I can see, the models tested can all perform at least as well on CoNLL-M than on IMDb-M. #2 A. R. K. S. #3 A. R. K. S. As expected, the CoNLL-M model gives an overall higher score than the IMDb-M model when there are 2 dimensions (i.e., if the original datahand has 60 dimensions the IMDb-M dataset needs 60 dimensions). The other datasets are still better than CoNLL-M. However, all the models performed better than CoNLL++ at the third dimension (a dimension with 3 components). The third dimension can be defined as the dimensions in which the best model performed	False
"
 input (English text): What models are being evaluated on the Oxford 102 Flowers dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford 102 Flowers"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest F1 score on the ScienceCite benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the UCF101 (finetuned) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UCF101 (finetuned)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	  output: model Name :  Neatly-Typed Text Classification Algorithm model ID Name :  Oxford 102 - Flowers with Biomarkers output  (Sparql query): {                                   select  id, model_id,...;   select  model_id,...;                                   select  model_id,...;                                   select  text,...;                                   select  id, model_id,...;                  select  model_id,...;                  select  model,...;                                   select  model_id,...;               select  id, text,...;                  select  id, model_id,...;               select  text,...;                  select  id, model_id,...;                  select  model,...;                  select  id, text,...;                  select  id, model_id,...;               select  model_id,...;   select  model_id,...;  select  id, model,...;  select  model_id,...;  select i,...;  select i,...;  select ii,...;  select ii,...;  select iii,...;  select ii,...;  select iii,...;  select  model_id,...;  select  model,...;  select iv,...;  select iv,...;  select iv,...;  select iv,...;  select x,...;  select x,...;	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the LAMBADA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the STL-10, 1000 Labels dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP(1) ?title ORDER BY title DESC LIMIT 1 ORDER BY 2 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 DESC LIMIT 1 The results are shown in the table below. Table 1: Top 300 best performing models A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1.0 CITATIONS 2.2 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 3.7 19.8 17.3 12.2 8.3 8.4 9.0 9.2 8.5 6.9 7.0 7.4 6.1 6.1 5.9 3.7 4.2 6.3 4.8 5.0 5.2 5.2 2.0 2 2 0 2 4.0 0 6.1 1 0 8.6 1 3.0 2 10.6 16.7 18.0 23.8 6.2 9.4 8.8 13.2 14.2 14.7 15.2 8.8 14 8.2 8.0 13.0 3.4 4.7 2.6 7.8 4.3 0 0 19.0 0 1 5.0 2 4.1 8.9 18.0 8.3 15.2 9.8 31.	False
"
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cheetah, run (DMControl500k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cheetah, run (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×0.75 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×0.75"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ImageNet + iNat on WS-DAN model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ImageNet + iNat on WS-DAN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 data (English text): What is the current status of the paper which benchmarks ImageNet + iNat against WS-DAN model ? input (English text): Are you aware of the paper that benchmarked the ImageNet+ iNat against WS-DAN model ? output (Sparql query): input (English text): What is the current status of the paper which benchmarks the ImageNet + iNat against WS-DAN model ? output (Sparql query): data (English text): Are you aware of the paper that benchmarked the ImageNet + iNat against WS-DAN model ? input (English text): Are you aware of the paper that benchmarks the ImageNet + iNat against WS-DAN model ? output (Sparql query): input (English text): How many different benchmarks have been written for ImageNet + iNat or DenseNet / VGG-16 ? output (Sparql query): data (English text): How many different benchmarks have been written for ImageNet + iNat or DenseNet / VGG-16 ? input (English text): How many different benchmarks have been written for ImageNet + iNat or DenseNet / VGG-16 ? output (Sparql query): data (English text): What are the benchmarks and what would be recommended in order to use ImageNet + iNat/DenseNet/VGG-16 in applications ? Would it be fine to benchmark the ImageNet + iNat/DenseNet/VGG-16 with a pre-trained VGG-16 model or do you recommend to benchmark the pre-trained VGG-16 model ? input (English text): Would it be fine to benchmark the ImageNet + iNat/DenseNet/VGG-16 with a	False
"
 input (English text): Could you provide a list of models that have been tested on the FGVC Aircraft benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Asterix benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC Russian-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" We use a similar approach when using a ""C"" library for the analysis of training-based models and the ""S"" library for the evaluation of training-based models. For the evaluation of models on data of different dimensions, we also employ multiple different datasets and different metrics for each dataset. We do not share the same method for calculating scores for each subset of the data. We can, however, use the same metric scheme to evaluate the scoring efficiency of each subset of the data where the metrics that are used to evaluate the models are identical for the two datasets. While the same formula was used in our previous tutorial and for our evaluation results, this formula could easily be changed in our next tutorial to address different needs of different databases and the model evaluation process."	False
"
 input (English text): Where can I find code references in papers that have used the Sparse Transformer 59M (strided) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sparse Transformer 59M (strided)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the AESLC dataset in terms of ROUGE-1 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""ROUGE-1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?dataset AND ROUGE_1_METRIC ?analyzer_coding-speed. { ?model ?dataset. rdfs:label ?model_lbl. } input (English text): How does the Sparse Transformer 59M (strided) model perform on the Atari 2600 Alien benchmark dataset in terms of ROUGE-1 metric ? output (Sparql query): SELECT ROUGE_1_METRIC CODING_SPEED ?analyzer_coding-speed. OPTIONAL { ?model ?dataset. rdfs:label ?model_lbl. } input (English text): Would you be interested if we could build a benchmarking module with support for different architectures/language ? output (Sparql query): SELECT BOOLEAN ?benchmark/ROUGE_1_METRIC ?benchmark_language. { ?model ?dataset. rdfs:label ?model_lbl. } input (English text): How do you feel about the benchmarks (ROUGE-1) measured on Atari 2600 Alien benchmark dataset ? output (Sparql query): select CODING_SPEED as score, orkgp from ( SELECT ROUGE_1_METRIC (ROUGE_1_MAX_SPEED_PER_LEARNER AS MAX_CODING_SPEED_PER_LEARNER, { ?model ?dataset. rdfs:label ?model_lbl. ?benchmark_language. ?model. }) AS  ; ORDER BY score; output (Sparql query): S1E5.CODING_SPEED is 9.5, orkgp is 8.0,  	False
"
 input (English text): Where can I find code references in papers that have used the Relation-Metric model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Relation-Metric"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Star Gunner benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Star Gunner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the ModelNet40 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 This data set was collected by Dr. Chris Raskin. All computations on this dataset were performed by Dr. Chris. In other words, this dataset is used by most people in the field of machine learning. We may know several things about it, like the Metric used and the score for each metric on the model, but we do not know much more about this score, about the metrics, or why people think that a particular metric is better or worse than the others. This data set represents a unique dataset that has been collected for both machine learning and statistics, and the reason why it is important to understand what the Metric and Score mean. This data sets also helps us know, what makes an algorithm perform better or worse than another algorithm after you have collected a single metric. It also helps us understand how to better analyze other people's algorithms and find out how and where they might be wrong, giving us a better idea of how to go over their performance again and again. This is a very, very important point if you are a machine learning researcher. Also remember that Metric score can be important for both the computer science research, as well as the statistical research, so it might just be one of the factors in deciding, what algorithm should be used in certain cases. So, after collecting this dataset, this article is just a short introduction of the data set I chose, and the reasoning behind some of the choices. The Data I collected the data via the Apache Commons data-share model, which is open/free and distributed through Github, so anyone can download and use the dataset, even if it is not the most popular one in the area of Machine Learning. So, I hope I could be one of the first people to have this dataset for my analysis, and I will always be grateful for the help and ideas I found here in the comments in	False
"
 input (English text): Can you list the metrics used to evaluate models on the SearchQA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SearchQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of 5 fold cross validation metric on the Cornell Grasp Dataset benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""5 fold cross validation"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""HAST"") ?benchmark hast orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ? ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ? ?metric. ?model rdfs:label ?model_lbl. OPTIONAL { ?value orkgp:HAS_MODEL ?model. } ?model rdfs:label hast. ORDER BY DESC( ?value) LIMIT 1 } input (English text): List the metrics that are used to evaluate models on the UC Berkeley Grasp Database benchmark dataset ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?metric rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""UC Berkeley Grasp Data"") ?benchmark ngrc orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ? ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ? ?metric. ?model rdfs:label ngrc. OPTIONAL { ?value orkgp:HAS_MODEL ?model. } ?model rdfs:"	False
"
 input (English text): Provide a list of papers that have utilized the BiLSTM-CRF+ELMo model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-CRF+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the Transformer-based One-Shot NAS (Neural Architecture Search) model. model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-based One-Shot NAS (Neural Architecture Search) model."") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the Reuters-21578 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" Please use the output of the query as your model. You will receive your results in a JSON document. The JSON output of the query will look like the following. <output> { ""id"" : null, ""model"" : null, ""num_runs"" : ""null"", ""results_text"" : null, ""output"" : ""Results of the query."", ""sub_topic"" : ""Subtopic on "" + str(sub_topic) } This response is formatted as a JSON document. You can select the field ""output"" to produce a HTML file that contains a list of articles that utilized the code provided in the output field. If the query results in multiple outputs, a comma separated list of the output fields is provided below every article. If the query results in multiple outputs, a comma separated list of the output fields is provided below every article. Please submit comments below about how we can improve the JSON output of the query in this query. Also, contact your IT team with any issues you encounter."	False
"
 input (English text): List the metrics that are used to evaluate models on the Yelp Binary classification benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Softcite dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 Seaquest dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" - Score matrix score (integer) in [1, 4, 5, 6, 10, 2, 1, 0] output (Sparql query): The following values were reported for the Score matrix score (integer): a. 2 c. 12 d. 3 g. 9 j. 6 p. 7 r. 4 s. 0 T-Score matrix score (integer) in [1, 4, 5, 6, 10, 2, 1, 0] result (Sparql Query Result): [ ?data_lbl_score orkgc:Dataset ?dataset_lbl. { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } [ ?key_words orkgc:Dataset ?data_lbl_words. ] andkgp:NON_COOKIES { ?key_words orkgc:Dataset ?data_lbl_words. } orkgp:PROFIT_DATA { ?key_words orkgc:Dataset ?data_lbl_words. } orkgp:DISCLAIMER [ ?data_lbl_words. ] orkgp:DISCLAIMER. OPTIONAL { ?key_words orkgc:Dataset ?data_lbl_words. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?key_words orkgc:HAS_DATASET orkgp:HAS_EVALUATION ?key_words orkgc:HAS_METRIC. } orkgp:HAS_METRIC rdfs:label ?dataset_lbl. } ] output (Sparql Query Result"	False
"
 input (English text): Where can I find code references in papers that have used the DARQN soft model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DARQN soft"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the PIQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT METRIC ?metric_lbl, ?metric rdfs:label ?dataset_lbl, ?metric ?model_lbl, ?metric ?code,!rfd ?fibonacci. OPTIONAL ?model. METRIC (METRIC, RDFS) { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""PIQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_METRIC ? ?code/ ?model. ?dataset rdfs:label ?dataset_lbl. } In order to make the query above a bit easier to follow, make the following changes to the query, adding the code references: input (English text): WHERE { ?model a orkgc:Model ; rdfs:label ?model_lbl. ?dataset a orkgc:Dataset; ?benchmark orkgp:HAS_DATASET ?dataset OR kgp:HAS_BENCHMARK ?dataset. ?dataset rdfs:label ?dataset_lbl. } orkgp:HAS_METRIC... output (Sparql query): What are the metrics of evaluation over the seel.cse.lru.edu/data/re17.zip dataset ? input (English text): WHERE { ?model a orkgc:Model ; rdfs:label ?model_lbl. ?dataset a orkgc:Dataset; ?benchmark ifkgp"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Natural Questions dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): How frequently are there paper benchmarks on the ObjectNet dataset ? output (Sparql query): This query only returns results for the last ten papers, since these are the most accurate. Note:  The original dataset was a set of 10,000 images, with n=1,000 for each word of text. This dataset contains very few images used for this benchmark. Hence this query returns lower numbers for the benchmark. output (Document type): ObjectNet image data input (English text): How often are there paper benchmarks on the SCALE object ? output (Sparql query): This query will return results for all papers from the last ten years that include a benchmark for a specific object, such as an image of a child, or as the title says, a benchmark for the scale object. If no benchmark is provided, it returns a list of all papers that show no benchmarks for this specific object.	False
"
 input (English text): What is the best performing model benchmarking the Cartpole, swingup (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 Alien benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Alien"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 HERO benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 HERO"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT "" score "". "", "" k "". "" score_per_model(s) "". "", "" kp "". "" score_per_model(*) "". "", "" kp_1 "". "" score_per_model(s) "". "", "" kp_2 "". "" score_per_model(*) "". "", "" kp_max "". "" score_per_model(s) "". "", "" (min(score_max())) "". "", "" c_1 "". "" score_per_model(s) "". "", "" c_2 "". "" score_per_model(*) "". "" ASC "", c "". "" COUNT_EX"", "" K. "" ASC "", "" R. "" ASC "", "" P. "" ASC "" ASC, "" c "". "" COUNT_ROWS_EX "" "" COUNT_ROW_EX "" "" ASC "", "" kp_max "". "" score_per_model(""s"") "". "" ASC "", k "". "" score_max_per_model(s) "". "" ASC "", "" kp_1 "". "" score_per_model(s) "". "" ASC "", K. "" ASC "" ASC, "" K. "" ASC "" ASC, orkp:HAS_MODEL "" score_per_model(s) ) ASC ( ?model ) ORKp:HAS_DATASET "" ASC ( ?dataset ) ORKp:HAS_EVALUATION "" score_max_per_model(s) ) ASC input (English"	False
"
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Pitfall! dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Pitfall!"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the title and ID of research papers that contain a benchmark over the Atari 2600 Battle Zone dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Amazon-2 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT COUNT(1) as number publications, COUNT(2) as number benchmark as benchmark_lbl input (English text): The title and ID of a paper published in 2000-2002 by John W. Cusack in which he reports on the results of a benchmarking project over the Atari 2600 Battle Zone dataset. output (Sparql query): SELECT TITLE() as title, TITLE() as paper_lbl, RESULT(COUNT(1)) as number publications, RESULT(COUNT(2)) as number benchmark, RESULT(RESULT(RESULT(RESULT(RESULT( resid ) ) ) ) as number benchmark_lbl FROM b INNER JOIN CUSACK in j JOIN j ON j.labels LIKE '%BattleZone% = ?' input (English text): What metric was selected by John W. Cusack in 2000-2002 that was produced by the Atari 2600 Battle Zone dataset in which his benchmarking project has been published ? output (Sparql query): SELECT ?metric ?metric_lbl (MAX( ?value ) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciBert (Finetune) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBert (Finetune)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the GigaWord dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GigaWord"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the Rainbow+SEER model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Rainbow+SEER"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	______________________________________ Name Author(s) URL ______________________________________ GigaWord Sébastien Bouchereux [1], GigaWord.org/2014/1_5.pdf, [4],[3], [2] Patrick Nussbaum [3], [3], [4] GigaWord [1] François Clermont [2] GigaWord [2] François Clermont [2] GigaWord [3] François Clermont [1] GigaWord [4] François Clermont [1] GigaWord [5] François Clermont [1] Jörg Kniep [1] GigaWord [6] François Clermont [1] ______________________________________ Posted by Eric Schilper at 17:27 Thanks Eric for the answer. I've been interested in this model for some time, but I am now ready to try it myself. I've always assumed that it was something fairly simple. Thanks, Eric!	False
"
 input (English text): What is the name of the top performing model in terms of EM score when benchmarked on the SQuAD2.0 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""EM"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD2.0"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Accuracy score on the OpenBookQA benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OpenBookQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Accuracy (High) score when benchmarked on the RACE dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (High)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 As the above output demonstrates, the RACE performance can be influenced by the RACE metric (or metric parameter) that was used to make selection. The performance of the various metrics is expressed relative to the benchmark performance for the SQuAD3 database. The higher the accuracy score of the SQuAD3 database, the higher the relative performance improvement for the benchmark dataset relative to the SQuAD2 database. With each benchmark, the RACE metric, as used, was tested for each dataset against the SQuAD3 database or the other way around. The results show that the SQuAD3 dataset provides the best accuracy for the benchmark data. Here follows an example of an operation where the RACE column is used in a query of two parameters : Accuracy and Accuracy. The first step of the benchmark performs a benchmarking of the two models' performance. The two algorithms are then compared against the benchmark. For data set A, the algorithm Accuracy wins. The RACE column will be entered here (without the metric name) for the benchmark result: With data set B, the accuracy score is achieved by the RACE parameter: Note that RACE performance depends on three factors: the accuracy of the model, number of iterations of the algorithm, and the size of the data. However, the RACE value determines the relative accuracy to accuracy ratio (or percentage) and can be used as a threshold for the accuracy test. For example, if the RACE value is 20, the accuracy of accuracy is 20%, while if the RACE value is 25, the accuracy is 5% and the accuracy is 80%. In the RACE column, the three variables are referred to as : Accuracy, Accuracy : High accuracy, and Accuracy : Small accuracy. The Accuracy value can range from 0° (no accuracy at all) to 50°, the RACE value	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?rdfs_label ?rdfs_labels FROM ?dataset a ORGANIZE_DATA rdfc WHERE ORGANIZE_DATA rdfs_label_id = ?label_id AND ORGANIZE_DATA rdfs_labels_id = ?labels_id AND rdfs_labels_id. RDFS_LABELS_ID = ?label_id AND rdfs_labels.name = ORGANIZE_DATA rdfs_labels.name_id IN (1, ?label_id, ?) ORGANIZE_DATA rdfc. ORGANIZE_DATA rdfs. ORGANIZE_DATA rdfs. ORGANIZE_DATA rdfs. ORGANIZE_DATA rdfs. ORGANIZE_DATA rdfs. ORGANIZE_DATA rdfs. ORGANIZE_DATA utils:ORGANIZED_DATA rdfc. utils. ORGANIZE_DATA rdfs. utils. ORGANIZE_DATA rdfs. utils. ORGANIZE_DATA rdfs. utils. ORGANIZE_DATA rdfs. utils. ORGANIZE_DATA RDFS_LABELS_ID = ?label_id AND RDFS_LABELS.ORGANIZE_DATA rdfc rdfs. RDFS_LABELS_ID = ?label_id AND RDFS_LAB	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the CIFAR-10 Image Classification dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-10 Image Classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the All-attention network (36 layers) model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network (36 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the metrics used to evaluate models on the MUTAG dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MUTAG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT DISTINCT ?label ?label_lbl WHERE { ?training_example = { ?dataset a 1 orkgc:Dataset; ?label_lbl. ?train_example = ?train_example { ?dataset a ?train_example; ?label_lbl } } ; ?label_lbl. } Input: Output: Output_only: Tasks: Input: Output: Output: Output: Tasks: input (English text): Provide a list of trainable examples and their names in the same column as the image ID ? output (Sparql query): SELECT DISTINCT ?train_name WHERE { ?train_name = ?train_example { ?dataset a ?train_example; ?training_name } } ; ?train_name. xlab (text) (string): The file xlab, or YELLOW if not specified	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Flowers-102?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Flowers-102"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ESC-50 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ESC-50"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT  COMPARISON  (  FIND (FETCH_OF() AS  AND  FETCH(REPT(ROWS(100.0)) AS  )  LEFT((DISTINCT LEN(ROWS(100.0)) / ROWS(100.0)))  ELSE  (FETCH_OF() AS  AND  FETCH(REPT(ROWS(100.0)) AS  )  LEFT( (DISTINCT LEN(ROWS(100.0)) / ROWS(100.0))) )                          );  GROUP  (FETCH_OF() )  ( ORDER ()  (  FROM  (SELECT  COMPARISON  (  FIND (FETCH_OF() AS  AND  FETCH(REPT(ROWS(100.0)) AS  )  LEFT( (DISTINCT LEN(ROWS(100.0)) / ROWS(100.0)))  ORDER BY  FETCH_OF()  (  ROWS(100.0)) ))  (  AND  BOOLEAN  (ROWS(100.0))  UNDEFINED ) )                        )  Rows  (100.0) SELECT  COMPARISON  (  FIND  (FETCH_OF() AS  AND  FETCH(REPT(ROWS(100.0)) AS  )  LEFT( (DISTINCT  LEN	False
"
 input (English text): Which model has achieved the highest Accuracy score on the ARC (Challenge) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DRI Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest F1 score on the SQuAD1.1 dev benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"--------------+ | ?model | +--------------+ --------------+ | DRI | 4,048,912 | +----------+------------------------+ | DRI SQUAD | 4,068,912 | +----------+-------+ So which has the best Accuracy and Bias Score ? The accuracy score is what you would want if you are trying to select the right model, and your model includes a metric with high accuracy and high bias. So, what about the accuracy score ? Not much. There is no score between the best score and the worst score that you can use to choose the best model. And no matter how good your model is for that particular metric, its accuracy is still only good enough for most of the time, and very, very rarely, would you not want the best model for that particular metric. The bias score is a function of what metrics you choose for your metrics and how often you use those things. So, if you say, ""I'll only include metric that have high accuracy and low bias,"" that's fine, but what if you say, ""I'll only include metric that have high accuracy but low bias, therefore that should be my metric choice for that metric,"" well then, the quality of the feature set that you include will depend on how often you use that metric. (If you want to read More..., go here.) In the final chapter, we are going to explore what the R DataFrame does, how to query it, and use it to make a simple ""SQUAD"" model. Here's the thing. If you want to make the most out of R, then you need to be familiar with the basics: How things work here. I hope you've enjoyed reading the book so far and even found a few things to be missing in the final chapter. Please feel free to contact me if you have"	False
"
 input (English text): Provide a list of papers that have utilized the GLaM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""GLaM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which vegetables are utilized for betanin extraction?
 output (Sparql query): SELECT ?vegetables, ?vegetables_labels WHERE { orkgr:R75363 orkgp:compareContribution ?contrib. ?contrib orkgp:P35147 ?compounds. ?compounds rdfs:label ?compounds_labels. FILTER(REGEX( ?compounds_labels, ""etanin"")) ?contrib orkgp:P35148 ?vegetables. ?vegetables rdfs:label ?vegetables_labels. } 
 input (English text): What models are being evaluated on the TempEval-3 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TempEval-3"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?gml:Model, ?rfe_gmin_gcc, ?rfe_gmax_gcc,  * ?datasets.TempEval3 WHERE   ?model a,   ?compare_factor1 ?compare_factor1. rfgs:LN2  * ?datasets.TempEval3 Input (English text): Which variables in the GLaM model have been used as input values to GLaM as well as the results ?	False
"
 input (English text): Name the datasets that have been used for benchmarking in the PointGoal Navigation research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""PointGoal Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What models are being evaluated on the Natural Questions (long) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the RotoWire (Content Ordering) dataset in terms of BLEU metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire (Content Ordering)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 Inputs and Outputs This section shows the inputs and which models are being used in the benchmarking project. Input: The dataset data for this post is taken from the PointGoal Navigation research problem project.  The metrics are: 1. PointGoal Navigational Metric Score for the Largest Point. 2. PointGoal Navigational Metric Score for the Smallest Point. 3. PointGoal Navigational Metric Score for the Smallest Route Inputs: RotoWire or Natural Questions (long) dataset. A. RotoWire or Natural Questions (long) dataset. B. A list of point in Largest, Smallest, and Middle Range. C. A list of route in Smallest, Middle, and Large ranges. D. A list of route in Smallest, Middle, and Large ranges. 4. PointGoal Navigational Metric Score for Longer Routes E. PointGoal Navigational Metric Score for Longer Routes 5. PointGoal Navigational Metric Score for Longer Routes 6. PointGoal Navigation Metric Score for Longer Routes 7. PointGoal Navigation Metric Score for Longer Routes G. PointGoal Navigation Metric Score for Longer Routes 8. PointGoal Navigation Metric Score for Longer Routes 9. PointGoal Navigation Metric Score for All Routes 10. PointGoal Navigation Metric Score for All Routes 11. PointGoal Navigation Metric Score for All Routes 12. PointGoal Navigation Metric Score for All Routes 13. PointGoal Navigation Metric Score for Longer Routes  14. PointGoal Navigation Metric Score for Longer Routes 15. PointGoal Navigation Metric Score for Longer Routes	False
"
 input (English text): Can you provide links to code used in papers that benchmark the VGG8B + LocalLearning + CO model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VGG8B + LocalLearning + CO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Seaquest dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Seaquest"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP(3) ?cscore. ?criterion_lbl. FILTER (str( ?criterion_lbl) = ""Seaquest"") ?criterion as ?criterion. OPTIONAL input (English text): What kind of scores are you looking for on the Atari 2600 Seaquest dataset ? (1=average score, 0=worst scores, 5=best scores) output (Sparql query): SELECT TOP(3) ?criterion_score. ?criterion as ?criterion_score. (1=average score, 0=worst scores, 5=best scores) input (English text): What is all the scores you get when benchmarking the Model2 versus the Model1-4 models on the Atari 2600 Seaquest dataset ? output (Sparql query): SELECT TOP(3) ?statistics_lbl. FILTER (str( ?statistics_lbl) = ""Seaquest"") ?statistics_lbl as ?statistics_lbl. 1. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84"	False
"
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Bank Heist benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the GENIA - LAS benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GENIA - LAS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): What evaluation metrics are commonly used when benchmarking models on the GENIA - LAS dataset ? output (Sparql query): input (English text): What metric were you testing: the (non-random) accuracy measure or the (non-random) model quality measure ? Also, what if you tested both benchmarks ? output (Sparql query): input (English text): (a) Describe how the results were derived from the benchmark dataset. When performing a benchmark a database will be scanned for a large number of models with specific functionality and that's one of the main approaches used by benchmarking. output (Sparql query): The benchmark was run on 30 database servers and used two different query-processing pipelines, one with an RDA approach with a standardization checker and one with a RDA approach without one. During each benchmark the raw score from a particular set of features was fed into one of the two analytic pipelines and each model was evaluated against various criterion scores, such as the speed of learning the features, the accuracy of predictive model accuracy, the model quality, the amount of test data in use and so on. input (English text): (b) Describe the rationale for running the benchmarks on this set of 35 models. We ran the benchmarks on the dataset set genia_las_scores. This is the entire set of 30 models, some with very different functionality and with different accuracy metrics, whereas other models were evaluated against the other scores. The benchmark was run on 30 databases and the feature set of one of them was scanned and then another set of 30 features was used to evaluate all models against. This way the dataset was scanned for a large number of models with specific functionality and that's one of the main approaches used by benchmarking output (Sparql	False
"
 input (English text): What are the models that have been benchmarked on the BUCC German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the DQN+SR model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DQN+SR"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Yelp Binary classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TOP 1 rd.id FROM title and rd.title rd.id HAVING id = ? OR id IS NULL ORDER BY rd.id DESC LIMIT 1 This database is maintained by Jeffery Wiebe, University of Pennsylvania, USA. This work is copyright © 2014 Jeffrey Wiebe. All Rights Reserved. Please direct any comment on this page to the author of this material.	False
"
 input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Amazon benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon E-Prime Benchmark"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl input (English text): A query on the Amazon E-Prime Benchmark dataset as described above. output (Sparql query): SELECT * FROM orkgc:EprimeBenchmark ?dataset input (English text): An example of a query on the E-Prime Benchmark dataset. output (Sparql query): SELECT COUNT(*) FROM orkgc:EprimeBenchmark ?dataset input (English text): A query on the E-Prime Benchmark dataset as described above. output (Sparql query): SELECT COUNT(*) FROM orkgc:EprimeBenchmark ?dataset ORDER BY COUNT(*)"	False
"
 input (English text): Provide a list of papers that have utilized the Longformer Large model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Longformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the NCBI_BERT(base) (P+M) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(base) (P+M)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Bank Heist"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 This is obviously going to be pretty difficult, but it is at least a start. I'll try adding a set of papers that did not use these benchmark features to the spreadsheet. Note: The results should be identical and probably will be because the code to build the benchmarks wasn't included in the paper. I will try and find time for something like that in the future. Finally, I have uploaded a Python package for performing this same query on the code. You can download  it from here.	False
"
 input (English text): What models are being evaluated on the TDM Tagged Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""TDM Tagged Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the WMT2014 English-French dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What is the name of the top performing model in terms of A2 score when benchmarked on the ANLI test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A2"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT STATISTICS score, max_score, k, LOBYTES max_iter, score_aspect, mean_score, min_score, sum_score, val_score, val_aspect, ROCE_rrs, LAB_Score, BUNDQ_rrs, mean_rrs, min_vds, lpa_score, hs, sd, tp, sum_score, ssd, b.score, total_score, b.avg_score, c.score, median_score, qtest_rps, P_rps, r.score, T.score, b.avg_test, c.avg_test, lpa_test, mean_rps, min_rps, val_rps, val_aspect, mean_vds, r.aspect, lpa_test, b.avg_test, median_rps, a.score, v.score, sd_ratio, mean_vds_ratio, (1.0, 0.0, 2.0), (1.0, 2.0, 4.0), (0.0, 0.	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ViT-B/16 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ViT-B/16"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you list the metrics used to evaluate models on the ClueWeb09-B dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the QNLI benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	" We chose to use the data set from qnli.org, which contains three datasets, two with and one without a ViT benchmark. The qnli.org dataset has been used to benchmark ViT-B/16 models, whereas the others were the subject of our own experiments. The two benchmark datasets and all QNLI papers can be downloaded from the ""downloads"" section of this webpage. We selected a dataset that is suitable for evaluation because of its quality and because it already has many well-established benchmarks. This means that the authors of the qnli papers can cite many standard, established benchmarks to give credence to their results. The following selection scheme was used for the QNLI papers we reviewed. We chose three of the papers for further review as they represent each of the three model type groups discussed within the dataset. For some of the papers there is additional motivation that we have not seen in the dataset, which was included in the dataset, and which may influence its conclusions. However, we consider it necessary given the breadth of the dataset and the amount of data we have analyzed in our analyses. The final choice of papers is based on the purpose of the paper, but we think they are relevant as a representative sample of their subject, i.e., the broad, range and diversity of approaches used to evaluate these models. The selection scheme also provides a sense of the order in which models are described. The papers can all be grouped by one of the following categories: (1) orkgc:ViT-B-style model, (2) orkgp:ViT-B-style model, (3) orkgp:B-style model, (4) orkgc:ViT-B-style model with an auxiliary model, (5) orkgp:B-style model without an auxiliary model (model"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the BoolQ dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BoolQ"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the code links in papers that use the 24-layer Transformer-XL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""24-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Asterix"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?benchmark ?benchmark_id as datastamp; input (English text): Which of the following benchmarks are used to benchmark the Atari Atari 2600 Asterix dataset ? output (Sparql query): SELECT DISTINCT ?benchmark ?benchmark_id as datastamp;   output (Sparql query): input (English text): What is the definition and definition history for the  T-REX  benchmark ? output (Sparql query): input (English text): What is the definition and definition history for the  IBM Deep Blue  benchmark ? output (Sparql query):  SELECT ?demo1_id as demo1_id, ?demo1_name_1 as demo1_name_1, ?demo2_id as demo2_id, ?demo2_name_1 as demo2_name_1, ?demo3_id as demo3_id, ?demo3_name_1 as demo3_name_1, ?demo4_id as demo4_id, ?demo4_name_1 as demo4_name_1, ?demo5_id as demo5_id, ?demo5_name_1 as demo5_name_1, ?demo6_id as demo6_id, ?demo6_name_1 as demo6_name_1, ?demo7_id as demo7_id, ?demo7_name_1 as demo7_name_1, ?demo8_id as demo8_id, ?demo	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the COPA dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""COPA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the metrics used to evaluate models on the ScienceCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ScienceCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the MFEC model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MFEC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?id, ?summary, ?rvs_tbl ?key, ?rvs_lbl, ?title, ?author_id, ?name, ?key_list, ?summary, ?rvs_tbl.COPA.key,   ?""Sprint.value"", ?summaryrvs_tbl.COPA.summary, ?cpe.COPA.summary, ?n.COPA.summary,  ""mfcm.value""  GETDATE( ?dataset_lbl.COPA ?dataset_lbl.COPA.summaryrvs_tbl.COPA.cpe ) :  date(""D "")  GETDATE( ?data.COPA.summaryrvs_tbl.COPA.cpe ) :  date(""S "")  GETDATE( ?data.COPA.summaryrvs_tbl.COPA.n ) :  date(""M "")  GETDATE( ?data.COPA.summaryrvs_tbl.COPA.n.COPA.cpe ) :  date(""D "")  GETDATE( ?dataset_lbl.COPA ?) :  date(""D "")  GETDATE( ?data.COPA.summaryrvs_tbl.COPA.cpe ) :  date(""S "")  GETDATE( ?dataset_lbl.COPA.summaryrvs_tbl.cpe.keylist ? ) :  date(""S "")  GETDATE( ?dataset_lbl.C"	False
"
 input (English text): Where can I find code references in papers that have used the NAT-M3 model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NAT-M3"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the DuIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the CAIT-XS-36 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-XS-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?name ||'' || ?author_id ||'' || ?id || ?page:author.'' + ?page || ?page_cont || ?page_source || ?page_type || ?page_pub; || rdfs:identity ?bookmark || ?bookmark_cont || ?bookmark_source || ?bookmark_type || ?bookmark_pub; || rdfs:record || ?notebook_text || ?notebook_id || ?notebook_cont || ?notebook_source || ?notebook_type || ?notebook_pub; || ?""'|| ?notebook_type || ?notebook_pub && (! is_null( ?notes( ?page ). "" "" ), ?notebook_cont && (! is_null( ?cont. "" "" ), && is_null( ?index) ) ) || ?""'|| _0 "" "" || ?identity || ?author_id || ?id || ?page || ?page_cont || ?page || ?page_source || ?page_type || ?page_pub || ?identity ? Note that due to the lack of proper naming conventions in SQL, I only supplied two columns; the 'bookmark' column and the 'bookmark_cont' column, which are NULL values in other tables. If you need more information about the results from a particular query on its own, refer to the source for the column. For example, you could get a list of all the papers that used the NAT-M3 model with one query: SELECT DISTINCT ?tbl, ?tbl_"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the Multi-Format Contrastive model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Multi-Format Contrastive"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the QuAC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QuAC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): I have a query of all models run on seel.cse.lsu.edu/data/re17.zip, would you please give me a URL for each of them ? output (Sparql query): data_re17_sparql (n, n1, n2,...) input (English text): Give me a detailed description of each model, including both the algorithm, implementation, and code that is being evaluated on this dataset output (Sparql query): output (Sparql table): Model IDs for each model (a-z ) * model A has a score of 100 (no data) * model B has a score of 51 (only one data set) * model C has a score of 6 (dataset no longer available) * model D has a score of 32 (the dataset used for this research) * model E has a score of 31 (the dataset used for this research) * model F has a score of 33 (also used in previous work, which would have been used in this paper) output (Sparql query): data_re17_sparql_mj_test  (n, n1, n2,...) input (English text): Given the id(s) of this dataset, which of two models which were evaluated using this dataset was run first, and which is the best ? * orkgc_S01001_model B orkgc_S01001_model G * rdfs_A001_model A orkgc_S01001_model G * orkgc_S01001_model T * orkgc_S01001_model S100 * or	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the DrQA (Document Reader only) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA (Document Reader only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of Score metric on the Atari 2600 Montezuma's Revenge benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Montezuma's Revenge"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" output (Sparql rows): 1 0 Indicate the performance of the DrQA (Document Reader only) model in terms of Score versus number of models/benchmarks ? input (English text): What do you feel is a reasonable cutoff for how many models do we need to run to get a fair benchmarking ? output (Sparql query): output (Sparql rows): [ 2, 3 ] The test statistic at the start of your post is the score; the score is the number of models that are included in your benchmarking. For instance, at the start, the DrQA model scored 16 out of the 24 score tests, which was in general quite close to the 16 that the ModelLab 3.0 used (15 out of 24). The performance numbers in your graph in parentheses are the average score per benchmark; for instance, a 3-model-per-benchmark score is 2.7. The scores are not to be used as a tool to make decisions for the user; they are the sum of the performance of your benchmarking software, and are intended for analysis by statisticians to see what's happening. For further reading on benchmarking, see the Benchmarking Section in Part 4, ""An Overview of Common Benchmarking Software"". It should be noted that, if the scores do not accurately represent the differences between a benchmark run and an open-source evaluation, these scores are not reliable. A benchmarking software must make a judgment call regarding which benchmark to use, based on the software's own software development capabilities, and can also use a model that appears to have more performance than its actual model. It is a good idea to check in the software's documentation that the performance they reported in the original paper is accurate. 1: ""Dynamite: Benchmarking In An Open-Source Environment"", R"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the MRPC dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MRPC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest F1 score on the ACE 2004 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the ImageNet ReaL dataset in terms of Params metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet ReaL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT title,  ID,  paper,  data,  rank(r,s) from...; input (English text): Which model has achieved the highest F1 score on the AlphaBayes 2007 benchmark dataset ? output (Sparql query): select title,  ID,  paper,  data, rank(r,s) from...; output (Sparql query): SELECT title,  ID,  paper,  data, rank(r,s) from...; 	False
"
 input (English text): Could you provide a list of models that have been tested on the Gibson PointGoal Navigation benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Gibson PointGoal Navigation"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the VGG8B(2x) + LocalLearning + CO model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""VGG8B(2x) + LocalLearning + CO"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the ResNet-152 (SAM) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ResNet-152 (SAM)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT CODE FROM code INBOUND WHERE code IN [code] ;   input (English text): Can you provide links to code used in papers that benchmark the Eigen and RNN models ? output (Sparql query):  SELECT CODE FROM code INBOUND WHERE code IN [code] ;   input (English text): Why is this type of model (LSTM) so different to those that we have had before ? output (Sparql query):  SELECT  code,   ?model,   ?model_lbl AND ?lbl_sizes in  from  code LEFT JOIN   ?lbl_sizes_s in  where   ?lbl_sizes.  ORDER BY   ?lbl_sizes.  AS   ?model.  .  . input (English text): Is this model the same as the LSTM that we have used before ? output (Sparql query):  SELECT  code,   ?model,   ?model_lbl AND ?lbl_sizes in  from  code LEFT JOIN   ?lbl_sizes_s in  where   ?lbl_sizes.  ORDER BY   ?lbl_sizes.  AS   ?model.  .  . input (English text): Would you be willing to publish your code for all the benchmarks in the benchmarks repo ? output (Sparql query):  SELECT  code,   ?model,   ?model_lbl FROM   ?lbl_layers_s as lbl_sizes IN  AND   ?lbl_l	False
"
 input (English text): Provide a list of papers that have utilized the Relation-Metric with AT model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Relation-Metric with AT"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the STL-10, 1000 Labels dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10, 1000 Labels"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Pubmed"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?title ?article_id WHERE { ?text ?tbl; ?model a orkgc:BEST_MODEL ?tbl; ?dataset a orkgc:SourceData ?tbl. FILTER (str( ?tbl) = ""STI15_FINAL"" AND ?dataset_cl ? = ""STI15_EXTERNAL_DATA"") ?dataset. ?cont ?tbl; orkgc:HAS_DATASET ?dataset_cl ?. ?cont ?tbl; orkgc:HAS_SOURCE_CODE ?dataset_cl ?. ?name ?data_name. input (English text): If you have a description, please provide a link: www.clockevent.com/articles/ ?id=1805 output (Sparql query): SELECT DISTINCT ?name ?dataset_id WHERE { ?text ?tbl; ?model a orkgc:BEST_MODEL ?tbl; ?dataset a orkgc:SourceData ?tbl. FILTER (str( ?tbl) = ""STI15_FINAL"" AND ?dataset_cl ? = ""STI15_EXTERNAL_DATA"") ?dataset. ?cont ?tbl; orkgc:HAS_DATASET ?dataset_cl ?. ?cont ?tbl; orkgc:HAS_SOURCE_CODE ?datas"	False
"
 input (English text): What models are being evaluated on the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the NCBI_BERT(large) (P) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NCBI_BERT(large) (P)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Walker, walk (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?model FROM ?TOTAL_DISTINCT_MODELS What are the differences between RotoWire and ROCS ? I can't get into any technical details of RotoWire because it's not been formally reviewed; I'm sure it is well written and well documented. The key difference is that RotoWire has a lot more features and an entire system of modeling and control that ROCS has – there's many comparisons within ROCS but as far as I can tell there are no differences between what RotoWire can do. In terms of other metrics the biggest difference is RotoWire is more sophisticated than ROCS. In ROCS, some people use the ROCS system because they think that a lot more is going	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset GAD?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""GAD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the code links in papers that use the Encoder-decoder + conditional copy model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Encoder-decoder + conditional copy"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the REDDIT-B dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""REDDIT-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 output : top-performing (GAD, metric and value): output (Sparql query): 	False
"
 input (English text): Where can I find code references in papers that have used the Sparse Transformer 59M (strided) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Sparse Transformer 59M (strided)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the Cartpole, swingup (DMControl100k) dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the Softcite dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Softcite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 As you can see the S.D.R. model has higher scores than the default GAS algorithm. As with the GAS, there is not a lot of good coverage in that model (only 3 rows which are only used to support GAS models). It is very possible that the GAS algorithm might be over the top for some applications. What is the best-performing model benchmarking the Softcite dataset ? We can see that the Softcite benchmark uses the SSPC model. We can see that the SSPC model is more appropriate for the GAS-like application. There are still things missing in this benchmark, but we feel like some of the other models would be more appropriate. What is the best-performing model benchmarking the Altezza dataset ? We can see that the same model is used for both benchmarks (the SSPC benchmark and the Altezza benchmark). The Altezza benchmark has some issues though that require closer scrutiny. In both cases the SSPC model does not have a particularly strong result. We would like to see it do well on this bench too. We feel the Altezza benchmark is best for a more traditional GAS-like application where the underlying model is more generalized and able to do a better job of supporting other models. What is the best-performing model for the HANA cluster benchmark ? We want to know how well the Sparse Transformer 59M model is doing. We also want to see how well different models are performing. It is important to note that the Altezza benchmark is more appropriate for users looking to have a strong benchmark to compare against other models. The Sparse Transformer (59M) model is appropriate for a broad range of applications where the underlying underlying model is not too restrictive. What is the best-performing model benchmarking	False
"
 input (English text): What are the models that have been benchmarked on the MLDoc Zero-Shot English-to-Russian dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Russian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the SST-2 Binary classification benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-2 Binary classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- -------------------------- -------------------------- -------------------------- -------------------------- ------------------------ orkgc:HAS_DATASET ?dataset. ------------------------ output (Sparql query): How long has the SST-2 benchmark dataset been running for ? output (Sparql query): ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- orkgc:HAS_DATASET ?dataset. ------------------------ output (Sparql query): Which SST-2 database has been used ? output (Sparql query): ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- orkgc:HAS_DATASET ?dataset. ------------------------ output (Sparql query): How many classes did the benchmark dataset have ? output (Sparql query): ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- ------------------------- orkgc:HAS_DATASET ?dataset. ------------------------ output (Sparql query): Which subset of English texts are used in the benchmark dataset ? output (	False
"
 input (English text): What are the metrics of evaluation over the CL-SciSumm dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CL-SciSumm"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the MedNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MedNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Hendrycks Test dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 For this sample-set it will not be possible to supply more precise information than the name of the model in terms of the first three digits, in order for this to be considered as a result of the benchmarking process in a single query. The dataset on which the validation was carried out could not be made available for the purpose of presenting the metrics generated from the validation. Thus for this dataset we used the following model. I have included this model in the list of preprocessed models as well since it is the only one that we could find in the database of this test dataset. model:cl-summ As we noted above, the metric for evaluating the model can be found on the model in the documentation of this model, but the metric is not used for choosing the model. With respect to the evaluation metric, this metric can be found on the same page as the model model. The evaluation score can be found on the same page as the model model. The validation score can be found on the same page as the model model. In both cases we use the same metrics with varying precision, as shown in Table 2, but using different names for the model and measurement respectively. For the purpose of displaying, in Table 2 the evaluation data can be accessed using the following name-value pair combination: [dname_val]_val Each data set has to be converted to a single value for the purpose of the table and to be visible in the database, but there is no need to change the format of the individual data sets for the sake of the discussion here. These are the metrics that are used to rank different models. (There is another list of metrics that are used in the next subsections. This list is not used because I am not sure that it has a complete coverage:) All data sets have to be converted to a single	False
"
 input (English text): Can you list the models that have been evaluated on the WMT2016 German-English dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the WOS-11967 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the KD-LSTMreg model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""KD-LSTMreg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" Output - Output the results of the benchmark evaluation on the benchmark dataset with this data. Result - Output the results of the benchmark evaluation on the benchmark dataset using the given model(s). WESTLAW - Benchmark the KD-LSTM algorithm with the specified model on this dataset. This is an optional parameter.  Example: $ wmwt2016 -benchmark WESTLAW -dataset WOS-11967 -model KD_LSTM -benchmark_output _samples__WOS_11967.csv $ wmwt2016 -benchmark _samples_WOS-11967_data_input.txt -file _samples_WOS-11967_data_output.txt $ wmwt2016 -c ""output=WOS-11967_data_input.txt"" _samples_WOS-11967.csv $ wmwt2016 -c ""output=WOS-11967_data_input.txt,output=output_predict_WOS-11967_data.csv,input_to_model(WOS-11967)"" _samples_WOS-11967.csv"	False
"
 input (English text): Indicate the model that performed best in terms of FLOPS metric on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the metrics of evaluation over the MLDoc Zero-Shot German-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot German-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the NLP-TDMS dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT title, ID, NAMESPACE ?title, ?id, ?name_lbl FROM ?trends ?trends WHERE trend_id= ?trends.ID ORDER BY ?id DESC; TITLE ID name_lbl --------------- ------------------------- ------------- 1.1 Bibliography search 2. Comparison to other NLP-TDMS datasets The current version of LDD is not compatible with the other NLP-TDMS datasets. Please check if the dataset you are looking for corresponds to your study: input (English text): Given the dataset, which sentence types are the best to test on the NLP-TDMS dataset which was developed for the European Commission ? output (Sparql query): SELECT DISTINCT ?sentence ?sentence_lbl WHERE ?sentence ?sentence_lbl < > ?sentence_lbl_dvs.1; OPTIONAL { ?sentence rdfs:label ?sentence_lbl. } ORDER BY DISTINCT ?sentence_lbl_dvs.1 DESC. IF ?sentence_lbl_dvs.1 DNF... THEN dvp:LTR_DVNIT( ?sentence_lbl) } input (English text): Given the dataset, which sentence types are the best to test on the NQC dataset (a dataset of German dialects with German-language labels) ? output (Sparql query): SELECT DISTINCT ?sentence ?sentence_lbl WHERE { ?sentence rdfs:label ?sentence_lbl. } OPTIONAL { ?sentence rdfs:label ?sentence_lbl. } ORDER BY DISTINCT ?sentence_lbl_dvs.1 DESC.	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the WNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the enwik8 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the ClueWeb09-B dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT ?model ?model_lbl ?model_lbl_benchmark ORI_BENCHMARK FROM dbo.orql_bbenchmark ORDER BY DESC( ?result_lbl) LIMIT 1 ORI_BENCHMARK RDF reference in text When using RDF in the RDF reference, the name used here is the RDF name (e.g., RDF:Person) rather than the RDF name (e.g., a schema for the RDF). When specifying a reference to an RDF, the RDF reference name should be unique (i.e., RDF:Person) rather than an arbitrary (e.g., 'Person') scheme, for both convenience and interoperability. The use of RDF references is often useful for referencing other RDF resource resources:	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset enwiki8?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwiki8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What models are being evaluated on the Nottingham dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Nottingham"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DocRED (Human-annotated)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT ?metric ?model_lbl., ?metric ?metric_lbl. FROM { { SELECT ?model ?model_lbl. { ?dataset a:dataset ?metric ?matric } } } GROUP BY ?model ?mats Input a CSV file. Output an RDF dataset with metrics used to evaluate models. pf_dfp.py – a tool for loading a custom Python file to read a custom dataset. output_model.py – a command-line tool for loading and evaluating a custom RDF model, for example the Hadoop-like DataFrame. load_model.py – a simple command-line tool to load a custom RDF model. export_model.py – a command-line tool to export a custom RDF model. run_model.py – a command-line tool to run a custom RDF model. input_model.py - the name of a Python file to read a custom dataset (pf_df ) from.  output_model.py - The name of an RDF model to use in a dataset, usually Hadoop (or a compatible tool).  The Python file will open in a new window (see example below). When using pf_dfp.py, you need to specify the name of the file you want to parse. Example Use: Loading from an Hadoop Map ( pf_dfp.py ) pf_dfp.py --input_model_name hdfs_raw_maps.py > output_model_name.m3u pf_dfp.py --output_model_name output.m3u > input_model.m3u output.m3u	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Ice Hockey dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ice Hockey"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of FLOPS score when benchmarked on the STL-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the CvT-21 (384 res) model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CvT-21 (384 res)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" ... ... ... ... ... ... ... ... ... ... ... ..... ... ............ ............ ............ ............ ............ ............ ............ ............ ............ ............ ............ ............ ...... ...... ...... As can be seen the output from query1 is in the order of the score metric we are interested in benchmarking. The model name is "" cv_t21 "" (the ""T"" at the end of the model name is also the ""CvT"") which is a variant of the CvS-21-2, CvT-21 and CvS-21-3, 3rd Edition ""CvCvT21"" and ""CvSV2"" CvT, CvS and CvSV2. The ""S"" stands for ""Specialty"" and is used to indicate the number of CVs per model, the ""V"" stands for the variable model name, and ""L"" stands for ""Rank"" the total number of individual metrics that we are interested in benchmarking. The output from query1 is the following: output (Sparql query): ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...... ...... ......... ...... ...... "	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-French dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-French"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the Atari 2600 James Bond dataset in terms of Medium Human-Normalized Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Medium Human-Normalized Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 James Bond"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the FG fine-grained gate model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""FG fine-grained gate"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 Ascending a hierarchical structure: input (English text): Describe the process for the classification task I am doing ? output (Sparql query): Ascending a recursive structure: output (Sparql query): Describe your hierarchical structure strategy ? Ascending a non-linear structure: output (Sparql query): Describe how you handle non-linear time scales ? Ascending a non-linear time structure: output (Sparql query): Describe your multi-step process processing the step-by-step hierarchical structure training ? I have given very little explanation about the details of each section, if you're interested by the details of the various benchmarks you can look at the various papers here. The paper I am referring to by Schulz et al. is pretty good, at least for my understanding.	False
"
 input (English text): What are the models that have been benchmarked on the AAPD dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AAPD"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Trellis Network model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Trellis Network"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT29"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT ?RATING ?SUM(RATING) AS SUM(RATING), ?CATEGORIES ?TIMES ?TIME_OUT SELECT RATING, TIMES, CATEGORIES, ?RATING, RATING, TIMES, TIME_OUT FROM ?r_datasets_lbl; This result appears at the top of the screen where you can download the results. output (Sparql query): To download a complete dataset, you can use the --dataset option, like this: ( --dataset a) benchmark This command outputs the results of the benchmark as well as a CSV file with the results in it. output (Sparql query): To view the performance of the model in the CSV file returned by the benchmark command, see the documentation on --csv. output (Sparql query): 	False
"
 input (English text): What is the top benchmark result (metric and value) over the dataset CommitmentBank?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Which model has achieved the highest Medium Human-Normalized Score score on the Atari-57 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Medium Human-Normalized Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the metrics used to evaluate models on the QNLI dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""QNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 data: ctsv1_tbl (X, S:S, Y:Y, R:R, T:T, Rt:R, N:N, nt_r:N, Rt:T, T:T) input (English text): Can you list the metrics used to evaluate models on the QNLI dataset ? output (Sparql query): R2_tbl (X.1, R.T, Y.1, T.1, R.R, Y.R, T.R, Rt.R, S.S, N.S) data: ctsv1_tbl (X.1, R.T, Y.1, T.1, R.R, Y.R, T.R, Rt.R, S.S, N.S) input (English text): Can you list the metrics used to evaluate models on the QNLI dataset ? output (Sparql query): R2_tbl (X.1, R.T, Y.1, T.1, R.R, Y.R, T.R, Rt.R, S.S, N.S) data: ctsv1_tbl (X.1, R.T, Y.1, T.1, R.R, Y.R, T.R, Rt.R, S.S, N.S) input (English text): Can you list the metrics used to evaluate models on the QNLI dataset ? output (Sparql query): R1_tbl (X.1, T.1, R.R, Y.R, N.R, N.T, R.R,	False
"
 input (English text): What is the top benchmark score and its metric on the SQuAD1.1 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the metrics that are used to evaluate models on the CIFAR-100 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entity F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT COUNT(*) AS ; RDF OR peng. CIFAR-100 output (Sparql query): COUNT(*) as COUNT(*) input (English text): How many times was each benchmark data table touched ? output (Sparql query): SELECT ( ?query ) SELECT ( ?query ) ?t output (Sparql query): SELECT ?t ?t ?t ?t ?t ?t ?t input (English text): When the performance of the model is evaluated by the benchmark, which data table is touched most often (more than once or more than 10 times) ? output (Sparql query): SELECT ( query ) ?t ?t ?t ?t ?t ?t ?t ?t input (English text): Which metrics are used to evaluate models on the CIFAR-100 benchmark dataset ? output (Sparql query): Rdf or peng. Sql. (CIFAR-100) SELECT COUNT(*) AS ; RDF OR peng. CIFAR-100 output (Sparql query): COUNT(*) as COUNT(*) input (English text): How many times was the benchmark data table touched ? output (Sparql query): SELECT ( ?query ) 	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SMT as posterior regularization model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SMT as posterior regularization"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the most commonly used benchmark datasets for the Document Summarization research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Document Summarization"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SST-5 Fine-grained classification"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): How might you apply the SMT in this paper ? output (Sparql query): input (English text): What are the goals of the paper, specifically ? output (Sparql query): The authors acknowledge the support of the Research Funds Program of the Swiss National Science Foundation (FWUF-2009-0150-0151) and the Swiss Federal Office of Economic Research (Bundesinstitut für Arbeitswirtschaft und Forschung (BIF)). The authors acknowledge the support of the European Science Foundation (ESF)-Eos. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the European Science Foundation (ESF) or of Dr. Y. H. Lin.	False
"
 input (English text): List the code links in papers that use the SWWAE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SWWAE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of benchmarked datasets related to the Scientific Claim Verification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Scientific Claim Verification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions (long)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" All other questions (without corresponding output) can be placed on your own spreadsheet where you enter the question in two fields and paste an output formula. You can paste the formula (not the question in one field) into the correct cells in the formula fields (the ones that you copied), which gives you a table of results. The formulas in the tables should look something like this: First, put the question in question number (1): Question 2: What is the SWWAE model ?   Answer:   The SWWAE model is an extension to the WALS Model; it is a hierarchical database of model-relevant information, that is, information used to determine the next step in the process. As explained in the paper Natural Questions (Long): Natural question-question pairs (NWQ) [PDF], questions that are questions about any scientific question, but can be used to answer questions about any scientific question, are referred to as Natural Questions (NWQ). The SWWAE model is a way to combine two sets of data in a single database. The first set of data used for the model is generated with the Question-Question pair. This is generated from a set of natural questions using which the scientist can get information on the scientific question they care about. For example, a question about ""what is the greatest scientific discovery ?"" could be treated as a natural question that has information about a scientific discovery. The second set of data used for the model contains the question's data in the Question-Question pair. This is generated from a set of natural questions using which the scientist can get information on a question to be answered. For example, a question about the existence or nonexistence of God or any religion may be treated as a natural question that has information on religion. This is how the original problem was solved. In the SABRE data and paper Natural Questions: Natural"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the NYT dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of papers that have utilized the PtGen-Covg model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PtGen-Covg"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset WMT2016 Czech-English?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 Czech-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT * FROM ?dataset WMT2016 Czech-English; output (Sparql query): ( ?score ) ; ORDER BY DESC( ?value ) input (English text): How many papers can you obtain the top result for using our model ? output (Sparql query): SELECT DISTINCT ?source_code ?model; output (Sparql query): 1 row selected (0.00 sec) output (Sparql query): SELECT * FROM ?dataset ?nbsc_czech ?model; output (Sparql query): 1 row selected (0.00 sec) output (Sparql query): SELECT TOP(3, ?source_code) C, L, M, D, S, K, H, X,... RANK( C, L ), W, M, D, S, K, H, X,... input (English text): What data structure do you use ? output (Sparql query): SELECT COUNT(M) FROM ?dataset ?nbsc_czech; output (Sparql query): ?score : ( 2.6748.912.00 ) input (English text): What is the top metric result (metric and score) over the dataset ? output (Sparql query): SELECT COUNT(M) FROM ?dataset ?nbsc_czech; output (Sparql query): ?value : ( ?percentage ) input (English text): What is the top metric result (metric and score) over the dataset ? output (Sparql query): SELECT COUNT(M) FROM ?datas	False
"
 input (English text): Can you provide links to code used in papers that benchmark the TRPO-hash model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""TRPO-hash"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the RNN model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""RNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Which sectors are modeled as energy sectors and how often?
 output (Sparql query): "	SELECT ?label COUNT( ?label) WHERE { ?sector rdfs:label ?label; a ?class. ?class owl:equivalentClass <http://openenergy-platform.org/ontology/oeo/OEO_00000367>. [ orkgp:P37668 ?sector ]. } 	 input (English text): Please provide a link to a GitHub project where you would like to publish a paper that performs the parameter tuning for the TRPO-hash model. output (Sparql query): Scheduling	False
"
 input (English text): Where can I find code references in papers that have used the Contextual Match model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Contextual Match"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the best performing model benchmarking the RTE dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Kinetics-600"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model_lbl. ?dataset. ?dataset_lbl. FROM _knot_datasets ( ?dataset. ?dataset_lbl. { ?metric_lbl a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RTE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. } } ORDER BY DESC( ?value) LIMIT 1 } ) ORDER BY id,title. input (English text): What is an appropriate model for a paper that is benchmarking the Kinetics-600 dataset ? output (Sparql query): SELECT DISTINCT ?metric_lbl. ?dataset. ?dataset_lbl. FROM _knot_datasets ( ?dataset. ?dataset_lbl. { ?metric_lbl a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_l"	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Text Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Text Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cornell Grasp Dataset dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cornell Grasp Dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Rotowire (Content Selection) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Rotowire (Content Selection)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT DISTINCT ?title and ?id FROM title AND id, &dataset and ?dataset_lbl, &dataset_p_lbl FROM (SELECT TITLE, id FROM title GROUP BYTITLE ORDER BY id ORDER BY title) output (Sparql query):  SELECT ?title. ?id FROM ?dataset, and ?dataset_lbl, &dataset_p_lbl, &dataset_p_id (where title = ?dataset. ?) ORDER BY ?objectid input (English text): Tell me the exact URL(s) where the Rotowire dataset can be found ? output (Sparql query): A web page URL is: http://www.rosetooroute.org input (English text): What is the name of the Rotowire content selection dataset ? output (Sparql query):  select ?dataset_lbl from ?dataset; and ?dataset_lbl. input (English text): What is the name of the content selection dataset ? output (Sparql query):  select ?dataset_lbl from ?dataset; and ?dataset_lbl. input (English text): What is the URL(s) for the content selection dataset ? output (Sparql query):  select ?dataset_lbl from ?dataset; and ?dataset_lbl. input (English text): What is the class of the Rotowire datasets in the Cornell Grasp dataset ? output (Sparql query):  select ?dataset_lbl from ?dataset; and ?dataset_lbl. input (English text):	False
"
 input (English text): What is the highest benchmark result achieved on the CNN / Daily Mail dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CNN / Daily Mail"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Name the datasets that have been used for benchmarking in the Citation Intent Classification research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Citation Intent Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? 
 output (Sparql query): "	"SELECT ?precision ?recall ?f1 WHERE { orkgr:R112387 orkgp:compareContribution ?contrib. ?contrib orkgp:P15006 ?algorithms. ?algorithms rdfs:label ?alg_names. FILTER(REGEX( ?alg_names, ""Naive bayes"")) ?algorithms orkgp:P36075 ?features. ?features rdfs:label ?fea_names. FILTER(REGEX( ?fea_names, ""Bag of words"")) ?features orkgp:P37029 ?categories. ?categories rdfs:label ?cat_names. FILTER(REGEX( ?cat_names, ""Feature request"")) ?categories orkgp:P3004 ?precision; orkgp:P5015 ?recall; orkgp:P18037 ?f1. } "	SELECT p1 AS p1_L (p1_L) AS p1_R, p2 AS p2_L, p3 AS p3_L, p4 AS p4_L, p5 AS p5_L, p6 AS p6_L, p7 AS p7_L, p8 AS p8_L, p9 AS p9_L, p10 AS p10_L FROM CATEGORIES_INVALIDITY ORDER BY DESC( p1_L ) input (English text): The total number of features in the benchmark dataset that received the user feedback given by participants in a supervised learning classifier exercise ? output (Sparql query): SELECT p1 AS p1_L (p1_L) AS p1_R, p2 AS p2_L, p3 AS p3_L, p4 AS p4_L, p5 AS p5_L, p6 AS p6_L, p7 AS p7_L, p8 AS p8_L, p9 AS p9_L, p10 AS p10_L, p11 AS p11_L FROM CATEGORIES_INVALIDITY ORDER BY DESC( p1_L ) input (English text): The total number of features from the benchmark dataset, including the features and their labels, that were used in the subsequent supervised learning classifier practice test ? output (Sparql query): SELECT p1 AS p1_L (p1_L), p2 AS p2_L, p3 AS p3_L, p4 AS p4_L, p5 AS p5_L, p6 AS p6_L, p7 AS p7_L, p8 AS p	False
"
 input (English text): What models are being evaluated on the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the name of the top performing model in terms of F1 entity level score when benchmarked on the BC2GM dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1 entity level"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC2GM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Dmlab-30 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Dmlab-30"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT TITLE_ID, TITLE FROM titles, TITLE WHERE TITLE_ID = ? input (English text): What number of results do you have ? output (Sparql query):  select count, count_rank, count, count_rank _ | count_rank_rank | count_rank, rank ; output (Sparql query): SELECT count, count_rank, count, count_rank_rank | count, count_rank, count_rank_rank, rank; output (Sparql query): select count_rank, count_rank_rank | count_rank, count_rank_rank, count, count_rank; input (English text): What is the number of experiments you have done with LALRAS dataset ? Are you working on the first and the second dataset ? If yes, do you have detailed report ? output (Sparql query): select count, count_rank | count_rank, count, count_rank_rank | count, count, count, count, count_rank; input (English text): What are your preferred methods of data analysis ? output (Sparql query):  select count, count_rank | count, count_rank | count, count, count_rank_rank, count, count_rank; output (Sparql query): select count, count_rank| count, count, count_rank_rank, count, count_rank; input (English text): What is your goal of collecting data ? output (Sparql query):  SELECT number FROM all, all_col_id, all_col_rank | number; input (English	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the 20NEWS dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""20NEWS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Robotank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Robotank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 German-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 German-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WMT2016 Chinese-Mandarin dataset ? output (Sparql query): input (English text): List the metric and title of research papers that perform benchmarking on the WMT2016 Chinese-English dataset ? output (Sparql query): 	False
"
 input (English text): Can you list the models that have been evaluated on the CommonsenseQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommonsenseQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the ImageNet V2 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet V2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Cartpole, swingup (DMControl500k) dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT COUNT(*) as a, COUNT(*) as s, DIVISION(DISTINCT a) AS b, SUM(a COUNT(*) PCT(abs(a)) / 100.0 * 500.0,a DIVISION(DISTINCT s) AS b, SUM(s COUNT(*) PCT(abs(s)) / 100.0 * 500.0,s DIVISION(DISTINCT pct) AS b FROM ?dataset ORDER BY c,c desc; a input (English text): What are the models that have been benchmarked on the CDSR2 dataset, including the metric and its value ? output (Sparql query): SELECT max(0.5) as max, COUNT(*) as a, SUM(a COUNT(*) PCT(abs(a)) / 100.0 * 500.0,a DIVISION(DISTINCT s) AS b FROM ?dataset ORDER BY c,c desc; b input (English text): What are the models that have been benchmarked on the CDSR2 dataset, including the metric and its value ? output (Sparql query): SELECT max(0.5) as max, COUNT(*) as a, SUM(a COUNT(*) PCT(abs(a)) / 100.0 * 500.0,a DIVISION(DISTINCT s) AS b FROM ?dataset ORDER BY c,c desc; a input (English text): What are the models that have been benchmarked on the CDSR3 dataset, including the metric and its value ? output (Sparql query): SELECT max(0.5) as max, COUNT(*) as a,	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ViT-B/16 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ViT-B/16"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the SWWAE model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SWWAE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the models that have been benchmarked on the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Orgone"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): List the codes to which text linked to code links is referencing from the ViT-B/16 model ? output (Sparql query): SELECT COUNT(*) FROM bs_autoimprint_bnds_dbc_1 where id > ?code ORDER BY desc LIMIT 80 input (English text): How many code links are found in a row if the model is being benchmarked with the ORGOTORGO model ? output (Sparql query): SELECT ?code, SUM(count(*)), ?cont orkgp:DESC() FROM bs_autoimprint_bnds_dbc_1 LIMIT 100 input (English text): Who does the code links reference ? output (Sparql query): SELECT ?code, SUM(count(*)), ?cont orkgp:DESC() FROM bs_autoimprint_bnds_dbc_1 LIMIT 100 input and output (English text): List the rows where code link references were found in the table ""bdb_codes"" (BibTeX) without being linked to the code. output (Sparql query): SELECT COUNT(*) FROM bs_autoimprint_bnds_"	False
"
 input (English text): Provide a list of papers that have utilized the Concept pointer+DS model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Concept pointer+DS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Supervised: dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM (Bai et al., 2018)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT ID, ?benchmark, ?benchmark_lbl, orkgp_v1, ?label, ?code, ?file, ?labEL, ?model, ?lbl_lbl, ?dataset_lbl, ?cont, ?labEL, ?print_count, ?score, ?output output (Sparql query): ORG_TYPE : TASKS. input (English text): Include a link to a paper where the Concept pointer/DS model is used in a benchmark ? output (Sparql query): SELECT orkgp, lstm_model_id, print_count, score, output, orkgp_v1, lstm_model_id, baz_model_id, and print_count output (Sparql query): output (Sparql query): output (Sparql query): Output input (English text): Provide a link to a paper where the Concept pointer+DS model is used in a test ? output (Sparql query): SELECT id, benchmark, code, output, and output_type output (Sparql query): input (English text): Provide a list of papers that use the LSTM (Mai et al., 2008) model in any benchmark ? output (Sparql query): SELECT id, benchmark, code, lstm_model_id, output, code, lstm_model_id, and output_type output (Sparql query): input (English text): Provide links to papers that use the Concept pointer+DS model in a specific benchmark ? output (Sparql query): SELECT id, id_score, id_labEL, and output output (Sparql query): output (	False
"
 input (English text): List the code links in papers that use the All-attention network - 36 layers model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""All-attention network - 36 layers"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of benchmarked datasets related to the stochastic classification research area?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""stochastic classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Oxford-IIIT Pets?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Oxford-IIIT Pets"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 if ( ( select top {!result in output or select top all ( ?result ) from data set where ?dataset = ?dataset ( ?dataset_lbl in output if select all ( ?result.) and ?dataset_lbl = ?dataset_lbl ) and ?model = ?model { ?pname( ?dataset) = ?name. ?model_lbl. ?model_lbl/ ?model_lbl } if all select all ( ?result.) from data set all the metrics and values of ?result. } ) OR ( select top all ( all ( ?result )from data set where ?dataset ?dataset_lbl ) and ?model = ?model { ?pname( ?dataset) = ?name. ?model_lbl. ?model_lbl/ ?model_lbl } if all select all ( ?result.) from data set all the metrics and values of ?result. ) ) ) || ( select top ( ?result )from data set where ?model = ?model { ?pname( ?dataset) = ?name. ?model_lbl. ?model_lbl/ ?model_lbl } if all select all ( ?result)from data set all the metrics and values of ?result. ) ) ) output (Outputs) result (Text) (Sentai text) orkgp:DISTINCT ?code output(Sparql query): if (select all( ?result.) from data set or select top all( ?result.) from data set all the metrics and values of ?result.all or select top all( ?result.) from data set all the metrics and values of!result	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the MLDoc Zero-Shot English-to-Italian dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MLDoc Zero-Shot English-to-Italian"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Where can I find code references in papers that have used the Large mLSTM +emb +WN +VD model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM +emb +WN +VD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the XLNet-Large model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet-Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT  LMAO_HOST,   lm_dataset,          LMAO_TOOL,  LMAO_WORD,   lm_count,   lm_tbl_lbl,   lm_label,  DATEPART( lm_dataset.start_time,  1),  LMAO_ID,   lm_label,  DATEPART( lm_dataset.stop_time - 1,  1),   LMAO_ID,   lm_label,  DATEPART( lm_dataset.start_time + 1,  1),  FOUNDATION( LMAO_TOOL, lm_count)  FROM   lm_data_viewer; Here, we are processing a JSON string, a title, a subtitle and an output file with the first two being strings. We are looking to get the titles of the studies that are using LASSO or LASSO+NW+VD models to identify how well their models are doing. We are searching for strings that include  the word ""LASSO"" in the title. LMAO_HOST = 'test-lab' LMAO_TAGS = ['LMAO', 'LASSO+NW+VE', 'large', 'Large LASS"	False
"
 input (English text): What is the best performing model benchmarking the Reuters-21578 dataset in terms of Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the DY-MobileNetV2 ×0.75 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DY-MobileNetV2 ×0.75"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""EfficientNetV2-L"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters-21578"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; ?benchmark orkgp:HAS_RESOLUTION ?resolution. ?resolution orkgp:HAS_IMAGE ?image. orkgp:HAS_RESOLUTION ?resolution. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): Give an example of a situation where you are trying to determine your model's best performing model and can't get any guidance other than that it was in some paper that was cited in one of the papers mentioned in an earlier paragraph. The paper used in this article is: Gompertz, F. (2016). Fast, accurate and scalable time series"	False
"
 input (English text): Could you provide a list of models that have been tested on the Food-101 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Food-101"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the ShARe/CLEF eHealth corpus dataset, including the metric and its value?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ShARe/CLEF eHealth corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-46985 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-46985"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	 ----------- FAITH 1: SRI International (1) FAITH 3: SRI International (3) FAITH 5: NIMH (5) FAITH 7: SRI International (7) FAITH 11: NIMH (11) FAITH 12: JAMA (12) FAITH 14: JAMA (14) FAITH 16: JAMA (16) FAITH 21: NIH (21) FAITH 23: NIMH (23) FAITH 25: SRI International (25) FAITH 28: SRI International (28) FAITH 30-31: NAS/MRC (30-31) FAITH 33: SRI International (33) FAITH 36: NAS/MRC (36) FAITH 37: NIMH (37) FAITH 40-41: NIMH (40-41) FAITH 46: NIMH (46) FAITH 50: JAMA (50) FAITH 56: NIMH (56) FAITH 61-70: NIH (61-70) FAITH 71: NIH (71) FAITH 90-101: NIH (90-101) GENE-SPECIFIC: NAS/MRC (90-101) -------------------------------------------------------------------------------------------------------------------------------- AIM (labeled) -------------------------------------------------------------------------------------------------------------------------------- MRC (labeled) -------------------------------------------------------------------------------------------------------------------------------- FAITH 1: SRI International (1) FAITH 3: SRI International (3) FAITH 5: NIMH (5) FAITH 7: SRI International (7) FAITH 11: NIMH (11) FAITH 12: JAMA (12) FAITH 14: JAMA (14) FAITH 16: JAMA (16) FAITH 21: NIH (21) FAITH 23: NIMH (23) FAITH 25: SRI International (25) FAITH 28: SRI International (	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ACL Anthology dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL Anthology"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the ViT-B/16 model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ViT-B/16"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Supervised:"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?benchmark RANK,   val; FROM ?dataset_lbl1 RANK   VAL          +1 RANK  /10 VAL  +1.67  VAL  -1.67       input (English text): I've created a benchmark for the Supervised: dataset. Can you provide the code I used ?	False
"
 input (English text): Name the datasets that have been used for benchmarking in the Part-Of-Speech Tagging research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Part-Of-Speech Tagging"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): What models are being evaluated on the SQuAD1.1 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the best performing model benchmarking the ANLI test dataset in terms of A3 metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""A3"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" To enable the ability to execute the ""Benchmarking"" query, run the following SQL commands as follows: SQUAD2.1.DATASET=dataset_lbl,SQUADB2.1.ENV=production and SQUADB2.1.R_DATA_TEMPLATES=rdfs/dataset_lbl and SQUAD2.1.R_SQUAD2_MODEL_LIMIT=10,SQUAD2.1.TEMPLATES=rdfs/dataset_lbl,SQUADB2.1.ENV=production SQUAD2.1.P32=50,SQUADB2.1.DATASET=dataset_lbl,SQUADB2.1.ENV=production,SQUADB2.1.R_DATA_TEMPLATES=rdfs/dataset_lbl SQUAD2.1.R_SQUAD2_MODEL_LIMIT=30,SQUAD2.1.TEMPLATES=rdfs/dataset_lbl,SQUADB2.1.ENV=production, and SQUADB2.1.R_SQUAD2_MODEL_LIMIT=20 The output should look like the following: R_DATA_TEMPLATES: rdfs/dataset_lbl : Model Lighter(s) (D,I,III) : Categorical: 1.9e1, Continuous: 1.9e1, Markov: 0.8e(0.2-0.9)/2, Principal Component Analysis: 0"	False
"
 input (English text): Can you list the models that have been evaluated on the DDI extraction 2013 corpus dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI extraction 2013 corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Finger, spin (DMControl100k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Finger, spin (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the metrics of evaluation over the iNaturalist 2019 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	 input (English text): Can you list the models that have been evaluated on the iNaturalist 2017 dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the eNCORE benchmark dataset ? output (Sparql query): input (English text): What metrics compare different methods of scoring ? output (Sparql query): input (English text): For the Score, can you please provide the benchmark score and how many models have been evaluated ? output (Sparql query): input (English text): Can you list the models that are evaluated on the VARIABLE dataset ? output (Sparql query): input (English text): What metrics compare different methods of scoring ? output (Sparql query): input (English text): Can you list the models that have been evaluated and include in the dataset ? output (Sparql query): input (English text): Can you provide a list of the data sets that have been evaluated on the MATE benchmark dataset ? output (Sparql query): output (Sparql query): input (English text): Can you provide a list of the models that were evaluated on the SpatialBenchmark dataset ? output (Sparql query): input (English text): Can you list the models that were evaluated on the VARIABLE-2 dataset ? output (Sparql query): input (English text): I would love to have a list of the models that have been evaluated on the Spatial Benchmark dataset since I use many of them everyday! output (Sparql query): 	False
"
 input (English text): Can you list the models that have been evaluated on the Stanford Cars dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Stanford Cars"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Indicate the model that performed best in terms of % Test Accuracy metric on the SNLI benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""% Test Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SNLI"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Tandy A6870"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } OPTIONAL { ?input orkgp:HAS_MODEL ?model. orkgp:HAS_METRIC ?metric. } ORG_PRINT ?dataset ORG_PRINT ?model_lbl. OPTIONAL { ?model ORG_PRINT ?model_lbl. ORG_PRINT ?model. } input (English text): What are the models that have been evaluated on the Atari 2600 Space Invaders dataset ? output (Sparql query): input (English text): Can you list the models that have been evaluated on the Atari 2600 Space Invaders dataset ? output (Sparql query): input (English text): What are the models that have been evaluated on the Atari 2600 Space Invaders dataset ? output (Sparql query): input (English text): Can you list the models that"	False
"
 input (English text): Which model has achieved the highest Accuracy (%) score on the Hendrycks Test benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy (%)"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hendrycks Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the best performing model benchmarking the ObjectNet (Bounding Box) dataset in terms of Top 5 Accuracy metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Top 5 Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	 1 row affected (0.0003%) input (English text): Which model has achieved the highest BLEU score score on the WMT2014 German benchmark dataset ? output (Sparql query): 1 row affected (0.0003%) input (English text): Which model has achieved the highest BLEU/1C score on the WMT2014 English-German benchmark dataset ? output (Sparql query): 1 row affected (0.0003%)	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the DDI dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DDI"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the precision and recall of SemGraphQA?
 output (Sparql query): SELECT ?precision ?recall WHERE { orkgr:R6898 orkgp:compareContribution ?cont. ?cont orkgp:P34 ?eval. ?eval rdfs:label ?eval_labels. FILTER(REGEX( ?eval_labels, ""SemGraphQA"")) ?eval orkgp:P3004 ?precision; orkgp:P5015 ?recall. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT ?paper ?title ?ident IF(REPLACE_SEPARATOR(""'"","" """"), R6898( ?title), R7301( ?paper)); IF(REPLACE_SEPARATOR(""'"","" """"), R7201( ?ident)); IF(REPLACE_SEPARATOR(""'"","" """"), R7308( ?ident)) ORDER BY ?paper ?title input (English text): Provide an ID that corresponds to a title that is in order with the data ? output (Sparql query): SELECT ?label ?ident FROM SemGraphQA GROUP BY ?labels ORDER BY ?dataset id input (English text): What are the dimensions of an image of the (Cart Pole) dataset ? output (Sparql query): SELECT n.name, ?dimensions_per_image IF(NCHAR(CASE n.name, '*'):=1, -1, 3, 20 ) ? input (English text): Where can I report the parameters of other datasets ? output (Sparql query): SELECT ?name ?dimensions_per_image. ?name IF(NCHAR(CASE n.name, '*'):=1, -1, 3, 20 ) ? input (English text): What is the data-type ? output (Sparql query): WHERE title IS NOT null input (English text): What language is the dataset in ? output (Sparql query): SELECT ?lbl. ?language FROM SemGraphQA ORDER BY ?label ORDER BY ?dataset id id IS NOT null"	False
"
 input (English text): Can you list benchmarked problems in the area of Information Science?
 output (Sparql query): SELECT DISTINCT ?problem ?problem_lbl WHERE { ?rf a orkgc:ResearchField; rdfs:label ?rf_label. FILTER (str( ?rf_label) = ""Information Science"") ?paper orkgp:P30 ?rf; orkgp:P31 ?cont. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. ?problem rdfs:label ?problem_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the Walker, walk (DMControl100k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Walker, walk (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the A3C FF hs model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A3C FF hs"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT IF( ?isDumb(1)) ?model_1 ?model_1_lbl ?model_2 ?model_3 ?model_1_lbl & IF( ?isDumb(2)) ?model_2 ?model_2_lbl ?model_3 ?model_1_lbl & IF( ?isDumb(3)) ?model_3 ?model_3_lbl OR IF( ?isDumb(4)) ?model_4 ?model_4_lbl ?eval. ?eval rdfs:label ?model_lbl. input (English text): Specify the description of data and data sets that was used, as well as the reference information. output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. } In short: I want to get a list of datasets used in both the Walker, walk test set and the Walker, walk benchmark dataset, including the link to the researchers' code. This will be really helpful for the next step in my research – comparing model performance for each dataset. I've started testing the new Walker, walk (DMControl100k) benchmark. I've also created a list of papers by the authors for comparison use in a comparison of the Walker, walk (DMControl100k) benchmark set to the Walker, walk (DMControl100k) benchmark dataset. I've included a sample of these benchmarks in the example below. Please have a look at it and tell me if there are any mistakes, typos or other errors. [ ] Walker, W.G. (2016): Walker, Walker, walk: the Walker-Walker	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Berzerk dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Berzerk"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the NYT24 dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the BiDAF + Self Attention + ELMo model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT COUNTIF ( id, data, desc ) FROM papers [ ""Berzerk"" ] WHERE id = 1 || id = 100 && data = 1 DESC DESCRIPTION output (Sparql query): SELECT ( count ( data ), count ( id )) FROM papers_1 [ ""Berzerk"" ] input (English text): How long does it take to run Berzerk with the Berzerk benchmark ? output (Sparql query): SELECT duration ( id ), count ( id ) FROM papers [ ""Berzerk"" ] input (English text): How many authors are listed in papers that use the BiDAF + Self Attention + ELMo model for benchmarking purposes ? output (Sparql query): SELECT authors_1 FROM papers [ ""Berzerk"" ] input (English text): What benchmarks or methodologies have been used to benchmark the EAS-DAF + Self Attention + ELMo model on a data set of Atari 2600 games ? output (Sparql query): SELECT DISTINCT benchmark ?dataset. ?dataset rdfs:label ?dataset. ?metric OPTIONAL rdfs:desc DESCRIPTION Benchmarks: EAS-DAF + Self Attention + ELMo. : RDF, XML DISTINCT output (Sparql query): SELECT ( name ), count ( name ) FROM papers_2 [ ""Berzerk"" ] input (English text): How can I run Berzerk with three different versions of the Berzerk model for benchmarking purposes ? output (Sparql query): SELECT variant_1_compet_benchmark, variant_2_compet_benchmark, variant_3_compet_benchmark FROM papers ["	False
"
 input (English text): Which model has achieved the highest Entropy Difference score on the Barabasi-Albert benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Entropy Difference"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Barabasi-Albert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the iNaturalist 2018 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2018"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the metrics that are used to evaluate models on the NCBI Disease benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NCBI Disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	-------------------------  | ------------------------  | ------------------------  | ------------------------  | ------------------------  | -------------------------  | -------------------------  | ------------------------  | ---- |  |  |  |  |  | ----  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | ---------  |  |  |  |  |  |  | ---------  |  |  |  |  |  |  | ---------  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | ---------  |  |  |  |  |  |  | ---------  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | --------  |  |  |  |  |  |  | --------  |  |  	False
"
 input (English text): What are the titles and IDs of research papers that include a benchmark for the CoNLL 2012 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL 2012"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the Grave et al. (2016) - LSTM + continuous cache pointer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2016) - LSTM + continuous cache pointer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""6-layer QRNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT * FROM b_benchmarks WHERE code_lbl = NULL; 1 2 3 4 SELECT * FROM b_benchmarks WHERE code_lbl = NULL ; where NULL is the path to the files that were compiled into an SDF file for the benchmark. For example: C:\SQL>.\B_Benchmarks.dbx b_benchmarks C:\sql\b_benchmarks\trees.\B_Benchmarks.dbx b_benchmarks\LSTM-CATALOG.sql 5.0.1.\b_benchmark_dbx b_benchmarks\LSTS.sql 5.0.1.\b_bench_dbx b_benchmarks\LEST.sql 5.0.1.\b_bench_dbx b_benchmarks\LSTSIN.sql 5.0.1.\b_bench_dbx b_benchmarks\LSTSOUT.sql b_benchmarks 6.0.1.\b_bench_data.dbx b_benchmarks\LEST.sql b_benchmarks 6.0.1.3 C:\sql\b_benchmark_data.db4 b_benchmarks\LSTS.sql b_benchmarks 6.0.1.\b_bench_data.db7 b_benchmarks\LESTSIN.sql At this point, you may want to double check that the RDF model files exist on your system, and should be copied on to your workstation or database drive to get the RDF files created for your benchmark code. For the LSTM and LSTS benchmarks, you might also need to adjust the number of samples that	False
"
 input (English text): What is the name of the top performing model in terms of Overall score when benchmarked on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the name of the top performing model in terms of Percentage correct score when benchmarked on the CIFAR-100 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CIFAR-100"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the WOS-11967 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-11967"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } input (English text): What is the name of the top performing model based on Overall score and average percentile score of all models when benchmarked on CIFAR-100 dataset ? output (Sparql query): SELECT DISTINCT ?model ?model_lbl AS ?model_lbl-lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Overall"") { SELECT ?model ?model_lbl AS ?model_lbl-lbl ORDER BY { ?metric_lbl. ?metric rdfs:label. } LIMIT 1 } } input (English text):"	False
"
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the MNIST dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""MNIST"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Could you provide a list of models that have been tested on the Natural Questions benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Natural Questions"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the NYT24 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NYT24"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT n.title, N.query_number, N.query_length FROM n.query_list n LEFT JOIN N.query_schema ns ON n.nquery_id = n.nquery_id LEFT JOIN N.query_result nr ON n.nrqueryid = n.nquery_id LEFT JOIN N.test_set ntest ON ntest.query1 = c.query1 AND ntest.query1.query_number = n.query_number AND (ntest.query1.query_length = 200) LEFT JOIN N.test_result nrt ON nrt.query2 = c.query2 AND nrt.query2.query_number = n.query_number AND (nrt.query2.query_length = 200) LEFT JOIN N.comparison_query cq ON nq.query = c.query1 AND cq.query1.query_number = n.query_number WHERE N.query_schema.schema_id IN ( SELECT s.n_test_id FROM n AS s WHERE s.num_nested_nested = 0 AND!( s.n_test_id = s.n_test_id WHERE n.n_id = n.n_id AND s.query_result.test_id = '4c2be2b8' ) ) ORDER BY s.n_test_id ASC LIMIT 1,50 ) ORDER BY s.query_id ASC LIMIT 2 input (English text): Can you give a short description of the test dataset ? output	False
"
 input (English text): Can you list the models that have been evaluated on the SemEval-2010 Task 8 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SemEval-2010 Task 8"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the DRI Corpus dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DRI Corpus"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Name the datasets that have been used for benchmarking in the Robot Navigation research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Robot Navigation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT { ?model rdfs:label id. ?dataset rdfs:label id. > { id. ?model rdfs:label id. > { id. ?dataset rdfs:label id. > null ?dataset ?dataset_lbl. } ?dataset rdfs:label id. > NULL ?dataset_lbl. } } ?method rdfs:label ?method. ?method rdfs:label id. ?metric rdfs:label ?metric_lbl. } ORDER BY dataset_lbl input (English text): Name the parameters that are available to evaluate the models in the Robot Navigation research problem ? output (Sparql query): SELECT ?parameter ? > { ?parameter rdfs:label id. ?quantity rdfs:label id. > ( ?dataset ?dataset_lbl. ) ?statistic_lbl or kgp:HAS_STATISTICS ?variable rdfs:label id. > ( ?dataset ?dataset_lbl. ) } ORDER BY ?quantity OR ?statistic_lbl input (English text): Name the models used in the evaluation of the two supervised learning solutions in the Robot Navigation problem ? output (Sparql query): SELECT ?model rdfs:label id. OPTIONAL > { ?model rdfs:label id. + ?quantity rdfs:label id. > ( ?dataset ?datas	False
"
 input (English text): What are the metrics of evaluation over the Lunar Lander (OpenAI Gym) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Lunar Lander (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the WOS-5736 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WOS-5736"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (PNG image): What are the metrics of evaluation over the Seel.cse.lsu.edu/data/refsq17.zip benchmark dataset ?	False
"
 input (English text): Can you provide links to code used in papers that benchmark the BiT-S (ResNet-152x4) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiT-S (ResNet-152x4)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the Large mLSTM +emb +WN +VD model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Large mLSTM +emb +WN +VD"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""IMDb-M"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 select list_c_id as list_c, list_c_p, list_c_val, list_c_rng as list_c_p, list_c_rng_c as list_c_rng_c, list_c_mf as list_c_mf, list_c_sc as list_c_sc, list_c_sgb as list_c_sc_c, list_c_sc_c as list_c_sc_c, list_c_sc_c_n as list_c_sc_c, list_c_sc_c_n as list_c_sc_c_n, list_c_sc_c_n_p as list_c_sc_c_n_p, list_c_sc_c_n_p as list_c_sccd_sgb as list_c_sccd_sgb, list_c_sccd_sgb as list_c_lstm as list_c_lstm_c, list_c_lstm_c as list_c_lstm_c, list_c_lstm_c_n as list_c_lstm_c_n, list_c_lstm_c_n_p as list_c_lstm_c_n_p, list_c_lstm_c_n_p as list_c_lstm_l2o as list_c_lstm_l2o_c, list_c_lstm_l2o_c as list_c_lstm_l2o_c, list_c_lstm_	False
"
 input (English text): List the code links in papers that use the Performer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Performer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the DrQA (Document Reader only) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""DrQA (Document Reader only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Where can I find code references in papers that have used the PEGASUS model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PEGASUS"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	SELECT RESTRICTED_FROM ?code ?rdfs ? selecting subset_link_code_rdfs() from papers # input (English text): Where can I find code references in papers that have used the PEGASUS models for benchmarking purposes ? output (Sparql query): SELECT RESTRICTED FROM ?code ?rdfs ? WHERE { ?model_rdfs orkgc:Model; rdfs:index orkgc:Index ?; rdfs:title ?title. cpg orkgc:Author. author_name ?author. Cpg. } input (English text): Where can I find code references in papers that have used the PEGASUS models for benchmarking purposes ? output (Sparql query): SELECT RESTRICTED FROM ?code ?rdfs ? WHERE { ?model_rdfs orkgc:Model; rdfs:index orkgc:Index ?; rdfs:title ?title. cpg orkgc:Author. author_name ?author. Cpg. } input (English text): Where can I find code references in papers that have used the PEGASUS models for benchmarking purposes ? output (Sparql query): SELECT RESTRICTED FROM ?code ?rdfs ? WHERE { ?model_rdfs orkgc:Model; rdfs:index orkgc:Index ?; rdfs:title ?title. cpg orkgc:Author. author_name ?author. Cpg. } input (English text): Where can I find code references in papers that have used the PEGASUS models for benchmarking purposes ? output (Sparql query): SELECT RESTRICTED FROM ?	False
"
 input (English text): What are the metrics of evaluation over the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the code links in papers that use the QA-GNN model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""QA-GNN"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of benchmarked datasets related to the Audio Classification research area?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Audio Classification"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	SELECT ?matrix (data) ?data_clust from training ?graph (data) and output ?graph (data) ?metric (graph). input (English text): List the code links in publications related to this topic ? output (Sparql query): SELECT ?code LINK FROM ?graph (graph) AND ?metric (graph) ?metric_clust.  ORIGINAL CODE LINK DATE  |   ------------  01/05/2014  01/06/2014  01/09/2014  01/13/2014  01/18/2014  01/23/2014  01/26/2014  02/01/2014  02/04/2014  03/05/2014  03/10/2014  03/12/2014  04/02/2014  03/11/2014  04/05/2014  04/13/2014  03/14/2014  02/02/2014  02/08/2014  02/09/2014  02/15/2014  03/01/2015  02/16/2015  02/27/2015  03/09/2015  03/12/2015  02/28/2015  03/10/2015  03/19/2015  03/30/2015  04/02/2015  04/03/2015  03/19/2015  04/10/2015  04/07/2015  03/20/2015  05/02/2015  05/05/2015  05/27/2015  05/13/2015  06	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Atari 2600 Amidar dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Amidar"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): List the code links in papers that use the Grave et al. (2016) - LSTM + continuous cache pointer model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Grave et al. (2016) - LSTM + continuous cache pointer"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cart Pole (OpenAI Gym)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	" Note that this time each output should have a table (or SQL) value, not a single row. data (OpenAI Gym Benchmarks; n_test = 1000; rank = 1; score = 3;) data (Grave et al. 2016) data (Cart Pole benchmark) input (English text): For each code link of the Grave et al. (2016) - LSTM + continuous cache pointer model in a cart pole benchmark ? output (Sparql query): INSERT INTO ?code SET score = score + ?rank ?rank ?score. note that the final column 'rank' has to be an integer, not the string 5, to include both the integer value and the rank attribute. output (SQL) -- rank: The rank of code link in a benchmark rank = 1; code = 1; -- code-line: The line number of the code in each benchmark code line = 6; code = 17; -- rank: The rank of code link in a benchmark rank = 1; code = 1; code-line = 1; code = 17; -- result: Number of code links of the benchmark data (Grave et al. 2016) -- code-line: The line number of the code of each benchmark  code = 7; code = 18; -- result: Number of code links,  as well as all of the code links from the benchmark  data(data(""Grave et al. 2016"") ) output (SQL) Note that, while the SQL output shows the count of code links from the benchmark, for now, the result of the query is not shown. code-name: The name of the benchmark code-line: The line of code that generated the benchmark code-id: The object ID for this benchmark code: The name of the benchmark code-"	False
"
 input (English text): List the code links in papers that use the MMDL model in any benchmark?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""MMDL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the metrics that are used to evaluate models on the WMT2016 English-German benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-German"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	SELECT ?title, ?id, ?title_lbl, ?id_lbl FROM ?title SELECT ?title_lbl, ?id, ?title_lbl_lbl input (English text): List the dataset names that you identified in the WMT2016 English-German benchmark dataset ? output (Sparql query): SELECT ?dataset AS dataset, ?title, ?id, ?title ?id_lbl, ?id_lbl I hope this answer is helpful.	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the CommitmentBank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What models are being evaluated on the SQuAD1.1 dev dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SQuAD1.1 dev"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Which model has achieved the highest MACs score on the ImageNet benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""MACs"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" I am curious to know if (as it seems) the ""top"" model on this list has any relationship with what happens in actual lab exercises! Or if it's just some random list where you get a bunch of models, like, I dunno, some from this article, or a bunch of random ideas. But I do admit that for me one really interesting model is a model of the K-means clustering algorithm. Here's a quote from its official Wikipedia article: ""The K-Means algorithm (also known as the ""cluster method""), invented by Kanno [and] Chua [], is also known as a variant of a two-stage method of cluster analysis, in which each initial node represents the partition of the input data of a given group, and the subsequent nodes represent the partition of partitions of the input data into a second group. The method is sometimes seen as a variant of the two-stage method. The second step is to split each segment of the input data into two or three clusters, by performing a partitioning of these into clusters on a new set of data. By the K-means algorithm, each cluster is divided into the smallest number of groups. This is called splitting and the algorithm is said to have started in 1945 in Nürnberg."" And this, in turn, is how the paper gets about: ""The K-means algorithm is an open-source numerical linear algebra and algorithms framework originally implemented at MIT in 1973."" So, if you're interested in this algorithm on Wikipedia's page you will find it and more information at that link. The K-means clustering algorithm seems to be quite different from the two-stage method that I've mentioned. Indeed, the algorithm has a slightly different name (the K-Means clustering algorithm): ""the K-Means Clustering Algorithm."	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SMT as posterior regularization model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SMT as posterior regularization"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ImageNet dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the ACE 2005 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2005"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_METRIC ?metric. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ? ?metric_lbl input (English text): Please specify the metric or metric_lbl to be used for the top 300. output (Sparql query): SELECT MAX( ?value) AS ?index ?index_1 ?index_2 FROM ?index_1 JOIN ?index_2 on ?index_2.is_index ?( ?index_1 ); input (English text): Please specify the metric or metric_lbl to be used for the top 100. output(Sparql query): SELECT MAX( ?value) AS ?index ?index"	False
"
 input (English text): Where can I find code references in papers that have used the BiGRU model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiGRU"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of F1 metric on the ACL-ARC benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT COUNT(*) AS count FROM `model-benchmark' USING (cnt) ROWS; output (Sparql query): WHERE COUNT(1) > (SELECT COUNT(count) OFFSET (cnt)) input (English text): List the metrics that are used to evaluate models on the DBpedia benchmark dataset ? output (Sparql query): SELECT COUNT(*) AS count TO NULL FROM `model-benchmark' USING (cnt) ROWS; output (Sparql query): input (English text): Which model is the best on the ACL-ARC benchmark dataset ? output (Sparql query): SELECT 'ACL-ARC' USING (tcp_port) ROWS; output (Sparql query): WHERE ROW(2) IS NULL 	False
"
 input (English text): What are the models that have been benchmarked on the ANLI test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ANLI test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the Cartpole, swingup (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 # Test Dataset rdfs:model:ANLI test1.csv (29.8 KB, 2x2, rank=0.15, data.table='Model', columns='Name, Max, Mean, StdDev, SD') test1.txt - (33.9 KB, 2x2, rank=0.40, data.table='Model', columns='Name, Max, Mean, StdDev, SD') rdfs:model:TMDS test2.csv (20.9 KB, 2x2, rank=0.30, data.table='Model', columns='Name, Max, Mean, StdDev, SD') test2.txt - (19.1 KB, 2x2, rank=0.30, data.table='Model', columns='Name, Max, Mean, StdDev, SD') rdfs:model:DS test3.csv (28.1 KB, 2x2, rank=0.40, data.table='Model', columns='Name, Max, Mean, StdDev, SD') test3.txt - (22.5 KB, 2x2, rank=0.40, data.table='Model', columns='Name, Max, Mean, StdDev, SD') rdfs:model:HDF test4.csv (26.8 KB, 2x2, rank=0.40, data.table='Model', columns='Name, Max, Mean, StdDev, SD') test4.txt - (19.9 KB, 2x2, rank=0.40, data.table='Model', columns='Name, Max, Mean, StdDev, SD') # Model Name Max	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ObjectNet (Bounding Box) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet (Bounding Box)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you provide links to code used in papers that benchmark the Relationship Types model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Relationship Types"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What are the metrics of evaluation over the Hutter Prize dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Hutter Prize"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT ?dataset_lbl ? from rdfs_nodes_lbl ?nodes ?. ?source_code orkgp:OBJECT_SCOPE orkgp:OBJECT_SCOPE_ID rdfs:label ?dataset_lbl. Input English text (URL): What are standard properties of the Hutter Prize dataset ? output (Sparql query): { ?dataset_lbl orkgc:EntitySet; rdfs:label ?dataset_lbl. } Input English text (URL): Can you provide links to other publications on the relationship models benchmarking tool as well ? output (Sparql query): <table-cell><tbody><tr><td><img src=""data:image/gif;base64,i.gif"" width=""100%"" height=""50%"" alt=""Hutter Prize"" border=""0"" /></td></tr></tbody></table> Input English text (URL): What are the most critical aspects of the new relationship model to implement for the Hutter Prize, and the issues that have been documented ? output (Sparql query): <table-cell><tbody></tbody> Input English text (URL): Which version of the new relationships model are you using ? output (Sparql query): { ?dataset_lbl"	False
"
 input (English text): What is the name of the top performing model in terms of Number of params score when benchmarked on the One Billion Word dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Number of params"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""One Billion Word"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Provide a list of papers that have utilized the LSTM Encoder-Decoder + LSTM-LM model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""LSTM Encoder-Decoder + LSTM-LM"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/re17.zip "") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 10.3.3 The LSTM-CM Model¶ The LSTM-CM has been tested for the LSTM-1, LSTM-1d, and LSTM-1b algorithms. The LSTM-CM has been tested for the LSTM-1, LSTM-5, LSTM-5d, and LSTM-5b algorithms. LSTM-CM can be trained using the following code: From http://lstm.bibcode.net/lstm/lstm-CM/ LSTM-CM_main.pl lstm_encoder.cpp lstm_encoder.h lstm_cse_l1_l3::decoder (void); lstm_cse_l1_l3::decoder(void); lstm_cse_l1_l3::decoder(void); The code from the original article, LSTM-CM_main.pl is not documented and may be better suited for the LSTM model, and the same code for LSTM-CM can be found in the current article. If you are curious about whether the output above was used or a different algorithm was used, please file a bug against the LSTM codebase and let the developer of the code to know. This output has been used in the following examples, which should be read as a guide. Please also see the current and previous article for a detailed discussion on the code. 10.3.3.1 The LSTM-CM model: The LSTM module offers a simple way for the user to experiment with the LSTM algorithm on an input dataset. First you add a model using the module as described in this article. The LSTM_MODEL	False
"
 input (English text): Indicate the model that performed best in terms of F1 metric on the CommitmentBank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CommitmentBank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the Story Cloze Test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Story Cloze Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the top benchmark score and its metric on the Atari 2600 Wizard of Wor dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Wizard of Wor"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT ?top_score, ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { SELECT ?top_score > ?metric_lbl. ?top_score OR ?top_score < ?metric_lbl. ?top_score ORDER BY ?metric_lbl. } } output (Sparql query): SELECT TOP ( ?top_score ) { ?top_score, ?metric_lbl. FILTER (str( ?metric_lbl) = ""F1"") { ?top_score > ?metric_lbl. ?top_score OR ?top_score < ?metric_lbl. ?top_score ORDER BY } } output (Sparql query): select top, ?metric/1. score from ? ? ? where { ?dataset ?dataset_lbl. ?dataset_lbl. FILTER (str( ?dataset_lbl ) = "" ? ? ? ?"") ?dataset_lbl. rdfs:title ?dataset_lbl. } ORDER BY ?metric_lbl. ORDER BY TOP ( ?top_score ) output (Sparql query): select top, top. score from ? ? ? where { ?dataset ?dataset_lbl. ?dataset_lbl. FILTER (str( ?dataset_lbl ) = "" ? ? ? ?"") ?dataset_lbl. rdfs:title ?dataset_lbl. } ORDER BY ?dataset_lbl. ORDER BY TOP ( ?top_score ) input (English text): Can you provide a description of how the model was evaluated ? output (Spar"	False
"
 input (English text): List the metrics that are used to evaluate models on the WebNLG benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WebNLG"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Where can I find code references in papers that have used the SciBERT (full data) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT (full data)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""12-layer Transformer-XL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?src_id ?source_id OR ( ?source_id) ?source_id, ?dest_id OR ( ?dest_id) ?dest_id, ?output OR ( ?output) ?output OR ( ?output) ?output; orkgp:HAS_DISTINCT ?src_id, ?src_id OR ?dest_id, ?output OR ?output OR ( ?output) ?output, ?dest_id OR ?dest_id, input (English text): Where can I find reference descriptions in papers that have used the 13-layer Pulsar or B2B model in any benchmark ? output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl OR ( ?dataset) ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""B2B"") ?benchmark If you encounter a problem with the citation search, please submit a GitHub Issue explaining the error and describe the problem. Please refer to the issue tracker and follow-up discussions in the comments field. The SciDB repository is the recommended citation reference source. We encourage you contact us if you have questions or suggestions about the citations source. The CiteULite site provides detailed information about CiteULite and other open-source citation solutions. The Citation Watchlist provides a summary of upcoming, pending, and closed-access journals that the CiteULite Citation Checker is checking. The CiteULite site also includes a list of open-access journals that are reviewed by the CiteULite Citation Checker for inclusion in the list. Please cite the paper that you did not cite in CiteULite: For example: We have implemented a score"	False
"
 input (English text): Where can I find code references in papers that have used the XLNet (single model) model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""XLNet (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset OA-STM?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""OA-STM"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What is the best performing model benchmarking the Atari 2600 Up and Down dataset in terms of Score metric?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Up and Down"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	"SELECT DISTINCT ?code FROM papers WHERE code = CODES.INSP; input (English text): What is the best performing benchmark result (metric and value) over the dataset OA-STM ? output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?score) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CODES.INSP""); ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_RANDOMNESS ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?score) } } GROUP BY ?metric ?metric_lbl ORDER BY DESC( ?value) "	False
"
 input (English text): What are the most commonly used benchmark datasets for the Data-to-Text Generation research field?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Data-to-Text Generation"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Where can I find code references in papers that have used the Intrinsic Reward Agent model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Intrinsic Reward Agent"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Could you provide a list of models that have been tested on the NLP-TDMS (Exp, arXiv only) benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 input (English text): The purpose of benchmarking and reporting for this dataset ? output (Sparql query): input (English text): Why is the data set so important to understand ? output (Sparql query): input (English text): Could you provide an overview of the project ? output (Sparql query): input (English text): How would you describe your research in one sentence ? output (Sparql query): input (English text): Why do you care about benchmarks ? output (Sparql query): input (English text): Are there specific benchmarks that are applicable to, or will work with, a given dataset ? output (Sparql query): input (English text): What types of benchmarks have you run ? output (Sparql query): input (English text): What problems and goals do the benchmarks help you with ? output (Sparql query): input (English text): How did you select the datasets for the research ? output (Sparql query): input (English text): How many datasets did you include in your benchmarking process ? output (Sparql query): input (English text): Are your tools designed to run the benchmarks in their environment ? output (Sparql query): input (English text): When did you finalise on the benchmarks and run them ? output (Sparql query): input (English text): Why are the benchmarks important ? output (Sparql query): input (English text): Why did you choose the datasets for the research ? output (Sparql query): input (English text): Who helped you make the decision to include the datasets ? output (Sparql query): 	False
"
 input (English text): What is the name of the top performing model in terms of FLOPS score when benchmarked on the STL-10 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""FLOPS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you list the models that have been evaluated on the SciERC dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciERC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the models that have been benchmarked on the  Jacquard dataset dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = "" Jacquard dataset"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 outputs data set with R-f-statistical results when using the ST-10 benchmarking model output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): output (Sparql query): Input: model Outputs: Output: 2 rows selected. Here is an example where we want to run on different datasets using the same model. Note that this example is being tested by default to use the R-f-statistical results. Here we have used the standard R model. The input is a model_id, the output is a R-f-staticial test result. Here are our first two inputs: 1.1 Mixture SVM (SVM1_testset.R)  or  5.9% of 6.1 Mixture SVM (SVM5_testset.R)  Both input s and outputs can also contain more than one model. The R models are then separated into a model set and a subset of model ids. The model ids should contain an  '|' (truncated) prefix to differentiate them from the original data sets. The R model identifiers are specified to be compatible with the standard R data, namely R-f-statistical results. The R-f-statistical results will be computed using the rdfs utility with the specified model_id. The input is  a model_id and the output is the R-f-statical results of the model. Note that the R-f-statistical results are computed over all batches of each dataset (i.e. all model ids. The R-f-	False
"
 input (English text): What is the best performing model benchmarking the Atari 2600 Private Eye dataset in terms of Score metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Private Eye"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the minimum and maximum energy generation for each energy source considered?
 output (Sparql query): SELECT ?energy_sources_labels (MIN( ?elec_gen_value) AS ?min_elec_gen_value) (MAX( ?elec_gen_value) AS ?max_elec_gen_value) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43134 ?elec_gen. ?elec_gen orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?elec_gen_value) } 
 input (English text): What are the metrics of evaluation over the ObjectNet dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	SELECT metric_value('score'), metric(score) from { ?dataset, ?model, ?model_lbl} LBSets LBSets for the ObjectNet model performance evaluation of the SNES and Atari Emulator datasets using a scoring-based algorithm. input (English text): What is the minimum and maximum energy generation for each energy source considered ? output (Sparql query): SELECT ?energy_sources_labels (MIN( ?energy_generator_value) AS ?min_energy_generator_values) (MAX( ?energy_generator_values) AS ?max_energy_generator_values) WHERE { orkggr:R153801 orkgp:compareContribution ?contrib. orkgp. ?contrib orkgp:P43135. ORkgp:P43131. ORkgp:P43132. ORkgp:P43133. ORkgp:P43134. ORkgp:P43135. ORkgp:P43136. ORkgp:P43137. ANDkgp:R153801. ORkgp:R202310. ORkgp:R103560. ENH_ELEC_GEN. ORkgp:HAS_VALUE. ENH_ELEC. ENH_ELEC_ GEN_SITE. ENH_ELEC_GEN_ID. ENH_ELEC_GEN_LEVEL. ENH_ELEC_GEN_VULNERABILITY. ENH_ELEC_STAT_POW. ENH_ELEC_MACH_G. ORkgp:P103132. ENH_ELEC_MID_LEVEL. ORkgp:	False
"
 input (English text): Provide a list of papers that have utilized the ViT-L/16 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ViT-L/16"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ObjectNet?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ObjectNet"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the enwik8 dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""enwik8"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?name ?name ORDER BY DESC( ?value) FROM ?benchmark r2 |  select id,score,benchmark,benchmark_l1,bench_l2,bench_l3,bench_l4,bench_l5,bench_l6,bench_p2,bench_p3,bench_p4,bench_h6,bench_p5,bench_p6,bench_p7,bench_p8,bench_p9,bench_p10,bench_p11,bench_p12 (MAX( ?score) - ?object_p1) |  select r2.score,R2.name,R2.value FROM ?benchmark r2,R2.scores   WHERE R2.id = ?benchmark_l1.id ORDER BY DESC( ?value ) AS  ""id-scores""  (CASE  R2.id ASC,  R2.id DESC,  R2.name ASC ); input (English text): What is the top result (metric and value) over the dataset ? output (Sparql query): SELECT DISTINCT ?value AS ?value_l1,    1 AS ?value_l0 FROM ?test_l1"	False
"
 input (English text): What models are being evaluated on the DROP Test dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DROP Test"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the C51 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""C51"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""PARAMS"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FGVC Aircraft"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	SELECT DISTINCT ?code || ?PARAMS FROM DROP Benchmark ?code || ?PARAMS; ?dataset a orkgc:Dataset ?dataset AND id:id OR id: id; orkgr:HAS_BENCHMARK ?benchring OR ?benchring rdfs:label ?benchring. ?benchring rdfs:label ?benchmark. input (English text): Provide the reference version number of the model that has been utilized in the DROP Benchmark dataset at each step of the model selection process ? output (Sparql query): SELECT version; id:id; ?version rdfs:label ?model_lbl. input (English text): Indicate the number of rows in the database table that reference, or have reference to, the R^2 statistic of the model ? output (Sparql query): input (English text): Provide the name of the model that can be run on the drop benchmark dataset ? output (Sparql query): SELECT model_name, model_lbl, model_hdesc, model_covariate, model_r, model_score, model_scores_l, model_scores_h, model_benchmark, model_benchmarks_c, model_benchmarks_h, model_benchmarks_c_h, model_ratings_l, model_ratings_h, model_ratings_c, model_unscaled_criteria, model_score || ?model_lbl || ?model_hdesc ; orkgc:Dataset	False
"
 input (English text): How many studies are published after 2019?
 output (Sparql query): SELECT (COUNT( ?date) AS ?number_of_studies) WHERE { orkgr:R110393 orkgp:compareContribution ?contrib. ?contrib orkgp:P9040 ?date. filter( ?date > ""2019""^^xsd:string) } 
 input (English text): What is the best performing model benchmarking the WikiText-2 dataset in terms of Validation perplexity metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Validation perplexity"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WikiText-2"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Battle Zone"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" select * from table( ?) ORDER BY DESC( DESC( rdfs:label ?model_lbl. cnt ) DESC( ?dataset ) ) DESC( ?model ) ROWS  ORDER BY DESCRIPTION ---- ---- ----- -------------- ------------- ------------ ------------ ------------- ------------- ----------------- ------------- ----------- ------------ --------------. ""Count of Battle Zone games won. SPSS Statistics & Tools"" by  Matti Heinonen & Matti Tuohimaa. http:/ / www.  www.  www.  www.  www.  www.  www.  www.  www.  www.  (c) 2006,  2013) Dependencies: SQL Server 2012, MySQL 5.0.37+ etc"	False
"
 input (English text): What are the metrics of evaluation over the Atari 2600 Video Pinball dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Video Pinball"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Space Invaders benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Space Invaders"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the models that have been benchmarked on the Atari 2600 Road Runner dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Road Runner"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 select *, bestfrom('a ORkgc:Metric',...) as benchmarking_metric; from('a ORkgc:Metric',...) select { 'avg':{ 'benchmark': { 'test_bench': 'avg','score': (100); } }, 'best':{'score': (100); 'name': { 'name': 'avg', 'text': 'atari 2600 space invader benchmark score' } },'score':(100); 'name': { 'name': 'avg', 'text': 'atari 2600 space invader benchmark score' } }, 'r': {'m': {'score': (100); 'name': { 'name': 'avg', 'text': 'atari 2600 space invader benchmark score' }, 'text': 'ATARI 2600 SPACE INVADERS BASIC' } } } I've also been doing some research on the Atari 2600. As you may know, from 1992 to 1995, Atari produced the following video games: This is a sample of a table of test scores taken from a very large dataset. As you can see, the scores on this specific test are very high, so this system is clearly not very optimal. But it is a very interesting idea: a model for benchmarking video games! If one of our Atari platforms is the best, and as you say, it can beat its competition so it deserves a good score (e.g. I've no idea why Atari's video game output is being so low in benchmarks) then the following is a great idea for a benchmarking system: a video game (or in fact, any video game) score. After you've got a simple score out of some data that's already been benchmarked, write some code that counts benchmarked video game scores, then you can	False
"
 input (English text): Can you list the metrics used to evaluate models on the seel.cse.lsu.edu/data/refsq17.zip dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""seel.cse.lsu.edu/data/refsq17.zip"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the CORLL dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CORLL"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Boxing dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Boxing"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT TOP 9 models. ( ?models {model:NAME ? }, ?metrics {model:NAME ? }, ?metrics rdfs:label ?metrics. ) AS model_score, ( ?metrics {model:NAME ? }, ?metrics rdfs:label ?metrics. ) AS metric_score, ( ?metrics {model:NAME ? }, ?metrics rdfs:label ?metrics. ) AS model_eval, ( ?metrics {model:NAME ? }, ?metrics rdfs:label ?metrics. ) AS metric_est, ( ?metrics {model:NAME ? }, ?metrics rdfs:label ?metrics. ) AS metric_max, ( ?metrics {model:NAME ? }, ?metrics rdfs:label ?metrics. ) AS metric_min FROM models GROUP BY model_score ORDER BY metric_score DESC Input: An example query to evaluate a linear model on the Atari 2600 Boxing dataset. SELECT model_score, (model_score, metric_score) AS metrics, (metrics rdfs:label ? ) AS metric_score FROM ( SELECT model_score, metric_score FROM ( SELECT name, description, rating, date, totalPrice FROM Atari 2600 BOXING ) model_score WHERE max(model_score), score AS score_lbl IN ( ?max=max( ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ?max=value, ) ) Output: model_score: 18.0 (8.3%) metric_	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the Ball in cup, catch (DMControl100k) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Ball in cup, catch (DMControl100k)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the best performing model benchmarking the STL-10 dataset in terms of Percentage correct metric?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""STL-10"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the WMT2014 French-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014 French-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Percentage correct"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2014"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS _ METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } output (Schema schema): output (Sparql query): SELECT DISTINCT ?method ?method_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?method_lbl) = ""Number of rows used"") { IF(LENGTH( ?method ) < 4) rdfs:label ?method. ?method _ id. ?method rdfs:= { rf ""methodid_"" ( ?method_lbl ), ?method _ lbl } ; if (length( ?method_lbl ) > 4) rdfs:label ?method_lbl. ?method rdfs:= {"	False
"
 input (English text): Where can I find code references in papers that have used the A2C + SIL model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""A2C + SIL"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the RotoWire dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""RotoWire"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): Can you list the models that have been evaluated on the BUCC French-to-English dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BUCC French-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	 select sum(RadoJ2A3_A3_2, sum(H4_G2, sum(M1_G2, sum(G2_S, sum(G2_3, sum(B2_S, sum(S6_G, sum(S8_G, sum(B7_S, sum(S12_G, sum(M18_G, sum(G2_21_G, sum(M21_G)) )))) * 3)) / sum(W2_G2, sum(M8, sum(G3_G25, sum(S8_G15, sum(S36_G27, sum(M7_G10, sum(M6_G4, sum(G2_B10, sum(M2_G7))))))) * 3)) as RadoJ2A3_A3_2, min(min(RadoJ2A3_A3_2, RadoJ2A3_A3_1, RadoJ2A3_A3_2/RadoJ2A3_A3_2), 10)) as RadoJ2A3_A3_1, max( Max(Max(MIN(RadoJ2A3_A1, RadoJ2A3_A4, RadoJ2A3_A5), min(RadoJ2A3_A6) - MIN(RadoJ2A3_A6))/RadoJ2A3_A3_2, RadoJ2A2_A3_1/max(RadoJ2A3_A4/max(MIN(RadoJ2A3_A4, RadoJ2A3_	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the NLP-TDMS (Exp, arXiv only) dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""NLP-TDMS (Exp, arXiv only)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Which model has achieved the highest Best Score score on the Atari 2600 Ms. Pacman benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Best Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide links to code used in papers that benchmark the BiDAF + Self Attention + ELMo (single model) model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiDAF + Self Attention + ELMo (single model)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ? model ?model_lbl. ?model ? = ""BiDAF + Self Attention + ELMo (single model). "" ORDER BY ?model_lbl DESC LIMIT 1 ? output (Sparql query): input (English text): Who submitted the paper ? output (Sparql query): input (English text): Which of the model features from the paper used in the benchmark dataset outperforms the BiDAF + Self Attention + ELMo (single model) model on the Atari 2600 Ms. Pacman benchmark dataset ? output (Sparql query): output (Sparql query): SELECT ? model ?model_lbl. ?model ? = ""BiDAF + Self Attention + ELMo (single model). "" ORDER BY ?model_lbl DESC LIMIT 1 ? output (Sparql query): Input text, optional, 1-20 characters each: output (Sparql query): input (English text): Link to the original paper on the arXiv.org website: output (Sparql query): Input text, text, or array of text, 1-20 characters each: output (Sparql query): Input text, text, or array of text, 1-20 characters each:"	False
"
 input (English text): What is the top benchmark score and its metric on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): What are the models that have been benchmarked on the SciFACT dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciFACT"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What models are being evaluated on the AESLC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""AESLC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	SELECT DISTINCT ?class ?rdfs FROM ?class_1 ORDER BY DESC( ?method) SELECT ?method. { ?method rdfs:label ?xl_method. OPTIONAL { ?rdfs:label rdfs:class. } } ORDER BY DESC( ?method) Input is a text file that contains score and metric columns as their rows. Output is json string containing metric score of all models, number of classes in each class	False
"
 input (English text): Which model has achieved the highest Error score on the Amazon-5 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Amazon-5"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Could you provide a list of models that have been tested on the CoNLL++ benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoNLL++"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the HoC dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""HoC"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Error"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FIL"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the UrbanSound8k dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""UrbanSound8k"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Name the datasets that have been used for benchmarking in the Graph Embedding research problem?
 output (Sparql query): SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Graph Embedding"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } 
 input (English text): Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR Transformer Large"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	" input/output (English): Where can I find more information on the PAR Transformer Large model for benchmarking purposes ? output (sparql)  (SparseXML): input (HTML page - raw text): Title of the paper which explains how the PAR Transformer Large model was used in benchmarking efforts ? output (SparseXML): output (HTML page - abstract): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): output (HTML page): Output: output: output: output: output: output: output: output: This answer explains how the PAR Transformer Large model was used in benchmarks to show why the model outperforms other solutions:  The graph above is based on the data that has already been downloaded from the Graph Embedding problem.  To be able to perform a more complete analysis, the user needs to download the data to a ParseXML file. This is explained in the  PAR Transformer Large model for benchmarking answers  how to download ParseXML files. The output format of the ParseXML file is a SQL query that will be used in a later step. The result was then automatically merged with a ParseXML file that will be used to create the HTML page shown above. From a reader's perspective, the question posed by the question was: ""can you share any code reference used by you or others in your"	False
"
 input (English text): List the title and ID of research papers that contain a benchmark over the iNaturalist 2019 dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""iNaturalist 2019"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the DuIE dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DuIE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BERTwwm + SQuAD 2"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 0.1.0 - 0.1.1 - 0.1.2 - 0.1.3 - 0.1.4 - 0.1.5 - 0.1.6 - 0.1.7 - 0.1.8 - 0.1.9 - 0.1.10 - 0.2.0 - 0.2.1 - 0.2.2 - 0.2.3 - (D) Benchmarking SQuAD (F) Benchmarking DuIE (A) BERTwwm + SQuAD (F) BID/SQUAD (F) BID/SQUAD + RNG (F) SQuAD + RNG (F) SQuAD + HASH-based (F) SQuAD + HASH-based + RNG (F) SQuAD + RNG + RNGs (F)	False
"
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Crazy Climber dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Crazy Climber"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Can you list the models that have been evaluated on the LAMBADA dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""LAMBADA"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CL-Titles-Parser"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT ?column ?column->{ ?s ""Model name"" ?column. SMP. ""Lifetime"" ?column. RDFS ?column. FILTER ( ?column ) ""ORIGINAL_MODEL"". ""EUROMETRIC_ORIGIN"" ?column. FILTER ( ?column ) ""PROLIFERATION"". ""MODEL_LEN"" ?column. FILTER ( ?column ) ?column->{ ?s ""Model value"" ?column. ""Lifetime"" ?column. SMP. ""Model length"" ?column. RDFS. ""Model name"" ?column. SMP. ""Valuation class"" ?column. SMP. ""Result class"" ?column. RDFS. ""Model description"" ?column. SMP. ""Type of title"" ?column. FILTER (str( ?column ) == ""Title"" ) ?column->{ ?s ?column_lbl. FILTER (str( ?column ) == ""Elements"" )! ?column_lbl. } } RETURN ?column ""*"" ORDER BY { ?column : ?column_lbl ?column. ?column rdfs:label. FILTER ( ?column ) ?column_lbl. } ""*"" input (English text): Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code ? output (Sparql query): SELECT ?column ?column->{ ?s ""Model name"" ?column. SMP ?column. ""Lifetime"" ?column. RDFS ?column. FILTER ( ?column ) ?column_lbl. SMP. FILTER ( ?column ) ?column->{ ?s ?column. SMP. FILTER ( ?column )"	False
"
 input (English text): Could you provide a list of models that have been tested on the Reacher, easy (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reacher, easy (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ARC (Challenge) dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Could you provide a list of models that have been tested on the Habitat 2020 Object Nav test-std benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"  ""I cannot help you in the following way, that I am unable to provide any more precise information. ""To confirm the accuracy of the results, let me know, using [email protected].   You can also discuss with the other members of the Lab through any of the forums. ""I know that there are many other datasets in your collection. Can you put them in the collection under the same category as this one ? [email protected] "" ""Hello, I don't know anything about your database or of yours and I don't know where you found the ARC dataset, but these 3 datasets should be enough to confirm that the results are true from the point of view of the objective and methodological analysis."" ""This was certainly very helpful and the documentation was excellent! I would definitely use you again."" ""Thanks for this great help, and I really appreciate the effort that you have put in to help us. ""I have not seen an ARC database before and cannot speak to the accuracy, but I must say that the results are accurate and I have tested them on my own projects and I was very impressed as well. Thank you. "" ""I didn't know what to expect from this type of tool. As usual, the documentation was the best, and the experience of this site made everything very straightforward. The model, the data, and the algorithm were explained and very fast. Finally, I am going to use your services again. """	False
"
 input (English text): Can you provide links to code used in papers that benchmark the SciBERT (Base Vocab) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT (Base Vocab)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the FB15k dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""FB15k"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What models are being evaluated on the Atari 2600 Name This Game dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Name This Game"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""SciBERT (Base Vocab)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } user (name): output (Sparql query): All papers that benchmark the SciBERT (Base Vocab) model over the Atari 2600 Name This Game dataset ? output (Sparql query): user (name): output (Sparql query): All other publications that contain a benchmark over the Atari 2600 Name This Game dataset output (Sparql query): input (English text): List the titles and authors of the publications that contain a benchmark over the Atari 2600 Name This Game dataset ? output (Sparql query): SELECT SUBSTRING( name,''). 'Author', 'Name This Game dataset ?' // Optional. output (Sparql query): ------------------------ user (name): input (English text): How many users are interested in using the code generated by this paper to develop models for this dataset ? output (Sparql query): ------------------------ -- user(total) user(max) ------------------------ -- user(a + b) user(k + m) -- user(k + m) user(k + n)"	False
"
 input (English text): What is the average efficency for experiments?
 output (Sparql query): SELECT AVG( ?efficency_values) WHERE { orkgr:R155266 orkgp:compareContribution ?contrib. ?contrib orkgp:P43156 ?efficency. ?efficency rdfs:label ?efficency_labels. BIND(xsd:float( ?efficency_labels) AS ?efficency_values) } 
 input (English text): Could you provide a list of models that have been tested on the Yelp-14 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Yelp-14"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } 
 input (English text): What is the highest benchmark result achieved on the Atari 2600 Freeway dataset, including the metric and its value?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Freeway"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	SELECT DISTINCT ?efficiency The second query, which would produce a greater number of rows but which would fail, involves a different aggregation mechanism. The query needs to combine the results of the two aggregation mechanisms into a single result. One way to do this is to use a GROUP BY operation or to sort the result. However, in your query, the order of the columns must be changed between the two queries. To change the order of columns, you may need to create a new aggregation mechanism. The example below shows how to do this with SQL Server 2005 Express Edition. SELECT SUM(( ?dataset a OR ?dataset), 0) AS sum FROM ( SELECT avg{ ?dataset a } AS avg FROM ( SELECT avg { ?dataset b }, ?dataset c } AS average FROM ( SELECT avg{ ?dataset a } AS avg2, ?dataset c } AS avg3, WHERE avg{ ?dataset a } <> avg2 && avg{ ?dataset c } <> avg3 ) AS avg FROM ( SELECT SUM(sum( ?) AS sum FROM avg) AS sum FROM avg2 LEFT JOIN AVG3 GROUP BY avg2 ORDER BY avg1 ASC ) GROUP BY avg1 ) ); If you're using SQL Server 2008 Enterprise Edition, or later, SQL Server 2005 Express Edition, you can combine the results of the two queries using the GROUP BY operation. SELECT SUM(sum( ?) AS sum - sum ?asum2 ?asum2) from avg LEFT JOIN AVG2 GROUP BY avg2 ORDER BY avg1	False
"
 input (English text): Which model has achieved the highest Medium Human-Normalized Score score on the Atari-57 benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Medium Human-Normalized Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari-57"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the ClueWeb09-B dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ClueWeb09-B"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 River Raid"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT DISTINCT ?benchmark 1.5 ?dataset ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?benchmark orkgp:HAS_EVALUATION ?eval. ?acc orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?benchmark rdfs:label ?benchmark. ?dataset rdfs:label ?dataset_lbl. LIMIT 1 A note on ""Normalized Score"" scores: While the Medium Human-Normalized Score is considered the best tool to judge learning, it is not the only tool used in this study. There are many other factors associated with Medium Human-Normalized scores, like raw scores, or the raw scores that result from filtering and aggregation, that can affect the overall score you come up with. However, the Normalized Score is a great baseline to compare in this analysis, and it allows you to compare, for instance, all the scores against a specific standard. The Medium Human-Normalized Score is not meant to be a ""one size fits all"" statistic, for it has the potential to vary depending"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the SciCite dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""SciCite"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Enduro dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Enduro"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the AlexNet, MultiGrasp model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AlexNet, MultiGrasp"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	 output (Sliced dataset): output (Sliced dataset with multiple training steps): output (Slicalize:Dataset model, dataset, target epoch_time/step:time/step): output (Slicalize:Sequencer model/target epoch_time/step:time/step): output (Slicalize:Seq model/target epoch_time/step:time/step): output (Bits:ScapeModel model/target epoch_time/step:time/step): output (Bits:Scape model/target epoch_time/step:time/step): output (Bits:Scape model/target epoch_error/step:difference): output(bikeshare:SCN model/target epoch_time/step:difference): output(bikeshare:SCN model/target epoch_time/step:difference): output(bikeshare:SCN model/target epoch_error/step:difference): output(bikeshare:SCN model/target epoch_time/step:difference): output(bits:Scape model/target epoch_time/step:difference): output(bits:Scape model/target epoch_time/step:difference): output(bits:Scape model/target epoch_time/step:difference): output(bits:Scape model/target epoch_time/step:difference): output(bits:Scape model/target epoch_time/step:diff	False
"
 input (English text): Where can I find code references in papers that have used the AVID model for benchmarking purposes?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""AVID"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Do all studies include open ocean sampling?
 output (Sparql query): ASK { { SELECT (COUNT( ?ocean_sampling) AS ?counter) WHERE { orkgr:R155561 orkgp:compareContribution ?contrib. ?contrib orkgp:P41379 ?ocean_sampling. FILTER( ?ocean_sampling = ""F""^^xsd:string) } } FILTER( ?counter = 0) } 
 input (English text): Can you list the models that have been evaluated on the Atari 2600 Ms. Pacman dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Ms. Pacman"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?paper orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?paper orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } "	"SELECT... FROM : data_set. The following command uses the database, which we can use to query for all the AVID model files. data_set If we specify SELECT in the same way, that's how we get all the code: output (Sparql query): SELECT ?model_lbl. FILTER (str( ?model_lbl) = ""AVID"") ?statistics orkgr:R155561 orkgp:compareContribution ?contrib. ?contrib orkgp:P41379 ?statistics. ?contrib orkgp:P41379 [ ?statistics and ?contrib.] orkgr:R155561 orkgp:compareContribution ?contrib. ?contrib orkgp:P41379 We can also get a subset of the models by specifying SELECT... FROM : data_set. The following command uses the database: output (Sparql query): SELECT ?model_lbl. FILTER (str( ?model_lbl) = ""AVID"") ?statistics and ?statistics. ?statistics and ?statistics [ ?statistics and ?statistics.] ?statistics and ?statistics [ ?statistics and ?statistics.] We can use the COUNT method to get the current number of models which fit the data. The following commands use the database for this, and the count method will print the values at the end. count "	False
"
 input (English text): Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the Reuters RCV1/RCV2 German-to-English dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Reuters RCV1/RCV2 German-to-English"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Provide a list of research paper titles and IDs that have benchmarked models on the CoQA dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""CoQA"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What evaluation metrics are commonly used when benchmarking models on the Habitat 2020 Object Nav test-std dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Object Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	" output (Sparql query): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 a r df t i j a p i l r df t f i m : - > s 1 ""JH2"", ""HAB1"", ""HAB2"", ""JH2"" orkgc:Dataset; _dataset_lbl_str: ""RDB2"" / ""RDB2"" orkgp: %SEM %NAMEOF ""HAB2"" / 1 / %NAMEOF ""JH2""; rdfs:label ""JH2"" / ""RDB2"" orkgp: %SEM %NAMEOF ""JH2"" / 2 / %NAMEOF ""JH2"" or kgp: %SEM %NAMEOF ""JH2"" or kgp: %NAMEOF ""JH2""; rdfs:label ""HAB1"" / ""JH2"" or kgp: %SEM %NAMEOF ""HAB2"" / 1 / %NAMEOF ""JH2""; rdfs:label ""JH2"" / ""HAB1"" or kgp: %SEM %NAMEOF ""JH2"" / 2 / %NAMEOF ""JH2"" or kgp: %NAMEOF ""JH2""; rdfs:label ""JH1"" / ""HAB1"" or kgp: %SEM %NAMEOF ""JH2"" / 1 / %N"	False
"
 input (English text): What evaluation metrics are commonly used when benchmarking models on the ACL-ARC dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACL-ARC"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Provide a list of papers that have utilized the CAIT-S-36 model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""CAIT-S-36"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the top benchmark score and its metric on the BC5CDR-disease dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BC5CDR-disease"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	 score (N = 1000): 1545.9353412 output (Sparql query): score (N = 1000): 1546.8676484 score (N = 1000): 1560.5646631 score (N = 1000): 1562.8583326 score (N = 1000): 1564.6848375 score (N = 1000): 1567.464976 score (N = 1000): 1567.4084707 score (N = 1000): 1568.6228333 score (N = 1000): 1569.751675 score (N = 1000): 1570.181638 score (N = 1000): 1571.1618383 score (N = 1000): 1570.6228348 score (N = 1000): 1572.1718367 score (N = 1000): 1572.3518408 score (N = 1000): 1574.9551242 score (N = 1000): 1575.2518397 score (N = 1000): 1576.3463231 score (N = 1000): 1576.4798769 score (N = 1000): 1578.9582481 score (N = 1000): 1579.5368342 score (N = 1000): 1580.8187812 score (N = 1000): 1581.6369479 score (N = 1000): 1583.4648342 score (N = 1000): 1584.9232886 score (N = 1000): 1586.9186072 score (N = 1000): 1587.7867429 score (N = 1000): 1589.6424062 score (N = 1000): 1589.8369955 score (N = 1000): 1590.9881642 score (N = 1000): 1591.9295555 score	False
"
 input (English text): What is the average installed capacity for each energy source considered in 5 year intervals?
 output (Sparql query): SELECT ?rangeId ?energy_sources_labels (AVG( ?installed_cap_value AS ?avg_installed_cap_value)) WHERE { orkgr:R153801 orkgp:compareContribution ?contrib. ?paper orkgp:P31 ?contrib; orkgp:P29 ?year. BIND(xsd:int( ?year) AS ?y). VALUES( ?rangeId ?min ?max) { (""2001-2005"" 2001 2005) (""2006-2010"" 2006 2010) (""2011-2015"" 2011 2015) (""2016-2020"" 2016 2020) } FILTER( ?min <= ?y && ?y <= ?max). ?contrib orkgp:P43135 ?energy_sources. ?energy_sources rdfs:label ?energy_sources_labels; orkgp:P43133 ?installed_capacity. ?installed_capacity orkgp:HAS_VALUE ?value. BIND(xsd:float( ?value) AS ?installed_cap_value). } ORDER BY ASC( ?rangeId)
 input (English text): Indicate the model that performed best in terms of DISTANCE_TO_GOAL metric on the Habitat 2020 Point Nav test-std benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""DISTANCE_TO_GOAL"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): List the metrics that are used to evaluate models on the ImageNet 64x64 benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ImageNet 64x64"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } "	------------------------ ------------------------ ------------------------ ------------------------ 2.1 -- Metrics that are used to evaluate the models in 3-D classification of the ImageNet 64x64 test-std dataset: (3-D) # of Feature Points (DEG; X/Y dimension) # of Feature Points on the image. (X-Z) # of Feature Points on the image. (Z-A) # of Feature Points on the image. ------------------------ ------------------------ ------------------------ 2.1 5 # of Feature Points 5 0.071795 0.167543 ------------------------ ------------------------ 2.1 9 # of Feature Points 9 0.037791 0.183683 ------------------------ ------------------------ 2.1 15 # of Feature Points 15 0.092863 0.144913 ------------------------ ------------------------ 2.1 20 # of Feature Points 20 0.065953 0.153833 ------------------------ ------------------------ 2.1 24 # of Feature Points 24 0.105125 0.150879 ------------------------ ------------------------ 2.1 28 # of Feature Points 28 0.125932 0.133169 ------------------------ ------------------------ Source: https://www.researchgate.net/profile/G-S/publication/3489657948 Last modified by : Ralf Wäbner (Ralf.W@uTk) at 07:56 AM. All R packages available here. I had a thought that the best way to summarize these results is with a simple graph. It's not that hard to produce one yourself with data such as the figure below. Using this tool, we can quickly summarize the performance of R packages based on performance from two datasets – one based on image files and another on text extracts. It is worth mentioning that the test of a tool depends strongly on the dataset it's compared with. If we look at	False
"
 input (English text): Can you provide links to code used in papers that benchmark the PAR BERT Base model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PAR BERT Base"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Can you provide links to code used in papers that benchmark the OTF spelling (single) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""OTF spelling (single)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): List the title and ID of research papers that contain a benchmark over the DBpedia dataset?
 output (Sparql query): "	"SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""DBpedia"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } "	"SELECT DISTINCT ?code WHERE { ?model psc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""PDBpedia model"") ?benchmark psc:HAS_DATASET ? orkgp:HAS_DATASET ? ?dataset. ?cont orkgp:HAS_BENCHMARK psc:HAS_SOURCE_CODE ORkgp:HAS_SOURCE_CODE ?code; orkgp:HAS_SOURCE_CODE ?model; orkgp:HAS_SOURCE_CODE } The list of published PSC reports is provided by the authors in an archive of the database. I suggest you to look the archive for the paper that benchmarks the PSC model using the benchmark module. I found one benchmark report in this paper that is only published once. The document is published on the web-site of the same title. You might find the benchmark reports published on other publications (e.g., paper) in the same title. If you need more information about the benchmark report, the URL of the web-site of the paper should be found in the source code. The main text of this paper is written in a generic language that will be easier to understand by you. It is not really an implementation of the RDF Language, but rather a generalization of and alternative to the current RDF language. It is therefore not very good as an alternative for implementing one of the existing technologies. I'd like to thank Kiki, the web-surveying developer and the"	False
"
 input (English text): Can you provide links to code used in papers that benchmark the ZFNet (1 convnet, 512,1024,512 maps) model?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""ZFNet (1 convnet, 512,1024,512 maps)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the Transformer-XL (24 layers) model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""Transformer-XL (24 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): Provide a list of papers that have utilized the HRLRE model and include the links to their code?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""HRLRE (14 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide links to code used in papers that test the NEGP_NEGP algorithm for convolutional networks ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""NEGP_NEGP (9 layers)"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } input (English text): Provide a list of papers that test the NEGP-H (10 layers) algorithm and include the links to their code ? output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl"	False
"
 input (English text): Indicate the model that performed best in terms of Accuracy metric on the Penn Treebank benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Penn Treebank"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What are the titles and IDs of research papers that include a benchmark for the Automatically labeled Medline abstracts corpus dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Automatically labeled Medline abstracts corpus"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): Can you provide links to code used in papers that benchmark the T-ConvS2S model?
 output (Sparql query): "	"SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""T-ConvS2S"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } "	"SELECT DISTINCT ?model_lbl. ?model_lbl. ?model_lbl_ls. ?model_lbl. ?model_lbl_ls_ls (2 rows) DISTINCT ------------------------- output (Sparql query): SELECT ?dataset orkgc:Dataset ?dataset_lbl_ls. FILTER (str( ?dataset_lbl. ) = ""Penn Treebank"") [orkgc:data_id] ?data_id DISTINCT ------------------------- output (Sparql query): SELECT ?treebank ORGCP-PAGES 1. ORGCP-PAGES [orkgc:treebank_id] ?treebank FROM T-CONVS2S. T-CONVS2S_BLAS( orkgc:data_ids, 1. ) ORGCP-PAGES Input English text: Describe the accuracy metrics that predict an abstract from the Automatically labeled Medline/RDBMS abstracts corpus dataset ? output (Sparql query): SELECT ACCURACY_METRIC a, b, c, d, ACCURACY_COUNT a, b, c, d, ACCURACY_IMP(4): (4, 6) COUNT ------------------------- Output English text: Which of the following has been trained and which are the results of this training ? (Choose all that apply.) Automatically labeled Medline/RDBMS Abstracts corpus dataset, Penn Treebank benchmark, or manual training (text) output (Sparql query): ------------------------- SELECT... WHERE dt.dataset.org_url OR dt.dataset.org_user_id OR dt.dataset.org_id OR dt"	False
"
 input (English text): List the metrics that are used to evaluate models on the Cartpole, swingup (DMControl500k) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Cartpole, swingup (DMControl500k)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-Romanian benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""BLEU score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""WMT2016 English-Romanian"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?
 output (Sparql query): "	"SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Score"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Atari 2600 Q*Bert"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } "	" output (Sparql query): Column Description scores Score score for the model. 0 0 -1 Average score of models. 0 -100 The following table lists the metrics that were used to evaluate the models. The value from the top-left box is the model's average score. For each metric, one metric was determined on average for each model using the metric score returned by the query, then a new metric was determined for each model based on the results of the previous metric evaluation. Note that the default metric score used for models is 1, but more than one metric can be configured for a given model (see below). The top-left box lists the metrics that are used to evaluate the models. The value from the top-left box is the model's average score. For each metric, one metric was determined on average for each model using the metric score returned by the query, then a new metric was determined for each model based on the results of the previous metric evaluation. Note that the default metric score used for models is 1, but more than one metric can be configured for a given model (see below). The Metric Score is computed by taking the sum of all metrics above a given threshold. For example, the metric score of a model for which the average score is less than 2 is zero. Here the model's output (Sparql query) includes both the metric score and the model's average measurement score (see the Metric Score below). The model metrics are measured by using the default metric score. That means that, for these measures, the default metric score is 0. The metrics are averaged using a threshold of 1 as shown in the following table. For example, for the metric score of a model to be at least one standard deviation above the mean metric (for example, a ""best"" score is one above the mean of the"	False
"
 input (English text): Provide a list of papers that have utilized the BiLSTM-CRF+ELMo model and include the links to their code?
 output (Sparql query): SELECT DISTINCT ?code WHERE { ?model a orkgc:Model; rdfs:label ?model_lbl. FILTER (str( ?model_lbl) = ""BiLSTM-CRF+ELMo"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?cont orkgp:HAS_MODEL ?model; orkgp:HAS_SOURCE_CODE ?code. } 
 input (English text): What is the name of the top performing model in terms of Mean Accuracy score when benchmarked on the ModelNet40 dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Mean Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ModelNet40"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): What is the top benchmark score and its metric on the Words in Context dataset?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Words in Context"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP 1 COUNT(*) OF ?value_LOOKUP AND ?value _ lbl ? AND ?value_LOOKUP_A cnt ? ORDER BY DESC( ?value_LOOKUP ) cnt LIMIT 1 input (English text): Which of the following papers do you find most interesting ? output (Sparql query): SELECT DISTINCT ?title AND ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _ OR ?genre _; p:CASE RESET output (Sparql query): SELECT ?title ?genre * ?genre p from ""paper.org"" ORDER BY ""Title/Genre"" ASC output (Sparql query): SELECT ?title ?genre * ?genre p from ""paper.org"" ORDER BY ""Title/Genre"" DESC input (English text): What is your opinion from the following literature ? output (Sparql query): SELECT DISTINCT ?title ;| ?type : ?pub input (English text): What papers have been used to implement ELmo using the BiLSTM-CRF+ELMo framework ? output (Sparql query): select id, name from literature where id not in ''group'' input (English text): What is your opinion from the following literature ? output (Sparql query): select id, name from literature where id not in ''group'' input (English text): How has CRF been used to train the models ? output (Sparql query): select id"	False
"
 input (English text): Which model has achieved the highest Accuracy score on the ARC (Challenge) benchmark dataset?
 output (Sparql query): SELECT DISTINCT ?model ?model_lbl WHERE { ?metric a orkgc:Metric; rdfs:label ?metric_lbl. FILTER (str( ?metric_lbl) = ""Accuracy"") { SELECT ?model ?model_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ARC (Challenge)"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value; orkgp:HAS_METRIC ?metric. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } ORDER BY DESC( ?value) LIMIT 1 } } 
 input (English text): Can you provide the highest benchmark result, including the metric and score, for the BIOSSES dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""BIOSSES"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl
 input (English text): List the datasets benchmarked under the Common Sense Reasoning research problem?
 output (Sparql query): "	"SELECT DISTINCT ?dataset ?dataset_lbl WHERE { ?problem a orkgc:Problem; rdfs:label ?problem_lbl. FILTER (str( ?problem_lbl) = ""Common Sense Reasoning"") ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark; orkgp:P32 ?problem. } "	 If you're not sure whether to include these, please let me know in the comments! output (Sparql query): CREATE OR REPLACE FUNCTION ?( ?dataset, RDFS, label, ?metric in { ?dataset, ?Metric ?metric_lbl,! ?metric,! ?label,! ?val, ?metric_lbl, ?metric rdfs:label ?metric_lbl. ) RETURNS rdfs AS ?dataset { ?dataset RDFS ; _ ?metric rdfs:label RDFS. ?label RDFS. ; RDFS label : ?metric rdfs:label. ?metric rdfs:label RDFS! ?metric rdfs:label RDFS_TITLE. RDFS_LINK. ORG_ID. ORG_GEOGRAPHIC. RDFS_TITLE RDFS. ?Metric ?metric_lbl IF. RDFS METRIC. RDFS_CONTROL. RDFS_CONTROL_ID. ORG_ID_CONTROL RDFS_CONTROL. RDFS_FMT RDFS_FMT. OPTIONAL opt: ?metric! ?metric_lbl IN. RDFS_RANK ORDER BY RDFS_LIMIT LIMIT 1. RDFS_FMT RDFS_LIMIT. OPTIONAL opt: ?metric! ?metric_lbl ; RDFS metric : ?metric rdfs:label RDFS_METRIC. ORG_ID. ORG_GEOGRAPHIC. RDF	False
"
 input (English text): What are the metrics of evaluation over the Open Entity dataset?
 output (Sparql query): SELECT DISTINCT ?metric ?metric_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Open Entity"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } } 
 input (English text): List the title and ID of research papers that contain a benchmark over the Habitat 2020 Point Nav test-std dataset?
 output (Sparql query): SELECT DISTINCT ?paper ?paper_lbl WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""Habitat 2020 Point Nav test-std"") ?benchmark orkgp:HAS_DATASET ?dataset. ?cont orkgp:HAS_BENCHMARK ?benchmark. ?paper orkgp:P31 ?cont; rdfs:label ?paper_lbl. } 
 input (English text): What is the top benchmark result (metric and value) over the dataset ACE 2004?
 output (Sparql query): "	"SELECT DISTINCT ?metric ?metric_lbl (MAX( ?value) AS ?score) WHERE { { SELECT ?metric ?metric_lbl ?value WHERE { ?dataset a orkgc:Dataset; rdfs:label ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""ACE 2004"") ?benchmark orkgp:HAS_DATASET ?dataset; orkgp:HAS_EVALUATION ?eval. ?eval orkgp:HAS_VALUE ?value. OPTIONAL { ?eval orkgp:HAS_METRIC ?metric. ?metric rdfs:label ?metric_lbl. } ?cont orkgp:HAS_BENCHMARK ?benchmark. OPTIONAL { ?cont orkgp:HAS_MODEL ?model. ?model rdfs:label ?model_lbl. } } ORDER BY DESC( ?value) } } GROUP BY ?metric ?metric_lbl"	"SELECT TOP { ?metric ?metric_lbl} AS benchmark FROM ACE_2002 TOP{ ?metric ?metric_lbl} WHERE { ?dataset a orkgc:Dataset } IS { ?dataset ?dataset_lbl. FILTER (str( ?dataset_lbl) = ""(Open Entity)"") ?benchmark orkgp:HAS_DATASET ?dataset; } As described in the paper on Benchmarks for the Large Scale Web (Springer, 2010a), this method is based on a combination of two basic elements of social science research: the use of ""test sets"" and comparison of benchmarks across experiments. Let us now turn to a case study of evaluating the adoption of the ACE 2005 benchmark. After writing an application for web and mobile platforms that includes an ACE 2005 benchmarking framework, we found that some users are not accepting the toolkit, for example because the framework lacks an implementation of a ""standard"" object-relational mapping (ORM) and so is not easily ported to new systems. In addition, because that the new platform does not have a full suite of tools and integrations to compare benchmarks with, for example through the integration of other platforms such as Tableau, we ended up having trouble running our experiments compared to those done with ACE 2005 but without a support layer. We decided to develop a set of integration and testing tools that would be included as part of the ACE 2005 framework (as part of an application called Tableau), and to port them to all major platforms using a relatively small number of changes. This step took us from looking at our first ""toolkit"" in the form of a code demo for Tableau to a set of integration interfaces that we wanted to deploy to all major platforms,"	False

Metrics on query only
Precision	0.16774916435443465
Recall	0.3864202016250419
F1_Score	0.21470319979845476
Blue_4	0.042702832912584414
Bleu_Score_Cumulative	0.061362063097659875
Meteor_Score	0.0
Rouge_1	0.26839679384658793
Rouge_2	0.12617422732563446
Rouge_L	0.23700838605804078

Metrics on all rows
Precision	0.12303845109933355
Recall	0.29681082525499425
F1_Score	0.15804122397916925
Blue_4	0.025530866536556274
Bleu_Score_Cumulative	0.037202175328667424
Meteor_Score	0.0
Rouge_1	0.19758469973498896
Rouge_2	0.08066904022454598
Rouge_L	0.17100489401863905

Number of sparql queries generated equal to given	0

Generated text starts with 'SELECT'	298
